<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文为ICLR 2023发表的 Vocoder 的代表性论文。">
<meta property="og:type" content="article">
<meta property="og:title" content="BigVGAN">
<meta property="og:url" content="http://example.com/2023/07/20/BigVGAN/index.html">
<meta property="og:site_name" content="海阔天空蓝">
<meta property="og:description" content="本文为ICLR 2023发表的 Vocoder 的代表性论文。">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2023-07-20T06:47:51.000Z">
<meta property="article:modified_time" content="2023-07-25T02:39:01.227Z">
<meta property="article:author" content="Alan Sun">
<meta property="article:tag" content="text-to-speech (TTS), speech synthesis, 跆拳道(TKD), 舞狮(lion dancing), 单板滑雪，花样轮滑">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2023/07/20/BigVGAN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>BigVGAN | 海阔天空蓝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">海阔天空蓝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个玩过n种运动的语音合成算法攻城狮</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2023/07/20/BigVGAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Alan Sun">
      <meta itemprop="description" content="记录工作与生活">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="海阔天空蓝">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BigVGAN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-07-20 14:47:51" itemprop="dateCreated datePublished" datetime="2023-07-20T14:47:51+08:00">2023-07-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-07-25 10:39:01" itemprop="dateModified" datetime="2023-07-25T10:39:01+08:00">2023-07-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">工作</span></a>
                </span>
            </span>

          
            <div class="post-description">本文为ICLR 2023发表的 Vocoder 的代表性论文。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="bigvgan一种具有大规模训练的通用神经声码器">BIGVGAN：一种具有大规模训练的通用神经声码器</h1>
<h2 id="摘要">摘要</h2>
<p>尽管最近在基于生成对抗性网络（GAN）的声码器方面取得了进展，其中该模型根据声学特征生成原始波形，但在各种记录环境中为众多扬声器合成高保真音频是一项挑战。在这项工作中，我们介绍了BigVGAN，这是一种通用的声码器，可以很好地推广到各种分布外的场景，而无需进行微调。我们在GAN生成器中引入了<strong>周期激活函数</strong>和<strong>抗混叠表示</strong>，这为音频合成带来了所需的<strong>电感偏置（inductive bias）</strong>，并显著提高了音频质量。此外，我们以高达112M个参数的最大规模训练我们的GAN声码器，这在文献中是前所未有的。我们识别并解决了大规模音频GAN训练中的故障模式，同时在不过度正则化的情况下保持高保真度输出。我们的BigVGAN只接受过清洁语音（LibriTTS）培训，在各种零样本（非分发）条件下都能实现最先进的性能，包括看不见的扬声器、语言、录音环境、歌唱声音、音乐和乐器音频。我们在以下位置发布代码和模型：https://github.com/NVIDIA/BigVGAN.</p>
<h2 id="introduction">1. Introduction</h2>
<p>深度生成模型在对原始音频建模方面取得了显著的成功。成功的方法包括自回归模型、基于流的模型、基于GAN的模型和扩散模型。</p>
<p>在这些方法中，基于GAN的声码器可以生成以mel声谱图为条件的高保真原始音频，同时在单个GPU上合成比实时快数百倍。然而，由于模型容量有限，现有的GAN声码器仅限于在清洁环境中录制适量声音的环境。当模型以不同录音环境中看不见的说话人的mel频谱图为条件时，音频质量可能会严重降低。实际上，一个通用声码器，可以为分布外的样本进行零样本生成，在许多实际应用中都非常有价值，包括多个说话人的文本到语音转换、神经语音克隆，和神经音频编解码器。在这些应用中，神经声码器还需要很好地推广在各种条件下记录的音频。</p>
<p>在文本生成和图像合成中，放大模型大小以实现零样本性能是一个显著的趋势，但在音频合成中尚未探索。尽管基于似然的模型由于其简单的训练目标和稳定的优化而更容易缩放，但我们使用大规模GAN训练来构建我们的通用声码器，因为GAN声码器具有以下优点：i）与自回归或扩散模型相比，它是完全平行的，并且只需要一次正向传递就可以产生高维波形。ii）与基于流的模型相比，它没有强制执行任何架构约束（例如，仿射耦合层），以保持潜在和数据之间的双射。在给定相同数量的参数的情况下，这种架构约束可能会限制模型容量。</p>
<p>在这项工作中，我们提出了BigVGAN，一种大的Vocoding GAN，它可以在没有微调的情况下实现高保真度的分布外（OOD）生成。具体而言，我们做出了以下贡献： 1. 我们在生成器中引入<strong>周期性激活</strong>，为音频合成提供所需的电感偏置（inductive bias）。受其他领域提出的方法的启发，我们证明了周期激活在音频合成中的显著成功。 2. 提出了一种用于复杂音频波形建模的抗混叠多周期合成（AMP）模块。AMP组合具有可学习周期性的多个信号分量，并使用低通滤波器来减少高频伪像。 3. 我们在不规范生成器和鉴别器的情况下，通过模拟大规模GAN训练的失败模式，成功地将BigVGAN扩展到了112M参数。经验见解与Brock等人在图像领域不同（2019）。例如，正则化方法在音频合成中引入了相位失配伪影。 4. 我们证明，对于分布内和分布外样本，具有14M参数的BigVGAN基底优于具有可类比大小的最先进的神经声码器。特别是，对于各种OOD场景下的零样本生成，具有112M参数的BigVGAN在很大程度上优于最先进的模型，包括各种不可见录音环境中的不可见扬声器、新颖语言、歌唱声音、音乐和乐器音频。</p>
<p>我们将论文的其余部分整理如下。我们在§2中讨论了相关工作，并在§3中介绍了BigVGAN。我们在§4中报告了实证结果，并在§5中总结了论文。</p>
<h2 id="related-work">2. Related Work</h2>
<p>我们的工作建立在用于图像和音频合成的最先进的GANs的基础上。GAN首次被提出用于图像合成。从那时起，通过优化的架构或大规模训练获得了令人印象深刻的结果。</p>
<p>在音频合成中，以前的工作侧重于改进鉴别器架构或添加新的辅助训练损失。MelGAN引入了多尺度鉴别器（MSD），该鉴别器使用平均池在多个尺度上对原始波形进行下采样，并在每个尺度上分别应用基于窗口的鉴别器。它还通过鉴别器的l1特征匹配损失来加强输入mel谱图和生成的波形之间的映射。相比之下，GAN-TTS使用了一组在不同大小的随机窗口上操作的鉴别器，并使用条件鉴别器在调节器和波形之间进行对抗性映射。并行WaveGAN将单个短时傅立叶变换（STFT）损失扩展到多分辨率，并将其添加为GAN训练的辅助损失。通过引入多分辨率STFT损失，进一步改进了MelGAN。HiFi GAN重用了MelGAN的MSD，并引入了用于高保真合成的多周期鉴别器（MPD）。UnivNe使用多分辨率鉴别器（MRD），该鉴别器以多分辨率频谱图为输入，可以锐化合成波形的频谱结构。相反，CARGAN将部分自回归纳入生成器中，以提高基音和周期精度。</p>
<p>在这项工作中，我们专注于改进和扩大生成器的规模。我们介绍了音频合成的周期性电感偏差，并解决了非自回归生成器架构中的特征混叠问题。我们的建筑设计与时间序列预测、隐式神经表示和图像合成的最新结果有关。请注意，不同的生成器架构对于单说话人神经声码可以同样好地执行。我们证明，在具有挑战性的条件下，改进生成器架构对于通用神经语音编码至关重要。</p>
<p>由于存在明显的挑战，通用神经语音编码的成功有限。在之前的工作中，WaveRNN已被应用于通用声码任务。建立了基于流的通用声码器模型。最近发现GAN声码器是一个很好的候选者。</p>
<h2 id="method">3. Method</h2>
<p>在本节中，我们介绍了GAN声码器的预备知识，然后介绍了BigVGAN。如图1所示，有关体系结构的详细描述，请参阅附录A。</p>
<h3 id="gan声码器简介">3.1 GAN声码器简介</h3>
<p><strong>生成器</strong> 生成器网络以mel频谱图或其他特征作为输入，并输出相应的原始波形。在之前的研究中，已经应用了几种生成器架构，包括WaveNet，或卷积网络，其通过残差块堆栈将mel频谱图逐步上采样为高分辨率波形。我们选择HiFi GAN生成器作为基线架构。我们相信所提出的技术也适用于其他生成器架构。</p>
<p><strong>鉴别器</strong> 最先进的GAN声码器通常包括几个鉴别器，以引导发生器合成相干波形，同时最大限度地减少人类耳朵无法检测到的感知伪影。重要的是，每个鉴别器包含多个子鉴别器，在波形的不同分辨率窗口上操作。例如，HiFi GAN 应用了两种类型的鉴别器：i）多周期鉴别器（MPD），其中将一维信号重塑为具有不同高度和宽度的二维表示，以通过二维卷积分别捕获多个周期结构。ii）多尺度鉴别器（MSD），其中每个子鉴别器通过时域中的平均池来接收不同频率的下采样的一维信号。提出使用多分辨率鉴别器（MRD）在时频域上应用鉴别器，该鉴别器由几个子鉴别器组成，这些子鉴别器对具有不同STFT分辨率的多个二维线性频谱图进行操作。我们还发现，用MRD代替MSD提高了音频质量，同时减少了音调和周期伪影。</p>
<p><strong>训练目标</strong> 我们的训练目标与HiFi GAN类似，只是将MSD替换为MRD。它包括最小平方对抗性损失、特征匹配损失和mel谱图上的谱l1回归损失的加权和。我们在附录B中留下了每个损失和超参数的详细信息。</p>
<h3 id="周期性电感偏置">3.2 周期性电感偏置</h3>
<p>已知音频波形表现出高周期性，并且可以自然地表示为原始周期分量的组成（即，狄利克雷条件下的傅立叶级数）。这建议我们需要为生成器架构提供所需的电感偏置。然而，当前的非自回归GAN声码器仅依赖于膨胀卷积的层来学习不同频率下所需的周期分量。它们的激活函数（例如Leaky ReLU）可以产生具有必要非线性的新细节，但不提供任何周期性的电感偏置。此外，我们发现Leaky ReLU在波形域的外推表现不佳：尽管该模型在训练时可以在可见的录音环境中生成高质量的语音信号，但在分布外的场景中，如看不见的录音环境、非语音发声和乐器音频，其性能会显著下降。</p>
<p>我们通过应用最近提出的称为Snake函数的周期激活，将周期性的适当电感偏置引入生成器，定义为<span class="math inline">\(f_\alpha(x) = x + \frac{1}{\alpha}sin^2(\alpha x)\)</span>，其中<span class="math inline">\(\alpha\)</span>是控制信号周期分量频率的可训练参数，<span class="math inline">\(\alpha\)</span>越大，频率越高。<span class="math inline">\(\sin^2(x)\)</span>的使用确保了单调性，并使其易于优化。证明了这种周期性激活对温度和金融数据预测具有改进的外推能力。</p>
<p>在BigVGAN中，我们使用Snake激活<span class="math inline">\(f_{\alpha}(x)\)</span>和信道可训练参数<span class="math inline">\(\alpha \in \R^h\)</span>，这些参数定义了每个一维卷积信道的周期频率。采用这种具有学习频率控制的周期函数形式，卷积模块可以自然地拟合具有多个周期分量的原始波形。我们证明了所提出的基于Snake的生成器对训练过程中看不到的分布外音频样本更具鲁棒性，这表明在通用声码任务中具有强大的外推能力。示例见图2和附录D；使用Snake激活的BigVGAN基本无滤波器比HiFi GAN更接近地面实况样本。</p>
<h3 id="防泄密代表">3.3 防泄密代表</h3>
<p>Snake激活为建模原始波形提供了所需的周期性电感偏置，但它可以为连续时间信号产生任意高频细节，这些细节不能用网络的离散时间输出来表示，这可能导致混叠伪影。这种副作用可以通过应用低通滤波器来抑制。抗混叠非线性通过沿时间维度将信号上采样2×，应用Snake激活，然后将信号下采样2×来操作，这是受奈奎斯特-香农采样定理启发的常见实践。每个上采样和下采样操作都伴随着低通滤波器，该低通滤波器使用具有Kaiser窗口的窗口sinc滤波器。详见附录A。</p>
<p>我们将这种滤波后的Snake非线性应用于生成器内的每个残差扩张卷积层，以获得离散时间一维信号的抗混叠表示。该模块被命名为抗混叠多周期合成（AMP）。如图1所示。我们发现，结合滤波激活可以减少合成波形中的高频伪影；如图2所示，请参见BigVGAN基本无滤波器与BigVGAN基础（带滤波器）。我们将证明，它在各种客观和主观评价方面提供了显著的改进。请注意，我们还探索了抗锯齿上采样层，但这会导致显著的训练不稳定性，并导致大型模型的早期崩溃。详见附录C。</p>
<h3 id="采用大规模训练的bigvgan">3.4 采用大规模训练的BigVGAN</h3>
<p>在本小节中，我们通过将生成器的模型大小扩大到112M个参数来探索通用声码的局限性，同时保持GAN训练的稳定性和作为高速神经声码器的实用性。我们从我们的改进生成器开始，使用具有14M参数的可比HiFi GAN V1配置，表示为BigVGAN基础。我们通过增加每个块的上采样块和卷积通道的数量来增加BigVGAN基数。BigVGAN基础使用4个上采样块以[8，8，2，2]的比率对信号进行256×上采样。每个上采样块都伴随着具有扩张卷积的多个残差层，即AMP模块。我们进一步将256×上采样划分为6个块[4，4，2，2，2]，以进行更细粒度的特征细化。此外，我们将AMP模块（类似于HiFi GAN中的MRF）的通道数量从512个增加到1536个。我们将具有1536个通道和112M个参数的模型表示为BigVGAN。</p>
<p>我们发现，HiFi GAN中使用的<span class="math inline">\(2×10^{−4}\)</span>的默认学习率会导致BigVGAN训练的早期训练崩溃，其中鉴别器子模块的损失在数千次迭代后立即收敛为零。将学习率减半至1×10−4能够减少此类失败。我们还发现，大批量有助于减少训练中的模式崩溃。为了在训练效率和稳定性之间取得良好的平衡，我们只将批量大小从通常的16倍增加到32倍，因为神经声码器可能需要数百万步才能收敛。注意，这个推荐的批量大小仍然比图像合成的批量大小（例如，2048）小得多，因为神经声码具有强的条件信息。</p>
<p>即使有了上述变化，大型BigVGAN在训练初期仍然容易崩溃。我们在训练过程中跟踪了每个模块的梯度范数，并发现抗混叠非线性显著放大了MPD的梯度范数。因此，BigVGAN生成器在训练的早期接收到发散梯度，导致不稳定性和潜在的崩溃。我们在附录C的图4中可视化了每个模块的梯度范数。我们通过将梯度的全局范数裁剪为<span class="math inline">\(10^3\)</span>来缓解这个问题，该全局范数接近112M BigVGAN生成器的平均梯度范数。这种梯度剪裁防止了生成器的早期训练崩溃。请注意，梯度裁剪被发现对缓解图像合成的训练不稳定性无效（见Brock等人（2019）中的附录H），但它在我们的努力中非常有效。</p>
<p>除了上述努力，我们还探索了其他方向，包括改进模型架构的各种方法、稳定GAN训练的谱归一化，这对图像域中的大规模GAN训练至关重要，以及提高模型泛化能力的数据增加。不幸的是，在我们的研究中，所有这些试验都导致了更差的感知质量。详细信息可在附录C中找到。我们希望我们所学到的这些实际经验教训对未来的研究工作有用。</p>
<h2 id="results">4 Results</h2>
<h3 id="训练数据">4.1 训练数据</h3>
<p>我们使用原始采样率为24kHz的LibriTTS数据集进行训练。与之前只采用在清洁环境中记录的子集的研究不同，我们使用了所有训练数据，包括来自不同记录环境的子集，这在文献中是前所未有的。我们发现，训练数据的多样性对于实现使用BigVGAN的通用神经声码的目标很重要。对于OOD实验，如果需要，我们使用librosa包提供的kaiser-best算法将音频重新采样到24kHz。</p>
<p>传统的STFT参数被设计为具有有限的频带[0，8]kHz，通过切断高频细节以便于建模。相反，我们使用频率范围[0，12]kHz和100波段对数mel频谱图来训练所有模型（包括基线），这也用于最近对通用声码的研究。我们在之前的工作中设置了其他STFT参数，具有1024 FFT大小、1024 Hann窗口和256帧跳大小。</p>
<h3 id="模型">4.2 模型</h3>
<p>我们使用1M步的训练配置训练所有BigVGAN模型，包括消融模型和基线HiFi GAN。我们使用批量大小为32，分段大小为8192，初始学习率为<span class="math inline">\(1×10^{−4}\)</span>。所有其他配置，包括优化器、学习率调度器和损失项的标量权重，都遵循HiFi GAN的官方开源实现，没有修改，除了BigVGAN用MRD代替MSD作为鉴别器。所有型号均使用NVIDIA DGX-1和8个V100 GPU进行训练。有关详细的超参数，请参阅附录A中的表6。</p>
<p>我们包括了与SC-WaveRNN的比较，SC是一种最先进的基于WaveRNN的自回归通用神经声码器，使用官方实现。我们还包括两个流行的基于流的模型：WaveGlow和WaveFlow，使用它们的官方实现。对于分发外测试，我们包括UnivNet-c32的非官方开源实现，它使用train-clean-360子集进行训练，据报道在相同的训练配置下优于HiFi-GAN。详见附录E。</p>
<p>我们包括了与SC-WaveRNN的比较，SC是一种最先进的基于WaveRNN的自回归通用神经声码器，使用官方实现。我们还包括两个流行的基于流的模型：WaveGlow和WaveFlow，使用它们的官方实现。对于分发外测试，我们包括UnivNet-c32的非官方开源实现，它使用train-clean-360子集进行训练，据报道在相同的训练配置下优于HiFi-GAN。详见附录E。</p>
<p>表1总结了用于生成24kHz音频的基于流和GAN声码器的合成速度。我们省略了SC-WaveRNN，因为它要慢得多。具有14M参数的BigVGAN-base模型可以合成比实时快70.18倍的音频，这比HiFi GAN相对较慢，因为滤波后的Snake函数需要更多的计算。HiFi GAN和BigVGAN比基于流的模型更快，因为它们是完全平行的（WaveFlow具有部分自回归），并且具有更少的层（WaveGlow具有96层）。我们的BigVGAN具有112M参数，可以合成比实时速度快44.72倍的音频，并有望成为一种高速神经声码器。</p>
<h3 id="评价指标">4.3 评价指标</h3>
<p>我们收集的客观指标旨在测量地面实况音频和生成的样本之间的各种类型的距离。我们提供了5种不同的度量：1）多分辨率STFT（M-STFT），它测量多个分辨率之间的光谱距离。 2）语音质量感知评估（PESQ），一种广泛采用的语音质量自动评估。3）具有动态时间扭曲的梅尔倒谱失真（MCD），其测量梅尔倒谱之间的差异。4）周期性误差，以及5）浊音/清音分类的F1分数（V/UV F1），其被认为是来自非自回归GAN声码器的主要伪像。</p>
<p>传统的5-尺度平均意见得分（MOS）不足以对通用声码器进行主观评估，因为该度量需要区分在各种环境中记录的不同说话者身份的话语。例如，该模型可能总是输出一些非常自然的“平均”声音，这不是优选的，但在MOS评估中仍然可以得到人类工作者的高度评价。因此，我们还进行了5尺度相似性平均意见得分（SMOS）评估，其中要求参与者在并排听了基本事实音频和来自模型的样本后给出这对音频的相似性得分。SMOS提供了一种改进的方法来评估给定样本与地面实况的接近程度，其中地面实况录音可以具有不同的说话者身份，包含听众看不见的语言，并且可以在各种声学环境中录制。SMOS还直接适用于非语音样本，例如音乐。我们对Mechanical Turk进行了MOS和SMOS评估。更多详细信息见附录G。</p>
<h3 id="libritts-结果">LibriTTS 结果</h3>
<p>我们报告了BigVGAN的性能以及使用上述客观和主观指标在LibriTTS上评估的基线模型。我们对dev-clean和dev-ether进行了全面的客观评价，对test-clean与test-ether的组合进行了主观评价。LibriTTS的开发和测试部分包含训练期间看不见的说话人，但录制环境包含在训练部分中。</p>
<p>表2显示了LibriTTS上的分布中测试结果。除HiFi GAN之外的基线模型表现明显较差。这表明GAN声码器是最先进的通用神经声码器。BigVGAN显著改进了所有客观指标。特别是，与具有相同参数量的HiFi GAN（V1）相比，BigVGAN-base表现出持续改进的客观得分，这表明它对波形数据具有更好的周期性电感偏差。</p>
<p>HiFi GAN（V1）、BigVGAN底座和BigVGAN在MOS方面表现相当好，而无需并排收听地面实况音频。当听众可以并排比较模型样本和真实音频时，BigVGAN基础在SMOS（+0.05）方面明显优于HiFi GAN（V1），112M BigVGAN在SMOS方面明显优于高保真GAN（+0.11），因为它具有高的模型容量，可以进一步利用不同的训练数据获得更好的质量。</p>
<p>在附录E中，我们还报告了其他结果，包括UnivNet和基于LibriTTS开发集、未发现的VCTK和LJSpeech数据的BigVGAN消融模型。</p>
<h2 id="看不见的语言和多变的录音环境">4.5 看不见的语言和多变的录音环境</h2>
<p>在本小节中，我们通过测量BigVGAN在不可见数据集中具有不同类型记录环境的各种不可见语言的零样本性能来评估其通用语音编码能力。基于表2中的结果，我们仅将基于GAN的声码器作为最先进的基线。我们收集了三类公开可用的多语言数据集，根据记录环境中的噪声类型进行分类。 - 在无声的录音室环境中录制的资源不足的语言集：爪哇语、高棉语、尼泊尔语和巽他语。我们使用从组合数据集中随机选择的50个音频片段，这些片段在不同语言之间具有相等的平衡。 - 多语言TEDx语料库：包含西班牙语、法语、意大利语和葡萄牙语的TEDx演讲集。我们从IWSLT’21测试集中随机选择了50个语言均衡的音频片段。我们通过添加MS-SNSD中的随机环境噪声来模拟看不见的录制环境设置，如机场、咖啡馆、胡言乱语等。</p>
<p>表3总结了三种不同类型的未发现数据集的SMOS结果。我们只做了SMOS评估，因为数据集有人类听众看不见的语言，如果不与地面实况记录进行并排比较，很难确定质量。对于资源不足的干净语言数据集，模型之间的性能差距并不大。这表明在整个LibriTTS训练集上训练的通用声码器在干净的录音环境下对看不见的语言是鲁棒的。对于两种类型的看不见的记录环境（模拟或真实世界），BigVGAN的性能都大大优于基线模型。小容量BigVGAN基础也显示出与基线相比的改善，具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.05）。这表明，由于采用了AMP模块改进的发生器设计，BigVGAN对看不见的记录环境的鲁棒性明显更强。在附录F中，我们进一步证明了就多种语言的字符错误率（CER）而言，BigVGAN是语言上最准确的通用声码器。</p>
<p>我们用在train-clean-360子集上训练的预训练检查点测试了UnivNet的开源实现。与Jang等人的报告相反，UnivNet-c32优于HiFi GAN，我们发现在整个LibriTTS数据集上训练的未修改的HiFi GAN能够匹配或优于UnivNet-c32。我们还在LibriTTS上对UnivNet-c32进行了完整的训练，发现它并没有从更大的训练数据中受益。详细分析见附录E。</p>
<h2 id="分布外稳健性">4.6 分布外稳健性</h2>
<p>在本小节中，我们通过测量分布外数据的零样本性能来测试BigVGAN的稳健性和外推能力。我们使用MUSDB18-HQ进行了SMOS实验，这是一个多音轨音乐音频数据集，包含人声、鼓、贝斯、其他乐器和原始混音。测试集包含50首歌曲和5首曲目。我们为每首曲目和歌曲收集持续时间为10秒的歌曲中期剪辑。</p>
<p>表4显示了来自5个轨道的SMOS结果及其来自MUSDB18-HQ测试集的平均值。BigVGAN模型显示出在更宽的频带覆盖范围内大大提高了零样本生成性能，而基线模型无法在有限的频率范围外生成音频，并遭受严重失真。歌声（人声）、器乐音频（其他）和歌曲的完整混音（混音）的改进最为深刻，而鼓和低音的改进则不那么显著。</p>
<p>我们还对从真实世界录制环境中的YouTube视频中获得的音频进行了实验。BigVGAN还表现出对各种类型的分布外信号（如笑声）的鲁棒性。我们为我们的演示页面提供音频样本。</p>
<h2 id="消融研究">4.7 消融研究</h2>
<p><strong>模型架构</strong> 为了衡量BigVGAN生成器的有效性，我们根据MUSDB18-HQ数据对BigVGAN的消融模型进行了SMOS测试。表4显示，消融模型在各种场景下表现出明显的退化，如仪器音频（其他，混合）。从平均SMOS评级来看，1）禁用Snake激活的抗锯齿滤波器比BigVGAN基础表现更差，2）去除滤波器和Snake活化（即用MRD代替MSD训练的香草HiFi GAN）甚至比仅Snake消融模型更差，两者都具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.01）。这表明Leaky ReLU不够健壮，无法外推到学习的频率范围之外，并且在具有挑战性的设置中，混叠伪影会降低音频质量。结果表明，由于周期性归纳偏差和抗混叠特征表示的无缝集成，BigVGAN生成器对分布外场景表现出强大的鲁棒性和外推能力。关于BigVGAN中抗锯齿效果的可视化，请参见附录D。</p>
<p><strong>大模型</strong> 我们比较了具有最大112M参数的HiFi GAN和BigVGAN。我们使用与BigVGAN相同的训练设置来训练112M HiFi GAN。我们在MUSDB18-HQ的混合测试集上对两个模型进行了成对测试，这是一个具有挑战性的分布外数据。我们要求参与者在两个模型的样本之间选择声音更好的音频。测试显示，58%的评分投票给BigVGAN而不是大的HiFi GAN，并且BigVGAN的质量大于大的HiFi GAN，具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.01）。结果进一步验证了BigVGAN在大规模环境中的架构优势。</p>
<p><strong>大型训练数据</strong> 为了验证使用大型训练数据的重要性，我们使用不太多样化的纯语音数据集训练我们的BigVGAN，使用相同的训练配置进行1M个步骤：1）LibriTTS的train-clean-360子集，或2）VCTK数据集。表5显示，在不太多样化的数据上训练BigVGAN显示了LibriTTS评估集上的客观指标和主观SMOS的退化。结果验证了使用不同训练数据的重要性，并证明了BigVGAN在大规模数据集上的有效性。</p>
<h1 id="结论">5 结论</h1>
<p>这项研究以前所未有的数据、模型和评估规模探索了通用神经语音编码的局限性。我们通过各种自动和人工评估来分析不同场景下的表现，包括看不见的演讲者、语言、录音环境和分布外的数据。我们通过引入具有学习频率控制的抗混叠周期激活函数，为BigVGAN提供了一种改进的生成器架构，该函数为波形生成注入了所需的电感偏置。基于改进的生成器，我们演示了最大的GAN声码器，它在各种OOD条件下具有强大的零样本性能，包括看不见的录制环境、歌唱声音和乐器声音。我们相信，BigVGAN结合从大规模培训中吸取的实际经验教训，将激励未来的通用声码努力，并提高现实世界应用的最先进成果，包括语音克隆、语音转换、语音翻译和音频编解码器。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/12/11/vc/" rel="prev" title="有关VC, SSL以及Feature Distanglement的论文总结">
      <i class="fa fa-chevron-left"></i> 有关VC, SSL以及Feature Distanglement的论文总结
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/07/25/SUPERB/" rel="next" title="SUPERB">
      SUPERB <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#bigvgan%E4%B8%80%E7%A7%8D%E5%85%B7%E6%9C%89%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E7%9A%84%E9%80%9A%E7%94%A8%E7%A5%9E%E7%BB%8F%E5%A3%B0%E7%A0%81%E5%99%A8"><span class="nav-number">1.</span> <span class="nav-text">BIGVGAN：一种具有大规模训练的通用神经声码器</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#introduction"><span class="nav-number">1.2.</span> <span class="nav-text">1. Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#related-work"><span class="nav-number">1.3.</span> <span class="nav-text">2. Related Work</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#method"><span class="nav-number">1.4.</span> <span class="nav-text">3. Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#gan%E5%A3%B0%E7%A0%81%E5%99%A8%E7%AE%80%E4%BB%8B"><span class="nav-number">1.4.1.</span> <span class="nav-text">3.1 GAN声码器简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%91%A8%E6%9C%9F%E6%80%A7%E7%94%B5%E6%84%9F%E5%81%8F%E7%BD%AE"><span class="nav-number">1.4.2.</span> <span class="nav-text">3.2 周期性电感偏置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%98%B2%E6%B3%84%E5%AF%86%E4%BB%A3%E8%A1%A8"><span class="nav-number">1.4.3.</span> <span class="nav-text">3.3 防泄密代表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%87%87%E7%94%A8%E5%A4%A7%E8%A7%84%E6%A8%A1%E8%AE%AD%E7%BB%83%E7%9A%84bigvgan"><span class="nav-number">1.4.4.</span> <span class="nav-text">3.4 采用大规模训练的BigVGAN</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#results"><span class="nav-number">1.5.</span> <span class="nav-text">4 Results</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">1.5.1.</span> <span class="nav-text">4.1 训练数据</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.5.2.</span> <span class="nav-text">4.2 模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-number">1.5.3.</span> <span class="nav-text">4.3 评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#libritts-%E7%BB%93%E6%9E%9C"><span class="nav-number">1.5.4.</span> <span class="nav-text">LibriTTS 结果</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9C%8B%E4%B8%8D%E8%A7%81%E7%9A%84%E8%AF%AD%E8%A8%80%E5%92%8C%E5%A4%9A%E5%8F%98%E7%9A%84%E5%BD%95%E9%9F%B3%E7%8E%AF%E5%A2%83"><span class="nav-number">1.6.</span> <span class="nav-text">4.5 看不见的语言和多变的录音环境</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%88%86%E5%B8%83%E5%A4%96%E7%A8%B3%E5%81%A5%E6%80%A7"><span class="nav-number">1.7.</span> <span class="nav-text">4.6 分布外稳健性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B6%88%E8%9E%8D%E7%A0%94%E7%A9%B6"><span class="nav-number">1.8.</span> <span class="nav-text">4.7 消融研究</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">2.</span> <span class="nav-text">5 结论</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Alan Sun</p>
  <div class="site-description" itemprop="description">记录工作与生活</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">56</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
    有<span id="busuanzi_value_site_uv"></span>人来过
</span>
</div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alan Sun</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
