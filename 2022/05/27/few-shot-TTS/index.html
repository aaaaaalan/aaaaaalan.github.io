<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="总结zero&#x2F;one&#x2F;few-shot TTS的所有论文">
<meta property="og:type" content="article">
<meta property="og:title" content="few-shot-TTS">
<meta property="og:url" content="http://example.com/2022/05/27/few-shot-TTS/index.html">
<meta property="og:site_name" content="海阔天空蓝">
<meta property="og:description" content="总结zero&#x2F;one&#x2F;few-shot TTS的所有论文">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2022-05-27T06:14:31.000Z">
<meta property="article:modified_time" content="2023-05-11T03:37:04.901Z">
<meta property="article:author" content="Alan Sun">
<meta property="article:tag" content="text-to-speech (TTS), speech synthesis, 跆拳道(TKD), 舞狮(lion dancing), 单板滑雪，花样轮滑">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://example.com/2022/05/27/few-shot-TTS/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>few-shot-TTS | 海阔天空蓝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">海阔天空蓝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个玩过n种运动的语音合成算法攻城狮</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2022/05/27/few-shot-TTS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Alan Sun">
      <meta itemprop="description" content="记录工作与生活">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="海阔天空蓝">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          few-shot-TTS
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-05-27 14:14:31" itemprop="dateCreated datePublished" datetime="2022-05-27T14:14:31+08:00">2022-05-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-05-11 11:37:04" itemprop="dateModified" datetime="2023-05-11T11:37:04+08:00">2023-05-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">工作</span></a>
                </span>
            </span>

          
            <div class="post-description">总结zero/one/few-shot TTS的所有论文</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Zero-One-Few-shot-TTS的所有论文"><a href="#Zero-One-Few-shot-TTS的所有论文" class="headerlink" title="Zero/One/Few-shot TTS的所有论文"></a>Zero/One/Few-shot TTS的所有论文</h1><p>研究Few-shot TTS有一段时间了，想要系统化的了解一下这个行业的发展情况，因此在这里总结一下所有有关Zero/One/Few-shot TTS / Voice clone的相关论文。</p>
<p>Motivation: 采集尽可能少量目标说话人的声音，克隆目标说话人的音色，制作tts引擎，重点需支持中文。</p>
<p>Github List:</p>
<ol>
<li>[x] 实时中文语音克隆 Mocking Bird <a target="_blank" rel="noopener" href="https://github.com/babysor/MockingBird/blob/main/README-CN.md">Code</a></li>
<li>[x] Neural Voice Cloning with a Few Samples (Sercan Ö. Arık, Baidu, 2018 Dec.) <a target="_blank" rel="noopener" href="https://github.com/SforAiDl/Neural-Voice-Cloning-With-Few-Samples">Code1</a> | <a target="_blank" rel="noopener" href="https://github.com/IEEE-NITK/Neural-Voice-Cloning">Code2</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1802.06006.pdf">Paper</a></li>
<li>[x] Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (Jia Ye, Google, 2019 Jan.) <a target="_blank" rel="noopener" href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1806.04558.pdf">Paper</a></li>
<li>[x] One model to speak them all (Mutian He, 港科技 &amp; 微软 2021 Jul.)  <a target="_blank" rel="noopener" href="https://github.com/mutiann/few-shot-transformer-tts">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.03541">Paper</a></li>
<li>[x] Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation (Dongchan Min, KAIST, 2021 Jun, ICML) <a target="_blank" rel="noopener" href="https://github.com/keonlee9420/StyleSpeech">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.03153">Paper</a></li>
<li>[x] SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model (Edresson Casanova, University of Sa ̃o Paulo, 2021 Jun, Interspeech 2021)</li>
<li>[x] YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone (Edresson Casanova, Universidade de Sa ̃o Paulo, 2022 Feb ) <a target="_blank" rel="noopener" href="https://github.com/Edresson/YourTTS">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2112.02418">Paper</a></li>
<li>[x] Zero-shot VITS Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus <a target="_blank" rel="noopener" href="https://github.com/hcyspeech/TransferTTS">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.15447.pdf">Paper</a></li>
<li>[x] AdaSpeech <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.00993.pdf">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.00993.pdf">Paper</a> || AdaSpeech2 <a target="_blank" rel="noopener" href="https://github.com/rishikksh20/AdaSpeech2">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.09715.pdf">Paper</a></li>
<li>Deepfakes for Video Conferencing Using General Adversarial Networks (GANs) and Multilingual Voice Cloning <a target="_blank" rel="noopener" href="https://github.com/ghatoledipak/Deepfakes-for-Video-Conferencing-Using-General-Adversarial-Networks-GANs-and-Voice-Cloning">Code</a></li>
<li>[x] Unet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning (Rui Li, CloudMinds Inc, ICASSP 2022) <a target="_blank" rel="noopener" href="https://github.com/CMsmartvoice/One-Shot-Voice-Cloning">Code</a> | <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2109.11115.pdf">Paper</a></li>
<li>[x] TorToiSe (jbetker, April 2022) <a target="_blank" rel="noopener" href="https://github.com/neonbjb/tortoise-tts/tree/main/tortoise">Code</a> | <a target="_blank" rel="noopener" href="https://nonint.com/2022/04/25/tortoise-architectural-design-doc/">Intro</a> | No training methodology</li>
</ol>
<p>Other Large Audio Models:<br><a target="_blank" rel="noopener" href="https://github.com/liusongxiang/Large-Audio-Models">https://github.com/liusongxiang/Large-Audio-Models</a></p>
<h3 id="Prompt-based-Audio-Synthesis"><a href="#Prompt-based-Audio-Synthesis" class="headerlink" title="Prompt-based Audio Synthesis"></a>Prompt-based Audio Synthesis</h3><ul>
<li>[x] NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers(2023), Kai Shen et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.09116.pdf">[PDF]</a></li>
<li>[x] FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model(2023), Ruiqing Xue et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.02939v3.pdf">[PDF]</a></li>
<li>[x] VALL-E X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling (2023), Ziqiang Zhang et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2303.03926.pdf">[PDF]</a></li>
<li>Noise2Music: Text-conditioned Music Generation with Diffusion Models(2023), Qingqing Huang et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2302.03917">[PDF]</a></li>
<li>[x] Spear-TTS: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision(2023), Eugene Kharitonov et al. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2302.03540">[PDF]</a></li>
<li>MusicLM: Generating Music From Text(2023), Andrea Agostinelli et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.11325">[PDF]</a></li>
<li>[x] InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt (2023), Dongchao Yang et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.13662.pdf">[PDF]</a></li>
<li>[x] AudioLDM: Text-to-Audio Generation with Latent Diffusion Models(2023), Haohe Liu et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.12503">[PDF]</a></li>
<li>Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion(2023), Flavio Schneider et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.11757">[PDF]</a></li>
<li>[x] Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models(2023), Rongjie Huang et al. <a target="_blank" rel="noopener" href="https://text-to-audio.github.io/paper.pdf">[PDF]</a></li>
<li>[x] VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers (2023), Chengyi Wang et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2301.02111.pdf">[PDF]</a></li>
<li>[x] Diffsound: Discrete Diffusion Model for Text-to-sound Generation (2022), Dongchao Yang et al.<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.09983.pdf">[PDF]</a></li>
<li>[x] VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature (2022), Chenpeng Du. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.00768.pdf">[PDF]</a></li>
<li>[x] DiscreTalk: Text-to-Speech as a Machine Translation Problem (2020), Tomoki Hayashi. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.05525.pdf">[PDF]</a></li>
<li>[x] DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders (2022), Yanqing Liu <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2207.04646.pdf">[PDF]</a></li>
<li>[x] PromptTTS: Controllable Text-to-Speech with Text Descriptions (2022), Zhifang Guo <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2211.12171.pdf">[PDF]</a></li>
</ul>
<h3 id="Audio-Language-Models"><a href="#Audio-Language-Models" class="headerlink" title="Audio Language Models"></a>Audio Language Models</h3><ul>
<li>[x] AudioLM: a Language Modeling Approach to Audio Generation(2022), Zalán Borsos et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.03143">[PDF]</a></li>
</ul>
<h3 id="Audio-SSL-and-UL-models"><a href="#Audio-SSL-and-UL-models" class="headerlink" title="Audio SSL and UL models"></a>Audio SSL and UL models</h3><ul>
<li>[] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations(2019), Alexei Baevski et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1910.05453.pdf">[PDF]</a></li>
<li>MuLan: A Joint Embedding of Music Audio and Natural Language (2022) Qingqing Huang et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2208.12415">[PDF]</a></li>
<li>[] W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training (2021) <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2108.06209">[PDF]</a></li>
<li>[] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (2021) Wei-Ning Hsu et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.07447.pdf">[PDF]</a>,</li>
<li>[] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (2020), Alexei Baevski et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2006.11477.pdf">[PDF]</a></li>
<li>[] Data2vec: A general framework for self-supervised learning in speech, vision and language (2022), Alexei Baevski et al. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2202.03555.pdf">[PDF]</a></li>
<li>[x] SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing (2022), Junyi Ao. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.07205.pdf">[PDF]</a></li>
<li>[x] CLAP: Learning Audio Concepts From Natural Language Supervision (2022), Benjamin Elizalde <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2206.04769.pdf">[PDF]</a></li>
<li>[x] AudioGen: Textually Guided Audio Generation (2023), Felix Kreuk. <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2209.15352.pdf">[PDF]</a></li>
<li>[x] WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing (2022), Sanyuan Chen <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.13900.pdf">[PDF]</a></li>
<li>[] Wav2vec: Unsupervised Pre-training for Speech Recognition (2019), Steffen Schneider <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1904.05862.pdf">[PDF]</a></li>
<li>[x] A Brief Overview of Unsupervised Neural Speech Representation Learning (2022), Lasse Borgholt <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.01829.pdf">[PDF]</a></li>
<li>[x] ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers (2022), Kaizhi Qian <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2204.09224">[PDF]</a></li>
<li>[x] IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion (2022), Wendong Gan <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2201.00269.pdf">[PDF]</a></li>
<li>[x] SoftVC: A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion (2022), Benjamin van Niekerk <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.02392.pdf">[PDF]</a></li>
</ul>
<h3 id="Encodec-models"><a href="#Encodec-models" class="headerlink" title="Encodec models"></a>Encodec models</h3><ul>
<li>[x] High Fidelity Neural Audio Compression (2022), Alexandre Défossez <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.13438.pdf">[PDF]</a></li>
<li>[x] SoundStream: An End-to-End Neural Audio Codec (2021), Neil Zeghidour <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.03312.pdf">[PDF]</a></li>
<li>[x] HIFI-CODEC: GROUP-RESIDUAL VECTOR QUANTIZATION FOR HIGH FIDELITY AUDIO CODEC <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2305.02765.pdf">[PDF]</a></li>
</ul>
<h3 id="System-level"><a href="#System-level" class="headerlink" title="System-level"></a>System-level</h3><ul>
<li>[x] AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head (2023), Rongjie Huang <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2304.12995.pdf">[PDF]</a></li>
</ul>
<p>无 Code List:</p>
<ol>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1809.10460.pdf">Sample Efficient Adaptive Text-to-Speech</a> (Yutian Chen, DeepMind &amp; Google, 2019 Jan, ICLR 2019)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2005.08484">Attentron: Few-shot Text-to-Speech Exploiting Attention-based Variable Length Embedding</a> (Seungwoo Choi, Hyperconnect, 2020 Aug., Interspeech 2020)</p>
</li>
<li><p><a href="">BOFFIN TTS: Few-Shot Speaker Adaptation by Bayesian Optimization</a>(<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2002.01953">https://arxiv.org/abs/2002.01953</a>) (Henry B. Moss, Amazon, 2020 Feb., ICASSP 2020)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2102.00151.pdf">Expressive Neural Voice Cloning</a> (Paarth Neekhara, 2021 Jan, 加州大学圣地亚哥分校, PMLR 2021)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2103.04699.pdf">CUHK-EE VOICE CLONING SYSTEM FOR ICASSP 2021 M2VOC CHALLENGE</a> (Daxin Tan, 港中文, 2021 Jul)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.04040.pdf">Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech</a> (Sung-Feng Huang, National Taiwan Uni, 2021 Nov,  IEEE/ACM Transactions on Audio, Speech, and Language Processing)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1907.04448.pdf">Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning</a> (Yu Zhang, Google, 2019 Jul.)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2104.01818.pdf">The Multi-Speaker Multi-Style Voice Cloning Challenge 2021</a> (Qicong Xie, ASLP, 2021 Apr., ICASSP 2021)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://sci-hub.se/10.1109/icassp39728.2021.9414727">Dian: Duration Informed Auto-Regressive Network for Voice Cloning</a> (Wei song, JD, 2021 May, ICASSP 2021)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2204.03421.pdf">Self supervised learning for robust voice cloning</a> (Konstantinos Klapsas, Innoetics, 2022 Apr, submitted to Interspeech 2022)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2203.09708.pdf">Improve few-shot voice cloning using multi-modal learning</a> (Haitong Zhang, NetEase Games, 2022 Mar, 2022 IEEE International Conference on Acoustics, Speech and Signal Processing)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2110.03347.pdf">Cloning one’s voice using very limited data in the wild</a> (Dongyang Dai, (SAMI), ByteDance, 2021 Oct)</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2111.12890.pdf">V2C: Visual Voice Cloning</a> (Qi Chen, 阿德莱德大学（澳洲），华南理工大学, 2021 Nov., )</p>
</li>
</ol>
<p>综述类:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2022/18991/final/fin_irjmets1644437365.pdf">Voice cloning</a>(Saiesh Prabhu Verlekar, Shree Rayeshwar Institute Of Engineering And Information Technology Goa 2022 Feb, IRJETS 2022)</li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/05/23/vscode/" rel="prev" title="vscode">
      <i class="fa fa-chevron-left"></i> vscode
    </a></div>
      <div class="post-nav-item">
    <a href="/2022/05/30/few-shot-TTS-details/" rel="next" title="few-shot-TTS-details">
      few-shot-TTS-details <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Zero-One-Few-shot-TTS%E7%9A%84%E6%89%80%E6%9C%89%E8%AE%BA%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">Zero&#x2F;One&#x2F;Few-shot TTS的所有论文</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Prompt-based-Audio-Synthesis"><span class="nav-number">1.0.1.</span> <span class="nav-text">Prompt-based Audio Synthesis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Audio-Language-Models"><span class="nav-number">1.0.2.</span> <span class="nav-text">Audio Language Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Audio-SSL-and-UL-models"><span class="nav-number">1.0.3.</span> <span class="nav-text">Audio SSL and UL models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Encodec-models"><span class="nav-number">1.0.4.</span> <span class="nav-text">Encodec models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#System-level"><span class="nav-number">1.0.5.</span> <span class="nav-text">System-level</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Alan Sun</p>
  <div class="site-description" itemprop="description">记录工作与生活</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">54</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
    有<span id="busuanzi_value_site_uv"></span>人来过
</span>
</div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alan Sun</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
