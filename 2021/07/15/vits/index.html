<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本文为ICML2021发表的end-to-end TTS的代表性论文，且论文代码已经开源。">
<meta property="og:type" content="article">
<meta property="og:title" content="VITS">
<meta property="og:url" content="http://example.com/2021/07/15/vits/index.html">
<meta property="og:site_name" content="海阔天空蓝">
<meta property="og:description" content="本文为ICML2021发表的end-to-end TTS的代表性论文，且论文代码已经开源。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/images/vits_fig1.png">
<meta property="og:image" content="http://example.com/images/vits_result1.png">
<meta property="og:image" content="http://example.com/images/vits_result2.png">
<meta property="og:image" content="http://example.com/images/vits_tab3.png">
<meta property="og:image" content="http://example.com/images/vits_fig2.png">
<meta property="og:image" content="http://example.com/images/vits_fig3.png">
<meta property="og:image" content="http://example.com/images/vits_tab4.png">
<meta property="article:published_time" content="2021-07-15T03:04:26.000Z">
<meta property="article:modified_time" content="2021-08-17T06:17:14.131Z">
<meta property="article:author" content="Alan Sun">
<meta property="article:tag" content="text-to-speech (TTS), speech synthesis, 跆拳道(TKD), 舞狮(lion dancing), 单板滑雪，花样轮滑">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/images/vits_fig1.png">

<link rel="canonical" href="http://example.com/2021/07/15/vits/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>VITS | 海阔天空蓝</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">海阔天空蓝</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">一个玩过n种运动的语音合成算法攻城狮</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/07/15/vits/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Alan Sun">
      <meta itemprop="description" content="记录工作与生活">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="海阔天空蓝">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          VITS
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-15 11:04:26" itemprop="dateCreated datePublished" datetime="2021-07-15T11:04:26+08:00">2021-07-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-17 14:17:14" itemprop="dateModified" datetime="2021-08-17T14:17:14+08:00">2021-08-17</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%B7%A5%E4%BD%9C/" itemprop="url" rel="index"><span itemprop="name">工作</span></a>
                </span>
            </span>

          
            <div class="post-description">本文为ICML2021发表的end-to-end TTS的代表性论文，且论文代码已经开源。</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech"><a href="#Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech" class="headerlink" title="Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"></a>Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>基于VAE，GAN，Normaling flows的端到端TTS神经网络结构。文章简单综述了end2end TTS的发展情况，且指出当前的single-stage模型大多相较于multi-stage模型较差。模型的创新点在首次采用VAE的方式并行生成语音。模型实验效果从MOS和推理效率上都远好于目前的baseline模型。</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.06103.pdf">Paper</a> | <a target="_blank" rel="noopener" href="https://jaywalnut310.github.io/vits-demo/index.html">Demo</a> | <a target="_blank" rel="noopener" href="https://github.com/jaywalnut310/vits">Code</a> | ICML 2021 | Rate: 🌟🌟🌟🌟🌟</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最近提出了几种支持单阶段训练和并行采样的端到端文本到语音（TTS）模型，但它们的样本质量与两阶段 TTS 系统的样本质量不匹配。在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。我们的方法采用变分推理，并通过归一化流程和对抗性训练过程进行增强，从而提高了生成建模的表达能力。我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量和随机持续时间预测器的不确定性建模，我们的方法表达了自然的一对多关系，其中可以以多种方式以不同的音高和节奏说出文本输入。对 LJ Speech（单个说话者数据集）的主观人类评估（平均意见分数或 MOS）表明，我们的方法优于最好的公开可用的 TTS 系统，并实现了与Ground-Truth实况相当的 MOS。 </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文本到语音 (TTS) 系统从给定文本通过多个组件合成原始语音波形。 随着深度神经网络的快速发展，除了文本规范化和音素化等文本预处理之外，TTS 系统管道已被简化为两阶段生成建模。 第一阶段是从预处理的文本中生成中间语音表示，如 Melspectrograms 或语言特征，第二阶段是生成以中间表示为条件的原始波形。 每个两级管道的模型都是独立开发的。</p>
<p>基于神经网络的自回归 TTS 系统已经显示出合成真实语音的能力，但它们的顺序生成过程使得现代并行处理器难以充分利用。为了克服这个限制并提高合成速度，已经提出了几种非自回归方法。在文本到谱图生成步骤中，尝试从预训练的自回归教师网络中提取注意力图，以降低学习文本和谱图之间对齐的难度.最近，基于似然的方法通过估计或学习最大化目标梅尔谱图的可能性的比对来进一步消除对外部比对器的依赖。同时，在第二阶段模型中探索了生成对抗网络（GAN）。基于 GAN 的前馈网络具有多个鉴别器，每个鉴别器区分不同尺度或周期的样本，实现高质量的原始波形合成。</p>
<p>尽管并行 TTS 系统取得了进展，但两级管道仍然存在问题，因为它们需要顺序训练或微调才能进行高质量的生产，其中后期模块使用早期模型生成的样本进行训练。此外，它们对预定义中间特征的依赖妨碍了应用学习到的隐藏表示来获得性能的进一步改进。最近，几项工作，即 FastSpeech 2s 和 EATS ，提出了有效的端到端训练方法，例如对短音频剪辑而不是整个波形进行训练，利用梅尔谱图解码器来帮助文本表示学习，并设计专门的谱图损失来减轻目标和生成语音之间的长度不匹配问题。然而，尽管通过利用学习的表示可能提高性能，但它们的合成质量落后于两阶段系统。 </p>
<p>在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。使用变分自编码器 (VAE)，我们通过潜在变量连接 TTS 系统的两个模块，以实现高效的端到端学习。为了提高我们方法的表达能力，以便可以合成高质量的语音波形，我们将标准化流应用于我们的条件先验分布和波形域的对抗训练。除了生成细粒度的音频之外，TTS 系统表达一对多关系也很重要，在这种关系中，文本输入可以以多种方式以不同的变化（例如，音调和持续时间）说出。为了解决一对多的问题，我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量的不确定性建模和随机持续时间预测器，我们的方法可以捕获无法用文本表示的语音变化。</p>
<p>与最好的公开可用的 TTS 系统 Glow-TTS和 HiFi-GAN相比，我们的方法获得了更自然的语音和更高的采样效率。 我们公开演示页面和源代码。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/images/vits_fig1.png" alt="vits_fig1"></p>
<p>在本节中，我们将解释我们提出的方法及其架构。 所提出的方法主要在前三个小节中描述：条件 VAE 公式； 来自变分推理的对齐估计； 用于提高合成质量的对抗性训练。 本节末尾描述了整体架构。 图 1a 和 1b 分别显示了我们方法的训练和推理过程。 从现在开始，我们将把我们的方法称为端到端文本到语音（VITS）对抗学习的变分推理。</p>
<h3 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>VITS 可以表示为条件 VAE，其目标是最大化数据 $log p_θ (x|c)$ 的难以处理的边际对数似然的变分下界，也称为证据下界 (ELBO: the evidence lower bound)：</p>
<script type="math/tex; mode=display">
log p_θ (x|c) ≥ E_{q_{φ(z|x)}}  [log p_θ (x|z)−log \frac{q_φ(z|x)} {p_θ (z|c)}]</script><p>其中 $p_θ(z|c)$​ 表示给定条件 $c$​ 的潜在变量 $z$​ 的先验分布，$p_θ(x|z)$​ 是数据点 $x$​ 的似然函数，$q_φ(z|x)$​ 是近似后验分布。 训练损失是负的 ELBO，可以看作是重建损失的总和 $- log p_θ(x|z)$​ 和 KL 散度 $log q_φ(z|x) - log p_θ(z|c)$​，其中 $z ∼ q_φ(z|x)$​​。</p>
<h4 id="重构损失"><a href="#重构损失" class="headerlink" title="重构损失"></a>重构损失</h4><p>作为重建损失中的目标数据点，我们使用梅尔谱图而不是原始波形，用 $x_{mel}$ 表示。 我们通过解码器将潜在变量 $z$ 上采样到波形域 $\hat y$，并将 $\hat y$ 变换到梅尔谱图域 $\hat x_{mel}$。 然后将预测和目标梅尔谱图之间的 $L_1$ 损失用作重建损失：</p>
<script type="math/tex; mode=display">
L_{recon} = ∥x_{mel} − \hat x_{mel}∥_1</script><p>这可以看作是最大似然估计，假设数据分布为拉普拉斯分布并忽略常数项。 我们定义了梅尔谱域中的重建损失，以通过使用近似人类听觉系统响应的梅尔尺度来提高感知质量。 请注意，来自原始波形的梅尔谱图估计不需要可训练的参数，因为它只使用 STFT 和线性投影到梅尔尺度上。 此外，估计仅在训练期间使用，而不是在推理期间使用。 在实践中，我们不会对整个潜在变量 $z$ 进行上采样，而是使用部分序列作为解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p>
<h4 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL-散度"></a>KL-散度</h4><p>先验编码器 $c$ 的输入条件由从文本中提取的音素 $c_{text}$ 和音素和潜在变量之间的对齐 $A$ 组成。 对齐是一个硬单调注意矩阵，带有 $|c_{text}| × |z|$ 维度表示每个输入音素扩展到与目标语音时间对齐的时间。 因为对齐没有真实标签，我们必须在每次训练迭代时估计对齐，我们将在第 2.2.1 节中讨论。 在我们的问题设置中，我们旨在为后验编码器提供更多高分辨率信息。 因此，我们使用目标语音 $x_<br>{lin}$ 的线性频谱图而不是梅尔频谱图作为输入。 请注意，修改后的输入不会违反变分推理的属性。 KL散度则为：</p>
<script type="math/tex; mode=display">
L_{kl} =logq_φ(z|x_{lin})−logp_θ(z|c_{text},A),</script><script type="math/tex; mode=display">
z ∼ q_φ(z|x_{lin}) = N(z; μ_φ(x_{lin}), σ_φ(x_{lin}))</script><p>分解正态分布用于参数化我们的先验和后验编码器。 我们发现增加先验分布的表现力对于生成真实样本很重要。 因此，我们应用了归一化流 $f_θ$ ，它允许在分解的正态先验基础之上，按照变量变化规则将简单分布可逆变换为更复杂的分布分配：</p>
<script type="math/tex; mode=display">
p_{\theta} (z|c) = N(f_{\theta} (z);μ_{\theta} (c),σ_{\theta} (c))􏰀􏰀􏰀 | det \frac {∂f_θ(z)} {∂ z}􏰀􏰀􏰀|,</script><script type="math/tex; mode=display">
c = [c_{text}, A]</script><h3 id="对齐估计"><a href="#对齐估计" class="headerlink" title="对齐估计"></a>对齐估计</h3><h4 id="MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索"><a href="#MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索" class="headerlink" title="MONOTONIC ALIGNMENT SEARCH 单调对齐搜索"></a>MONOTONIC ALIGNMENT SEARCH 单调对齐搜索</h4><p>为了估计输入文本和目标语音之间的对齐 A，我们采用了单调对齐搜索 (MAS)（Kim 等人，2020 年），这是一种搜索对齐的方法，该方法可以最大化由归一化流 f 参数化的数据的可能性：</p>
<script type="math/tex; mode=display">
A = arg max_{\hat A} log p(x|c_{text} , \hat A) = arg max_{\hat A} log N(f(x); μ(c_{text}, \hat A), σ(c_{text}, \hat A))</script><p>在人类按顺序阅读文本而不跳过任何单词的事实之后，候选对齐被限制为单调和非跳过。 为了找到最佳对齐方式，Kim 等人 (2020) 使用动态规划。 在我们的设置中直接应用 MAS 很困难，因为我们的目标是 ELBO，而不是确切的对数似然。 因此，我们重新定义 MAS 以找到最大化 ELBO 的对齐，这简化为找到最大化潜在变量 $z$ 的对数似然的对齐：</p>
<script type="math/tex; mode=display">
arg max_{\hat A} log p_θ(x_{mel}|z) − log \frac {q_φ(z|x_{lin})} {p_θ(z|c_{text}, \hat A)}
= arg max_{\hat A} log p_θ(z|c_{text}, \hat A)</script><script type="math/tex; mode=display">
= log N(f_θ(z); μ_θ(c_{text}, \hat A), σ_θ(c_{text}, \hat A))</script><p>由于等式 5 与等式 6 相似，我们可以不加修改地使用原始 MAS 实现。 附录 A 包括 MAS 的伪代码。</p>
<h4 id="来自文本的持续时间预测"><a href="#来自文本的持续时间预测" class="headerlink" title="来自文本的持续时间预测"></a>来自文本的持续时间预测</h4><p>我们可以通过对估计对齐$\sum_j A_{i,j}$的每一行中的所有列求和来计算每个输入标记 $d_i$ 的持续时间。 正如之前的工作（Kim 等人，2020 年）所提出的那样，持续时间可用于训练确定性持续时间预测器，但它无法表达一个人每次以不同语速说话的方式。 为了生成类似人类的语音节奏，我们设计了一个随机持续时间预测器，使其样本遵循给定音素的持续时间分布。随机持续时间预测器是基于流的生成模型，通常通过最大似然估计进行训练。 然而，最大似然估计的直接应用是困难的，因为每个输入音素的持续时间是 1) 一个离散整数，需要对其进行反量化以使用连续归一化流，以及 2) 一个标量，它阻止了高维 可逆性引起的变换。我们应用变分反量化（Ho et al., 2019）和变分数据增强（Chen et al., 2020）来解决这些问题。 具体来说，我们引入两个随机变量 $u$ 和 $ν$，它们具有与持续时间序列 $d$ 相同的时间分辨率和维数，分别用于变分去组化和变分数据增强。 我们将 $u$ 的支持度限制为 $[0, 1)$，以便差 $d - u$ 成为正实数序列，并且我们将 $ν$ 和 $d$ 通道级联以形成更高维的潜在表示。 我们通过近似后验分布 $q_φ(u, ν|d, c_{text})$ 对两个变量进行采样。 由此产生的目标是音素持续时间的对数似然的变分下界：</p>
<script type="math/tex; mode=display">
logp_θ(d|c_{text}) ≥ E_{q_φ (u,ν |d,c_{text} )} log \frac {p_θ(d-u,v|c_{text})} {q_φ (u, ν |d, c_{text} )}</script><p>训练损失 $L_{dur}$ 是负变分下界。 我们将停止梯度算子 (van den Oord et al., 2017) 应用于输入条件，以防止反向传播输入的梯度，以便持续时间预测器的训练不会影响其他模块的训练。</p>
<p>取样程序比较简单； 通过随机持续时间预测器的逆变换从随机噪声中采样音素持续时间，然后将其转换为整数。</p>
<h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器 $D$，用于区分解码器 $G$ 生成的输出和真实波形 $y$。 在这项工作中，我们使用了两种成功应用于语音合成的损失类型； 用于对抗训练的最小二乘损失函数 (Mao et al., 2017)，以及用于训练生成器的附加特征匹配损失 (Larsen et al., 2016)：</p>
<script type="math/tex; mode=display">
L_{adv}(D) = E_{(y,z)}􏰄 [(D(y) − 1)^2 + (D(G(z)))^2],</script><script type="math/tex; mode=display">
L_{adv}(G) = E_z [(D(G(z)) − 1)^2] ,</script><script type="math/tex; mode=display">
L_{fm}(G) = E_{(y,z)} [\sum_{l=1}^T \frac {1} {N_l} ∥D^l (y) − D^l (G(z))∥_1 ]</script><p>其中 $T$​ 表示鉴别器中的总层数，$D^l$​ 输出具有 $N_l$​ 个特征的鉴别器第 $l$​ 层的特征图。 值得注意的是，特征匹配损失可以看作是在鉴别器的隐藏层中测量的重建损失，建议作为 VAE 逐元素重建损失的替代方案。</p>
<h3 id="最终的损失函数"><a href="#最终的损失函数" class="headerlink" title="最终的损失函数"></a>最终的损失函数</h3><p>通过 VAE 和 GAN 训练的结合，训练我们的条件 VAE 的总损失可以表示如下：</p>
<script type="math/tex; mode=display">
L_{vae} = L_{recon} +L_{kl} +L_{dur} +L_{adv}(G)+L_{fm}(G)</script><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>所提出模型的整体架构由后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器组成。 后验编码器和鉴别器仅用于训练，不用于推理。 附录 B 中提供了架构详细信息。</p>
<h4 id="2-5-1-后验编码器"><a href="#2-5-1-后验编码器" class="headerlink" title="2.5.1 后验编码器"></a>2.5.1 后验编码器</h4><p>对于后验编码器，我们在 WaveGlow 和 Glow-TTS 中使用非因果 WaveNet 残差块。 WaveNet 残差块由带有门控激活单元 (gated activation unit) 和跳过连接 (skip connection) 的扩张卷积层 (dilated convolutions) 组成。 块上方的线性投影层产生正态后验分布的均值和方差。 对于多说话人的情况，我们在残差块中使用全局调节来添加说话人嵌入。 </p>
<h4 id="2-5-2-先验编码器"><a href="#2-5-2-先验编码器" class="headerlink" title="2.5.2 先验编码器"></a>2.5.2 先验编码器</h4><p>先验编码器由处理输入音素 $c_{text}$ 的文本编码器和提高先验分布灵活性的归一化流 $f_θ$ 组成。 文本编码器是一个转换器 (Transformer) 编码器，它使用相对位置表示而不是绝对位置编码。 我们可以通过文本编码器和文本编码器上方的线性投影层从 $c_{text}$ 中获得隐藏表示 $h_{text}$，该线性投影层产生用于构建先验分布的均值和方差。 归一化流程是一堆仿射耦合层，由一堆 WaveNet 残差块组成。 为简单起见，我们将标准化流设计为一个体积保持变换 (volume-perserving transformation) ，雅可比行列式为 1。 对于多说话人的情况，我们通过全局条件将说话人嵌入添加到归一化流程中的残差块中。</p>
<h4 id="2-5-3-解码器"><a href="#2-5-3-解码器" class="headerlink" title="2.5.3 解码器"></a>2.5.3 解码器</h4><p>解码器本质上是 HiFi-GAN V1 生成器。 它由一堆转置卷积组成，每个卷积后面跟着一个多感受野融合模块（MRF）。 MRF 的输出是具有不同感受野大小的残差块的输出之和。 对于多说话人设置，我们添加一个线性层来转换说话人嵌入并将其添加到输入潜在变量 $z$。</p>
<h4 id="2-5-4-鉴别器"><a href="#2-5-4-鉴别器" class="headerlink" title="2.5.4 鉴别器"></a>2.5.4 鉴别器</h4><p>我们遵循 HiFi-GAN中提出的多周期鉴别器的鉴别器架构。 多周期鉴别器是基于马尔可夫窗口的子鉴别器的混合，每个子鉴别器都对输入波形的不同周期模式进行操作。</p>
<h4 id="2-5-5-随机持续时间预测器"><a href="#2-5-5-随机持续时间预测器" class="headerlink" title="2.5.5 随机持续时间预测器"></a>2.5.5 随机持续时间预测器</h4><p>随机持续时间预测器根据条件输入 $h_{text}$ 估计音素持续时间的分布。 为了对随机持续时间预测器进行有效的参数化，我们用扩张的和深度可分离的卷积层堆叠残差块。 我们还将神经样条流应用于耦合层，该流通过使用单调有理二次样条采用可逆非线性变换的形式。 与常用的仿射耦合层相比，神经样条流以相似数量的参数提高了变换表达能力。 对于多说话人设置，我们添加一个线性层来转换扬声器嵌入并将其添加到输入 $h_{text}$。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在两个不同的数据集上进行了实验。我们使用 LJ Speech 数据集与其他公开可用的模型和 VCTK 数据集进行比较，以验证我们的模型是否可以学习和表达不同的语音特征。 LJ Speech 数据集由单个说话人的 13,100 个短音频剪辑组成，总长度约为 24 小时。音频格式为 16 位 PCM，采样率为 22 kHz，我们使用它时没有进行任何操作。我们将数据集随机分成训练集（12,500 个样本）、验证集（100 个样本）和测试集（500 个样本）。 VCTK 数据集包含大约 44,000 个短音频片段，由 109 位英语为母语的人发出，带有各种口音。音频剪辑的总长度约为 44 小时。音频格式为 16 位 PCM，采样率为 44 kHz。我们将采样率降低到 22 kHz。我们将数据集随机分为训练集（43,470 个样本）、验证集（100 个样本）和测试集（500 个样本）。</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>我们使用可以通过短时傅立叶变换 (STFT) 从原始波形中获得的线性频谱图作为后验编码器的输入。 变换的FFT大小、窗口大小和跳跃大小分别设置为1024、1024和256。 我们使用 80 波段梅尔尺度频谱图进行重建损失，这是通过将梅尔滤波器组应用于线性频谱图而获得的。 </p>
<p>我们使用国际音标 (IPA) 序列作为先验编码器的输入。 我们使用开源软件将文本序列转换为 IPA 音素序列，并且在实现 Glow-TTS 之后，转换后的序列中穿插着空白标记。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>网络使用 AdamW 优化器进行训练，其中 $β_1 = 0.8$、$β_2 = 0.99$ 和权重衰减 $λ = 0.01$。 学习率衰减按每个 epoch 的 $0.999^{1/8}$ 因子进行调度，初始学习率为 $2 × 10^{−4}$。 继之前的工作之后，我们采用了窗口生成器训练，这是一种仅生成一部分原始波形的方法，以减少训练期间的训练时间和内存使用量。 我们随机提取窗口大小为 32 的潜在表示片段以馈送至解码器，而不是馈送整个潜在表示，并且还从地面实况原始波形中提取相应的音频片段作为训练目标。 我们在 4 个 NVIDIA V100 GPU 上使用混合精度训练。 每个 GPU 的批量大小设置为 64，模型最多训练 800k 步。</p>
<h3 id="用于比较的实验设置"><a href="#用于比较的实验设置" class="headerlink" title="用于比较的实验设置"></a>用于比较的实验设置</h3><p>我们将我们的模型与最好的公开可用模型进行了比较。 我们使用自回归模型 Tacotron 2 和基于流的非自回归模型 Glow-TTS 作为第一阶段模型，将 HiFi-GAN 作为第二阶段模型。 我们使用了他们的公开实现和预训练的权重。 由于两阶段 TTS 系统理论上可以通过顺序训练实现更高的合成质量，我们将微调的 HiFi-GAN 包括了高达 100k 步的预测输出 第一阶段模型。 我们根据经验发现，在教师强制模式下，使用 Tacotron 2 生成的梅尔谱图对 HiFi-GAN 进行微调，与使用生成的梅尔谱图微调相比，Tacotron 2 和 Glow-TTS 的质量都会更好。 Glow-TTS，因此我们在 Tacotron 2 和 Glow-TTS 中附加了更好的微调 HiFi-GAN。</p>
<p>由于每个模型在采样过程中都有一定程度的随机性，我们在整个实验过程中固定了控制每个模型随机性的超参数。 Tactron 2 的 pre-net 中的 dropout 概率设置为 0.5。 对于 Glow-TTS，先验分布的标准偏差设置为 0.333。 对于 VITS，随机持续时间预测器的输入噪声标准差设置为 0.8，我们将比例因子 0.667 乘以先验分布的标准差。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="语音合成质量"><a href="#语音合成质量" class="headerlink" title="语音合成质量"></a>语音合成质量</h3><p>我们进行了众包 MOS 测试来评估质量。 评分者聆听随机选择的音频样本，并以 1 到 5 的 5 分制对它们的自然度进行评分。评分者被允许对每个音频样本进行一次评估，我们将所有音频片段标准化以避免幅度差异对分数的影响。 这项工作中的所有质量评估都是以这种方式进行的。</p>
<p><img src="/images/vits_result1.png" alt="vits_result1"></p>
<p>评估结果如表 1 所示。 VITS 优于其他 TTS 系统，并实现了与Ground-Truth实况相似的 MOS。 VITS (DDP) 采用 Glow-TTS 中使用的相同确定性持续时间预测器架构而不是随机持续时间预测器，在 MOS 评估中在 TTS 系统中得分第二高。 这些结果意味着 1）随机持续时间预测器比确定性持续时间预测器生成更真实的音素持续时间；2）我们的端到端训练方法是制作比其他 TTS 模型更好的样本的有效方法，即使保持相似 持续时间预测器架构。</p>
<p><img src="/images/vits_result2.png" alt="vits_result2"></p>
<p>我们进行了一项消融研究以证明我们方法的有效性，包括先验编码器中的归一化流和线性尺度频谱图后验输入。 消融研究中的所有模型都经过最多 30 万步的训练。 结果如表 2 所示。去除先前编码器中的归一化流导致 1.52 MOS 比基线降低，这表明先验分布的灵活性显着影响合成质量。 用梅尔谱图替换后验输入的线性尺度谱图导致质量下降（-0.19 MOS），表明高分辨率信息对于 VITS 提高合成质量是有效的。</p>
<h3 id="泛化至多说话人TTS"><a href="#泛化至多说话人TTS" class="headerlink" title="泛化至多说话人TTS"></a>泛化至多说话人TTS</h3><p><img src="/images/vits_tab3.png" alt="vits_tab3"></p>
<p>为了验证我们的模型可以学习和表达不同的语音特征，我们将我们的模型与 Tacotron 2、Glow-TTS 和 HiFi-GAN 进行了比较，以显示扩展到多说话人语音合成的能力。 我们在 VCTK 数据集上训练模型。 我们将说话人嵌入添加到我们的模型中，如第 2.5 节所述。 对于 Tacotron 2，我们扩展说话人嵌入并将其与编码器输出concat起来，对于 Glow-TTS，我们参照2.5，应用了全局调节global conditioning。 评估方法与第 4.1 节中描述的相同。 如表 3 所示，我们的模型实现了比其他模型更高的 MOS。 这表明我们的模型以端到端的方式学习和表达各种语音特征。</p>
<h3 id="语音多样性"><a href="#语音多样性" class="headerlink" title="语音多样性"></a>语音多样性</h3><p>我们验证了随机持续时间预测器产生了多少不同长度的语音，以及合成样本具有多少不同的语音特征。</p>
<p><img src="/images/vits_fig2.png" alt="vits_fig2"></p>
<p><img src="/images/vits_fig3.png" alt="vits_fig3"></p>
<p>类似于 Valle 等人。 （2021），这里的所有样本都是从一句话<em>How much variation is there?</em>生成的。图 2a 显示了每个模型生成的 100 个话语的长度的直方图。由于确定性的持续时间预测器，Glow-TTS 仅生成固定长度的话语，我们模型中的样本遵循与 Tacotron 2 相似的长度分布。图 2b 显示了在多说话人设置中，使用五个说话者身份，生成的 5*100 个话语的长度，以展示模型能够学习到说话人相关的音素持续时间。图 3 中使用 YIN 算法 (De Cheveigne ́ &amp; Kawahara, 2002) 提取的 10 个话语的 F0 轮廓显示我们的模型生成具有不同音高和节奏的语音，图 3d 中的每个不同说话者身份生成的五个样本证明我们的模型为每个说话者身份表达了非常不同的语音长度和音调。请注意，Glow-TTS 可以通过增加先验分布的标准偏差来增加音高的多样性，但相反，它会降低合成质量。</p>
<h3 id="合成速度"><a href="#合成速度" class="headerlink" title="合成速度"></a>合成速度</h3><p><img src="/images/vits_tab4.png" alt="vits_tab4"></p>
<p>我们将模型的合成速度与并行的两阶段 TTS 系统 Glow-TTS 和 HiFi-GAN 进行了比较。 我们测量了整个过程的同步经过时间，以从 LJ Speech 数据集的测试集中随机选择 100 个句子的音素序列生成原始波形。 我们使用了一个批处理大小为 1 的 NVIDIA V100 GPU。结果如表 4 所示。由于我们的模型不需要用于生成预定义中间表示的模块，因此其采样效率和速度得到了极大的提高。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="端到端文本转语音"><a href="#端到端文本转语音" class="headerlink" title="端到端文本转语音"></a>端到端文本转语音</h3><p>目前，具有两级管道的神经 TTS 模型可以合成类人语音。 但是，它们通常需要使用第一阶段模型输出来进行训练或微调的声码器，这会导致训练和部署效率低下。 他们也无法获得端到端方法的潜在好处，该方法可以使用学习的隐藏表示而不是预定义的中间特征。</p>
<p>最近，单阶段端到端 TTS 模型被提出来解决更具挑战性的任务，即直接从文本生成原始波形，其中包含比梅尔频谱图更丰富的信息（例如，高频响应和相位）。FastSpeech 2s 是 FastSpeech 2 的扩展，它通过采用对抗性训练和帮助学习文本表示的辅助梅尔谱解码器来实现端到端并行生成。然而，为了解决一对多问题，FastSpeech 2s 必须从作为训练输入条件的语音中提取音素持续时间、音调和能量。 EATS（Donahue 等人，2021 年）也采用对抗性训练和可微分对齐方案。为了解决生成语音和目标语音之间的长度不匹配问题，EATS采用通过动态规划计算的软动态时间规整损失(soft DTW loss)。 Wave Tacotron（Weiss 等人，2020 年）将标准化流(Normalizing flows)与 Tacotron 2 相结合，形成端到端结构，但仍保持自回归。上述所有端到端 TTS 模型的音频质量都低于两级模型。</p>
<p>与前面提到的端到端模型不同，通过利用条件 VAE，我们的模型 1) 学习直接从文本中合成原始波形，而无需额外的输入条件，2) 使用动态规划方法 MAS 进行搜索 最佳对齐而不是计算损失，3）并行生成样本，4）优于最好的公开可用的两阶段模型。</p>
<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>VAE (Kingma &amp; Welling, 2014) 是使用最广泛的基于似然的深度生成模型之一。 我们对 TTS 系统采用有条件的 VAE。 条件 VAE 是一种条件生成模型，其中观察到的条件调节用于生成输出的潜在变量的先验分布。 在语音合成中，Hsu 等人。 (2019) 和 Zhang 等人 (2019) 结合 Tacotron 2 和 VAE 来学习说话风格和韵律。 BVAE-TTS（Lee 等人，2021 年）基于双向 VAE 并行生成梅尔谱图（Kingma 等人，2016 年）。 与之前将 VAE 应用于第一阶段模型的工作不同，我们将 VAE 应用于并行的端到端 TTS 系统。</p>
<p>Rezende &amp; Mohamed (2015), Chen 等(2017) 和 Ziegler &amp; Rush (2019) 通过使用标准化流增强先验和后验分布的表达能力来提高 VAE 性能。 为了提高先验分布的表示能力，我们将归一化流添加到我们的条件先验网络中，从而生成更真实的样本。</p>
<p>与我们的工作类似，Ma 等人 (2019) 提出了一种条件 VAE，在非自回归神经机器翻译 FlowSeq 的条件先验网络中对流进行归一化。 然而，我们的模型可以明确地将潜在序列与源序列对齐这一事实与 FlowSeq 不同，后者需要通过注意力机制学习隐式对齐。 我们的模型通过 MAS 将潜在序列与时间对齐的源序列进行匹配，从而消除了将潜在序列转换为标准正常随机变量的负担，这允许标准化流的更简单架构。</p>
<h3 id="非自回归TTS中的持续时间预测"><a href="#非自回归TTS中的持续时间预测" class="headerlink" title="非自回归TTS中的持续时间预测"></a>非自回归TTS中的持续时间预测</h3><p>自回归 TTS 模型通过其自回归结构和一些技巧（包括在推理和启动期间保持丢失概率）生成具有不同节奏的多样化语音。 另一方面，并行 TTS 模型依赖于确定性持续时间预测。 这是因为并行模型必须在一个前馈路径中预测目标音素持续时间或目标语音的总长度，这使得很难捕获语音节奏的相关联合分布。 在这项工作中，我们提出了一种基于流的随机持续时间预测器，它可以学习估计音素持续时间的联合分布，从而并行生成不同的语音节奏。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项工作中，我们提出了一个并行的 TTS 系统 VITS，它可以以端到端的方式学习和生成。我们进一步引入了随机持续时间预测器来表达不同的语音节奏。由此产生的系统直接从文本合成自然发声的语音波形，而无需经过预定义的中间语音表示。我们的实验结果表明，我们的方法优于两阶段 TTS 系统并达到接近人类的质量。我们希望所提出的方法将用于许多语音合成任务，其中使用了两阶段 TTS 系统，以实现性能提升并享受简化的训练过程。我们还想指出，即使我们的方法在 TTS 系统中集成了两个分离的生成管道，仍然存在文本预处理的问题。研究语言表征的自监督学习可能是去除文本预处理步骤的一个可能方向。我们将发布我们的源代码和预训练模型，以促进未来许多方向的研究。</p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2021/07/08/awesome-bloggers/" rel="prev" title="Awesome-bloggers">
      <i class="fa fa-chevron-left"></i> Awesome-bloggers
    </a></div>
      <div class="post-nav-item">
    <a href="/2021/07/20/end2end/" rel="next" title="End-to-end TTS & VC 文章总结">
      End-to-end TTS & VC 文章总结 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech"><span class="nav-number">1.</span> <span class="nav-text">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%84%9F%E6%83%B3"><span class="nav-number">1.1.</span> <span class="nav-text">感想</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.2.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.3.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Method"><span class="nav-number">1.4.</span> <span class="nav-text">Method</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%98%E5%88%86%E6%8E%A8%E6%96%AD"><span class="nav-number">1.4.1.</span> <span class="nav-text">变分推断</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overview"><span class="nav-number">1.4.1.1.</span> <span class="nav-text">Overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%87%8D%E6%9E%84%E6%8D%9F%E5%A4%B1"><span class="nav-number">1.4.1.2.</span> <span class="nav-text">重构损失</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#KL-%E6%95%A3%E5%BA%A6"><span class="nav-number">1.4.1.3.</span> <span class="nav-text">KL-散度</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E9%BD%90%E4%BC%B0%E8%AE%A1"><span class="nav-number">1.4.2.</span> <span class="nav-text">对齐估计</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#MONOTONIC-ALIGNMENT-SEARCH-%E5%8D%95%E8%B0%83%E5%AF%B9%E9%BD%90%E6%90%9C%E7%B4%A2"><span class="nav-number">1.4.2.1.</span> <span class="nav-text">MONOTONIC ALIGNMENT SEARCH 单调对齐搜索</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%9D%A5%E8%87%AA%E6%96%87%E6%9C%AC%E7%9A%84%E6%8C%81%E7%BB%AD%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B"><span class="nav-number">1.4.2.2.</span> <span class="nav-text">来自文本的持续时间预测</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%B9%E6%8A%97%E8%AE%AD%E7%BB%83"><span class="nav-number">1.4.3.</span> <span class="nav-text">对抗训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%80%E7%BB%88%E7%9A%84%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.4.4.</span> <span class="nav-text">最终的损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="nav-number">1.4.5.</span> <span class="nav-text">模型结构</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-1-%E5%90%8E%E9%AA%8C%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">1.4.5.1.</span> <span class="nav-text">2.5.1 后验编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-2-%E5%85%88%E9%AA%8C%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">1.4.5.2.</span> <span class="nav-text">2.5.2 先验编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-3-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="nav-number">1.4.5.3.</span> <span class="nav-text">2.5.3 解码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-4-%E9%89%B4%E5%88%AB%E5%99%A8"><span class="nav-number">1.4.5.4.</span> <span class="nav-text">2.5.4 鉴别器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-5-5-%E9%9A%8F%E6%9C%BA%E6%8C%81%E7%BB%AD%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B%E5%99%A8"><span class="nav-number">1.4.5.5.</span> <span class="nav-text">2.5.5 随机持续时间预测器</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.5.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">1.5.1.</span> <span class="nav-text">数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-number">1.5.2.</span> <span class="nav-text">预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.5.3.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E6%AF%94%E8%BE%83%E7%9A%84%E5%AE%9E%E9%AA%8C%E8%AE%BE%E7%BD%AE"><span class="nav-number">1.5.4.</span> <span class="nav-text">用于比较的实验设置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">1.6.</span> <span class="nav-text">实验结果</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E8%B4%A8%E9%87%8F"><span class="nav-number">1.6.1.</span> <span class="nav-text">语音合成质量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B3%9B%E5%8C%96%E8%87%B3%E5%A4%9A%E8%AF%B4%E8%AF%9D%E4%BA%BATTS"><span class="nav-number">1.6.2.</span> <span class="nav-text">泛化至多说话人TTS</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AF%AD%E9%9F%B3%E5%A4%9A%E6%A0%B7%E6%80%A7"><span class="nav-number">1.6.3.</span> <span class="nav-text">语音多样性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%90%88%E6%88%90%E9%80%9F%E5%BA%A6"><span class="nav-number">1.6.4.</span> <span class="nav-text">合成速度</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="nav-number">1.7.</span> <span class="nav-text">相关工作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AB%AF%E5%88%B0%E7%AB%AF%E6%96%87%E6%9C%AC%E8%BD%AC%E8%AF%AD%E9%9F%B3"><span class="nav-number">1.7.1.</span> <span class="nav-text">端到端文本转语音</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#VAE"><span class="nav-number">1.7.2.</span> <span class="nav-text">VAE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9D%9E%E8%87%AA%E5%9B%9E%E5%BD%92TTS%E4%B8%AD%E7%9A%84%E6%8C%81%E7%BB%AD%E6%97%B6%E9%97%B4%E9%A2%84%E6%B5%8B"><span class="nav-number">1.7.3.</span> <span class="nav-text">非自回归TTS中的持续时间预测</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BB%93%E8%AE%BA"><span class="nav-number">1.8.</span> <span class="nav-text">结论</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Alan Sun</p>
  <div class="site-description" itemprop="description">记录工作与生活</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">50</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
    有<span id="busuanzi_value_site_uv"></span>人来过
</span>
</div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Alan Sun</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
