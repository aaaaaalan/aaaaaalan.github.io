<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ICASSP 2020 Overview 思维导图</title>
    <url>/2021/06/16/ICASSP-2020-XMind-Overview/</url>
    <content><![CDATA[<p><img src="/images/ICASSP_2020.png" alt="ICASSP 2020 Overview"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>玩过的n=22种运动</title>
    <url>/2021/06/16/21-sports/</url>
    <content><![CDATA[<p>目前玩过的22种运动如下，更多运动探索中……</p>
<ul>
<li>跳绳</li>
<li>跳皮筋</li>
<li>丢沙包</li>
<li>跳房子</li>
<li>蹦床</li>
<li>爬健身器材</li>
<li>花样轮滑</li>
<li>游龙板</li>
<li>短跑</li>
<li>跳远</li>
<li>游泳</li>
<li>跆拳道</li>
<li>太极拳</li>
<li>健美操</li>
<li>舞狮</li>
<li>龙舟</li>
<li>瑜伽</li>
<li>攀岩</li>
<li>滑雪</li>
<li>滑冰</li>
<li>射箭</li>
<li>乒乓球</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>金载勋跆拳道- 2020年总结</title>
    <url>/2020/12/31/2020-jhk/</url>
    <content><![CDATA[<p>被两个极喜欢拖着我一起立flag折腾自己的狐朋狗友又一次立了一个flag，本想做一个视频，想到社会蓝的故事编辑技能树需要发光发亮，那我也来记录下这一年多的金载勋生涯吧。</p>
<p>2010年的暑假是我第一次接触tkd并深爱上跆拳道，似乎是2012年初二的暑假结束了跆拳道课程，止步于蓝带。印象中结束的最后一节课只有我一个学员上课，在我十分敬佩的葛教练指导下进行十分难得的1v1教学，上的课是当年我最爱的反应踢（教练随意出腿法脚靶，然后迅速反应）。当年留有贝克汉姆发型的我在葛教练的逼迫和鼓励下，顺利学会了俯卧撑、握拳俯卧撑和三指俯卧撑，也在一次次地习练中打开了横竖叉任督二脉。那几年的我，似乎达到了身体素质的局部巅峰。也是这种一次次突破的感觉，让我深爱习练tkd，深信自己也会成为一名优秀的跆拳道选手。然而，同伴的退出和教练的不停更换和面临初三中考的压力，我最后也选择了退出。尽管早已偷偷学会太极七章，尽管再没有机会弥补上痛失金牌的愿望，最终仍止步于蓝带。</p>
<p>中考后的我，还曾经和朋友一起回到道馆，尝试找回曾经的回忆，可笨重的自己和课堂上灵活的小朋友们一起上课后的感受只有“没有对比，就没有伤害”。即便还是曾经的那个道馆，年龄的增长还是劝退了我。</p>
<p>本想大学过后找到tkd社团就继续完成我的黑带梦想，可itf的发声方式，教练要改口为师范，以及从头再来的打击又一次将我劝退，充满棱角的我选择不愿意变更流派。虽然大学四年间每次路过道馆都会观察是不是wtf，有没有成人训练。可奈何都没有合适之选。我也逐渐转移了注意力到其他的运动上，从连续一个月每日中午晚上的舞狮训练上又找回了那种走火入魔的快乐。</p>
<p>直到2019年，工作上稍微稳定些的我继续开始寻觅自己的黑带梦想能够在哪里得以实现。在对比了解了不下十家道馆的我，被国际化的金载勋道馆所吸引了，于是预约了第一次的体验课。</p>
<p>尽管初次体验迟到了大概十五分钟，但耐心又漂亮温柔的前台小姐姐，1对1的体验，小阁楼式的装修，和隔板间的换衣房让我对这国际化道馆的好感印象不断上升。柴师范是我的试课师范，尽管距离当年的意气风发已久隔近十年，但当年对跆拳道的热爱使我还没有忘记腿法的基本动作，已学过的腿法都能够顺利完成。至于拳法的话，对于wtf的我就是一团糟了，只能在师范的指导下尝试模仿。</p>
<p>课上师范的鼓励和表扬和更加专业体系化的指导让我似乎找回了当年的快乐，也对这里的课程逐渐产生了信任。课后与丁师范近一个小时的聊天也让我在残酷的社会和异地漂泊的寒冷中找到了一丝温暖。唯一的顾虑还是itf or wtf。</p>
<p>我记得我有问是哪一种，dsf的回答十分巧妙而又深刻，两种都不是，但两种都会学，并且作为崔洪熙将军直系弟子的金载勋师范传授下来的课程，与市面上更流行普适的跆拳道课程不同，是更加地道和专业的。</p>
<p>另外印象深刻的一点是，我说为什么以前的课程都是一个半小时，现在一个小时感觉时间太短了，无法得到充分的练习。丁师范的回答十分坦诚，说“我们也不希望课程里面穿插很多无用的热身活动，比如跑步就跑了半个小时，如果排成一个半小时，那需要排的课更少了，对我们来说反而是轻松了。大家时间都很宝贵，我们还是希望能在课程里教给学员们更多的专业技能”。这一番言论在dsf绵绵细语下格外真诚，让我逐渐放下了心中对常规销售套路的防备心。</p>
<p>介意从头再来的我仍然问了那个问题，是否可以从以前的级别继续习练。dsf对此也是包容和支持，希望腰间的带色能够勉励我们自己不停努力，加紧训练，尽快追赶上自己腰间的色带。最终，csf对我天赋的鼓励和dsf对道馆体系化教学的讲解双面夹击，我放下了对教练称呼和训练体系改变的排斥，当场报名了半年的课程，希望能一展宏图。</p>
<p>然而，后面一个月内的课程大多是不习惯，不习惯课前没有充分的热身就开始进行腿法练习，不习惯每一次课前热身动作的重复无趣，不习惯不练习俯卧撑，不习惯打拳的时候抬手准备，不习惯做动作的时候要垫脚，不习惯要自己摸索动作结束的标准位置，不习惯很多腿法的动作细节，不习惯分散练习时无人指导的茫然，不习惯韧带没有拉开仍需要勉强完成一些大幅度的腿法动作。这一个月的习练甚至让我开始怀疑自己报名的冲动，最难受的是怀疑自己是不是并不适合这项运动，乃至开始咨询深圳内的其他成人道馆。</p>
<p>这一切直至遇到成长背景都十分相似，同龄又同样有执念的黑带胡哲才算得到一些开解。胡哲是我在跆拳道生涯中遇到过最让我敬佩的学员，尽管已经身为黑带还是在课后刻苦习练，尽管大学时候查有膝盖积水，仍旧在朗朗诗声中咬牙完成自己的横叉梦想，即便动作都已经非常熟练还是一遍又一遍的熟悉，同为wtf转方向，仍旧能够耐下心来完成新的体系的学习。这一切令我感到有些找回当年热血的感觉。课后我开始向她讨教腿法，品势的细节动作，曾经也是wtf的她帮助我开始逐渐熟悉了金载勋的训练体系。</p>
<p>说来很神奇，陌生人面前比较孤冷的我见到她之后话匣子似乎被打开了，她给我讲述大学中的一件件训练故事，分享给我大学的训练日记，文武双全的她让我开始感到崇拜。有她在的每一节课变得不再那么的孤单和艰辛，她不在的日子我都似乎又关闭了自己的说话功能，选择不去接触更多的人，把这一份热情和温暖唯独留给我欣赏的人。</p>
<p>除了同伴的收获，还有一次是杨师范的训练课也令我印象深刻。丁师范的课程还是比较偏向金载勋的训练体系，我还没有从幼时游戏式的热身，更多体能、全方面身体素质方面训练的热身中走出来，尤其在分散练习独自一人无人指导，也无熟悉同伴共同训练的时候倍感孤单，甚至觉得这样好似报名了一个健身房，得不到一些及时的指导和团队协作和竞争上的训练。可是ysf的课程是让排成两队，做一些交叉步，青蛙跳等素质练习的动作，逐渐让我找回了舒适圈，一些动作也得到了杨师范的认可让我逐渐找回了自信。课上的腿法练习中，杨师范总是可以观察一会，很准确的找出动作的问题所在，并给出改进方案，在尝试了师范的方式后，自我感觉也得到了显著的提升，那种成就感令我找回了一些快乐。那节课是我一个月以来最快乐的一次，从那以后开始甚至有点期待杨师范的课程。然而半年来都没有期盼到。</p>
<p>所幸的是，胡哲也与杨师范交情甚好，课后总是会在杨师范的指导下进行1对1柔韧度练习。我深知柔韧度对跆拳道训练的重要性，柔韧度很大程度上决定了腿法动作的幅度、标准度和灵活度，对挑战横叉的胡哲我眼中尽是钦佩和羡慕，所以选择留下来帮助胡哲按住一条腿。可长久以来都无法鼓起勇气说自己也想要完成这个柔韧度的训练。最终，选择报名了一个舞蹈软开度课程。课程结束也没有达到一些显著的提升。</p>
<p>在胡哲的横叉飞速进步中，很快就迎接来了过年，随着本命年的到来，居然是动荡全球的疫情风云。很早回到深圳的我还是不能够回到道馆继续训练。在家里的一个月时间中，我时不时会跟着keep进行一些简单的软开度训练，但都是2周没有见到效果就放弃了。</p>
<p>疫情期间一次公司聚餐中，意外接到了熟悉的dsf打来的电话，听到熟悉而亲切的声音的我激动但却不得不继续公司里的工作，因此和dsf解释说是否可以晚点联系。聚餐后匆忙给dsf打回去的我，打过去却是金载勋的总机电话，那一天时间里似乎都在期盼着dsf再次拨回来。又一次接到电话的我赶忙问何时可以开课，已经迫不及待了，dsf说会随政府安排，尽快，但是叮嘱说在家也要勤加练习哦。真是一个有爱有人情味的道馆。</p>
<p>终于熬到了四月底，又可以回到道馆了。之后的一个月就认识了肖宝杰小妹妹。对bj的印象是，总会有一个男孩子在楼上等着她😂好像不是很多话。</p>
<p>后面就迎来了开新的分馆，一直打探分馆主教练的我其实内心早就在想这样是不是就可以上更多的杨师范的课了，说不定还可以找杨师范帮忙拉开韧带。心里想着，似乎黑带梦想又重燃了希望。</p>
<p>分馆如期开张，杨师范也如期升职为分馆主教练，更开心的是，本舍不得在杨师范和丁师范中做个选择，海岸城的课程居然是两位主教练轮换教课，这也解除了我的一部分顾虑，不会因为总是上一个师范的课而感到略显重复枯燥。</p>
<p>宝杰也转到了分馆，并且在我第一次去的时候，发现宝杰也开始了自己的横叉磨练。杨师范似乎从我眼中看出了我的心思，便问我要不要也一起来，终于稍微鼓起勇气的我开始尝试。也在杨师范的“狠绝”下第二天就达到了一个小目标。</p>
<p>于是就这样开始了两个人的横叉之旅。印象很深的是一次我已经满头大汗了，杨师范还是心里一坚持，又加了一些强度，那一刻感觉两边的腿内侧的筋似乎开始燃烧了，然而神奇的是，从那以后两侧的筋就再也不痛了。但是，让我略生退却心思的是，有时候拉完韧带回到家，会导致前半夜腿痛，影响睡眠，甚至一次需要敷上止痛膏药才可以稍微缓解一些这种疼痛，并且有时候的腰痛也开始让我怀疑自己这把老骨头是不是回笼胯，是不是天生就是这么硬，因为小的时候也是竖叉很快就拉开了，横叉花费了多一倍的时间才拉开。</p>
<p>尽管心生疑虑，但是还是忍住不敢讲出口，怕师范会过于担心我的身体，我就更加放弃自己了。</p>
<p>这其中，印象很深的一次是，那次是隔了四五天没有压韧带，我心里很害怕退步了，太过紧张所以导致肌肉紧绷，大师兄也在一旁说“世上无难事，只要肯放弃”，丁师范也在一旁劝我不要勉强，最后就选择放弃了。回家路上我很严肃，责怪自己，又不知道怎样开解自己。只是心里充斥了恨铁不成钢。第二天都没有颜面去上杨师范的课了，感觉似乎辜负了师范。然而还是厚着脸皮去上了晚课，那一天，又到了一天一度的“地狱”时刻了，我心里仍旧是害怕，身体上还是诚实地去多热了热身，希望这样能缓解一些。宝杰已经完成了自己的每日任务，昨天就放弃的我，开始犹豫。杨师范给我喊了过去，一只很有力气的脚推在我的小腿上，说“奥兰你今天就别想跑了”。听了这话的我心里也想，不能再辜负师范的努力和期望了，眼镜一扔，眼睛一闭，就想豁出去了，一定要完成。哎，很神奇，很快就到达了目标了啊，没有想象中的那么痛啊？甚至比一周前还要好很多？？那天我真的十分感激杨师范帮我克服心中的魔鬼，终究是没有中断和放弃。</p>
<p>后面坚持和宝杰一起开横叉的时光艰辛而又互相勉励。柔韧度进步后在一些腿法上感觉更加收放自如，自我都感觉来到海岸城的这段时间实则进步了一大截。终于迎来了红蓝带考试，我也很骄傲的到后海馆给许久未见的其他同学和老师们看在杨师范指导半年下的飞速进步。</p>
<p>可惜后面宝杰韧带拉伤，我自己虽然心里不想放弃，但是也找不到主动去找杨师范的理由，所以也就随着宝杰的韧带拉伤中止了每天的撕心裂肺。</p>
<p>下半年的击破表演和红带考试是2020后半年的重点，两次都有各自的遗憾，但我似乎也开始慢慢学会开解自己的遗憾，不再懊恼和自责，而且在训练中多加补足，争取下一次能弥补遗憾。下半年的周六也增加了更多的体能训练，自我感觉在体能上也得到了长足进步，不再那么的担心考场中体力不支而无法呈现出最好的表现了。</p>
<p>宝杰应该是我跆拳道生涯里第二敬佩的学员。半年来的习练无论是在带色还是技术上的飞速进步是她每日坚持的成果，多次的韧带拉伤也没有打消她的韧带梦想，再加上实战时候的勇敢和坚定，生活中的不服输和挑战自我都让我这个废柴似乎又找回了一点年轻时的热血。</p>
<p>说到实战，胡姐姐头脑清晰，技术全面，实战经验丰富却仍谦虚勤练，宝杰呢，勇猛坚定，时机紧握，不畏级别高低，实战场上的自信令哪怕是较她级别高许多的学员都心生几分畏惧。这两个人不仅能打，还好打，每逢周六晚上都已经不想去练习枯燥的基本动作了，直接是一通暴打。红带时没有实战压力的我丝毫不想卷入这两个强者之间的战争。然而考完红带后，考试的压力让我不得不去面对自己畏惧的对抗互捶。</p>
<p>第一次参与其中的我还是一团混乱，找不到节奏，除了打不着别人就是被别人狂打头，僵硬的颈椎都放松了好几分。但是鼓起勇气的我在两个强者的陪伴下，再加上有事没事就开始在视频上学习对打技巧，几次习练后似乎找到了一些门道，克服了心中的恐惧，找到了适合自己的一些技巧，在和丁师范对打时丁师范也夸奖到进步了。</p>
<p>另一件让我感到十分温暖的事情是，在考黑红带前大约半个月的时间，偶然问到师范多久可以考黑带，当时大概是10月底，师范说，大概明年2月考黑红带，明年10月考黑带，当时已经学完考试内容的我略感疑惑，感觉自己每日的坚持没有了动力，习惯了临时抱佛脚的我觉得那为何不等快考试的几个月再加紧习练呢。于是是一个月的停练和心中放弃的魔鬼又跑了出来。偶然间和宝杰胡哲提到心中想法后，两位同伴一个苦口婆心的劝导，另一个还特地打来电话慰问。甚至二位还帮我将内心说不出口的想法讲给了师范，看看有没有解决办法。于是乎，师范通知，可以尽快准备考试了。说来也为自己的功利感到羞愧，本身想要踏实完成黑带考试，成为一名优秀黑带选手的我在面对选择的时候，还是选择了更加功利的早些得到黑带。但我很感激同伴的热情帮我争取这次机会，我也很感激师范的信任让我能够拥有这一次机会，因此也是倍加珍惜，尽量争取能够多加习练，争取能够多多出勤，不辜负同伴和师范的期望。也在持证教练肖教练的帮助下逐步跟上了品势内容。说到这里，不得不cue，肖教练真的是有教练的天赋，恩威并施，耐心又严厉，我猜她如果有此志向的话，一定会成为优秀的女教练。</p>
<p>世上无难事，只怕有心人。一句俗语，可也一直在陪伴我的成人跆拳道生涯。好汉不提当年勇，年轻的灵活敏捷早已该放到过去，对现实的困难发起挑战是要从几位师范、同伴身上学习的，“礼义廉耻，忍耐克己，百折不屈”曾经陪伴过我俯卧撑从0-n的突破，陪伴过我冬季3000m长跑的咬牙坚持，和许多运动生涯的艰辛瞬间。年长的我虽然心智在逐渐成熟，但对跆拳道的意志力却与幼时的自己不增反减。很感谢遇到的两位鸡血同伴挽救了一个废柴跆拳道选手，很感谢这一个又一个被迫立下的flag目前还没有轰然倒塌，很感谢我们即便面对质疑与世俗的不解，仍然坚持在自己热爱的道路上。</p>
<p>对成年后仍在这条道路的我，不期待三天打鱼，两天晒网的短暂热情，期待能够习得自律的习惯和稳步进步，期待能够像几位师范和同伴一样，热情不减，勇敢更多。许愿自己2021flag不倒，心愿达成。祝愿师范事业继续蒸蒸日上，生活温馨顺利。祝愿两位伙伴不忘初心，永远热忱。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>新心愿达成，新身份新快乐收获</title>
    <url>/2021/03/13/jhk-teaching-assistant-1st-time/</url>
    <content><![CDATA[<p>纪念一下收获了爱与能量的一天！</p>
<p>早上8点多起来，就是为了能不放同学鸽子，毕竟已经放了道友们好多次鸽子了，再放鸽子感觉我人品有问题，必须得去！所以就这样开启了神奇的一天。</p>
<p>早上8-9就起床了去看表演，自己做的很烂，朋友们都比我现场看的好很多哈哈哈哈，dsf剪辑也有点厉害啊，（就像胡哲说的，了解了一个人的人物传记之后，就会对这个人很有好感），我今天又成为dsf的迷妹了，感觉又在发着光。</p>
<p>但印象很深刻的是马玲的冲木前面几个动作巨好看，完美，养眼，赏心悦目以及自愧不如（可能这就是宝杰说的那种感受吧，她觉得很完美，但或许当事人不觉得）。然后就看到我的后踢又驼背了，是三个人里最差的，腰背部力量不够，身体没撑起来（难道这也是ysf晚上突然又cue我们做后踢辅助动作的原因之一吗？难道他也发现问题了？哈哈哈哈）</p>
<p>其他道馆，尽量发现的话还是有几个精品，但是普遍水平感觉没差很多，高级别很帅，看来tkd也是吃经验的。</p>
<p>10:15，换了便装的上身卫衣，匆匆赶到南山馆，还可以继续看一会，海岸城馆气势很大，哈哈哈哈，感觉果真什么师范带出什么样的学生。</p>
<p>10:50，收拾道具，迷茫，还傻乎乎的问ksf需不需要帮忙，没眼力见，在dsf问“你俩谁可以拍照”的时候，终于又勇敢了一把，舒坦爽快，也就开启了勇敢的一天。</p>
<p>11:00，开始30来个学员的儿童班课程，级别从白带到黑带，年纪从三四岁，到十来岁，看得头都大了，无法想象怎么上课，但是作为大头兵的我就哪里需要哪里搬就好了。</p>
<p>11:05，ksf整队敬礼，带队热身活动，迷茫，不知道该站哪，不知道该不该一起热身，不知道该不该帮助小朋友，这时的马师范已经开始自觉热身了，还得到了dsf的表扬，🍋，后面才知道原来msf之前有过这种经验，一回生二回熟哈哈哈</p>
<p>11:10，30个小朋友按级别分成了五队，五个助教，一人带一队，当场感叹师范简直是人才，这么简单的除法就解决了30个熵混乱的问题哈哈哈哈。尴尬的是，ksf喊我们三个过去的时候，作为打头的我愣住了，不知道该从哪个路径走过去，恍惚了几秒，一向超级超级super-nice温柔的ksf突然很严肃的说，助教要快一点啊！妈呀被嫌弃了有点尴尬，还是要硬着头皮迎接接下来的任务啊。</p>
<p>11:15，ksf做师范，助教帮忙拿脚把，妈呀第一脚就被踢，第二脚又被踢了，都怪我脚把拿多高我都不知道，而且小朋友身高一会高一会低，脚把位置一直在变动，我的天，为啥有的小孩踢左脚，有的右脚，有的正面，有的背面，有的格斗式，有的并脚，刚才ksf怎么做的？？？脑子一面空白，dsf过来旁边告诉我，你放松一点，不要脚把抓的那么紧，dsf也帮助小朋友指正动作，这时候我终于只需要做个不需要思考的拿把机器就好了，舒坦，但是dsf走了之后又陷入了慌张和迷茫。终于ksf喊停了，第一组动作，带小朋友一起划水结束哈哈哈哈</p>
<p>11:20，这一次的示范动作我很认真的观察ksf怎样站位，怎样准备的，怎样起腿，怎样结束动作，脚把位置多高，甚至关键点是什么。到我这边实践后，小朋友们还是乱七八糟，我开始学会说，换脚，面向我，背对我，左腿，右腿等简单的方位动作，终于捋顺了大家的完整动作，从一个方向出腿了。开始也去观察主要的问题点，并且提出合理的建议，或者做出简单的解决方案，才到我大腿个子的小朋友（看似也就三四岁），居然可以听懂我的简单指令？？？成功建立沟通连接了之后，感觉开始上手了。居然也可以听懂我的解决方案？？小朋友真厉害！！真的进步了一点点点！好棒！！！（虽然内心疯狂喜悦，肾上腺激素疯狂上飙，但是嘴巴上的我还是一个敷衍的表扬机器），“很好”，“还不错”，我自己都感觉自己的表扬十分敷衍，但是又学不会dsf的那种“对iiiiii了！”，“对iiiii，就是这样做嘛！”太厉害了，佩服的五体投地了。</p>
<p>渐入佳境的我完成了几组动作指导后，发现队伍有点乱，不知道该怎么管理，发现后面小朋友在自己玩，不知道该咋办，不敢凶他们，发现小朋友东张西望了，不知道该怎么勾回他们的注意力，只能先注意到在做动作的这个人身上了。</p>
<p>小朋友们的性格，脾气，也各式各样，各有千秋，蓝带高个子朋友打头，最高级别，年龄也相对较大一些，所以能够沟通，动作上也是最好的。所以多是表扬，和给他更多的一些挑战。比如旁边两组高级别都是跳后旋，他在我们这组里级别最高，我看他也可以尝试，就叫他尝试了一下，发现果真还不错、他的表情令我感觉他自己做了高级别的动作，也有点小满足，小骄傲，小自豪，哈哈哈，他的满足也是我的快乐，我也接收到快乐了。</p>
<p>绿带也是大腿高度的小男孩，从一开始就一直笑笑的，笑的人都要融化掉了，僵硬冰冷了这么多年的我（3年）感觉被暖到了，所以我也卸掉了社会中的面具，变得笑笑的，微笑也给我带来了满足和快乐。这个小男孩更逗的一点是，后面练拳法，我不知道为啥他总要连续快速做，本来只要做一个就好，我说你这咋还变成连环拳了，他笑笑继续他的连环拳。柴师范过来说，哎，你不可以这样对待姐姐哦，要好好表现，他果真听话了？？？开始一拳一拳做。原来他刚才不是没听懂或者不会做？柴师范一说他咋就懂了？？所以刚才是故意的？引起我的注意？？被小孩的聪明才智折服也骗到了，怪不得老师都喜欢偏爱坏学生，原来坏学生总是能很聪明的挑战到他的底线，让她觉得教育这件事都变有趣了，更想要管教好她，所以才会选择偏爱更有趣和更有挑战性的学生。哈哈哈哈哈神奇。</p>
<p>绿黄带小孩给我印象也很深刻，几次勾踢和后旋踢有点惊艳到了，感觉不是他这个级别应该领悟到的东西啊。哈哈哈，或许这就是低级别的快乐。总是会被夸奖。哈哈哈哈，反正黑红带以前我也总是被夸奖，自从要考黑带了就每日被diss。说回绿黄带，本来期望绿黄带应该不太能做这两个动作吧，居然甩了几下有模有样的？？？ksf也予以了多次的表扬和认可，原来不止我觉得这个有天赋，是个苗子。更逗乐的是，他的每次出拳之前，别人都是ha之后就出拳了，我不知道为啥，这个小孩看起来只有三四岁，他好像思考了有一两秒，攥紧了拳头，好像在蓄力的样子，打出的拳头，虽然那么小只，感觉也很有力气啊我的天，或许这就是传说中的天赋？？这个蓄力的动作太神奇了，感觉一个三四岁小孩都充满了神秘感，不知道他的小脑袋瓜在想什么。（哈哈哈这里有个插曲，就是，dsf跑过来，说，让姐姐看到你的力气哦！我心想，dsf真尊重我们，真社会，真客气，真乖，哈哈哈哈我都不好意思了，也不知道小宝贝心里怎么想的。）在思考出拳方向？还是在蓄力？不知道，但是打出的拳头还是不错的</p>
<p>还有多次dsf跑过来说，一开始说，你放松一点，你不要拿脚把那么紧张，你不要扎着马步，这一天腰都断了，放松自然一点。你要适时有感染性的鼓励一下小朋友，你要带有情绪的，“哇！真棒！”，“好棒啊”，做得好的甚至可以“give me five”。这个时候我感觉自己才是道场里的学生，dsf和小朋友都是我的老师，在教化着我怎么去应对这个不太擅长的局面，dsf是我的指路人，用他的经验告诉我怎样是对的，小朋友们是我的助教，陪我实践，给我反馈，一次又一次表情上的反馈告诉我这样做是不是对的。大多数都会是肯定我，我这样做确实是对的，他们很开心，他们有快乐，有成就感。天呐，没想到，我才是这堂课收获最多的人。</p>
<p>好了说回我队的小宝贝，绿带高个子男孩，看起来有四五年级了，很成熟稳重，容易沟通，还能在我搞不了小朋友的时候，帮我指导一下，只不过就是有点好像循规蹈矩，很认真做，但感觉还是不太灵动，所以会感觉有点死板，再加上最明显的问题就是速度慢了，所以对他的要求主要是提高速度，也是希望能提高灵动性，他一开始没有讲过话，后面中间一次他好像问了一句，是不是这样，那个时候我感觉好像互相产生信任了，我给他的回复也给了他安全感。他也更加卖力去做了。也很明显感觉速度快了动作舒服很多。</p>
<p>蓝绿带软萌小女孩，妈呀太可爱了，头发好多，软萌软萌的，不忍心碰她，哎，没想到协调性贼好，转身动作极其完美酷炫，又超乎想象了。小拳头不是打过来，好像是砸过来的，太可爱了，都不忍心纠正她可爱的动作了，哈哈哈哈。萌化。</p>
<p>最后好像还有一个绿带还是蓝绿带的小宝贝，他应该是做的都还不错，所以没有花心思给他改正动作的话，印象似乎没有太深刻，这可能就是那种平凡的孩子？自己很乖，很优秀，很听话，动作也很标准了，但是就是不容易给人留下印象了，突然有点代入一向“普普通通”“平平凡凡”的自己。</p>
<p>总而言之，这次助教课收获颇多！这种能量的突变确实力量巨大，从去之前的恐惧，担心是熊孩子，从小觉得自己没有孩子缘，担心自己管不住小孩，等等，居然全都得到了救赎，师范们把小朋友们带的很好，该听话的时候很听话不闹，小朋友们像小天使一般，用他们的笑容，单纯，善良，进取，努力，甚至调皮等都带给了我巨大能量，让我这一天都充满了能量，（也导致我现在话匣子打开都停不下来了），有点上头，有点快乐。</p>
<p>课后dsf暗示我说，我在楼下坐，你有什么技术上的交流可以来找我，我温习了今天的课程之后，留了十分钟，本想找dsf看一步对打，没想到坐下就开始聊教课体验，哈哈哈。我真诚的表达了自己的一些小想法，他也很快乐，似乎觉得给我的能量似乎也反馈给他了一部分快乐，我能从表情里看到他对于这件事安排的满意程度，主要是因为他觉得我乐在其中，很有收获。我说这个事情，有点有成就感啊，dsf说，我们做师范最大的就是成就感，blablabla说了一堆，总而言之就是表达我这种小想法的认可，有成绩感是正常的。应该要有成就感。然后我说我没有经验，很紧张，dsf说以后还可以找你来，我很开心，也爽快的答应了。甚至还说，如果你以后，敲代码敲烦了，不想敲了，就给我打电话，（略带哭腔）地说，师范，我代码敲的不快乐了，我马上给你安排助教课，给你找找成就感，然后你再回去快乐的敲代码。天啊啊啊啊啊啊啊啊，听到这句话的我真的彻底融化，在这个诺大的城市里，受了委屈，有师范给我兜底，我真的心怀感激，备感荣幸，也感受到了深深的安全感，就算捅了篓子，也有人有事情能救回我的感觉，这也太适合我这个时不时低沉但是内心始终温暖热血的表面丧人了。听完这话的我，虽然内心知道这一把年纪了，肯定是不会好意思这样做的，最多也就是师范需要我，我会很乐意去帮忙，但是内心也是着实被暖到了，感觉似乎在深圳这个城市里伪装的冰冷，今天被小朋友和师范们全都融化了。表达完我有点快乐有点上头的情绪后，师范似乎也放宽了心，说以后还会找我来，让我慢慢接触尝试，说不定就转师范了哈哈哈哈哈，这种职业道路的变更，虽然想过，想过无数次、很多次，数万次，但是太需要勇气了，风险极大，落差极大，自己的专业性和天赋能力也远远不足，只能慢慢来，先享受小天使们带给我的快乐和满足吧。我现在算是体会到dsf所说的大师兄是自己享受指导和帮助人的过程，我现在算是理解了，之前可能是我们理解错了吧。到现在也算又圆了一个小时候的跆拳道助教梦想哈哈哈哈，人生真的太神奇了，儿时的梦想居然一个一个接一个的实现了，这种肾上腺素飙升的感觉也太快乐了。</p>
<p>至此，先鞠躬敬礼感谢丁师范的信任给我这一次机会，感谢dsf在看我迷茫的时候一直耐心的指导我该怎样做，哪里不对，（比指导我技术的时候耐心多了哈哈哈哈），感谢小朋友们包容我这个新手助教，给我信任、微笑、力量和快乐，感谢杨师范包容我中途跑掉，还不回来上课了，晚上还跑过来蹭场地的同时，晚上还不计前嫌地帮我个人抠动作了有三四十分钟，感谢柴师范还是用他的一如既往的温暖温暖着我，感谢自己珍惜这次机会才得偿所愿，感谢心中有梦的自己这么多年还没有忘记最初的梦想，人间又值得了！！！！！</p>
<p>最后上升一些生活哲学，</p>
<ol>
<li>六个小朋友不同的性格，似乎都让我看到了社会和人生。</li>
<li>你带的学生，最后就会变成你自己的一面镜子。dsf这句话真的绝了。联想到现实更绝。ksf和dsf，dsf和cj，ysf和bj，hjj和我算是dsf和ysf的混交吧，hjj偏dsf，我算是混的比较平均的。哈哈哈哈也算是这种结合方式的首次尝试，也挺特别和快乐的。</li>
<li>tkd快乐今日double、乃至triple，hundredable，突然感觉被赋予了使命，感受到了教育的快乐</li>
<li>又一次感受到了几个师范的温柔，简直温柔了深漂的岁月，今天繁师范问我为什么成人还要练tkd，之前一个小朋友的爸爸也问过我，我第一次回答对黑带的执念（或许是我最初的梦想），今天回答工作之余的发泄和对现实生活的逃避，感觉有点官方现实，但也有点映射哲学了。现在总结一下，于我而言，最开始是对黑带的执念，小时候不断突破自己的回忆让我埋下种子，在更有时间精力的时候完成小时候未竟的梦想，在参加了之后完全是被金载勋吸引了，道馆师范对自己要求太高，包括技术上，心理上，各种综合素质都相对于其他成人道馆要成熟太多太多了。对深漂的自己是一种解脱，感觉像是在深圳的一个家，朋友之间没有利益冲突，可以很坦率的交朋友，师范燃烧自己，照亮他人，最关键的是，师范自身的经历，相近的年龄，能给自己带来很多人生的启迪，有时候工作不顺训练上身体解压，朋友上互相开导，师范也会给出建议以及甚至给你兜底，懂得教育心理学的师范们，即便你受欺负了，也能帮你解脱出来。女孩子独自一人在外的护身防卫，不要以为是花架子，金载勋不一样，不仅仅玩运动竞技场上的规则，尤其是成人班很强调实用性的，很多动作你是真实可以应用到实际情景的，上海两例女性自我保护事件，这个东西会给自己在外漂泊更多安全感。价格不贵的， 挠痒痒一样，对道馆来说，成人班只是他们积累更多经验，基本不赚钱的，你说这种性价比这么高的活动，不值得吗？时间也不会耽误很多，但是需要坚持，但是培养任何一项业务爱好和特长，都需要坚持。最后才是，减肥，运动，保持身材，柔韧性，协调性，力量等等很多问题。如果想要自己的生活，不仅仅是工作，家庭，没有别的味道了，那还是非常值得推荐的。总而言之，工作之外的生活，变得更加丰富多彩，快乐，充实和解压。</li>
</ol>
<p>对自己黑带的要求是：小于3次失误，击破、一步对打和品势因为只有一次机会，所以要0失误。横叉flag不要倒啊加油啊不要倒啊。</p>
<p>好了，放过自己，今天的输入太多，终于输出完毕，今天可以圆满结束了。晚安蓝蓝。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>BC challenge 2019 Top5队伍 技术分析</title>
    <url>/2020/03/09/bc2019top5/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>Frontend</strong></th>
<th><strong>Duration Modelling</strong></th>
<th><strong>Spectrogram modelling</strong></th>
<th><strong>Vocoder</strong></th>
<th><strong>Features</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>USTC-iflytek</strong> <strong>科大讯飞</strong></td>
<td>Tasks:  special marks procession, polyphones classification, breaks prediction focuses prediction.  Methodoly: Bidirectional Encoders Representations from Transformers (BERT)-based multi-task models</td>
<td>LSTM-RNN models autoregressive model structure,</td>
<td>A statistical parametric speech system (SPSS) GAN-based multi- task acoustic modeling Fundamental frequency (F0), 41 dimensional mel-cepstra (M- CEP), band aperiodicity (BAP) were adopted as the acoustic features</td>
<td>Wavenet   The acoustic feature used was the joint feature vector of Mel-cepstrum, F0 and the u/v decision. Multi-speaker dataset for argumentation</td>
<td>Text-side: Manual annotations: Pinyin(with tone), PW, PP, and focus position Speech-side: Frame-level acoustic features:</td>
</tr>
<tr>
<td><strong>DeepSound</strong> <strong>深声科技</strong></td>
<td>Tasks: text normalization, qingsheng, sandhi and erhua, : rule-based G2P: Bi-LSTM prosody prediction, PW, PPH, IPH: Bi-LSTM BiLSTM-based recurrent network (RNN) is used in the G2P module for polyphone and prosody prediction.</td>
<td>/</td>
<td>VQVAE. + a embedding+prenet oper- ation + GAN based postfiltering     (robust on the unclean dataset )</td>
<td>robust multi-speaker neural vocoder conditioned on the mel spectrograms</td>
<td>manual and auto- matic tagging operations: phoneme, tone, prosody and pause duration</td>
</tr>
<tr>
<td><strong>腾讯</strong></td>
<td>Festival front-end to predict phoneme, tone and other linguistic features   +   BERT sentence embeddings are generated by a pre-trained Bert model.</td>
<td>/</td>
<td>A multi-speaker model is trained first.</td>
<td>multi-speaker model trained first. Wavenet</td>
<td>linguistic feature (The HTS full-context label) and sentence embedding mel spectrograms + channel embedding</td>
</tr>
<tr>
<td><strong>灵伴</strong></td>
<td>text normalization, word segmentation, part-of-speech tagging, phonetic disambiguation word segmentation of the sentence, Part-of-Speeches (POS) of this word sequence and prosodic hierarchy</td>
<td>/</td>
<td>DNN-LSTM</td>
<td>Wavenet   ground-truth mel-spectrograms plus F0</td>
<td>spectral envelope, fundamental frequency (F0), contextual labels (phone-related and word-related features)</td>
</tr>
<tr>
<td><strong>Horizon</strong> <strong>南京团队</strong></td>
<td>The corresponding texts were manually embedded into 476-dimensional vectors using our own text an- alyzing system. The embedded vectors consisted of one-hot encoded phonemes, tones, part-of-speech, prosodic boundaries and the position information.   Prosody boundary: phoneme boundaries, syllable boundaries, phrase boundaries, secondary phrase boundaries</td>
<td></td>
<td>DCTTS[14] and Deep Voice 3[13]</td>
<td>WaveRNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>韵律建模思维导图</title>
    <url>/2020/09/20/prosody-modelling/</url>
    <content><![CDATA[<p><img src="/images/%E6%83%85%E6%84%9F%E5%BB%BA%E6%A8%A1.png" alt="韵律建模论文思维导图"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>语音顶会论文集总结</title>
    <url>/2020/12/03/speech-papers/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>会议地点</strong></th>
<th><strong>离线论文包</strong></th>
<th><strong>论文集网址</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Icassp 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Interspeech 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td><a href="https://www.isca-speech.org/archive/Interspeech_2020/">https://www.isca-speech.org/archive/Interspeech_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Neurips</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://proceedings.neurips.cc/">https://proceedings.neurips.cc</a></td>
<td></td>
</tr>
<tr>
<td><strong>BC 2020 &amp; VC 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://www.isca-speech.org/archive/VCC_BC_2020/">https://www.isca-speech.org/archive/VCC_BC_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Iclr 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech">https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>论文合集网址</strong></td>
<td>-</td>
<td>-</td>
<td><a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive">https://www.isca-speech.org/iscaweb/index.php/archive/online-archive</a></td>
<td>含interspeech、ssw等论文合集</td>
</tr>
<tr>
<td><a href="https://openreview.net/">https://openreview.net</a></td>
<td>含iclr、icml、acm等论文集</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Vocoders 模型总结</title>
    <url>/2020/12/03/vocoders/</url>
    <content><![CDATA[<p>语音合成声码器脉络总结如下，持续更新ing</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>Order</strong></th>
<th><strong>Model</strong></th>
<th><strong>Year</strong></th>
<th><strong>Institution</strong></th>
<th><strong>Conference</strong></th>
<th><strong>Inherited Model (Base model)</strong></th>
<th><strong>Corresponding Author (Team leader)</strong></th>
<th><strong>URL</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>WaveNet</td>
<td>2016.9</td>
<td>Google DeepMind</td>
<td>SSW 2016</td>
<td>CNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1609.03499.pdf">https://arxiv.org/pdf/1609.03499.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>WaveRNN</td>
<td>2018.6</td>
<td>DeepMind &amp; Google Brain</td>
<td>ICML 2018</td>
<td>RNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1802.08435.pdf">https://arxiv.org/pdf/1802.08435.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>WaveGlow</td>
<td>2018.10</td>
<td>Nvidia</td>
<td>ICASSP 2019</td>
<td>WaveNet</td>
<td>Rafael Valle</td>
<td><a href="https://arxiv.org/pdf/1811.00002.pdf">https://arxiv.org/pdf/1811.00002.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>LPCNet</td>
<td>2019.2</td>
<td>Mozilla, Google</td>
<td>ICASSP 2019</td>
<td>WaveRNN</td>
<td>Jean-Marc Valin</td>
<td><a href="https://arxiv.org/pdf/1810.11846.pdf">https://arxiv.org/pdf/1810.11846.pdf</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WaveGAN</td>
<td>2019.2</td>
<td>UC San Diego</td>
<td>ICLR 2019</td>
<td>GAN</td>
<td>Miller Puckette</td>
<td><a href="https://arxiv.org/pdf/1802.04208.pdf">https://arxiv.org/pdf/1802.04208.pdf</a></td>
</tr>
<tr>
<td><strong>6</strong></td>
<td>Multi-band WaveRNN</td>
<td>2019.4</td>
<td>Tecent AI Lab</td>
<td>Interspeech 2020</td>
<td>DurIAN, WaveRNN</td>
<td>Dong Yu</td>
<td><a href="https://arxiv.org/pdf/1909.01700.pdf">https://arxiv.org/pdf/1909.01700.pdf</a></td>
</tr>
<tr>
<td><strong>7</strong></td>
<td>MelGAN</td>
<td>2019.12</td>
<td>University of Montreal, Mila, Lyrebird AI</td>
<td>NeurIPS 2019</td>
<td>GAN</td>
<td>Yoshua Bengio</td>
<td><a href="https://arxiv.org/pdf/1910.06711.pdf">https://arxiv.org/pdf/1910.06711.pdf</a></td>
</tr>
<tr>
<td><strong>8</strong></td>
<td>SqueezeWave</td>
<td>2020.1</td>
<td>UC Berkeley</td>
<td></td>
<td>WaveGlow</td>
<td>Bichen Wu</td>
<td><a href="https://arxiv.org/pdf/2001.05685.pdf">https://arxiv.org/pdf/2001.05685.pdf</a></td>
</tr>
<tr>
<td><strong>9</strong></td>
<td>Parallel WaveGAN (PWG)</td>
<td>2020.2</td>
<td>LINE Corp., NAVER Corp.</td>
<td></td>
<td>GAN</td>
<td>Ryuichi Yamamoto</td>
<td><a href="https://arxiv.org/pdf/1910.11480.pdf">https://arxiv.org/pdf/1910.11480.pdf</a></td>
</tr>
<tr>
<td><strong>10</strong></td>
<td>Multi-band MelGAN</td>
<td>2020.5</td>
<td>西北工业大学，sogou</td>
<td></td>
<td>melgan, multi-band</td>
<td>Xielei</td>
<td><a href="https://arxiv.org/pdf/2005.05106.pdf">https://arxiv.org/pdf/2005.05106.pdf</a></td>
</tr>
<tr>
<td><strong>11</strong></td>
<td>FeatherWave</td>
<td>2020.10</td>
<td>Tecent</td>
<td>Interspeech 2020</td>
<td>MB LP, WaveRNN</td>
<td>Shan Liu</td>
<td><a href="https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf">https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf</a></td>
</tr>
<tr>
<td><strong>12</strong></td>
<td>WaveGrad</td>
<td>2020.10</td>
<td>Johns Hopkins University, Google Brain</td>
<td></td>
<td>CNN</td>
<td>Heiga Zen</td>
<td><a href="https://arxiv.org/pdf/2009.00713.pdf">https://arxiv.org/pdf/2009.00713.pdf</a></td>
</tr>
</tbody>
</table>
</div>
<p><a href="https://arxiv.org/pdf/2103.05236.pdf">GAN Vocoder: Multi-Resolution Discriminator Is All You Need</a></p>
<p>此篇论文尝试解释为什么近期涌现的GAN-based vocoders要好于过往的Flow-based或者Autoregressive的vocoders。文章通过消融实验分析认为原因主要在于Multi-Resolution Discriminator的设计使得GAN-based vocoders达到了一个新的水平。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ASRU 2019 语音合成相关论文</title>
    <url>/2021/06/17/asru-2019-papers/</url>
    <content><![CDATA[<div class="table-container">
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>论文题目</strong></th>
<th><strong>作者</strong></th>
<th><strong>单位</strong></th>
<th><strong>摘要</strong></th>
<th><strong>关键词</strong></th>
<th><strong>论文链接</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</td>
<td><em>Xuankai Chang</em>1,2<em>, Wangyou Zhang</em>2<em>, Yanmin Qian</em>2†<em>, Jonathan Le Roux</em>3<em>, Shinji Watanabe</em>1†</td>
<td>1Center for Language and Speech Processing, Johns Hopkins University, USA 2<strong>SpeechLab</strong>, Department of Computer Science and Engineering, <strong>Shanghai Jiao Tong University</strong>, China 3Mitsubishi Electric Research Laboratories (MERL), USA</td>
<td>MIMO-Speech, which extends the original seq2seq to deal with <strong>multi-channel input and multi-channel output</strong> so that it can <strong>fully model multi-channel multi-speaker speech separation and recognition</strong>. MIMO-Speech is a fully neural end-to- end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model.</td>
<td><strong>Overlapped speech recognition</strong>, end-to-end, neural beamforming, <strong>speech separation</strong>, curriculum learning.</td>
<td><a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</td>
<td><em>Fengyu Yang</em>1<em>, Shan Yang</em>1<em>, Pengcheng Zhu</em>2<em>, Pengju Yan</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1∗</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University</strong>, Xian, China 2Tongdun AI Lab</td>
<td>We introduce a novel self-attention based encoder with learnable Gaussian bias in Tacotron. The proposed approach has the ability to generate stable and natural speech with minimum language-dependent front-end modules.</td>
<td>Tacotron, end-to-end, speech synthesis, <strong>self-attention, Gaussian bias</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>LEARNING HIERARCHICAL REPRESENTATIONS FOR EXPRESSIVE SPEAKING STYLE IN END-TO-END SPEECH SYNTHESIS</td>
<td><em>Xiaochun An</em>1†<em>, Yuxuan Wang</em>2<em>, Shan Yang</em>1,2<em>, Zejun Ma</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1⇤</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University,</strong> Xi’an, China 2 ByteDance AI Lab</td>
<td>we introduce a hierarchical GST archi- tecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech.</td>
<td>Speaking style, disentangled representations, <strong>hierarchical GST,</strong> style transfer</td>
<td><a href="http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf">http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>BOOTSTRAPPING NON-PARALLEL VOICE CONVERSION FROM SPEAKER-ADAPTIVE TEXT-TO-SPEECH</td>
<td><em>Hieu-Thi Luong<strong>1,2</strong>, Junichi Yamagishi**1,2,3</em></td>
<td>1SOKENDAI (The Graduate University for Advanced Studies), Kanagawa, Japan 2National Institute of Informatics, Tokyo, Japan 3<strong>The University of Edinburgh</strong>, Edinburgh, UK</td>
<td>Bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.</td>
<td><strong>voice conversion,</strong> cross-lingual, speaker adaptation, transfer learning, text-to-speech</td>
<td><a href="https://export.arxiv.org/pdf/1909.06532">https://export.arxiv.org/pdf/1909.06532</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WAVENET FACTORIZATION WITH SINGULAR VALUE DECOMPOSITION FOR VOICE</td>
<td><em>Hongqiang Du<strong>1,2</strong>, Xiaohai Tian<strong>2</strong>,</em> <strong><em>Lei Xie***</em></strong>1*<strong>**</strong>, Haizhou Li*<strong>*</strong>2***</td>
<td>1School of Computer Science, <strong>Northwestern Polytechnical University</strong>, xi’an, China 2Department of Electrical and Computer Engineering, National University of Singapore, Singapore <a href="mailto:hongqiang.du@u.nus.edu">hongqiang.du@u.nus.edu</a>, <a href="mailto:eletia@nus.edu.sg">eletia@nus.edu.sg</a>, <a href="mailto:lxie@nwpu.edu.cn">lxie@nwpu.edu.cn</a>, <a href="mailto:haizhou.li@nus.edu.sg">haizhou.li@nus.edu.sg</a></td>
<td>We propose to use singular value decomposition (SVD) to reduce WaveNet parame- ters while maintaining its output voice quality. Specifically, we apply SVD on dilated convolution layers, and impose semi-orthogonal constraint to improve the performance.</td>
<td>Voice Conversion (VC), <strong>WaveNet, Sin- gular Value Decomposition (SVD)</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf</a></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION"><a href="#Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION" class="headerlink" title="Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION"></a>Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</h1><h2 id="MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf"><a href="#MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf" class="headerlink" title="MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）https://arxiv.org/pdf/1910.06522.pdf"></a>MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）<a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></h2><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>ASRU的best paper思路是挺清晰的，但是与其他会议发表的classic论文相比还是感觉有一些些差距，有一些模型的细节点来说，会感觉有些晦涩难懂，打分：🌟🌟🌟</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>MIMO-Speech，采用“多通道-输入”和“多通道-输出”，以建模 多通道 多说话人 情景下的语音分离和语音识别。</p>
<ul>
<li><p>模型结构由三部分组成：</p>
<p>1）单声道masking网络；</p>
<p>2）多源神经波束形成器；</p>
<p>3）多输出-语音识别模型；</p>
</li>
<li><p>学习策略：a curriculum learning strategy</p>
</li>
<li><p>实验结果：60% WER reduction</p>
</li>
</ul>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><ul>
<li><p>待解决问题：鸡尾酒聚会课题，分为 单通道 / 多通道 语音识别问题</p>
<ul>
<li><p>单通道多说话人语音分离：</p>
<p>1） Deep clustering (DPCL), 将时域单元映射到嵌入向量，再采用聚类算法将每个单元聚类到说话人源。此方法之后被嵌入到端到端训练框架中。</p>
<p>2）Permutation-invariant training (PIT)：用一个permutation-free目标函数来最小化重构损失。PIT之后被应用于多说话人ASR，在一个DNN-HMM混合ASR框架下。</p>
</li>
<li><p>多通道多说话人语音分离：</p>
<p>1）PIT，语音分离</p>
<p>2）unmixing transducer (a mask-based beamformer)，语音分离</p>
<p>3）DPCL：将inter-channel differences作为空间特征，和单通道频谱特征，语音分离</p>
</li>
</ul>
</li>
<li><p>本文贡献：多通道-多说话人-语音识别，输入multi-channel input (MI), 输出 multiple output (MO) text sequences, one for each speaker, 所以称为 MIMO-Speech。</p>
</li>
<li><p>可行性：最近的单说话人-远场-语音识别展示了 “神经波束”技巧对于去噪的价值，并且一些研究证实了end-to-end的可行性。[27] 进一步证实了神经波束方法在多通道端到端系统能够增强信号。</p>
</li>
</ul>
<h3 id="MIMO-Speech-2-pages"><a href="#MIMO-Speech-2-pages" class="headerlink" title="MIMO-Speech (2 pages)"></a>MIMO-Speech (2 pages)</h3><p><img src="/images/MIMO-Speech.png" alt="MIMO-Speech"></p>
<h4 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h4><ul>
<li>Stage 1: a single-channel masking network，通过预测多说话人和各通道噪音masks来实现<strong>预分离</strong></li>
<li>Stage 2: 多源 “神经波束” 来<strong>空间上分离多说话人的源头</strong></li>
<li>Stage 3: 端到端ASR来实现<strong>多说话人语音识别</strong></li>
</ul>
<p>创新点：masking network + neural beamformer，单目标函数进行模型训练。</p>
<p>Stage1 (Monaural masking network) 可以预分离开噪音和多说话人音源；Stage 2 生成多个beamforming filters $g^{i}(f)$ 用来分离和降噪输入的多声道信号；Stage3有多个说话人的encoder，和一个attention decoder组成，来生成多个说话人的文本序列输出。</p>
<h4 id="Data-scheduling-and-curriculum-learning"><a href="#Data-scheduling-and-curriculum-learning" class="headerlink" title="Data scheduling and curriculum learning"></a>Data scheduling and curriculum learning</h4><ul>
<li><p>问题痛点：端到端训练难以收敛</p>
</li>
<li><p>解决方案<strong>（Data scheduling）</strong>：随机从以下两个数据集中选择一个batch</p>
<p>1） 不仅采用多空间域的多说话人数据集</p>
<p>2） 也采用单说话人的数据集</p>
</li>
<li><p>细节<strong>（Curriculum learning）</strong> 配Algorithm：</p>
<p>1） 当选择到单说话人的数据集的时候，数据不经过 masking network 和 neural beamformer 模型，以加强end-to-end ASR模型的训练。</p>
<p>2） 计算出最大声和最小声说话人声音之间的信噪比SNR，然后按“升序”排列，从SNR=1的数据集开始训练</p>
<p>3） 将单说话人的数据集从短到长进行排序，让seq2seq模型首先学习短语句。</p>
</li>
</ul>
<h3 id="Experiments-3-pages"><a href="#Experiments-3-pages" class="headerlink" title="Experiments (3 pages)"></a>Experiments (3 pages)</h3><h4 id="3-1-Configurations"><a href="#3-1-Configurations" class="headerlink" title="3.1 Configurations"></a>3.1 Configurations</h4><h5 id="3-1-1-Neural-Beamformer"><a href="#3-1-1-Neural-Beamformer" class="headerlink" title="3.1.1 Neural Beamformer"></a>3.1.1 Neural Beamformer</h5><h5 id="3-1-2-Encoder-Decoder-Network"><a href="#3-1-2-Encoder-Decoder-Network" class="headerlink" title="3.1.2 Encoder-Decoder Network"></a>3.1.2 Encoder-Decoder Network</h5><h4 id="3-2-Performance-on-ASR"><a href="#3-2-Performance-on-ASR" class="headerlink" title="3.2 Performance on ASR"></a>3.2 Performance on ASR</h4><p>Motivation: 验证提出的模型好于baselines</p>
<p>Baselines / Ours</p>
<h4 id="3-3-Performance-on-Speech-seperation"><a href="#3-3-Performance-on-Speech-seperation" class="headerlink" title="3.3 Performance on Speech seperation"></a>3.3 Performance on Speech seperation</h4><p>Motivation: 验证 neural beamformer 学习了一个波束行为，能够用于语音分离。</p>
<h4 id="3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验"><a href="#3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验" class="headerlink" title="3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)"></a>3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)</h4><p>Motivation: 验证在实际情况下的模型性能。</p>
<h1 id="Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS"><a href="#Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS" class="headerlink" title="Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS"></a>Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</h1><h2 id="Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf"><a href="#Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf" class="headerlink" title="Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf"></a>Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 <a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></h2><h2 id="感想-1"><a href="#感想-1" class="headerlink" title="感想"></a>感想</h2><p>有些论文读起来会觉得高深莫测，但是有有部分价值可以吸取，打分：🌟🌟</p>
<h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>问题痛点：虽然对于英文来讲，现有的如Tacotron等模型已经能够实现端到端的语音合成过程，即从英文字母直接转换至语音。但是对于如中文这样的语言，仍旧需要繁复的前处理过程（如词边界、韵律边界等），使得这个文本处理前端的过程和传统方法一样复杂。</p>
<p>解决方案：为了保持生成语音的自然度、以及摒弃特定语言的特殊性，普通话语音合成过程中，我们引入了一个创新性的自注意力机制作为编码器，并且引入可学习的高斯bias到Tacotron中</p>
<p>实验结果：我们评估了不同的系统（在 有/无 韵律信息的情况下），结果显示提出的方法能够在最小的语言-依赖的前端模块的情况下，生成稳定和自然的语音。</p>
<h3 id="Introduction-1页"><a href="#Introduction-1页" class="headerlink" title="Introduction (1页)"></a>Introduction (1页)</h3><ul>
<li><p>待解决问题</p>
<p>传统的端到端方法包含复杂的特征提取过程，如：Part-of-speech tagging, pronunciation prediction, prosody labelling. 即便如Tacotron的端到端语音合成系统被提出，但是单纯的输入音素也无法使得语音合成模型得到良好的效果，所以学者提出嵌入PW、PPH、IPH，来进行韵律边界的特征建模。但是这使得违背了端到端语音合成的初衷，使得这个系统再次变得更加复杂。</p>
</li>
<li><p>本文贡献</p>
<p>1）<strong>全局韵律建模：</strong>由于self-attention被证明对于简单的音素序列进行全局建模时有良好的效果，所以本文尝试采用自-注意力机制作为编码器来获取全局的韵律信息。</p>
<p>2）<strong>局部韵律建模：</strong>至于局部的韵律信息，我们采用了一个可学习的高斯bias引入到自注意力机制中，因为Gaussian分布更加集中于当前位置的局部关系。</p>
</li>
</ul>
<h3 id="Proposed-SAG-Tacotron-1-5页"><a href="#Proposed-SAG-Tacotron-1-5页" class="headerlink" title="Proposed SAG-Tacotron (1.5页)"></a>Proposed SAG-Tacotron (1.5页)</h3><h4 id="3-1-Motivation"><a href="#3-1-Motivation" class="headerlink" title="3.1 Motivation"></a>3.1 Motivation</h4><ul>
<li><p>目的：为了采用最少的文本分析模块，所以引入自-注意力学习机制，来进行全局依赖建模。</p>
</li>
<li><p>方案：1）将Encoder的CBHG模块，用自-注意力机制替换；2）可学习的高斯bias来提升局部建模。</p>
</li>
</ul>
<p><img src="/images/SAG-Tacotron.png" alt="SAG-Tacotron"></p>
<h4 id="3-2-基于-自注意力机制-的-编码器"><a href="#3-2-基于-自注意力机制-的-编码器" class="headerlink" title="3.2 基于 自注意力机制 的 编码器"></a>3.2 基于 自注意力机制 的 编码器</h4><p>Encoder 的 Pre-net是一个3层-CNN + Batch Norm + ReLU，尽管自-注意力机制不包含序列信息，我们注入类似于Transformer的位置信息。</p>
<script type="math/tex; mode=display">
PE_{pos,2i}=sin(pos/10000^{2i/d})</script><script type="math/tex; mode=display">
PE_{pos, 2i+1} = cos(pos/10000^{2i/d})</script><p>其中，$pos$是当前位置，$d$是特征维度，$i$是当前维度。PE也被输入到自-注意力模块。自注意力模块包含了一个自注意力层+全连接层+tanh激活函数。残差连接被应用于上述层。</p>
<p>对于多头注意力机制的每一个$head_i$, 对于一个有$n$个元素的序列$x$，我们想要获得有相同长度n的隐状态向量$head_i$, 这里采用scale-product注意力机制。</p>
<script type="math/tex; mode=display">
Head_{i}=\sum_{j=1}^{n}ATT(Q,K)V</script><script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy)​</script><script type="math/tex; mode=display">
energy = \frac {QK^{T}} {\sqrt{d}}</script><p>最终的多头注意力机制为：</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_{1}, ..., head_{h})W^{O}</script><p>其中的$W^{O}$是最后一层线性层的参数矩阵。</p>
<h4 id="3-3-可学习的Gaussian-bias"><a href="#3-3-可学习的Gaussian-bias" class="headerlink" title="3.3 可学习的Gaussian bias"></a>3.3 可学习的Gaussian bias</h4><p>在序列-序列模型中，对于中文来讲，相近的位置是十分重要的。在这种情况下，我们想要为注意力机制提升编码器对于临近状态的局部贡献。</p>
<p><img src="/images/Gaussian-bias.png" alt="Gaussian-bias"></p>
<p>如上图所示，首先，假设一个以e5为中心的高斯bias，窗长为3（实际上，窗长是一个可学习的参数）。然后将注意力机制的分布通过高斯bias来进行正则化，以生成最终的分布。如图3所示，最终的分布是会在e5附近有更多的权重的。</p>
<p>具体来讲，Gaussian bias $G$被mask到energy上，即</p>
<script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy + G)</script><p>其中$G \in R^{N\times N}$，$G \in (-1;0]$ 度量了当前的query $x_i$与position $j$ 之间的关系：</p>
<script type="math/tex; mode=display">
G_{ij}=-\frac {(j - P_{i})^2}{2\sigma_i^2}</script><p>其中的$P_i$是$x_i$的中心位置，当给定输入序列 $x=(x_1, x_2, …, x_n)$，$\sigma_i$是标准差。如何选择合适的$P_i$和$\sigma_i$是关键。</p>
<script type="math/tex; mode=display">
P_i = N\cdot sigmoid(v_p^{T}tanh(W_{p}x_i))</script><script type="math/tex; mode=display">
D_i = N\cdot sigmoid(v_d^{T}tanh(W_{d}x_i))</script><p>其中$\sigma_i = \frac {D_i}{2}$, $W_p$ 和$W_d$是模型参数矩阵。</p>
<h3 id="Experiments-2-5页"><a href="#Experiments-2-5页" class="headerlink" title="Experiments (2.5页)"></a>Experiments (2.5页)</h3><h4 id="4-1-Basic-setups"><a href="#4-1-Basic-setups" class="headerlink" title="4.1 Basic setups"></a>4.1 Basic setups</h4><h4 id="4-2-System-comparison"><a href="#4-2-System-comparison" class="headerlink" title="4.2 System comparison"></a>4.2 System comparison</h4><ul>
<li>Baseline: Tacotron1</li>
<li>Baseline-prosody: Tacotron1 with complex inputs</li>
<li>SAE-Tacotron: Self-attention as encoder without Gaussian bias with simply inputs</li>
<li>SAG-Tacotron: Self-attention as encoder with Gaussian bias with simple inputs</li>
<li>Transformer with simple inputs</li>
</ul>
<p><img src="/images/simple_inputs.png" alt="simple_inputs"></p>
<p><img src="/images/complex_inputs.png" alt="complex_inputs"></p>
<h4 id="4-3-Model-details"><a href="#4-3-Model-details" class="headerlink" title="4.3 Model details"></a>4.3 Model details</h4><h4 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h4><h5 id="4-4-1-Robustness-test"><a href="#4-4-1-Robustness-test" class="headerlink" title="4.4.1 Robustness test"></a>4.4.1 Robustness test</h5><p>Motivation: 评估attention对齐（Repeats / Skips）的鲁棒性</p>
<h5 id="4-4-2-Prosody-analysis"><a href="#4-4-2-Prosody-analysis" class="headerlink" title="4.4.2 Prosody analysis"></a>4.4.2 Prosody analysis</h5><p>Motivation: 评估重读音节的pitch以及trajectory pattern of F0</p>
<h5 id="4-4-3-Objective-test"><a href="#4-4-3-Objective-test" class="headerlink" title="4.4.3 Objective test"></a>4.4.3 Objective test</h5><p>Motivation: 采用MCD评估学习到的频谱的质量，MCD越低越好。</p>
<h5 id="4-4-4-Subjective-test"><a href="#4-4-4-Subjective-test" class="headerlink" title="4.4.4 Subjective test"></a>4.4.4 Subjective test</h5><p>Motivation: 评估模型主管听测效果</p>
<p>评估方式：20个人，30句随机抽取的语音</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages</title>
    <url>/2021/06/21/mlms/</url>
    <content><![CDATA[<h1 id="Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017"><a href="#Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017" class="headerlink" title="Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)"></a>Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)</h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>这篇Google UK的论文，采用了一个MLMS的想法来解决低资源语种合成的问题，思想感觉上与采用IPA来统一音素特征是非常相似的，论文结构清晰，实验部分翔实充分，打分：🌟🌟🌟</p>
<h2 id="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK"><a href="#论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK" class="headerlink" title="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK"></a>论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>痛点：对于低资源语种来说，获取大量的训练数据是昂贵且困难的，通常是仅仅能获取到一小部分 / 或者没有数据集。</p>
<p>解决方案：本文提出了一种利用长短期循环神经网络的声学模型，目的是解决小语种语言数据缺失的问题。“说话人自适应”系统目的在于在多种语言间保持说话人的相似度，而本方法的突出特征是，模型构建成功后，系统不需要再重新训练以解决集外的语种，这是由于<strong>语言和说话人-不可知的建模方法和通用的语言特征集。</strong></p>
<p>实验结果：1）在12种语言上的实验结果显示，对于集外语种，系统仍能生成智能、自然的声音。2）当提供了少量训练数据的情况下，pooling the data有时能够提高整体的智能性和自然度。3）有时，构建一个zero-shot的多语种系统好于few-shot 单说话人单语种系统。</p>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><p>近期发展：近些年，统计参数语音合成的方法，从HMM转向了神经网络系统，2013年 Heiga Zen 发布了第一个采用前向DNN网络的语音合成系统（Google，ICASSP 2013），且合成效果优于HMM系统。之后的LSTM-RNN模型提升了语音合成的效果，并且最近的PCM生成模型（WaveNet）近一步提升了模型效果。</p>
<p>挑战：1）语音数据的获取。当从先验收集到少量数据时，说话人-自适应方法可以被采用，这时候，需要将模型在新说话人的少量数据集上进行fine-tune。但是这种方法不能用于zero-shot的情境。2）获取多说话人的广泛数据集，并且构建一个平均音色模型。但是这个方法不能应用于缺少足够语言信息的语种上。</p>
<p>本文课题：对于指定的低资源语种数据，有最小的语言表示信息。</p>
<p>解决方法：一个多语种声学模型被训练，其中目标语种的数据集未包含在训练数据集集内。</p>
<p>本文贡献：一个通用的MLMS（multi-lingual multi-speaker）模型被训练，并且是采用语言和说话人-不可知的方法。</p>
<h3 id="Multilingual-Architecture-1-page"><a href="#Multilingual-Architecture-1-page" class="headerlink" title="Multilingual Architecture (1 page)"></a>Multilingual Architecture (1 page)</h3><p>本文的优势：1）一个具象的输入特征空间，不需要在新语种上fine-tune；2）一个类似于单说话人的简单模型架构。</p>
<h4 id="2-1-文本特征"><a href="#2-1-文本特征" class="headerlink" title="2.1 文本特征"></a>2.1 文本特征</h4><h5 id="2-1-1-典型语言表示"><a href="#2-1-1-典型语言表示" class="headerlink" title="2.1.1 典型语言表示"></a>2.1.1 典型语言表示</h5><p>训练数据集是包含多种语言和口音的。首先将多语种全部转换至IPA。尽管这个转换过程有一些困难，如1）需要专家知识来做相应的转换，2）不能直接的转换。但是这个IPA还是能够为语言空间提供具象的特征。</p>
<h5 id="2-1-2-系统发育语言特征"><a href="#2-1-2-系统发育语言特征" class="headerlink" title="2.1.2 系统发育语言特征"></a>2.1.2 系统发育语言特征</h5><p>基于BCP-47标注，我们采用语言和边界识别特征来建模同语种的不同口音。+ 一个系统语言分类树</p>
<h4 id="2-2-LSTM-RNN-声学模型"><a href="#2-2-LSTM-RNN-声学模型" class="headerlink" title="2.2 LSTM-RNN 声学模型"></a>2.2 LSTM-RNN 声学模型</h4><p>给定语言特征后，LSTM-RNN时长模型的作用是预测每个音素的发音时长。然后再将这个时长和语言特征一同输入到声学模型。以预测音频波形。音频波形的平滑性，是采用RNN的循环单元来建模的。</p>
<p>由于本文需要处理更大数量的数据集和更加多样的语言特征，所以本文的模型与baseline的区别在于ReLU的单元数量和LSTM的层数，以及声学模型输出层的循环单元的个数。</p>
<h3 id="Experiments-2-page"><a href="#Experiments-2-page" class="headerlink" title="Experiments (2 page)"></a>Experiments (2 page)</h3><p>用于训练声学模型的数据集语料有超过800小时的语音，包含了37种不同的语言种类。这些语言属于原始的59组语言/地区对，一些语种，如英语，有不同的说话人数据集，对应不同的地域口音。对于一些口音（如EN-US）有多个说话人。一些音频是在消声室（anechoic chambers）录制的，而一些就是常规的录音室录制的（a regular recording studio）</p>
<h4 id="3-1-方法论：系统细节"><a href="#3-1-方法论：系统细节" class="headerlink" title="3.1 方法论：系统细节"></a>3.1 方法论：系统细节</h4><p>语音数据采用22.05KHz的数据集，LSTM-RNN模型输出的特征是音素的发音时长</p>
<h4 id="3-2-模型参数和评估"><a href="#3-2-模型参数和评估" class="headerlink" title="3.2 模型参数和评估"></a>3.2 模型参数和评估</h4><p>实验被设计为两种情景：</p>
<ul>
<li>模型被在除去12种语言的语料上训练（其中有6种，是毫无语料的情况）。但每一种被排除的语种（除了其中2种）都有“亲戚”语种在训练数据集中。在对这些被排除在外的目标语种进行语音合成。其中的模型称为H</li>
<li>用所有语种的数据集来训练模型。其中的模型称为I</li>
</ul>
<p>因为声学模型可以被speaker和gender identifying特征控制，所以以下实验被设计来观察如何影响合成质量。</p>
<ul>
<li>speaker和gender特征 unset （default，D），set to the highest quality female speaker (EN-US, F), highest quality male speaker (EN-GB, M), speaker of the closet language (C).</li>
<li>Setting the speaker and gender features for this speaker (S)</li>
</ul>
<p>实验评估：100句集外话术，每个人最多听100句话。每句话有1min的评估时间。每一种语言有8个评分者。</p>
<h4 id="3-3-实验结果和讨论"><a href="#3-3-实验结果和讨论" class="headerlink" title="3.3 实验结果和讨论"></a>3.3 实验结果和讨论</h4>]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>考古两篇TTS with NN/DNN的开山之作</title>
    <url>/2021/06/21/1st-dnn-tts/</url>
    <content><![CDATA[<h1 id="Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013"><a href="#Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013" class="headerlink" title="Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)"></a>Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)</h1><h2 id="https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf"><a href="#https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf" class="headerlink" title="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf"></a><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf</a></h2><h2 id="总结以及感想"><a href="#总结以及感想" class="headerlink" title="总结以及感想"></a>总结以及感想</h2><p>看了考古文对科研又产生了新的理解，目前的论文大多修修补补，灌水严重，实验部分不是很充分，无法印证论文的可复现性和实验结论的可靠性。这篇论文虽然采用的DNN技术还是最早期的神经网络系统架构，但是对于系统设计的每一个细小的结构都进行了充分的实验对比验证，得到了可靠的实验结论。在结论处也给同行留下了更多想象和探索的空间。强烈建议TTS从业者逐字逐行阅读本文，学习论文构思和写作的思想，并且了解深度学习在TTS领域应用的起源，至少从实验部分的objective /subjective evaluation可以学习到客观评估TTS合成效果的方法，使得自己的TTS研究更加扎实可靠。打分：🌟🌟🌟🌟🌟</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>痛点：传统的统计参数语音合成方式通常采用决策树，上下文依赖的HMM模型来表示给定文本的语音的概率密度。其中，语音参数是从概率密度中生成的来最大化它们的输出概率，然后再用生成的语音参数，重构语音波形。这种方法的缺点是，决策树对于建模复杂的上下文依赖关系是比较无效的。</p>
<p>解决方案：本文基于深度神经网络（DNN）。输入文本和声学表示的关系通过一个DNN来建模。DNN的使用能够解决许多传统方法的局限性。</p>
<p>实验结果：DNN效果优于HMM</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>基于HMM的参数方法在过去十年间盛行，相对于波形拼接算法来讲，它的优势在于能够灵活地替换音色，小的踪迹和鲁棒性。然而，它的主要局限性是合成语音的质量。Zen等提出了合成质量的三个主要特征：声码器，声学模型的准确度，和过拟合。本文的方法主要在于解决声学模型的准确度。</p>
<p>影响语音的一定数量的上下文特征包含音素、语言和语法特征，会在统计参数合成过程中，参与建模。典型系统包含50种上下文。因此，这些复杂的上下文依赖的有效建模是统计参数语音合成的关键点。为了解决这种上下文问题的标准方法是基于HMM的统计参数语音合成算法。对于每一种独立的上下文组合，都采用一个独立的HMM模型，作为一个上下文相关的HMM模型。通常来讲，对于这种全部上下文依赖的HMM模型来说，训练数据是不充分的，难以学习到一个稳定的模型，能够覆盖到所有所需的上下文组合。</p>
<p>为了解决这种问题，基于自上而下的决策树算法的上下文聚类被广泛使用。在这种方法中，上下文-依赖的HMM的状态被区分为多个“簇”，并且每个“簇”的分布参数是共享的。HMM模型的任务是通过二分类决策树，检验每一个HMM的上下文组合，其中一个上下文相关的二分类问题是与每一个 非叶子结点 相关的。“簇”的数量，也是 叶子结点 的数量，决定了模型的复杂程度。决策树通过序列化挑选能够在训练数据集上产生最高mle的分数来挑选问题。树的大小是通过一个预定义的mle阈值，一个模型复杂度惩罚，以及交叉验证来决定的。采用了上下文相关的问题和状态参数共享后，未知的上下文和数据稀疏性问题得到了有效解决。就像在语音识别中所成功解决的，基于HMM的方法自然地对于有丰富数据的上下文有较好的效果。</p>
<p>尽管基于上下文决策树的HMM模型在统计参数语音合成方法中是有效的。但是，有以下局限性：</p>
<ul>
<li>对于复杂上下文依赖如“XOR”，奇偶校验和复用问题，这种方法是无效的；</li>
<li>这种方法将输入的空间区分开，并且对于每个区域都采用了独立的参数，每个区域对应着一个决策树的叶子结点。这导致了分裂训练数据集，并且在聚类和估计分布的时候，使得每个簇的数据不充分。</li>
</ul>
<p>有一个相对大的决策树，并且分裂训练数据集都会导致过拟合，损害合成语音的质量。</p>
<p>为了解决上述局限性，本文采用基于DNN的结构。上述基于决策树的方案，建模了从 文本中抽取的语言上下文到语音参数的映射。在这里的决策树被一个DNN模型所替代。值得注意的是，自从90年代开始，NN就尝试被应用于TTS中。</p>
<h2 id="DNN-VS-Decision-tree-DT"><a href="#DNN-VS-Decision-tree-DT" class="headerlink" title="DNN VS Decision tree (DT)"></a>DNN VS Decision tree (DT)</h2><ul>
<li>DT 在表达输入特征的复杂关系时无效，如XOR、d 位奇偶校验函数、或者多路复用的问题。为了表达上述情境下的问题，决策树可能会十分巨大。然而，这些关系能够被DNN模型来具象表示</li>
<li>决策树致力于分割输入空间，对每个空间采用一组独立的参数和一个叶子结点。这样会导致在每个区域的数据数量少和较差的泛化性能。Yu et al 证明了在采用决策树建模时，一些较弱的输入特征如语音中词级别的重读会被丢失。由于DNN的权重是从整体的训练数据得到的，所以DNN会得到更好的泛化性能。DNN也提供了输入高维、多种输入特征的可能性。</li>
<li>相较于决策树，通过反向传播来训练一个DNN模型通常需要大量的计算过程。在预测过程中，DNN需要在每一层都有一个矩阵乘法，但是决策树仅仅需要通过一个输入特征的子集从根结点遍历树直至叶子结点。</li>
<li>决策树的推理是更加可解释的，DNN中的权重很难在直观上获得解释。</li>
</ul>
<h2 id="基于DNN的语音合成"><a href="#基于DNN的语音合成" class="headerlink" title="基于DNN的语音合成"></a>基于DNN的语音合成</h2><p>由于人类的发声系统是多层级的，才能够将文本信息转换为语音波形，所以本文尝试采用深度神经网络来进行语音建模。</p>
<p><img src="/images/DNN-based-tts.png" alt="DNN-based-tts"></p>
<p>上图展示了一个基于DNN的语音合成框架。输入文本首先被转换为输入特征序列$\{x^t_n\}$，其中的$x^t_n$表示在第$t$帧的第$n$维输入特征。输入的特征是对于文本上下文关系的二分类问题，包含如（e.g is-current-phoneme-aa?）和数值（e.g. 在短语中的单词数量，在当前音素序列的当前帧的相对位置，和当前音素的发音时长）</p>
<p>然后输入特征通过一个训练好的DNN，采用前向传播的方法被映射到输出特征$\{y^t_m\}$，其中$y^t_m$表示在第$t$帧的第$m$个输出特征。输出特征包含频谱和激励参数以及他们的时间导数（动态特征）。DNN的权重能够采用从训练数据集中抽取的成对的输入和输出特征来进行训练。类似于HMM的方法，这样是可以生成语音参数的。通过从DNN中设定预测的输出特征作为均值向量，再加上从所有训练数据预先计算的方差作为协方差矩阵，语音参数的生成算法能够生成平滑的语音参数特征轨迹，满足了静态和动态特征的统计情况。最终，一个语音合成模块通过得到的语音参数来生成语音波形。</p>
<p>注意到，文本分析，语音参数生成，和波形生成模块可以与HMM模型共享，<strong>即仅仅从上下文依赖关系的标签生成统计参数的过程需要被替换。</strong></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="4-1-实验条件"><a href="#4-1-实验条件" class="headerlink" title="4.1 实验条件"></a>4.1 实验条件</h3><p>实验数据：US-EN 女性语音数据，约33000条。语音分析的条件和方法论类似于Nitech-HTS2005系统的方法。语音数据首先从48KHz降采样到16KHz，然后每5ms抽取一次40维的Mel倒谱系数，$log F_0$，和5段非周期性系数。每一个观察向量包含40维的mel倒谱系数，$log F_0$，和5段非周期性系数，以及他们的delta和delta-delta特征。从左至右，包含5个状态的无跳过隐藏半马尔可夫模型 (HSMM)被采用。为了建模 $log F_0$序列包含了声学和非声学观察序列。一个多空间的密度分布被使用（multi-space probability distribution (MSD)）。基于决策树的上下文聚类的问题数量是2554个。在HMM系统中的决策树大小是通过改变模型复杂度惩罚因子$\alpha$来控制的（最小描述长度标准（MDL）是（$\alpha=16,8,4,2,1,0.5,0.375,or 0.25$）。当$\alpha=1$时，Mel频谱，$log F_0$和频带非周期性的叶子结点的数量分别是12342, 26209, 和401（总共有3209991个参数）。</p>
<p>基于DNN系统的输入特征包含表征类别语言上下文（例如音素身份、重音标记）的342个二分类特征和表征数字语言上下文（例如，单词中的音节数、短语中当前音节的位置）的25个数值特征。除去文本上下文相关的输入特征，还包含了3个用于粗略编码当前音素序列中当前帧位置的数值特征，以及一个用于估计当前音节时长的数值特征。输出特征与HMM系统基本一致。为了通过DNN模型建模$log F_0$序列，我们采用了显式发声建模（explicit voicing modeling）的方法来获取连续$F_0$ ，发声/不发声的二分类特征值被用于添加到输出特征，并且在不发声值中的 $log F_0$ 通过插值得到。为了降低计算成本，80%的静音段从训练数据中移除。DNN的权重被随机初始化，然后在最小化MSE的目标函数下得到最优化。优化策略为基于小批次的随机梯度下降（SGD）的后向传播算法。DNN的输入和输出特征均被正则化，其中输入特征被正则化至(0,1)分布，然后输出特征根据训练数据中的最大最小值被正则化至0.01-0.99隐藏层采用sigmoid激活函数。建模频谱和激励特征参数的DNN神经网络被训练。</p>
<p>在评估的语句中，语音参数通过语音参数生成算法被生成。在倒谱域采用了基于后过滤的频谱增强算法。语音波形通过source-filter模型来重构语音波形。</p>
<p><strong>为了客观评估HMM和DNN模型系统，MCD（mel-cepstral distortion）(dB)，Linear aperiodicity distortion (dB), 发声/不发声错误率（%），和$log F_0$的RMSE被使用。</strong>音素发音时长在后面被使用，我们挑选了173句训练集外的语句用于模型评估。</p>
<h3 id="4-2-客观评估"><a href="#4-2-客观评估" class="headerlink" title="4.2 客观评估"></a>4.2 客观评估</h3><p><img src="/images/5th-mcep-comparison.png" alt="5th-mcep-comparison"></p>
<p>上图绘制了Ground-Truth、HMM预测值和DNN预测值的第五个mel倒谱系数，从图中可以观察到，三个模型都可以产生合理的语音参数轨迹。</p>
<p>在客观评估中，我们调查了预测结果和DNN结构（1，2，3，4，5层）的关系，以及与每层神经元个数（256，512，1024，2048）的关系，下图展示了实验结果。</p>
<p><img src="/images/dnn-tts-exp-res.png" alt="dnn-tts-exp-res"></p>
<p>基于DNN的系统一直都比HMM系统要好在 “voiced/unvoiced error rate”和”aperiodicity prediction”。在MCD中，有多层的DNN模型要相似于或者好于HMM模型。然而，在$log F_0$的预测中，HMM在大多数情况下要好于DNN模型。其中，所有的不发音帧都被插值作为发音帧来建模。我们认为这种方法会降低$log F_0$的预测效果，因为这些插值的$F_0$对于DNN模型来说是一个bias。对于MCD和aperiodicity预测中，模型深度的提升比在每一层上增加神经元的个数更加有效。</p>
<p>以上的客观指标并不能评估合成语音的自然度，但是可以作为评估声学模型准确率的指标。</p>
<h3 id="4-3-主观评估"><a href="#4-3-主观评估" class="headerlink" title="4.3 主观评估"></a>4.3 主观评估</h3><p>173句话被评估，每个评估人最多评估30句话，这些话术是随机打乱的。每一对语音被5个人评估。评估人有带耳机。在听完一对语音后，评估人需要选择一个更喜欢的语音，如果觉得两个语音很相似的话，可以选择“中立”，在这个评估过程中，HMM系统和DNN系统采用相似的模型数量来被评估。DNN模型采用了4个隐藏层，神经元的个数也进行了多组实验（256，512，1024个神经元）</p>
<p><img src="/images/dnn-tts-mos.png" alt="dnn-tts-mos"></p>
<p>上表展示了实验结果，可以从以上结果看出，在相似的参数数量配置的情况下，DNN模型要远优于HMM模型。我们认为较好的MCD代表了更佳的效果。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章examined the use of the DNNs to perform speech synthesis. DNN模型有潜力解决传统DT-HMM模型的局限性。主观评估和客观评估均显示DNN能够实现较好的效果。HMM的一个优势在于模型参数较少，计算开销较小。在合成时，HMM模型便利决策树来找到每一个状态的参数。然而，本文提出的DNN算法是在每一帧进行输入到输出的预测，接下来的工作可以在如何降低DNN模型的计算开销，添加更多的输入特征包括一些弱特征如重读，并且可以探索如果获得一个更好的 $log F_0$建模方案。</p>
<h1 id="Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper"><a href="#Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper" class="headerlink" title="Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)"></a>Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)</h1><h2 id="https-arxiv-org-pdf-cs-9811031-pdf"><a href="#https-arxiv-org-pdf-cs-9811031-pdf" class="headerlink" title="https://arxiv.org/pdf/cs/9811031.pdf"></a><a href="https://arxiv.org/pdf/cs/9811031.pdf">https://arxiv.org/pdf/cs/9811031.pdf</a></h2><h2 id="感想与总结"><a href="#感想与总结" class="headerlink" title="感想与总结"></a>感想与总结</h2><p>这篇1996年的文章算是nn-tts的创世之作，文章采用了一个duration mode来预测音素的发音时长，以及一个phonetic network来预测每一个音素的声学特征，这个idea让我不由得想到当前的如Fastspeech等与这个想法如出一辙，同样的也是需要预测duration和音素的声学特征，但本文的行文思路尤其是实验部分，感觉没有上一篇论文更加翔实充分，所以打分的话我会给：🌟🌟🌟🌟</p>
<h2 id="Abstact"><a href="#Abstact" class="headerlink" title="Abstact"></a>Abstact</h2><p>传统的文本-语音转换通过拼接短的语音单元或者采用基于规则的系统来将语音的音素表示转换为声学表示形式，然后被转换为语音。本文描述了一种采用时延神经网络（time-delay neural network TDNN）的方案来进行音素-声学特征的建模，不需要额外的神经网络来控制生成语音的timing。这个神经网络系统相较于拼接算法可以降低对于系统存储资源的需求，对比于其他的商业系统表现良好。</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="1-1-Description-of-Problem"><a href="#1-1-Description-of-Problem" class="headerlink" title="1.1 Description of Problem"></a>1.1 Description of Problem</h3><p>文语转换通常包含了先将文本转换为语音参数，再将语音参数转换为语音波形。计算机交互可以采用对话沟通交互方式，也可以配置到移动端。拼接系统首先制作拼接数据库，然后再拼接时，调整音素的发音时长，平滑转接点来生成语音参数。拼接系统的主要问题是存储成本高昂。基于规则的合成方法将每一种可能的音素表示存储好目标声学参数。然后根据衔接点的情况来根据规则选择语音参数，主要问题是：拼接点不自然，因为转接规则倾向于只生成少量的转接风格。另外，大量的转接规则需要进行存储，会造成合成的机械音。</p>
<h3 id="1-2-Discussion-of-the-use-of-networks"><a href="#1-2-Discussion-of-the-use-of-networks" class="headerlink" title="1.2 Discussion of the use of networks"></a>1.2 Discussion of the use of networks</h3><ul>
<li><p>先前提到的两种方法都是语言-依赖的，而NN方法是语言-不相关的；</p>
</li>
<li><p>拼接系统的高昂存储成本导致了难以配置到移动端，而NN通过生成具象的表示方式，能够降低拼接系统的冗余性；</p>
</li>
</ul>
<p>下图展示了TTS的流程图（终于找到了现有的TTS流程设计的出处）</p>
<p><img src="/images/tts-diagram.png" alt="tts-diagram"></p>
<h2 id="系统描述"><a href="#系统描述" class="headerlink" title="系统描述"></a>系统描述</h2><h3 id="2-1-数据库"><a href="#2-1-数据库" class="headerlink" title="2.1 数据库"></a>2.1 数据库</h3><p>38岁男性，居住在Florida和Chicago。录制时采用了类似于近距离麦克风的DAT录制起。文本包含480个音素均衡的语句，是从Harvard sentence list中筛选出来的。除此之外，160句在其他情境下的话术也被录制了下来。录音音频通过数字方式转移到计算机，每句话对应一个音频。每句话被归一化，以使得每句话都在非静音段有相同的平均信号能量。文本信息被标记为音素、节奏和音调信息。</p>
<p>标记方式采用了类似于TIMIT数据库的标记方式（是ARPABET的变种），停止标记为关闭和释放作为单独的音素。这使得模型能够有效预测到停顿，以及开始。精准的对齐对于帧级的损失函数是有效的。</p>
<p>音素不是唯一的输入特征，音素时长，F0曲线（通常被音节重读和语法边界影响）。语法边界（syntactic labelling）标记了音节，词，短语，从句和句子的开始和结束时间。语法重读（lexical stress：primary, secondary, or none）被应用到词汇的每一个字母中。词性(function word (article, pronoun, conjunction or preposition) or content word)；每个词语都有一个层级(level)，基于生成F0的rule-based系统。</p>
<p>尽管语法（syntactic）和重读（lexical stress）对于语音的韵律变化很重要，但是这些信息没有完全决定了这些韵律变化。<strong>说话者对于语句的重读可能取决于句子中的对比度，比如在遇到陌生词汇的时候，可能会不自觉的重读。</strong>因此标记如此音调重读的实际位置到字幕上，或者词语间的强对比性是有效的。在英文中的标准是ToBI（Tone and Break Index）系统。</p>
<h3 id="2-2-从音素表示上生成片段时长"><a href="#2-2-从音素表示上生成片段时长" class="headerlink" title="2.2 从音素表示上生成片段时长"></a>2.2 从音素表示上生成片段时长</h3><p>神经网络的两个任务之一是去决策，从音素顺序和语法和韵律信息上，每一个音素的发音时长。</p>
<p>网络的输入大多数采用二分类数值，分类数值代表了采用1-out-of-n codes和一些通过bar codes表示的小的整数值。表示音素片段的输入数据包含音素片段，它的发音特点，字幕凸起的描述和包含片段的词语，以及片段接近的任何语法边界。网络结构被训练来生成时长的log。</p>
<p>时长预测网络结构如图2所示，网络有两个输入值（2和3），通过I/O block 1 和 2 输入。（Stream 2 包含了通过shift register来给一个音素提供上下文描述），stream 3 包含了仅仅用于一个特定音素时长的生成过程。当神经网络被用于生成时长的过程中，I/O block 6写入输出的数据流。在训练过程中，Block 6 读取目标值并且生成error value。Block 3、4和5是单层的神经网络模块，模块7、8和recurrent buffer控制了循环生成的机制。</p>
<p><img src="/images/duration-prediction.png" alt="duration-prediction"></p>
<h3 id="2-3-从音素和时长信息生成声学信息"><a href="#2-3-从音素和时长信息生成声学信息" class="headerlink" title="2.3 从音素和时长信息生成声学信息"></a>2.3 从音素和时长信息生成声学信息</h3><p>系统中使用的第二个神经网络从音素、语法和时长信息来生成语音参数信息。更精准地来说，网络从一个帧级的音素上下文信息生成语音10-ms帧的声学表示。</p>
<h4 id="2-3-1-网络输出-—-Coder"><a href="#2-3-1-网络输出-—-Coder" class="headerlink" title="2.3.1 网络输出 — Coder"></a>2.3.1 网络输出 — Coder</h4><p>神经网络不会直接生成语音，这个的计算资源十分昂贵，并且不太可能生成好的结果。该网络为声码器的分析-合成风格的合成部分生成数据帧。许多语音编码的研究致力于数据压缩的问题；然而，神经网络对于coder的需求没有被大多数的数据压缩技术所满足。具体来讲，将语音编码成每帧的数值向量是有价值的，这样的话，向量的每个元素对于每一帧都会有一个定义好的数值，因此用于训练的神经网络的错误度量是合适的。（例如，如果神经网络生成向量，并且错误度量相对于训练向量是较小的话，生成语音的质量，即通过running these vectors通过coder的合成部分的话，将得到较好的语音质量。）加权Euclidean距离被用作error criterion使得coder没有使用二分类输出值是明智的，并且根据其他的向量元素，向量元素的含义没有改变。</p>
<p>coder是LPC声码器的形式，采用线性频谱（line spectral frequencies）来表示filter coefficients和一个2-band的激励模型（不同的filter coefficients的表示形式被测试，模型对于线性频谱表现良好）。2-band激励模型是一个multi-band激励模型的变种，包含一个低频带的voiced band，和一个高频带的unvoiced band。两个bands之间的边界是coder之一的参数。F0 和 power of the voice signam是剩下的参数。F0在不发音的帧级，被插值为一个高频的数值。</p>
<h4 id="2-3-2-网络输入"><a href="#2-3-2-网络输入" class="headerlink" title="2.3.2 网络输入"></a>2.3.2 网络输入</h4><p>音素网络的输入包含了时长网络的所有输入，和时长网络输出的timing information。网络采用了一定数量的不同的输入coding技术。blocks 5，6，20和21采用了300毫秒的TDNN的风格输入窗口。窗口的采样不是均匀的，最优的采样区间是通过分析神经网络从TDNN窗口的不同部分来决策神经网络对信息的使用。Blocks 6 和20 处理了一组与输入音素相关的特征，Blocks 7和8为音素和语法边界编码时长和距离信息。网络的输入数据是二分类数据的混合，1-out-of-codes和bar codes</p>
<h4 id="2-3-3-网络结构"><a href="#2-3-3-网络结构" class="headerlink" title="2.3.3 网络结构"></a>2.3.3 网络结构</h4><p>决定好的网络结构需要大量的实验，也就需要大量的计算资源，然而本课题的复杂程度和数据集的大小使得训练时间成为了主要瓶颈。因此，一个 in house neural network simulator被开发来降低训练时间（多个月-&gt;几天），并且可以同时验证多个方法。一些神经网络的技术和理论通过这种方式被pass掉了。</p>
<p>最终的网络结构整合了TDNN、recurrent、和modular网络，和一些实验过程中演化的技巧。下图是当前方法的图示，其中六边形模块是I/O或者用户写入的子程序，方块是神经网络的模块。神经网络的模块采用后向传播来训练。网络的模块化是通过专家知识来手动调整的。</p>
<p>网络通过逐渐降低的学习率和momentum方法来训练，以一种新型的顺序和随机混合的训练模式来训练。训练的网络需要&lt;100 Kilobyters of 8-bit quantized weights ，对比于拼接算法，得到了显著降低。</p>
<p><img src="/images/phonetic-network.png" alt="phonetic-network"></p>
<h2 id="系统效果"><a href="#系统效果" class="headerlink" title="系统效果"></a>系统效果</h2><h3 id="3-1-语音质量和自然度"><a href="#3-1-语音质量和自然度" class="headerlink" title="3.1 语音质量和自然度"></a>3.1 语音质量和自然度</h3><p><img src="/images/tts-nn-exp-results.png" alt="tts-nn-exp-results"></p>
<p>上图展示了GT语音频谱和系统生成的语音频谱（生成的频谱没有采用ToBI标注系统）。为了对比更加清晰，有两种合成语音频谱被展示出来。第一种，phonetic 特征是网络预测的，而duration是真实的，为了仅仅展示phonetic network的效果。第二种，duration和phonetic都是预测的。对比实验发现，在语音接受度（Acceptability）上，本方法生成的质量远好于其他的系统。在片段的拟人度方面（Segmental Intelligibility），本方法仍旧有提升空间，而本次试验中的较差的数据可能是由于缺少单字语音样本所导致的。</p>
<p><img src="/images/tts-nn-table1.png" alt="tts-nn-table1"></p>
<h3 id="实时合成"><a href="#实时合成" class="headerlink" title="实时合成"></a>实时合成</h3><p>最开始模型是在Sun SPARCstation平台来通过ANSI C语言实现的。最近这个被插入到Power Macintosh 8500/120，PowerPC快速的乘法和加法使得合成器能够实时合成。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>本方法从Acceptability角度来看，是优于传统算法的，但是仍旧可以有一些提升，如数据库可以扩充，来获得更多的音调变化，包含更多的音素上下文特征，数据库可以包含更多的短语，单字，和长段的语句。在coder、network architecture、和训练方法上也可以做出一些提升。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages</title>
    <url>/2021/06/27/vqvae/</url>
    <content><![CDATA[<h1 id="低资源语种序列-序列语音合成无监督学习方法"><a href="#低资源语种序列-序列语音合成无监督学习方法" class="headerlink" title="低资源语种序列-序列语音合成无监督学习方法"></a>低资源语种序列-序列语音合成无监督学习方法</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>待解决问题：近些年，包含attention机制的序列-到-序列在TTS领域获得了广泛的成功。这些模型能够以一个大的标注的语料库来生成近似人的语音。</p>
<p>解决方案：然而，准备这样一个大的数据集是昂贵且耗费人力的，为了解决数据依赖的问题，我们提出了一种创新性的无监督预训练机制。具体来讲，首先，我们采用VQVAE模型来从大规模公开发表的，未标注的数据中抽取无监督语言单元。然后，我们采用&lt;无监督语言单元，语音&gt;对来预训练序列-序列TTS模型。最终，我们采用目标说话人的小数据量的<text, audio>数据，来fine-tune模型。</p>
<p>实验结果：主观和客观实验结果均显示，我们提出的方案可以采用相同数量的成对训练数据，合成更加智能化和自然的语音。除此之外，我们将我们提出的方案延伸到假设的低资源语言中，采用客观评估方法，验证模型的有效性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>序列到序列的TTS模型由一个编码器-解码器-attention的框架组成，能够生成自然语音。然而，训练这些S2S TTS模型需要成百上千的标注语音来生成近似人声的语音。尽管少量的数据需要被用来生成类人的语音，它限制了整体的自然度，并且模型容易造成不希望的错误。</p>
<p>尽管收集一个这样一个大型的标注语音语料库是昂贵的，耗费成本的，研究者开始调研TTS中数据的有效性。一些学者集中于采用少量数据集迁移TTS模型到新说话人。一些调研采用Speaker embeddings来在TTS中建模speaker identities。一些也探索使用一个speaker embeddings的组合并且fine-tune。一些人甚至致力于zero-shot 说话人自适应研究。</p>
<p>其他的一些学者探索通过通用数据来建模TTS模型。一些人尝试采用传统TTS的技术，将分布式的文本或语言信息引入TTS。一些人采用ASR数据或者通过数据筛选或分析找到的已有数据数据来训练TTS模型。最近，有学者提出了一个简单但是有效的半监督学习方法来仅仅采用语音预训练端到端TTS中的decoder。</p>
<p>目前有一些工作在研究低资源语种TTS的数据有效性，并且显示，训练一个多语种的统计参数语音合成方式，能够将adaption迁移到有小数据量的新语言。最近的工作调查显示从高资源的语种语言，迁移到低资源的语言也是有效的。</p>
<p>本文目的在于通过利用大量的，公开的，未标注的语音数据，来减轻S2S TTS训练中的数据需求量。我们提出了一个训练Tacotron2的无监督学习框架。具体来讲，我们首先通过VQ-VAE模型来从未标注语音中抽取无监督语言单元。然后通过使用&lt;无监督语言单元，语音&gt;对来预训练Tacotron。最终，我们采用目标说话人少量的<text, audio>文本语音对来fine-tune 模型。</p>
<p>我们的工作与[20]Y.-A Chung (ICASSP 2019) “Semi-supervised training for improving data efficiency in end-to-end speech synthesis”相关。然而，区别如下：</p>
<ul>
<li>我们的方法采用无监督学习方法抽取类似于音素的语言单元，使得有可能预训练整个TTS模型，然而[20]分开预训练模型的几个部分；</li>
<li>我们也在假设的低资源语言上，证实了方法的可行性；</li>
<li>最后，我们在实验中主要采用公开数据集，因此能够很容易被复现。</li>
</ul>
<h2 id="提出的方案"><a href="#提出的方案" class="headerlink" title="提出的方案"></a>提出的方案</h2><p>我们采用一个baseline Tacotron的模型架构，其中采用location-sensitive attention 和 从文本中生成的音素序列。为了将预测的频谱转换为语音，我们采用Griffin-Lim算法for fast experiment cycles，因为我们是关注于数据有效性的问题，而不是生成高保真的语音。在baseline模型中，模型是从0开始训练的，意外着模型的所有parameters全都是被paired data来训练的。</p>
<p><img src="/images/vqvae-algorithm.png" alt="vqvae-algorithm"></p>
<h3 id="2-1-半监督预训练"><a href="#2-1-半监督预训练" class="headerlink" title="2.1 半监督预训练"></a>2.1 半监督预训练</h3><p>在baseline Tacotron model中，模型应该同时学习到文本声学表示和他们之间的对齐。[20] 提出了两种模型预训练的方法来利用外部的文本和声学信息。对于文本表示来说，他们通过外部的word-vectors预训练了Tacotron的encoder，对于声学表示，他们通过未标注的语音，预训练了decoder。</p>
<p>[20]然后采用paired data来fine模型，在这一步，模型集中于学习textual representations和acoustic ones之间的对齐。</p>
<h3 id="2-2-无监督学习—-预训练"><a href="#2-2-无监督学习—-预训练" class="headerlink" title="2.2 无监督学习—-预训练"></a>2.2 无监督学习—-预训练</h3><p>尽管[13] 展示出提出的半监督预训练的方法能够合成更加智能的语音，但是它也发现同时分开训练编码器和解码器不会相较于仅仅预训练解码器带来更多提升。然而，仅预训练解码器和fine-tune整个模型有一个不匹配。为了避免这种不匹配带来的潜在损失，并且进一步通过仅仅使用语音来提高数据有效性，我们提出从未标注语音中抽取无监督语言单元来预训练整个模型。</p>
<p>我们提出的方法提供在了算法1中。整体的框架包含两个模型：一个无监督模型，用来抽取类似于音素的语言特征，和Tacotron模型。</p>
<h4 id="2-2-1-无监督语言单元"><a href="#2-2-1-无监督语言单元" class="headerlink" title="2.2.1 无监督语言单元"></a>2.2.1 无监督语言单元</h4><p>无监督语言表示在表示学习和特征解耦两个方面都带来了很大的提升。在它们之间，离散表示在语言和语音社区是较为流行的，因为直观上来看，语言和语音都是由有限的离散单元来组成的，例如文本中的字母和语音中的音素。本文利用VQ-VAE模型作为离散语言单元的抽取器。</p>
<p>在这种情况下，VQ-VAE模型作为一个类似于ASR的识别模型。然而，VQVAE模型和ASR模型的主要区别是VQVAE模型以一种无监督的方式来训练，然而ASR是采用一种有监督的方式。这种区别只要考虑到低资源语种的问题就会有所影响。当我们没有一个用于低资源语种的ASR模型时，这种提出的无监督方法对于提取低资源语种的语言表示单元是有意义的。</p>
<p>VQ-VAE模型有采用了一个encoder-decoder的框架，和一个码书（codebook dictionary）$e = C \times D $，其中$C$是字典中隐状态嵌入的数量，$D$是每一个嵌入的维度。编码器$E$输入原始语音波形 $x_{1:T}=x_1, x_2, …, x_T$作为输入，并且生成编码状态$z_{1:N}=E(x_{1:T})$，其中$N$依赖于文本时间长度$T$和编码器中下采样层的数量。然后，连续的隐状态表示$z_{1:N}$能够被映射到$\hat z_{1:N}$通过在字典中找到最近的预定义的离散嵌入$\hat z = e_k$，其中$k=argmin_j||z-e_j||$，$e_j$是在码书中的第$j$个嵌入，并且$j\in 1,2,…,C$。最终，隐嵌入$\hat z_{1:N}$和说话人嵌入$s$被一同输入到解码器$D$来重构语音波形$\hat x = D(\hat z, s)$。</p>
<p>因为模型输入和输出是相同的，模型能够以一种auto-encoder的方式来训练。然而，梯度不能够通过$argmin$计算来获取，因此直接采用梯度估计来近似。因此模型的整体loss为：</p>
<script type="math/tex; mode=display">
L = -log(x|\hat z(x), s) + ||sg(z(x))-e_j||^2_2 + \beta \ast ||z(x) - sg(e_j)||^2_2</script><p>其中第一项是negative log-likelihood用来更新整体模型的；第二项更新码书字典，其中<br>$sg$指代stop-gradient计算；第三项，指代承诺损失，鼓励编码器输出$z$来接近于码书嵌入，超参数$\beta$是用于给第三项增加一个权重。</p>
<p><img src="/images/vqvae-t2.png" alt="vqvae-t2"></p>
<h4 id="2-2-2-Tacotron预训练和fine-tune"><a href="#2-2-2-Tacotron预训练和fine-tune" class="headerlink" title="2.2.2 Tacotron预训练和fine-tune"></a>2.2.2 Tacotron预训练和fine-tune</h4><p>在VQVAE被训练后，我们抽取每句话的无监督语言单元。然后给无监督语言单元随机处理化一个嵌入表，通过查表得到的语言嵌入序列被用作Tacotron的输入。因此，我们能够通过<linguistic embedding, audio> 对来预训练Tacotron。</p>
<p>在模型被预训练后，我们采用一些paired语言数据来fine-tune模型。在这一步中，模型的输入是从文本中得到的音素序列。</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>数据集：LJSpeech</p>
<p>VQ-VAE：与“Unsupervised speech representation learning using WaveNet autoencoders”相似的网络结构</p>
<p>当训练VQ-VAE的时候，我们采用39维MFCC作为模型输入。在我们调研学习后，码书的大小是256，并且每一个码书embedding的嵌入是64维。jitter rate 和$\beta$是0.12和0.25。</p>
<p>[20]发现<strong>24-分钟语音是刚好不能构建一个Tacotron系统的语音的最小最大值</strong>。因此，我们集中于对比不同的仅仅采用24分钟paired数据训练的模型。</p>
<h3 id="3-2-在24-min数据上的结果"><a href="#3-2-在24-min数据上的结果" class="headerlink" title="3.2 在24-min数据上的结果"></a>3.2 在24-min数据上的结果</h3><p>模型如下：</p>
<ul>
<li>Tac：仅仅采用LJSpeech训练的Tacotron</li>
<li>T-Dec: 以半监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-VQ: 以本文提出的学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-phone: 以监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune，意指模型的上限。</li>
</ul>
<p>外部数据集：VCTK</p>
<p>其中在预训练T-Dec和T-VQ的时候，仅仅采用语音数据，对于T-Phone，采用VCTK的文本语音对数据。</p>
<p>本文采用了客观和主观两种方式，来评估实验结果。对于客观方式，我们采用了Dynamic-time-warping Mel-cepstral Distortion(DTW MCD)，度量了合成语音和真实语音之间的距离，越小越好。我们采用了<strong>大约20分钟的unseen数据作为评估数据</strong>。对于主观评估方式，我们采用20条unseen utterances执行了一系列的AB tests。20 raters(10男10女)是地道的普通话中文者，英文熟练。</p>
<h4 id="3-2-1-MCD客观评估"><a href="#3-2-1-MCD客观评估" class="headerlink" title="3.2.1 MCD客观评估"></a>3.2.1 MCD客观评估</h4><p><img src="/images/vqvae-results.png" alt="vqvae-results"></p>
<p> MCD结果如上表1所示，如[20]中所描述的，仅仅预训练decoder能够降低MCD。然而，提出的方法实现了最好的效果，MCD相较于baselineTacotron低了14.3%。我们也发现了T-VQ的结果十分接近Upper bound（T-Phone）</p>
<h4 id="3-2-2-AB偏好主观测试"><a href="#3-2-2-AB偏好主观测试" class="headerlink" title="3.2.2 AB偏好主观测试"></a>3.2.2 AB偏好主观测试</h4><p>AB tests的结果如表2所示，可以清晰看出预训练的技巧能够帮助提升模型性能。在Tacotron和预训练模型（T-Dec / T-VQ）中有一个较大的表现差距。我们发现采用LJSpeech数据从0开始训练模型能够很难得到智能数据，部分原因是LJSpeech的数据集质量也不是足够高。</p>
<p>在T-Dec和T-VQ的AB Test中，T-VQ获取了更好的表现，从不正式的听测中，我们注意到T-Dec合成的语音在智能性上更加中庸，T-VQ的智能性会更好。这显示通过无监督语言单元和语音来进行预训练能够进一步提升模型性能。原因是在提出的预训练的配置中，模型不仅仅能够学习到声学表示，也能够学习到对齐信息。尽管无监督的语言单元没有在fine-tune中使用，提出的预训练的方法对于textual representation learning也是有效的，因为这些无监督的语言单元被证明很像音素。</p>
<p>在Tac-VQ和T-phone的对比中，大多数的raters没有选择，但其中还是有20%的人在二者之中选择T-Phone。</p>
<h3 id="3-3-在其他数量数据上的实验结果"><a href="#3-3-在其他数量数据上的实验结果" class="headerlink" title="3.3 在其他数量数据上的实验结果"></a>3.3 在其他数量数据上的实验结果</h3><p>实验结果表示：</p>
<ul>
<li>在使用24min数据时，Tacotron与其他三个模型有很大差异</li>
<li>随着数据量增大，差异缩小，证明了pre-training的作用在降低</li>
<li>T-VQ和T-Phone始终比Tac和T-Dec的方法效果要好</li>
</ul>
<h3 id="3-4-低资源语种的实验结果"><a href="#3-4-低资源语种的实验结果" class="headerlink" title="3.4 低资源语种的实验结果"></a>3.4 低资源语种的实验结果</h3><p>本节验证提出的方法在2种低资源语种的实验效果。假设English和Mandarin Chinese是两种低资源语种。主要为了解决以下两个问题：</p>
<ul>
<li>此种方法能否在这种情境下提升数据有效性？</li>
<li>那些预训练的语种对于提出的方法更有效？与目标语种音素相近的还是不相关的？</li>
</ul>
<p>目标语种，英语的语料是LJSpeech，中文是内部数据集Xiaomin，新闻风格，女性。</p>
<p>训练VQ-VAE和预训练Tacotron的语言包含以下几种：韩语，日语，西班牙语，法语，德语。我们仅仅利用语音数据来训练VQ-VAE和预训练Tacotron。在训练VQ-VAE的时候，仅做了一个改动：码书的大小从256改变到512，因为这里采用了多语种的数据集。三种模型衍生如下：</p>
<ul>
<li>Tac：通过LJSpeech或者Xiaomin训练的Tacotron；</li>
<li>T-VQ-A：以本文提出的学习方式，采用亚洲数据集（韩语、日语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
<li>T-VQ-E：以本文提出的学习方式，采用欧洲数据集（西班牙语，韩语，德语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
</ul>
<p><img src="/images/low-resource-languages.png" alt="low-resource-languages"></p>
<p>To alleviate the burden of raters，我们仅仅评估MCD值。如表3和表4所示。我们提出的预训练的方式，能够有效提升合成语音的质量，对于低资源语种来说很有价值，因为语音的收集成本十分高。</p>
<p>除此之外，在English TTS中T-VQ-E结果好于T-VQ-A，在普通话实验中，T-VQ-A slightly out-performs T-VQ-E。这个结果展示出了，采用音素相近的语言来进行模型预训练，是更加有效的。此外，我们发现了随着fine-tune数据的增多，MCD的下降，这个结果与上一节结果是相似的。最后，通过对比English TTS中最好的模型T-VQ-E模型，和上一节中的T-VQ模型，我们发现这里尽管使用音素相似的语言，仍然会存在一个不可忽视的gap。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一种在序列-序列TTS中，提升数据有效性的无监督学习方案。本方案首先从大规模未转译外部数据中，抽取文本和声学特征表示。然后在采用S2S模型来训练。具体来讲，采用了此种预训练的方式，Tacotron能够采用较少的数据，生成较好的语音。尽管我们是采用Tacotron来进行试验的，我们坚信我们的方法在其他的sequence-to-sequence模型中，也应该有效。我们也证实了此种方法在低资源语种上的有效性，这样的话，不需要目标语种的语音，我们的方法能够提供一个显著的效果提升。尽管，我们假设了两种低资源语种，但我们相信此种方法能够泛化到真实的低资源语种。这个结果给单语种和多语种TTS系统增加了曙光。</p>
<p>未来前进方向：可以尝试其他的无监督学习方式；采用小数据量来adaptation neural vocoders也同样需要被调研。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised speech representation learning using WaveNet autoencoders</title>
    <url>/2021/06/28/unsupervised-representation-learning/</url>
    <content><![CDATA[<h1 id="Unsupervised-speech-representation-learning-using-WaveNet-autoencoders-https-export-arxiv-org-pdf-1901-08810"><a href="#Unsupervised-speech-representation-learning-using-WaveNet-autoencoders-https-export-arxiv-org-pdf-1901-08810" class="headerlink" title="Unsupervised speech representation learning using WaveNet autoencoders https://export.arxiv.org/pdf/1901.08810"></a>Unsupervised speech representation learning using WaveNet autoencoders <a href="https://export.arxiv.org/pdf/1901.08810">https://export.arxiv.org/pdf/1901.08810</a></h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>第一次完整的精读完一篇期刊文章，一开始还是自己逐字句翻译的，但后面发现这样太耗费时间了（主要是突然解锁了typora的copy-paste功能），所以这篇文章从模型部分开始主要是Google翻译的。通篇来说论文Idea很容易理解，模型结构主要展示在了图1的模型结构里面。但是后面采用了6页的科学实验来验证这个想法，并且分析这个想法中的每一步细节。总体来讲，与MelGAN这种A类的长篇论文来对比的话，我更青睐于MelGAN的这种论文，是由于读到本文的一些实验细节，并且我尝试从图中去分析实验现象的时候，个人感觉实验现象都相对来说有一些牵强，尽管实验内容很详细，但是没有很好的控制变量，以得到有效的实验结果，并且很多实验结果有点晦涩难懂。（并且这篇文章采用了4个TPU训练了一周，没有资源还是不要轻易去re-train了，或许这个方向也像BERT一样，需要投入大量的资金和精力才能够实现宇宙第一的效果，所以还是建议轻易不要挑战）。所以本次打分：🌟🌟🌟</p>
<h2 id="Abastract"><a href="#Abastract" class="headerlink" title="Abastract"></a>Abastract</h2><p>现状：无监督的语音表示学习可以通过对语音进行自编码来实现。目标是从语音中抽取出高层级语义特征，例如，音素id，与语音信号低层级的混淆信息如音高轮廓或者背景噪声无关。</p>
<p>解决方案：因为learned representation 被tuned来仅仅包含音素内容，我们借助于一个高容量的WaveNet decoder来从之前的样本点中推理encoder丢失的信息。除此之外，自编码器模型的行为依赖于被应用到隐状态表示的约束条件。我们将三种variants进行对比：一个简单的dimensionality reduction瓶颈特征，一个高斯VAE，和一个离散的VQ-VAE。</p>
<p>实验分析：我们通过speaker independence，预测音素内容的能力，准确重构独立频谱帧的能力三个方面来评估learned representation的质量。除此之外，对于采用VQ-VAE模型提取的离散编码，我们度量从它们映射到phonemes的难度。我们引入了一个“正则化”的机制，来强迫representations集中于语句的音素内容的建模，并且报告出效果是与Zerospeech 2017 无监督声学单元发现任务的前几名能够匹敌的。</p>
<p>关键字：autoencoder，speech representation learning，unsupervised learning, acoustic unit discovery</p>
<h2 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h2><p>Deep learning被高层级表示学习算法所触发，如stacked Restricted Boltzman Machines和Denoising Autoencoders. 然而，近期在计算机视觉，机器翻译，语音识别和语言理解的突破，依赖于大规模的标记数据集，而几乎没有利用无监督表示学习。这样有两点缺陷：1）大规模人工标注的数据集的需求经常使得deep learning models的发展十分昂贵。2）尽管一个深度模型能够擅长于解决一个given task，但是这种方法对于问题域产生有限的启发，主要的启发是从显著输入特征的可视化中组成的，一种仅仅能够应用于问题域的策略，很容易被人类所解释。</p>
<p>本文集中于评估和提高无监督语音表示学习。具体来讲，我们集中于从音素内容中学习能够区分说话人特征（特别是说话人性别和id）的表示，这种特性类似于从语音识别器学习到的内部表示特征。这样的特征在多个任务中都有价值，例如低资源语音识别（当仅仅小数据量的标签训练数据被提供）。在这种情景下，有限的数据可能足够基于无监督发现的表征训练一个声学模型，但是对于训练一个声学模型或者以一种有监督的方式来学习数据表征是不充足的。</p>
<p>我们集中于将自编码器学习到的表征应用到原始语音和频谱特征，并且采用LibriSpeech来调研learned representations的质量。我们tune learned 隐状态表征来仅仅编码phonetic content并且移除其他的混淆特征。然而，为了做信号重构，我们采用了一个自回归的WaveNet模型作为解码器， 来推理被编码器拒绝掉的信息。这个解码器的作用是一个inductive bias，使得编码器无需使用它的capacity来表示低层级细节，而是允许编码器集中于高层级的语义特征。我们发现当使用ASR特征，如MFCC作为输入，原始语音作为decoder targets的时候，能够得到最好的representations. 这强迫系统学习生成在特征抽取的过程中，被移除的sample level的细节。此外，我们注意到VQ-VAE模型可以生成在声学内容和说话人信息之间最好的分割。我们调研VQ-VAE的可视化直观性，通过将它们映射到phonemes，展示了模型超参数在可视化上的影响，并且提出了一个新的正则化机制，能够提高latent representation -&gt; phonetic content 的质量。最终，我们展示了模型在ZeroSpeech 2017声学单元发现任务上的有效性，度量了当一句话的音素发生了最小改变的时候，learned representation的区分度。</p>
<h2 id="Representation-learning-with-neural-networks-1-5-page"><a href="#Representation-learning-with-neural-networks-1-5-page" class="headerlink" title="Representation learning with neural networks (1.5 page)"></a>Representation learning with neural networks (1.5 page)</h2><p>神经网络是高层级信息处理的模型，常用多层计算单元来进行实现。每一层可以被理解为是一个特征抽取器，它的输出被传递到上游单元。特别是在图像领域，神经网络学习到的特征可以被展示出一个可视化原子的层级结构，能够与可视化的脑皮层组织在一定程度上匹配。相似地，当应用到语音领域时，神经网络倾向于在音乐，语音的低层级上学习到听觉的频谱特征。</p>
<p>A. 有监督特征学习</p>
<p>神经网络能够采用有监督或者无监督的方式学习到数据表征。在有监督的情况下，从大规模数据集中学习的特征是能直接被使用的，除了data-poor tasks。例如，在视觉领域，ImageNet中发现的特征也会被用作其他计算机视觉任务的输入表示。相似的，语音社区采用从在音素预测任务的网络中抽取的瓶颈特征也可以作为ASR的特征表示。相似的，在NLP，可以从机器翻译或者语言推理任务训练的网络抽取通用文本特征。</p>
<p>B. 无监督特征学习</p>
<p>在这篇文章中，我们关注于无监督特征学习。因为没有训练标签，所以我们采用自编码器，i.e. 网络任务是重构她们的输入。自编码器采用一个encoding network来抽取隐状态表示，然后这个隐状态表示被输入到一个解码器网络来恢复原始数据。理想情况下，隐状态表示保留了原始数据的显著特征，是容易分析和解决的，e.g. 通过解耦数据中不同因子的变化，并且摒弃虚假特征（噪音）。这些渴望的性质典型是通过一个包含正则化技巧和contraints or bottlenecks的明智的应用。因此，自动编码器学习到的表示会受到两种相互竞争的力量的影响。一方面，它会提供给解码器以必要信息为了完美的重构，因此会在隐状态提取尽可能多的输入数据特征模式。另一方面，contraints限制了一些信息需要被摒弃，避免了latent representation颠倒到微不足道。因此，瓶颈特征对于强迫网络学习到一个非-微不足道的数据转换是必要的。</p>
<p>降低隐表示的维度是应用到隐向量的最基本的约束，autoencoder扮演作为一个非线性的variant of 线性低维数据映射，如PCA和SVD。然而，这样的表示是很难直观解释的，因为输入的重构完全依赖于所有的隐状态特征。对比来讲，字典学习的技巧(dictionary learning techniques)，例如sparse和non-negative decompositions，从一个大池子里使用了小数据量的挑选特征来表示每一种输入特征，能够帮助他们的可解释性。基于VQ的离散特征学习能够被视为sparseness的一种极端形式，其中，重构仅需使用字典中唯一的一个元素。</p>
<p>VAE提出了一种特征学习的不同的表示方式，follows 概率框架。自编码网络结构是从一个latent variable generative model衍生的。首先，一个latent vector $z$ 被从一个先验概率$p(z)$ 采样（典型情况下，是一个多维的正态分布）。然后，数据采样点$x$是采用一个深度解码器神经网络来生成的，神经网络的参数是$\theta$, $x \sim p(x|z;\theta)$。然而，在最大似然估计训练中，计算精准的后验概率分布$p(z|x)$是很困难的。取而代之的是，VAE给后验概率引入了一个变分估计，$q(z|x;\phi)$，这个变分估计是通过一个参数为$\phi$的编码器神经网络建模的。因此VAE模拟了一个传统的自编码器的过程，其中，其中，编码器生成隐状态的分布，而不是deterministic encodings，然而解码器是从这个分布生成的样本上进行训练。编码和解码网络是联合训练的，以最大化一个lower bound on the log-likelihood of data point x:</p>
<script type="math/tex; mode=display">
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - \beta D_{KL}(q(z|x;\phi)||p(z))</script><p>以上公式中的两个terms分别是autoencoder的重构损失，以及一个对隐状态应用了penalty term。特别地，KL散度表示了网络中的信息数量，即隐表示携带的有关数据样本的信息数量。因此它作为一个隐表示的一个信息瓶颈，其中$\beta$控制了重构质量和表示相似度之间的trade-off。</p>
<p>VAE目标函数的另一种表示方式明确的限制了隐表示的信息：</p>
<script type="math/tex; mode=display">
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - max(B, D_{KL}(q(z|x;\phi)||p(z)))</script><p>其中常数$B$是指$q$中自由信息的数量，因为模型仅仅会被惩罚，如果它比隐状态的先验损失传递了超过B的信息。</p>
<p>近期提出的一种VQ-VAE的方式，用确定的量化特征，取代了连续和随机的隐向量。VQ-VAE维护了一些原型向量$e_i, i=1,…,K$。在前向计算的过程中，编码器生成的representations被原型向量中最近的一个替代。Formally, 让$z_e(x)$是编码器在量化之前的输出。VQ-VAE通过 $q(x)=argmin_i||z_e(x)-e_i||^2_2$找到最近的原型，并且将其使用作隐状态表征$z_q(x)=e_{q(x)}$输入到解码器中。当模型应用到downstream tasks时，learned representation 能够被当作一个distributed representation处理（其中的每一个采样点都被一个连续vector表示），或者一个离散的representation（每个sample都被一个原型ID / token ID表示）。</p>
<p>在反向传播时，loss对于预先量化向量的梯度通过straight-through估计器被估计。</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial z_e(x)} \approx \frac {\partial \mathcal{L}}{\partial z_q(x)}</script><p>In TensorFlow, 可以通过如下公式来实现：</p>
<script type="math/tex; mode=display">
z_q(x)=z_e(x)+stop\_gradient(e_{q(x)} - z_e(x))</script><p>这些原型，是通过延伸learning objective with terms 来最优化quantization来训练的。原型被强迫与它们所替换的vectors近一些，被称为承诺损失(commitment loss)，用于鼓励编码器生成靠近原型的vectors。如果没有承诺损失，VQ-VAE 训练可以通过生成unbounded magnitude来发散。因此，<strong>VQ-VAE模型的loss包含如下三项，NLL(negative log-likelihood) of the reconstruction, (use the straight-through estimator来反向传播), 2个VQ-相关的loss, 2) prototype 与 assigned vectors 之间的距离，3）commitment cost</strong></p>
<script type="math/tex; mode=display">
\mathcal{L} = log p(x|z_q(x)) + ||sg(z_e(x)) -e_{q(x)} ||^2_2 + \gamma|| z_e(x) - sg(e_{q(x)})||^2_2</script><p>其中，$sg(\cdot)$是<em>stop gradient</em>,来在反向传播的时候，清空gradient。</p>
<p>VQ-VAE中的量化是一个information bottleneck。编码器可以被理解为一个概率模型，puts all probability mass on the selected discrete token (prototype ID). 假设K个tokens服从均匀先验分布(uniform prior distribution)，KL 散度是常数，并且等于$log K$。因此KL项不需要被包含在VQ-VAE训练标准中，取而代之的，成为了一个与prototype inventory大小相关的超参数。</p>
<p>VQ-VAE被定性地展示出能够学习到一个区分开文本内容和说话人id的representation。除此之外，discovered tokens能够在有限的设置下被映射到phonemes。</p>
<p>C. 序列数据的自编码</p>
<p>序列数据，如语音或文本，经常包含了能够被生成模型探索的局部依赖信息。事实上，序列数据的纯自回归模型，基于历史数据预测下一个观测点，是非常成功的。对于文本来说，很容易想到与n-gram模型相关的卷积神经语言模型。相似地，WaveNet是一个时域采样点的自回归模型。</p>
<p>这样的自回归模型的缺点是，它们不能够生成数据的隐空间表示。然而，可以将如此的生成模型与一个隐表示抽取器来结合使用。目前有两种解决方案，1是编码器能够处理整句话术，生成一个隐状态向量，然后输入到自回归解码器中，2是编码器能够周期性的生成隐状态特征来输入到解码器中。我们采用方案2。</p>
<p>训练这样的latent variable和自回归的混合模型容易造成隐空间坍塌，其中，解码器学习时，忽略受限隐表示，仅仅使用自回归的无约束信号。对于VAE来说，可以给KL项增加一个权重来阻止这种隐空间坍塌，并且使用free-information formulation。VQ-VAE 自然地对潜在崩溃具有弹性，因为 KL 项是一个超参数，它没有使用给定模型的梯度训练进行优化。</p>
<h2 id="Model-description-2-pages"><a href="#Model-description-2-pages" class="headerlink" title="Model description (2 pages)"></a>Model description (2 pages)</h2><p><img src="/images/VQVAE-model.png" alt="VQVAE-model"></p>
<p>模型结构如图1所示。编码器（为了keep the autoencoder viewpoint， encoder可以理解为一层固定的信号处理层）读取原始语音采样点序列，或者是语音特征, 抽取出一个隐向量序列(a sequence of hidden vectors)，被输入到一个bottleneck来成为一个隐表示序列(a sequence of latent representations)。latent vectors的哪一个frequency 被抽取是通过number of strided convolutions applied by the encoder来决定的。</p>
<p>解码器通过采用WaveNet来条件于encoder抽取的latent representation和一个speaker embedding来重构语音。解码器显式条件于speaker identity使得encoder不需要在latent representation中捕捉说话人信息。具体来说，<strong>解码器1) 输入encoder outputs，2）选择性地应用随机正则化到latent vectors. 3）使用卷积函数来将neighboring time steps抽取的latent vectors 整合， 4）上采样至目标采样率。</strong> 语音采样点是采用WaveNet来进行重构的，整合了所有的条件信息，包括：<strong>autoregressive information</strong> about past samples, <strong>global information about</strong> <strong>the speaker</strong>, <strong>latent information</strong> about past and future samples extracted by the encoder. 我们发现encoder’s bottleneck和提出的regularization在提取数据表示时是关键的。如果没有bottleneck, 该模型倾向于学习一种简单的重建策略，该策略可以逐字复制未来的样本。我们还注意到编码器与说话人无关，只需要语音数据，而解码器也需要说话人信息。</p>
<p>我们考虑了三种bottleneck：1）简单的降维，2）具有不同潜在表示维数和不同容量的高斯 VAE，3）具有不同数量量化原型的 VQ-VAE。所有bottleneck都可选地跟随下面描述的 dropout 启发的时间抖动正则化(time-jitter regularization)。此外，我们使用原始波形、对数梅尔滤波器组（log-mel filterbank）和梅尔频率倒谱系数 (MFCC) 特征对不同的输入和输出表示进行试验，这些特征丢弃了频谱图中存在的音高信息。</p>
<p>A. Time-jitter regularization</p>
<p>是一个类似于RNN中的Zoneout dropout的本文自主创新的正则化的方法，用于防止过拟合，详情不作描述。</p>
<h2 id="实验-6页"><a href="#实验-6页" class="headerlink" title="实验 (6页)"></a>实验 (6页)</h2><p>我们在两个数据集上评估模型：LibriSpeech(clean subset)和ZeroSpeech 2017 Contest Track 1 data. 两个数据集的共性是：多说话人，清晰，的read speech(sourced from audio books)以16 kHz录制。除此之外，ZeroSpeech Challenge控制了每个说话人的数量，主要的数据是被其中的几个说话人来讲述的。</p>
<p>第 IV-B 节中介绍的初始实验比较了不同的bottleneck变体，并确定模型在图 1 所示的四个不同探测点处生成的连续潜在表示中保留了来自输入音频的哪些类型的信息。使用在每个探测点计算的表示，我们测量几个预测任务的性能：音素预测（每帧准确度）、说话人身份和性别预测准确度，以及频谱图帧的 L2 重建误差。我们确定 VQ-VAE 学习了在语音内容和说话者身份之间具有最强解缠结的潜在表示，并在以下实验中关注该架构。</p>
<p>在第 IV-C 节中，我们通过将每个离散标记映射到小标记数据集 (LibriSpeech dev) 强制对齐中最常见的对应音素来分析 VQ-VAE 标记的可解释性，并报告单独集上映射的准确性 （LibriSpeech 测试）。 直观地说，这捕获了单个令牌的可解释性。</p>
<p>然后，我们将 VQ-VAE 应用于第 IV-D 节中的 ZeroSpeech 2017 声学单元发现任务 [20]。 此任务评估表示相对于语音类的判别力。 最后，在第 IV-E 节中，我们测量了不同超参数对性能的影响。</p>
<p>A. 默认的模型超参数</p>
<p>我们最好的模型使用 <strong>MFCC 作为编码器输入</strong>，但在<strong>解码器输出处重建原始波形</strong>。 我们使用每 10 毫秒（即以 100 Hz 的速率）提取的标准 13 个 MFCC 特征，并用它们的时间一阶和二阶导数进行扩充。 这些特征最初是为语音识别而设计的，并且大多对音频信号中的音调和类似的混淆细节是不变的。 编码器有 9 个层，每层使用 768 个单元和 ReLU 激活，组织成以下组：2 个预处理卷积层，过滤器长度为 3 和残差连接，1 个步幅卷积长度减少层，过滤器长度为 4，步幅为 2（对信号进行下采样 因子二），然后是 2 个长度为 3 的卷积层和残差连接，最后是 4 个具有残差连接的前馈 ReLU 层。 产生的潜在向量以 50 Hz（即每隔一帧）提取，每个潜在向量取决于 16 个输入帧的感受野。我们还使用了一个具有两个长度缩减层的替代编码器，它以 25 Hz 的频率提取潜在表示，接受场为 30 帧。</p>
<p>当未指定时，潜在表示为 64 维，适用时限制为 14 位。 此外，对于 VQ-VAE，我们使用推荐的 γ = 0.25 [19]。</p>
<p>解码器应用了随机时间抖动正则化（参见第 III-A 部分）。 在训练期间，每个潜在向量被替换为它的任何一个邻居，概率为 0.12。 抖动的潜在序列通过具有滤波器长度 3 和 128 个隐藏单元的单个卷积层，以混合相邻时间步长的信息。 然后对该表示进行 320 次上采样（以匹配 16kHz 音频采样率），并与表示当前说话者的单热向量连接以形成自回归 WaveNet 的调节输入。 WaveNet 由 20 个因果扩张卷积层组成，每个层使用 368 个带有残差连接的门控单元，组织成两个“循环”，每层 10 层，扩张率为 1,2,4,…,29。 调节信号分别传递到每一层。 使用跳过连接将来自 WaveNet 每一层的信号传递到输出。 最后，信号通过 2 个 ReLU 层，256 个单元。 应用 Softmax 来计算下一个样本概率。 我们在 mu-law 压扩后使用了 256 个量化级别。</p>
<p>所有模型都在从训练数据集中均匀采样的 64 个序列的小批量上训练，长度为 5120 个时域样本（320 毫秒）。 <strong>在 4 个 Google Cloud TPU（16 个芯片）上训练一个模型需要一周时间。</strong> 我们使用 Adam 优化器，初始学习率为 4 × 10−4，在 400k、600k 和 800k 步后减半。 Polyak 平均应用于所有用于模型评估的检查点。</p>
<p>B. Bottleneck comparison</p>
<p>我们在 LibriSpeech 上训练模型，并分析在自动编码器瓶颈周围的隐藏表示中捕获的信息，如图 1 所示：</p>
<ul>
<li>$p_{enc}$ (768 dim) encoder output prior to the bottleneck,</li>
<li>$p_{proj}$ (64 dim) within the bottleneck after projecting to lower dimension,</li>
<li>$p_{bn}$ (64 dim) bottleneck output, corresponding to the quantized representation in VQ-VAE, or a random sample from the variational posterior in VAE, and</li>
<li>$p_{cond}$ (128 dim) after passing $p_{bn}$ through a convolution layer which captures a larger receptive field over the latent encoding.</li>
</ul>
<p>在每个探测点，我们在四个任务中的每一个上训练具有 2048 个隐藏单元的单独 MLP 网络：对整个片段的说话人性别和身份进行分类（在整个信号中平均汇集潜在向量之后），预测每一帧的音素类（每个潜在向量都制作几个预测），并在每个帧中重建对数梅尔滤波器组特征（再次从每个潜在向量预测几个连续帧）。从信号中捕获高层级语义内容的表示，同时对令人讨厌的低级信号细节保持不变，将具有高音素预测精度和高频谱重建误差。解开的表示还应该具有较低的说话人预测精度，因为该信息明确地提供给解码器调节网络，因此不需要在latent encoding中保留。由于我们主要对发现构建的表示中存在哪些信息感兴趣，因此我们报告了训练性能并且不调整探测网络以进行泛化。</p>
<p>图 2 展示了使用具有不同超参数（latent dimensionality和bottleneck bitrate）的三个瓶颈中的每一个的模型比较，说明了信息通过网络传播的程度。此外，图 3 突出显示了使用不同配置获得的语音内容和说话者身份的分离。</p>
<p>图 2 显示，每种瓶颈类型始终会丢弃 $p_{enc}$ 和 $p_{bn}$ 探针位置之间的信息，这可以从每个任务的性能降低中得到证明。 瓶颈还会影响前面层的信息内容。 特别是对于简单地降低维数的 vanilla 自动编码器 (AE)，$p_{enc}$ 的说话人预测精度和滤波器组重建损失取决于瓶颈的宽度，更窄的宽度会导致更多的信息在编码器的较低层被丢弃。 同样，与维数和比特率匹配的 VQ-VAE 相比，VQ-VAE 和 AE 在 $p_{enc}$ 上产生了更好的滤波器组重建和说话人身份预测，这对应于 VQ-VAE 的令牌数量的对数，以及与先验的 KL 散度 VAE，我们通过设置允许的空闲位数来控制。</p>
<p><strong>正如预期的那样，AE 丢弃的信息最少。 在 $p_{cond}$ 上，表示仍然对说话者和音素具有高度预测性，并且其滤波器组重建是所有配置中最好的。 然而，从无监督学习的角度来看，AE 潜在表示不太有用，因为它混合了源信号的所有属性。</strong></p>
<p>相比之下，VQ-VAE 模型产生的表示可以高度预测信号的语音内容，同时有效地丢弃说话者身份和性别信息。 在更高的比特率下，音素预测与 AE 一样准确。 滤波器组重建也不太准确。 我们观察到说话者信息主要在 $p_{proj}$ 和 $p_{bn}$ 之间的量化步骤中被丢弃。 在 $p_{cond}$ 表示中组合几个潜在向量会产生更准确的音素预测，但额外的上下文无助于恢复说话者信息。 这种现象在图 3 中突出显示。请注意，VQ-VAE 模型对瓶颈维度的依赖性很小，因此我们以默认设置 64 呈现结果。</p>
<p>最后，VAE 模型比简单的降维更好地分离说话人和语音信息，但不如 VQ-VAE。 VAE 比 VQ-VAE 更一致地丢弃语音和说话者信息：在 $p_{bn}$ 处，VAE 的音素预测不太准确，而其性别预测更准确。 此外，在 $p_{cond}$ 上结合更广泛的感受野的信息并不能像 VQ-VAE 模型那样提高音素识别。 对瓶颈维度的敏感性（如图 2 所示）也令人惊讶，与较宽的 VAE 瓶颈相比，较窄的 VAE 瓶颈丢弃的信息更少。 这可能是由于 VAE 的随机操作：为了提供与低瓶颈维度相同的 KL 散度，需要在高维度添加更多噪声。 这种噪声可能会掩盖表示中存在的信息。</p>
<p><strong>基于这些结果，我们得出结论，VQ-VAE 瓶颈最适合于学习潜在表示，这些表示捕获语音内容同时对底层说话者身份保持不变。</strong></p>
<p>C. VQ-VAE token interpretability</p>
<p>到目前为止，我们已经使用 VQ-VAE 作为量化潜在向量的瓶颈。在本节中，我们寻求对离散原型 ID 的解释，评估 VQ-VAE 令牌是否可以映射到音素，即语音的潜在离散成分。示例令牌 ID 显示在图 4 的中间窗格中，我们可以看到令牌 11 始终与瞬态“T”音素相关联。为了评估其他标记是否有类似的解释，我们测量了逐帧音素识别准确度，其中每个标记被映射到 41 个音素中的一个。我们使用 460 小时干净的 LibriSpeech 训练集进行无监督训练，并使用来自干净的开发子集的标签将每个标记与最可能的音素相关联。我们通过在干净的测试集上以 100 Hz 的帧速率计算逐帧电话识别准确度来评估映射。使用来自 s5 LibriSpeech 配方 [55] 的 Kaldi tri6b 模型从强制对齐中获得真实音素边界。</p>
<p>表 I 显示了在 LibriSpeech 上获得 VQ-VAE 令牌到音素的最佳准确度的配置的性能。 在两个时间点给出识别精度：在 200k 梯度下降步骤之后，当可以评估模型的相对性能时，以及在模型收敛后 900k 步之后。 我们没有观察到训练时间较长的过度拟合。 为所有帧预测最常见的静音音素将准确度下限设置为 16%。 在完整的 460 小时训练集上有区别地训练以预测具有与 25 Hz 编码器相同架构的音素的模型实现了 80% 的逐帧音素识别准确率，而没有时间减少层的模型将上限设置为 88%。</p>
<p>表 I 表明映射精度随着标记数量的增加而提高，使用 32768 个标记的最佳模型达到 64.5% 的精度。 然而，最大的准确度增益出现在 4096 个令牌时，随着令牌数量的进一步增加，收益递减。 该结果与 Kaldi tri6b 模型中使用的 5760 个绑定三音素状态大致对应。</p>
<p>我们还注意到，增加令牌的数量并不会轻易提高准确性，因为我们衡量的是泛化，而不是集群纯度。 在为每个帧分配不同标记的限制下，由于对我们建立映射的小型开发集过度拟合，准确性会很差。 然而，在我们的实验中，我们始终观察到提高的准确性。</p>
<p>D. 无监督ZeroSpeech 2017声学单元发现任务</p>
<p>ZeroSpeech 2017 语音单元发现任务评估representation区分不同声音的能力，而不是将representation映射到预定义语音单元的难易程度。因此，它是对上一节中使用的音素分类准确度度量的补充。 ZeroSpeech 评估方案使用最小对 ABX 测试，该测试评估模型区分三个音素长语音段对的能力，这些语音仅在中间音素（例如“get”和“得到了”）。我们在提供的训练数据（英语 45 小时，法语 24 小时，普通话 2.5 小时）上训练模型，并使用官方评估脚本对测试数据进行评估。为了确保我们不会过拟合 ZeroSpeech 任务，我们只考虑了在LibriSpeech 上找到的最佳超参数设置（参见第 IV-E 部分）。此外，为了最大限度地遵守 ZeroSpeech 约定，我们对所有语言使用了相同的超参数，在表 II 中表示为 VQ-VAE（每语言、MFCC、$p_{cond}$）。</p>
<p>在带有足够大训练数据集的英语和法语上，尽管使用了独立于说话人的编码器，但我们取得的结果比顶级参赛者更好。</p>
<p>结果与我们对 VQ-VAE 瓶颈执行的信息分离的分析一致：在更具挑战性的跨说话者评估中，最佳性能使用 $p_{cond}$ 表示，它结合了瓶颈表示（VQ-VAE）的几个相邻帧 ,（表 II 中的每个 lang、MFCC、$p_{cond}$））。 比较说话人内部和说话人之间的结果同样与第 IV-B 部分中的观察结果一致。 在说话人内部的情况下，没有必要从语音内容中分离说话人身份，因此 $p_{proj}$ 和 $p_{bn}$ 探测点之间的量化会损害性能（尽管在英语中，通过考虑 $p_{cond}$ 的更广泛的上下文来纠正这一点）。 在跨说话人的情况下，量化提高了英语和法语的分数，因为丢弃混杂说话人信息的收益抵消了一些语音细节的损失。 此外，丢弃的语音信息可以通过在 $p_{cond}$ 处混合相邻的时间步长来恢复。</p>
<p><strong>VQ-VAE 在普通话上的表现更差</strong>，我们可以将其归因于三个主要原因。 首先，训练数据集仅包含 2.4 小时或语音，导致过度拟合（参见第 IV-E7 节）。 这可以通过多语言训练得到部分改善，如 VQ-VAE，（所有语言，MFCC，$p_{cond}$）。 <strong>其次，普通话是一种声调语言，而默认输入特征 (MFCC) 会丢弃音高信息。</strong> 我们注意到在 mel filterbank 特征（VQ-VAE，（所有 lang，$f_{bank}$，$p_{proj}$））上训练的多语言模型略有改进。 第三，VQ-VAE 被证明不会在潜在表示中编码韵律。 比较各个探测点的结果，我们发现普通话是唯一一种 VQ 瓶颈会丢弃信息并降低跨说话者测试制度中性能的语言。 尽管如此，多语言预量化特征产生的精度与相当。</p>
<p>我们不认为需要更多无监督训练数据是一个问题。 未标记的数据非常丰富。 我们认为，需要并且可以更好地利用大量未标记训练数据的更强大的模型比性能在小数据集上饱和的更简单的模型更可取。 然而，增加训练数据量是否有助于普通话 VQ-VAE 学会丢弃更少的音调信息还有待验证（多语言模型可能已经学会这样做以适应法语和英语）。</p>
<p>E. Hyperparameter impact</p>
<p>所有 VQ-VAE 自动编码器超参数都使用多组网格搜索 (grid-search) 在 LibriSpeech 任务上进行了调整，优化了最高的音素识别精度。 我们还在 ZeroSpeech 挑战任务的英语部分验证了这些设计选择。 事实上，我们发现所提出的时间抖动正则化提高了所有输入表示的 ZeroSpeech ABX 分数。 使用 MFCC 或滤波器组特征会产生比使用波形更好的分数，并且当使用更多令牌时，模型始终会获得更好的分数。</p>
<p>1) 时间抖动正则化：在表 III 中，我们分析了时间抖动正则化对 VQ-VAE 编码的有效性，并将其与两种 dropout 变体进行比较：常规 dropout 应用于编码的各个维度和 dropout 随机应用于整个 在各个时间步进行编码。 常规 dropout 不会强制模型在相邻的时间步长中分离信息。 Step-wise dropout 促进了跨时间步独立的编码，并且性能比时间抖动略差6。</p>
<p>所提出的时间抖动正则化大大提高了令牌映射的准确性，并扩展了性能良好的令牌帧速率范围，包括 50 Hz。 虽然 LibriSpeech 令牌精度在 25 Hz 和 50 Hz 下相当，但更高的令牌发射频率对于 ZeroSpeech AUD 任务很重要，50 Hz 模型在该任务上明显更好。 这种行为是由于 25 Hz 模型容易忽略短音（第 IV-E6 节），这会影响 ABX 在 ZeroSpeech 任务上的结果。</p>
<p>我们还分析了 VQ-VAE、VAE 和简单降维 AE 瓶颈的四个探测点的信息内容，如图 5 所示。对于所有瓶颈机制，正则化限制了滤波器组重建的质量并提高了音素识别精度 在约束表示中。 然而，在 $p_{cond}$ 探测点中组合了相邻的时间步之后，这种好处就变小了。 此外，对于 VQ-VAE 和 VAE，正则化会降低性别预测的准确性，并使表示对说话者的敏感度略低。</p>
<p>2) 输入表示：在这组实验中，我们使用不同的输入表示来比较性能：原始波形、log-mel 频谱图或 MFCC。 原始波形编码器使用 9 个跨步卷积层，这导致令牌提取频率为 30 Hz。 然后，我们用常规的 ASR 数据管道替换了波形：每 10 毫秒从 25 毫秒长的窗口中提取 80 个对数梅尔滤波器组特征，从梅尔滤波器组输出中提取 13 个 MFCC 特征，两者都增加了它们的一阶和二阶时间导数。 在编码器中使用两个跨步卷积层导致这些模型的令牌速率为 25 Hz。 </p>
<p>结果报告在表III的底部。 高级特征，尤其是 MFCC，比波形表现更好，因为按照设计，它们会丢弃有关音高的信息并提供一定程度的说话人不变性。 使用这种简化的表示迫使编码器向解码器传输更少的信息，作为对更多说话者不变潜在编码的归纳偏置。</p>
<p>3）输出表示：我们构建了一个自回归解码器网络，重建滤波器组特征而不是原始波形样本。 受文本到语音系统最近进展的启发，我们实现了一个类似 Tacotron 2 的解码器，在自回归信息流上有一个内置的信息瓶颈，这被发现在 TTS 应用程序中至关重要。 与 Tacotron 2 类似，滤波器组特征首先由一个小的“预网络”处理，我们应用了大量的 dropout 并将解码器配置为并行预测多达 4 帧。 然而，这些修改最多产生 42% 的音素识别准确率，明显低于本文中描述的其他架构。 然而，该模型的训练速度要快一个数量级。</p>
<p>最后，我们分析了解码 WaveNet 的大小对 VQ-VAE 提取的表示的影响。 我们发现整体感受野 (RF) 的影响大于 WaveNet 的深度或宽度。 特别是，当解码器的感受野跨越大约 10 毫秒时，潜在表示的属性会发生很大的变化。 如图 6 所示，对于较小的 RF，调节信号包含更多说话人信息：性别预测接近 80%，而逐帧音素预测准确度仅为 55%。 对于较大的 RF，性别预测准确率约为 60%，而音素预测的峰值接近 65%。 最后，虽然重建对数似然随着 WaveNet 深度提高到 30 层，但音素识别准确度稳定在 20 层。 由于 WaveNet 的计算成本最大，我们决定保留 20 层配置。</p>
<p>4) Decoder speaker conditioning：WaveNet 解码器基于三个信息源生成样本：先前发出的样本（通过自回归连接）、对说话者或其他时间固定的信息的全局调节以及提取的时变表示 从编码器。 我们发现禁用全局speaker调节会使音素分类准确度降低 3 个百分点。 这进一步证实了我们关于 VQ-VAE 瓶颈引起的解缠结的发现，这使模型偏向于丢弃以更明确形式提供的信息。 在我们的实验中，我们使用了独立于speaker的编码器。 但是，使编码器适应speaker可能会进一步改善结果。 事实上，展示了使用说话人自适应方法对 ZeroSpeech 任务的改进。</p>
<p>5) 编码器超参数：我们尝试调整编码器卷积层的数量、滤波器的数量和滤波器长度。 一般来说，使用更大的编码器会提高性能，但是我们确定必须仔细控制编码器的感受野，性能最好的编码器对于每个生成的令牌可以看到大约 0.3 秒的输入信号。</p>
<p>可以使用两种机制控制有效的感受野：通过仔细调整编码器架构，或通过设计具有宽感受野的编码器，但将训练期间看到的信号段的持续时间限制为所需的感受野。 通过这种方式，模型永远不会学会使用其全部容量。 当模型在 2.5s 长段上训练时，感受野为 0.3s 的编码器的帧音素识别准确率为 56.5%，而感受野为 0.8s 的编码器的得分仅为 54.3%。 当在 0.3 秒的片段上训练时，两个模型的表现相似。</p>
<p>6) 瓶颈比特率：语音 VQ-VAE 编码器可以看作是使用非常低的比特率对信号进行编码。 为了达到预定的目标比特率，可以控制令牌率（即通过控制编码器跨步卷积中的下采样程度）和每一步提取的令牌数（或等效的比特数）。 我们发现标记率是一个必须谨慎选择的关键参数，在 50 Hz（56.0% 音素识别准确率）和 25 Hz（56.3%）下获得 200k 训练步骤后获得最佳结果。 准确率在较高的令牌率（100 Hz 时为 49.3%）时突然下降，而较低的令牌率会错过非常短的电话（12.5 Hz 时为 53% 的准确率）。 </p>
<p>与令牌的数量相比，VQ-VAE 嵌入的维度对表示质量具有次要影响。 我们发现 64 是一个很好的设置，小得多的维度会降低具有少量标记的模型的性能，而更高的维度会对具有大量标记的模型的性能产生负面影响。</p>
<p>为完整起见，我们观察到即使对于具有最大令牌库存的模型，整体编码器比特率也很低：50 Hz 时为 14 位 = 700 bps，这与经典语音编解码器的最低比特率相当。</p>
<p>7) 训练语料库大小：我们在 LibriSpeech 训练集的子集上试验了训练模型，大小从 4.6 小时 (1%) 到 460 小时 (100%) 不等。 对 4.6 小时的数据进行训练，音素识别准确率在 100k 步时达到 50.5% 的峰值，然后下降。 9 小时的训练在 180k 集上达到了 52.5% 的峰值准确率。 当训练集的大小增加超过 23 小时时，音素识别率在大约 90 万步后达到 54%。 通过对完整 460 小时的数据进行训练，没有发现进一步的改进。 我们没有观察到任何过度拟合，并且为了获得最佳结果，训练模型直到达到 900k 步而没有提前停止。未来一个有趣的研究领域将是研究增加模型容量以更好地利用大量未标记数据的方法。</p>
<p>数据集大小的影响在 ZeroSpeech Challenge 结果（表二）中也可见：VQ-VAE 模型在英语（45 小时的训练数据）和法语（24 小时）上表现良好，但在普通话上表现不佳（ 2.5 小时）。 此外，在英语和法语上，我们使用在单语数据上训练的模型获得了最好的结果。 使用对所有语言的数据联合训练的模型在普通话上获得了稍好的结果。 </p>
<h2 id="Related-Work-1页"><a href="#Related-Work-1页" class="headerlink" title="Related Work (1页)"></a>Related Work (1页)</h2><p>序列数据的 VAE 在 [49] 中被引入。 该模型使用 LSTM 编码器和解码器，而潜在表示由编码器的最后一个隐藏状态形成。 该模型被证明对自然语言处理任务很有用。 然而，它也证明了潜在表示崩溃的问题：当一个强大的自回归解码器与潜在编码的惩罚同时使用时，比如 KL 先验，VAE 倾向于忽略先验并表现得好像它是一个 纯自回归序列模型。 这个问题可以通过改变 KL 项的权重来缓解，并通过使用 word dropout 限制自回归路径上的信息量 [49]。 在确定性自动编码器中也可以避免潜在崩溃，例如 [64]，它将卷积编码器耦合到强大的自回归 WaveNet 解码器 [18]，以学习由来自各种乐器的孤立音符组成的音乐音频的潜在表示。</p>
<p>我们凭经验验证，根据说话人信息调节解码器会导致编码更具有说话人不变性。[54] 给出了一个严格的证明，这种方法产生的表示对于明确提供的信息是不变的，并将其与域对抗训练相关联，这是另一种旨在对已知干扰因素实施不变性的技术 [65]。</p>
<p>当应用于音频时，VQ-VAE 使用 WaveNet 解码器从建模信息中释放潜在表示，这些信息很容易从最近的过去 [19] 中恢复。 它通过使用具有统一先验的离散潜在代码来避免后折叠问题，从而导致恒定的 KL 惩罚。 我们采用相同的策略来设计潜在表示正则化器：我们没有使用可能导致潜在空间崩溃的惩罚项来扩展成本函数，而是依靠潜在变量的随机副本来防止它们的共同适应并促进稳定性 时间。</p>
<p>本文中引入的随机时间抖动正则化受到数据的缓慢表示 [48] 和 dropout 的启发，dropout 在训练神经元期间随机删除以防止它们的协同适应 [50]。 它也与 Zoneout [51] 非常相似，后者依赖于所选神经元的随机时间副本来规范循环神经网络。</p>
<p>几位作者最近提议使用使用变量层次结构的 VAE 对序列进行建模。 [66] 探索了一个分层潜在空间，它将序列相关变量与序列无关变量分开。 他们的模型被证明可以执行说话人转换并在存在域不匹配的情况下提高自动语音识别 (ASR) 性能。 [67] 为序列数据引入了一个随机潜在变量模型，该模型还可以产生解开的表示，并允许在生成的序列之间进行内容交换。 这些其他方法可能会受益于规范潜在表示以实现进一步的信息解开。</p>
<p>声学单位发现系统旨在将声学信号转换成一系列类似于音素的可解释单位。 它们通常涉及声学帧、MFCC 或神经网络瓶颈特征的聚类，使用概率先验进行正则化。 DP-GMM [68] 在高斯混合模型上强加了狄利克雷过程先验。 使用 HMM 时间结构为子语音单元扩展它会导致 DP-HMM 和 HDP-HMM [69]、[70]、[71]。 HMM-VAE 建议使用深度神经网络代替 GMM [72]、[73]。 这些方法通过 HMM 时间平滑和时间建模来强制执行自上而下的约束。 语言单元发现模型在类似单词的级别检测重复出现的语音模式，找到具有约束动态时间扭曲的常见重复段 [74]。</p>
<p>在分段无监督语音识别框架中，神经自动编码器用于将可变长度的语音段嵌入到一个公共向量空间中，在那里它们可以被聚类为单词类型 [75]。 [76] 用一个模型代替分段自动编码器，该模型可以预测附近的语音片段，并证明该表示与词嵌入共享许多属性。 结合无监督的分词算法和在单独的语料库 [77] 上发现的词嵌入的无监督映射，该方法产生了一个在不成对的语音和文本数据上训练的 ASR 系统 [78]。</p>
<p>ZeroSpeech 2017 挑战赛的几个条目依赖于神经网络来发现语音单元。 [61] 在使用无监督术语发现系统 [79] 找到的语音段对上训练自动编码器。 [59]首先对语音帧进行聚类，然后训练神经网络来预测聚类 ID，并将其隐藏表示用作特征。 [60] 使用在 MFCC 上训练的自动编码器发现的特征扩展了该方案。</p>
<h2 id="Conclusion-半页"><a href="#Conclusion-半页" class="headerlink" title="Conclusion (半页)"></a>Conclusion (半页)</h2><p>我们将序列自动编码器应用于语音建模并比较了不同的<strong>信息瓶颈，包括 VAE 和 VQ-VAE</strong>。 我们使用可解释性标准以及区分相似语音的能力仔细评估了诱导的潜在表示。 <strong>瓶颈的比较表明，使用 VQ-VAE 获得的离散表示保留了最多的语音信息，同时也是最大的说话人不变性。</strong> 提取的表示允许将提取的符号准确映射到音素，并在 ZeroSpeech 2017 声学单元发现任务中获得有竞争力的表现。 Cho 等人的 VQ-VAE 编码器和 WaveNet 解码器的类似组合。 在 ZeroSpeech 2019 [80] 中具有最佳的声学单元发现性能。</p>
<p>我们确定模型需要一个信息瓶颈来学习将内容与说话者特征分开的表示。 此外，我们观察到，<strong>通过使瓶颈强度成为模型超参数，或者完全去除它（如在 VQ-VAE 中）</strong>，或者通过使用自由信息 VAE 目标，可以<strong>避免由太强的瓶颈引起的潜在崩溃问题 .</strong></p>
<p>为了进一步提高表示质量，我们引入了一种<strong>时间抖动正则化方案，该方案限制了潜在代码的容量，但不会导致潜在空间的崩溃。</strong> 我们希望这可以类似地提高与其他问题域中的自回归解码器一起使用的潜在变量模型的性能。</p>
<p>VAE 和 VQ-VAE 都限制了潜在表示(latent representation)的信息带宽(information bandwidth)。 然而，VQ-VAE 使用量化机制，它确定性地强制编码等于原型，而 VAE 通过注入噪声来限制信息量。 <strong>在我们的研究中，VQ-VAE 比 VAE 产生了更好的信息分离。</strong> 然而，需要进一步的实验来充分理解这种影响。 <strong>特别是，这是量化的结果还是确定性操作的结果？</strong></p>
<p>我们还观察到，虽然 VQ-VAE 产生离散表示，但为了获得最佳结果，它使用了一个如此大的标记集，以至于为每个标记分配一个单独的含义是不切实际的。 特别是，在我们的 ZeroSpeech 实验中，<strong>我们使用了每个令牌的密集嵌入表示，这提供了比简单地使用令牌标识更细微的令牌相似性度量。 也许需要更结构化的潜在表示，其中可以以连续方式调制一小组单元。</strong></p>
<p>广泛的超参数评估表明，优化编码器和解码器网络的感受野大小对于良好的模型性能很重要。 多尺度建模方法可以进一步分离韵律信息。 我们的自动编码方法还可以与更专门用于语音处理的惩罚相结合。 在 [73] 中引入 HMM 先验可以促进潜在表示，从而更好地模仿语音的时间语音结构。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Auto-Encoding Variational Bayes and Tutorials</title>
    <url>/2021/06/30/vae/</url>
    <content><![CDATA[<h1 id="Auto-Encoding-Variational-Bayes-自编码变分贝叶斯-https-arxiv-org-pdf-1312-6114v5-pdf-—-Diederik-P-Kingma-Max-Welling-Machine-Learning-Group-Universiteit-van-Amsterdam-May-2014"><a href="#Auto-Encoding-Variational-Bayes-自编码变分贝叶斯-https-arxiv-org-pdf-1312-6114v5-pdf-—-Diederik-P-Kingma-Max-Welling-Machine-Learning-Group-Universiteit-van-Amsterdam-May-2014" class="headerlink" title="Auto-Encoding Variational Bayes 自编码变分贝叶斯 https://arxiv.org/pdf/1312.6114v5.pdf —- Diederik P. Kingma, Max Welling (Machine Learning Group, Universiteit van Amsterdam) May 2014"></a>Auto-Encoding Variational Bayes 自编码变分贝叶斯 <a href="https://arxiv.org/pdf/1312.6114v5.pdf">https://arxiv.org/pdf/1312.6114v5.pdf</a> —- Diederik P. Kingma, Max Welling (Machine Learning Group, Universiteit van Amsterdam) May 2014</h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>看了个Abstract就隐约感觉自己看不懂文章正文了。。。果断跳到tutorials（tutorials在下面）。看完tutorials后又回来想看看原文，看了四个章节，发现着实看不懂。。。没办法给分了。。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在存在具有难以处理的后验分布的连续潜在变量和大型数据集的情况下，我们如何在有向概率模型中执行有效的推理和学习？我们引入了一种可扩展到大型数据集的随机变分推理和学习算法，并且在一些温和的可微性条件下，甚至可以在棘手的情况下工作。我们的贡献是双重的。 首先，我们展示了变分下界的重新参数化产生了下界估计量，可以使用标准随机梯度方法直接优化该估计量。 其次，我们证明对于 i.i.d. 对于每个数据点具有连续潜在变量的数据集，通过使用建议的下限估计器将近似推理模型（也称为识别模型）拟合到棘手的后验模型，可以使后验推理特别有效。 理论优势体现在实验结果上。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>我们如何使用定向概率模型执行有效的近似推理和学习，这些模型的连续潜在变量和/或参数具有难以处理的后验分布？ 变分贝叶斯 (VB) 方法涉及对难以处理的后验的近似的优化。 不幸的是，常见的平均场方法需要期望 w.r.t. 的解析解。 近似后验，这在一般情况下也是难以处理的。 我们展示了变分下界的重新参数化如何产生下界的简单可微无偏估计量； 这种 SGVB（随机梯度变分贝叶斯）估计器可用于几乎任何具有连续潜在变量和/或参数的模型中的有效近似后验推断，并且可以直接使用标准随机梯度上升技术进行优化。</p>
<p>对于 i.i.d. 数据集和每个数据点的连续潜在变量，我们提出了自动编码 VB (AEVB) 算法。 在 AEVB 算法中，我们通过使用 SGVB 估计器优化识别模型使推理和学习特别高效，该模型允许我们使用简单的祖先采样执行非常有效的近似后验推理，这反过来又使我们能够有效地学习模型参数，而无需 每个数据点需要昂贵的迭代推理方案（例如 MCMC）。 学习到的近似后验推理模型还可用于许多任务，例如识别、去噪、表示和可视化目的。 当神经网络用于识别模型时，我们就得到了变分自动编码器。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>本节中的策略可用于为具有连续潜在变量的各种有向图模型推导出下界估计量（随机目标函数）。 我们将把自己限制在我们有一个 i.i.d 的常见情况下。 每个数据点具有潜在变量的数据集，以及我们喜欢对（全局）参数执行最大似然（ML）或最大后验（MAP）推断，以及对潜在变量进行变分推断。例如，可以直接将此场景扩展到我们还对全局参数执行变分推理的情况； 该算法放在附录中，但这种情况下的实验留待未来工作。 请注意，我们的方法可以应用于在线的非平稳设置，例如 流数据，但为了简单起见，我们在这里假设一个固定的数据集。</p>
<h3 id="2-1-问题情景"><a href="#2-1-问题情景" class="headerlink" title="2.1 问题情景"></a>2.1 问题情景</h3><p>让我们考虑一些由 $N$个 i.i.d. 组成的数据集 $X = \{x(i)\}^N_{i=1}$。 一些连续或离散变量 $x$ 的样本。 我们假设数据是由一些随机过程生成的，涉及一个未观察到的连续随机变量 $z$。 该过程包括两个步骤：（1）从一些先验分布 $p_{θ∗}(z)$ 生成一个值 $z(i)$； (2) 值 $x^{(i)}$ 是从一些条件分布 $p_{θ∗} (x|z)$ 生成的。 我们假设先验 $p_{θ∗} (z)$  和似然 $p_{θ∗} (x|z)$ 来自分布 $p_{θ} (z)$ 和 $p_θ (x|z)$ 的参数族，并且它们的 PDF 几乎在任何地方都是可微的 w.r.t. $θ$ 和 $z$。 不幸的是，这个过程中有很多是隐藏在我们看来的：真实参数 $θ∗$ 以及潜在变量 $z^{(i)}$ 的值对我们来说是未知的。</p>
<p>非常重要的是，我们没有对边际或后验概率做出常见的简化假设。 相反，我们在这里对一种通用算法感兴趣，该算法甚至在以下情况下也能有效工作：</p>
<ol>
<li>难处理性：边际似然 $p_θ(x) = 􏰇\int p_θ(z)p_θ(x|z)dz$ 的积分是难处理的（因此我们无法评估或区分边际似然）的情况，其中真正的后验 密度 $p_θ(z|x) = p_θ(x|z)p_θ(z)/p_θ(x)$ 是难以处理的（因此不能使用 EM 算法），以及任何合理的平均场 VB 算法所需的积分 也难治。 这些难以处理的问题很常见，出现在中等复杂的似然函数 $p_θ(x|z)$ 的情况下，例如 具有非线性隐藏层的神经网络。</li>
<li>大数据集：我们有太多的数据，批量优化成本太高； 我们希望使用小批量甚至单个数据点进行参数更新。 基于采样的解决方案，例如 Monte Carlo EM 通常会太慢，因为它涉及每个数据点的典型昂贵的采样循环。</li>
</ol>
<p>我们对上述场景中的三个相关问题感兴趣并提出了解决方案：</p>
<ol>
<li>参数 $θ$ 的有效近似 ML 或 MAP 估计。 参数本身可能很有趣，例如 如果我们正在分析一些自然过程。 它们还允许我们模拟隐藏的随机过程并生成类似于真实数据的人工数据。</li>
<li>给定参数 $θ$ 的观测值 $x$ 的潜在变量 $z$ 的有效近似后验推断。 这对于编码或数据表示任务很有用。</li>
<li>变量$x$的有效近似边际推断。 这使我们能够执行需要先验于 $x$ 的各种推理任务。 计算机视觉中的常见应用包括图像去噪、修复和超分辨率。</li>
</ol>
<p>为了解决上述问题，我们引入一个识别模型$q_{\phi}(z|x)$：对难处理的真实后验$p_θ(z|x)$的近似。 请注意，与平均场变分推理中的近似后验相反，它不一定是阶乘的，并且它的参数 $\phi$ 不是从某些封闭形式的期望中计算出来的。 相反，我们将介绍一种学习识别模型参数 $\phi$ 和生成模型参数 $θ$ 的方法。</p>
<p>从编码理论的角度来看，未观察到的变量 $z$ 具有作为潜在表示或代码的解释。 因此，在本文中，我们还将识别模型 $q_{\phi}(z|x)$ 称为概率编码器，因为给定一个数据点 $x$，它会在代码 $z$ 的可能值上产生分布（例如高斯分布），数据点 $x$ 来自该值 本来可以生成的。 类似地，我们将 $p_θ(x|z)$ 称为概率解码器，因为给定一个代码 $z$，它会在 $x$ 的可能对应值上产生分布。</p>
<h3 id="2-2-变分边界"><a href="#2-2-变分边界" class="headerlink" title="2.2 变分边界"></a>2.2 变分边界</h3><p>边际似然由单个数据点的边际似然总和 $log p_θ(x^{(1)}, · · · , x^{(N)}) = 􏰍\sum^N_{i=1} log p_θ(x^{(i)})$ 组成，每个都可以是 改写为：</p>
<script type="math/tex; mode=display">
log p_θ(x^{(i)}) = D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z|x^{(i)})) + \mathcal{L}(θ, \phi; x^{(i)})</script><p>第一个 RHS 项是近似值与真实后验值的 KL 散度。 由于这个 KL 散度是非负的，第二个 RHS 项 $\mathcal{L}(θ, \phi; x^{(i)})$ 被称为数据点 i 的边际似然的（变分）下界，可以写成：</p>
<script type="math/tex; mode=display">
log p_θ(x^{(i)}) ≥ \mathcal{L}(θ, \phi; x^{(i)}) = E_{q_{\phi}(z|x)} [− log q_{\phi}(z|x) + log p_θ(x, z)]</script><p>也可以写成：</p>
<script type="math/tex; mode=display">
\mathcal{L}(θ,\phi;x^{(i)} )=−D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z))+E_{q_{\phi}(z|x^{(i)})} [logp_θ(x^{(i)} |z)]</script><p>我们想要区分和优化下界 $L(θ,\phi;x^{(i)})$ w.r.t. 变分参数 $\phi$ 和生成参数 $θ$。 然而，下界的梯度 w.r.t. $\phi$ 有点问题。 这类问题通常的 (na ̈ıve) Monte Carlo 梯度估计量是： $∇_{\phi}E_{q_{\phi}(z)} [f(z)] = E_{q_{\phi} (z)} 􏰋f(z)∇_{q_{\phi} (z)} log q_{\phi}(z) ≃ \frac{1} {􏰍L} \sum^L_{l=1} f(z)∇_{q_{\phi}(z^{(l)})} log q_{\phi}(z^{(l)})$ 其中 $ z^{(l)} ∼ q_{\phi}(z|x^{(i)})$。 这个梯度估计器表现出非常高的方差（参见例如 [BJP12]），对于我们的目的来说是不切实际的。</p>
<h3 id="2-3-SGVB-估计器和-AEVB-算法"><a href="#2-3-SGVB-估计器和-AEVB-算法" class="headerlink" title="2.3 SGVB 估计器和 AEVB 算法"></a>2.3 SGVB 估计器和 AEVB 算法</h3><p>在本节中，我们将介绍一个实际估计器的下界及其w.r.t参数的导数。 我们假设近似后验形式为 $q_{\phi}(z|x)$，但请注意，该技术也适用于 $q_{\phi}(z)$ 的情况，即我们不以 $x$ 为条件的情况。 用于推断参数后验的全变分贝叶斯方法在附录中给出。</p>
<p>在 2.4 节概述的某些温和条件下，对于选定的近似后验 $q_{\phi}(z|x)$，我们可以使用（辅助）噪声变量$ε$的可微变换 $g_{\phi} (ε, x)$ 重新参数化随机变量 $􏰐\tilde z ∼ q_{\phi} (z|x)$ ：</p>
<script type="math/tex; mode=display">
􏰐z=g_{\phi}(ε,x), ε∼p(ε)</script><p>有关选择此类适当分布 $p(ε)$ 和函数 $g_{\phi}(ε,x)$ 的一般策略，请参见第 2.4 节。 我们现在可以对某函数 $f(z)$ w.r.t. $q_{\phi}(z|x) $的期望形成蒙特卡罗估计。 如下： </p>
<script type="math/tex; mode=display">
E_{q_{\phi}(z|x^{(i)})} [f(z)] = E_{p(ε)} [f(g_{\phi}(ε, x^{(i)}))] ≃ \frac{1}{L} \sum^L_{l=1} f(g_{\phi}(\epsilon^{(l)}, x^{(i)})), ε^{(l)} ∼ p(ε)</script><p>我们将这个技巧应用到变分下界，公式（2），可以生成通用“随机梯度变分贝叶斯 (SGVB) 估计器” $ \tilde L^A(θ, \phi; x^{(i)}) ≃ \mathcal{L}(θ, \phi; x^{(i)}):$ </p>
<script type="math/tex; mode=display">
\tilde L^A(θ, \phi; x^{(i)}) =\frac{1}{L} \sum^L_{l=1}log p_θ(x^{(i)}, z^{(i,l)}) − log q_{\phi}(z^{(i,l)}|x^{(i)})</script><p>其中 $z^{(i,l)} = g_{\phi}(ε^{(i,l)}, x^{(i)}), ε^{(l)} ∼ p(ε)$</p>
<p>通常，方程(3)的 KL 散度 $D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z))$可以通过解析积分（见附录 B），使得只有预期的重构误差 $E_{q_{\phi}(z|x^{(i)}) }􏰋logp_θ(x^{(i)}|z)$ 需要抽样估计。 KL 散度项可以被解释为正则化 $\phi$，鼓励近似后验接近先验 $p_θ(z)$。 这产生了第二个版本SGVB 估计量 $\tilde L^B(θ,\phi;x^{(i)}) ≃ L(θ,\phi;x^{(i)})$，对应于方程 (3)，它通常比通用估计量的方差更小：</p>
<script type="math/tex; mode=display">
\tilde L^B(θ,\phi;x^{(i)}) = - D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z)) + \frac{1}{L} \sum^L_{l=1}log p_θ(x^{(i)}, z^{(i,l)})</script><p>其中，$z^{(i,l)} = g_{\phi}(ε^{(i,l)}, x^{(i)}), ε^{(l)} ∼ p(ε)$</p>
<p>给定来自具有 $N$ 个数据点的数据集 $X$ 的多个数据点，我们可以构建完整数据集的边际似然下界，基于小批量：</p>
<script type="math/tex; mode=display">
L(θ,\phi;X) ≃\tilde L^M(θ,\phi;X^M) = \frac{N}{M}\sum^M_{i=1}\tilde L(θ,\phi;x^{(i)})</script><p>其中小批量 $X^M = \{x^{(i)}\}^M_{i=1}$ 是从具有 $N$ 个数据点的完整数据集 $X$ 中随机抽取的 $M$ 个数据点样本。 在我们的实验中，我们发现只要小批量大小 $M$ 足够大，每个数据点的样本数$L$ 可以设置为 1，例如 $M = 100$。可以取导数 $∇_{θ,\phi}L􏰐(θ;X^M)$，得到的梯度可以与随机优化方法（例如 SGD 或 Adagrad）结合使用。 有关计算随机梯度的基本方法，请参阅算法 1。</p>
<p>当查看 以上公式给出的目标函数时，与自动编码器的联系变得清晰。公式（7） 第一项是（近似后验与先验的 KL 散度）充当正则化器，而第二项是预期的负重建误差。 选择函数 $g_{\phi}(.)$ 使其将数据点 $x^{(i)}$ 和随机噪声向量 $ε^{(l)}$ 映射到来自该数据点的近似后验的样本： $z^{(i,l)} = g_{\phi}(ε^{(l)}, x^{(i)})$ 其中 $z^{(i,l)} ∼ q_{\phi}(z|x^{(i)})$。 随后，样本 $z^{(i,l)}$ 被输入到函数 $logp_θ(x^{(i)}|z^{(i,l)})$ 中，它等于给定 $z^{(i,l)}$数据点 $x^{(i)}$ 在生成式模型下的概率密度（或质量）。 该术语是自动编码器术语中的negative reconstruction error。</p>
<h3 id="2-4-重新参数化的技巧"><a href="#2-4-重新参数化的技巧" class="headerlink" title="2.4 重新参数化的技巧"></a>2.4 重新参数化的技巧</h3><h2 id="3-例子：VAE"><a href="#3-例子：VAE" class="headerlink" title="3 例子：VAE"></a>3 例子：VAE</h2><p>在本节中，我们将给出一个示例，其中我们将神经网络用于概率编码器 $q_{\phi} (z|x)$（生成模型 $p_θ (x, z)$ 的后验的近似值）以及参数 $\phi$ 和 $θ$ 与 AEVB 算法联合优化。</p>
<p>让潜在变量的先验是中心各向同性多元高斯 $p_θ(z) = \mathcal{N}(z;0,I)$。 请注意，在这种情况下，先验缺少参数。 我们让 $p_θ(x|z)$ 是一个多元高斯（在实值数据的情况下）或伯努利（在二进制数据的情况下），其分布参数是从 $z$ 用 MLP（一个全连接的神经网络，具有 单个隐藏层，见附录 C）。 请注意，在这种情况下，真正的后验 $p_θ(z|x)$ 是难以处理的。 虽然 $q_{\phi}(z|x)$ 的形式有很多自由度，但我们将假设真实的（但难以处理的）后验呈现为具有近似对角协方差的近似高斯形式。在这种情况下，我们可以让变分近似后验是一个具有对角协方差结构的多元高斯：</p>
<script type="math/tex; mode=display">
log q_{\phi}(z|x^{(i)}) = log \mathcal{N} (z; μ^{(i)}, σ^{2(i)}I)</script><p>其中 近似后验的均值和标准差 $μ^{(i)}$ 和 $σ^{(i)}$ 是编码 MLP 的输出，即数据点 $x^{(i)}$ 和变分参数 $\phi$ 的非线性函数（参见附录 C）。</p>
<p>如第 2.4 节所述，我们使用 $z^{(i,l)} = g_{\phi}(x^{(i)}, ε^{(l)}) = μ^{(i)} + σ^{(i)} ⊙ ε^{(l)}$ 其中 $ε^{(l)} ∼ \mathcal{N} (0, I)$。 $⊙$ 我们表示element-wise product。 在这个模型中，$p_θ(z)$（先验）和 q_{\phi}(z|x) 都是高斯分布的； 在这种情况下，我们可以使用 公式（7）的估计量，其中（7）无需估计即可计算和区分 KL 散度（参见附录 B）。 该模型的结果估计量和数据点 $x^{(i)}$ 为：</p>
<script type="math/tex; mode=display">
L(θ,\phi;x^{(i)})≃ 􏰏 \frac{1}{2}\sum^J_{j=1}(1+log((σ^{(i)}_j)^2)−(μ^{(i)}_j)^2 −(σ^{(i)}_j)^2 + \frac{1}{L} \sum^L_{l=1}􏰏logp_θ(x^{(i)}|z^{(i,l)})</script><p>其中，$z^{(i,l)} = μ^{(i)} + σ^{(i)} ⊙ ε^{(l)}, ε^{(l)} ∼ \mathcal{N} (0, I)$</p>
<p>如上所述和附录 C 中，解码项 $log p_θ (x^{(i)} |z^{(i,l)} )$ 是伯努利或高斯 MLP，取决于我们建模的数据类型。</p>
<h1 id="VAE-Tutorials-https-arxiv-org-pdf-1606-05908v2-pdf-—-Carl-Doersch-CMU-UC-Berkeley-Aug-2016"><a href="#VAE-Tutorials-https-arxiv-org-pdf-1606-05908v2-pdf-—-Carl-Doersch-CMU-UC-Berkeley-Aug-2016" class="headerlink" title="VAE Tutorials https://arxiv.org/pdf/1606.05908v2.pdf —- Carl Doersch CMU / UC Berkeley Aug 2016"></a>VAE Tutorials <a href="https://arxiv.org/pdf/1606.05908v2.pdf">https://arxiv.org/pdf/1606.05908v2.pdf</a> —- Carl Doersch CMU / UC Berkeley Aug 2016</h1><h2 id="感想-1"><a href="#感想-1" class="headerlink" title="感想"></a>感想</h2><p>直到2.1章节，公式（5）以上其实还能够看得懂，再往下的数学推断就有点迷茫，摸不清头脑了，所以这一次先记住公式（5）的内容，下一次继续深入理解吧。文章打分：🌟🌟🌟🌟🌟，果然初创论文以及相关的tutorials才是带来灵感的更多碰撞。</p>
<script type="math/tex; mode=display">
log P(X)- \mathcal{D} [Q(z|X)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z|X)||P(z)]</script><h2 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h2><p>在短短三年内，变分自动编码器 (VAE) 已成为最流行的复杂分布无监督学习方法之一。 VAE 很有吸引力，因为它们建立在标准函数逼近器（神经网络）之上，并且可以使用随机梯度下降进行训练。 VAEs 已经显示出在生成多种复杂数据方面的前景，包括手写数字 [1, 2]、人脸 [1, 3, 4]、门牌号码 [5, 6]、CIFAR 图像 [6]、 场景 [4]、分割 [7] 和从静态图像预测未来 [8]。 本教程介绍了 VAE 背后的直觉，解释了它们背后的数学原理，并描述了一些经验行为。 本文假设读者没有变分贝叶斯方法的先验知识。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>“生成建模” (Generative modelling) 是机器学习的一个广泛领域，它处理分布 P(X) 的模型，在一些潜在的高维空间 $\mathcal{X}$中的数据点 $X$ 上定义。 例如，图像是一种流行的数据，我们可以为其创建生成模型。 每个“数据点”（图像）都有数千或数百万个维度（像素），生成模型的工作是以某种方式捕获像素之间的依赖关系，例如，附近像素具有相似的颜色，并被组织成对象。 “捕获”这些依赖关系的确切含义取决于我们想要对模型做什么。 一种简单的生成模型允许我们以数值方式计算 P(X)。在图像的情况下，看起来像真实图像的 $X$ 值应该获得高概率，而看起来像随机噪声的图像应该获得低概率。 然而，像这样的模型不一定有用：知道一个图像不太可能并不能帮助我们合成一个可能的图像。</p>
<p>相反，人们通常关心生成更多与数据库中已有的示例类似但又不完全相同的示例。 我们可以从原始图像数据库开始，然后合成新的、看不见的图像。 我们可能会采用植物等 3D 模型的数据库，并生成更多的模型来填充视频游戏中的森林。 我们可以采用手写文本并尝试生成更多手写文本。 像这样的工具实际上可能对图形设计师有用。 我们可以通过说我们根据某个未知分布 $P_{gt}(X) $得到分布的样本 X 来形式化这个设置，我们的目标是学习一个我们可以从中采样的模型 P，使得 P 尽可能类似于 $P_{gt}$。</p>
<p>训练这种类型的模型一直是机器学习社区中的一个长期问题，而且经典地，大多数方法都具有三个严重缺陷之一。 首先，他们可能需要对数据结构进行强有力的假设。 其次，他们可能会进行严格的近似，从而导致次优模型。 或者第三，他们可能依赖于像马尔可夫链蒙特卡罗这样的计算昂贵的推理程序。 最近，一些工作在通过反向传播将神经网络训练为强大的函数逼近器方面取得了巨大进展。 这些进步催生了有前景的框架，这些框架可以使用基于反向传播的函数逼近器来构建生成模型。</p>
<p>最流行的此类框架之一是变分自动编码器 ，这是本教程的主题。 这个模型的假设很弱，而且通过反向传播训练很快。 VAE 确实进行了近似，但考虑到高容量模型，这种近似引入的误差可以说是很小的。 这些特点使其受欢迎程度迅速上升。</p>
<p>本教程旨在对 VAE 进行非正式介绍，而不是关于它们的正式科学论文。 它针对那些可能对生成模型有用，但在变分贝叶斯方法和 VAE 所基于的“最小描述长度”编码模型方面可能没有很强背景的人。 本教程最初是为加州大学伯克利分校和卡内基梅隆大学的计算机视觉阅读小组提供的演示文稿，因此偏向于视觉受众。</p>
<h3 id="1-1-预备知识：潜在变量模型"><a href="#1-1-预备知识：潜在变量模型" class="headerlink" title="1.1 预备知识：潜在变量模型"></a>1.1 预备知识：潜在变量模型</h3><p>在训练生成模型时，维度之间的依赖关系越复杂，模型训练就越困难。以生成手写字符图像的问题为例。为简单起见，我们只关心数字 0-9 的建模。如果字符的左半部分包含 5 的左半部分，则右半部分不能包含 0 的左半部分，否则该字符将非常明显地不像任何真正的数字。直观地说，<strong>如果模型在为任何特定像素分配值之前首先决定要生成哪个字符，这会有所帮助。这种决策正式称为潜在变量。</strong>也就是说，在我们的模型绘制任何东西之前，它首先从集合 [0, …, 9] 中随机采样一个数字值 z，然后确保所有笔画都匹配该字符。$z$ 被称为“潜在”，因为仅给定模型生成的字符，我们不一定知道潜在变量的哪些设置生成了字符。我们需要使用诸如计算机视觉之类的东西来推断它。</p>
<p>在我们可以说我们的模型代表我们的数据集之前，我们需要确保对于数据集中的每个数据点 $X$，都有一个（或多个）潜在变量设置，这会导致模型生成与 $X$ 非常相似的东西 . 正式地说，假设我们在高维空间 $\mathcal{Z}$ 中有一个潜在变量 $z$ 的向量，我们可以根据在 $\mathcal{Z}$  上定义的一些概率密度函数 (PDF) $P(z)$ 轻松地对其进行采样。 </p>
<p>然后，假设我们有一系列确定性函数 $f (z; θ)$，由向量$\theta$在空间$\mathcal{\Theta}$参数化，其中 $f :\mathcal{Z}×\mathcal{\Theta}→\mathcal{X}$。 $f$ 是确定性的，但如果 $z$ 是随机的且 $θ$ 是固定的，则 $f (z; θ)$ 是空间 X 中的随机变量。 我们希望优化 $θ$，以便我们可以从 $P(z)$ 中采样 z，并且很有可能，$f (z; θ)$ 将类似于我们数据集中的 $X$。</p>
<p>为了使这个概念在数学上精确，我们的目标是在整个生成过程中最大化训练集中每个 $X$ 的概率，根据：</p>
<script type="math/tex; mode=display">
P(X) = \int P(X|z; θ)P(z)dz.</script><p>在这里，$f (z; θ)$ 已被分布 $P(X|z; θ)$ 取代，这使我们能够通过使用总概率定律（the law of total probability）明确 $X$ 对 $z$ 的依赖关系。 这个框架背后的直觉——称为“最大似然”——是如果模型可能产生训练集样本，那么它也可能产生相似的样本，而不太可能产生不同的样本。 在 VAE 中，此输出分布的选择通常是高斯分布，即 $P(X|z;θ) = \mathcal{N}(X|f(z;θ),σ^2 ∗ I)$。也就是说，它的均值 $f(z;θ)$ 和协方差等于单位矩阵 $I$ 乘以某个标量 $σ$（这是一个超参数）。这种替换对于形式化直觉是必要的，即某些 $z$ 需要产生仅像 $X$ 的样本。一般来说，特别是在训练初期，我们的模型不会产生与任何特定 $X$ 相同的输出。通过高斯分布，我们可以使用梯度下降（或任何其他优化技术）通过使某些$z$的 $f(z; θ)$ 逼近$ X$来增加 P(X)，即在生成模型下逐渐使训练数据更有可能。 如果 $P(X|z)$ 是 Dirac delta 函数，这将是不可能的，就像我们确定性地使用 $X = f (z; θ)$ 一样！ 请注意，输出分布不需要是高斯分布：例如，如果 $X$ 是二进制的，那么 $P(X|z) $可能是由 $f (z; θ)$ 参数化的伯努利。 重要的性质很简单，$P(X|z)$ 可以计算，并且在 $θ$ 上是连续的。 从这里开始，我们将从 $f (z; θ)$ 中省略 $θ$ 以避免混乱。</p>
<h2 id="2-Variational-Autoencoders"><a href="#2-Variational-Autoencoders" class="headerlink" title="2 Variational Autoencoders"></a>2 Variational Autoencoders</h2><p>VAEs 的数学基础实际上与经典的自编码器关系不大，例如 稀疏自编码器或去噪自编码器。 根据图 1 所示的模型，VAE 近似最大化Equation 1。它们被称为“自动编码器”只是因为从该设置导出的最终训练目标确实具有编码器和解码器，并且类似于传统的自动编码器。 <strong>与稀疏自编码器不同，通常没有类似于稀疏惩罚的调整参数。</strong></p>
<p><img src="/images/vae_tutorials_fig1.png" alt="vae_tutorials_fig1"></p>
<p>与稀疏和去噪自编码器不同，我们可以直接从 $P(X)$ 中采样（无需执行马尔可夫链蒙特卡罗）。要解决方程 1，VAE 必须处理两个问题：如何定义潜在变量 $z$（即决定它们代表什么信息），以及如何处理 $z$ 上的积分。 VAE 对两者都给出了明确的答案。</p>
<p>首先，我们如何选择潜在变量 $z$ 以捕获潜在信息？ 回到我们的数字示例，模型在开始绘制数字之前需要做出的“潜在”决定实际上相当复杂。 它不仅需要选择数字，还需要选择绘制数字的角度、笔画宽度以及抽象的风格属性。 更糟糕的是，这些属性可能是相关的：如果写得更快，可能会产生更多倾斜的数字，这也可能会导致笔画更细。理想情况下，我们希望避免手动决定 $z$ 的每个维度编码的信息（尽管我们可能希望为某些维度手动指定它）。 我们还希望避免明确描述 $z$ 维度之间的依赖关系——即潜在结构。 VAE 采取了一种不同寻常的方法来处理这个问题：他们假设 $z$ 的维度没有简单的解释，而是断言 $z$ 的样本可以从一个简单的分布中抽取，即 $\mathcal{N} (0, I)$，其中 $I$ 是单位矩阵。 </p>
<p><img src="/images/vae_tutorials_dist_mapping.png" alt="vae_tutorials_dist_mapping"></p>
<p><strong>关键是要注意，任何 $d$ 维分布都可以通过取一组正态分布的 $d$ 变量并通过足够复杂的函数映射它们来生成样本。</strong>例如，假设我们想构造一个值位于环上的二维随机变量。如果 $z$ 是二维且正态分布的，则 $g(z) = z/10 + z/||z||$ 大致呈环形，如图 2 所示。因此，如果提供强大的函数逼近器，我们可以简单地学习一个函数，该函数将我们独立的、正态分布的 $z$ 值映射到模型可能需要的任何潜在变量，然后映射那些$X$ 的潜在变量。事实上，回想一下 $P(X|z; θ) = \mathcal{N} (X| f (z; θ), σ^2 ∗ I)$。如果$f (z; θ)$ 是一个多层神经网络，那么我们可以想象网络使用它的前几层将正态分布的 $z$ 映射到潜在值（如数字标识、笔画粗细、角度等）完全正确的统计数据。然后它可以使用后面的层将这些潜在值映射到一个完全渲染的数字。一般来说，我们不需要担心确保潜在结构存在。如果这种潜在结构有助于模型准确地再现（即最大化）训练集的可能性，那么网络将在某个层学习该结构。</p>
<p><img src="/images/vae_tutorials_fig3.png" alt="vae_tutorials_fig3"></p>
<p>现在剩下的就是最大化 Equation 1，其中 $P(z) = \mathcal{N} (z|0, I)$。 正如机器学习中常见的那样，如果我们可以找到 $P(X)$ 的可计算公式，并且我们可以采用该公式的梯度，那么我们就可以使用随机梯度上升来优化模型。 实际上，近似计算 $P(X)$ 在概念上很简单：我们首先对大量 $z$ 值 $\{z_1, …, z_n\}$ 进行采样，然后计算 $P(X) ≈ \frac{1}{n} ∑_i P(X|z_i)$。 这里的问题是在高维空间中，在我们准确估计 $P(X)$ 之前，$n$ 可能需要非常大。 要了解原因，请考虑我们的手写数字示例。假设我们的数字数据点存储在像素空间中，在 28x28 图像中，如图 3 所示。 由于 $P(X|z)$ 是各向同性高斯分布 (isotropic Gaussian)，因此 $X$ 的负对数概率是$f (z)$ 和 $X$ 之间的欧几里德距离的平方的比例 . 假设图 3(a) 是我们试图为其寻找 $P(X)$ 的目标 ($X$)。 产生图 3(b) 所示图像的模型可能是一个糟糕的模型，因为这个数字不像 2。因此，我们应该设置我们的高斯分布的 $σ$ 超参数，使得这种错误的数字不会 对 $P(X)$ 做出贡献。 另一方面，产生图 3(c)（与 $X$ 相同但向下和向右移动半个像素）的模型可能是一个很好的模型。 我们希望这个样本对 $P(X)$ 有贡献。然而不幸的是，我们不能同时拥有它：<strong>$X$ 和图 3(c) 之间的平方距离是 $.2693$（假设像素范围在 0 和 1 之间），但在 $X$ 和图 3(b) 之间它只是 $.0387$。 这里的教训是，为了拒绝像图 3(b) 这样的样本，我们需要将 $σ$ 设置得非常小，这样模型需要生成比图 3(c) 更像 X 的东西！</strong> 即使我们的模型是一个准确的数字生成器，我们也可能需要对数千个数字进行采样，然后才能生成与图 3(a) 中的 2 非常相似的 2。 <strong>我们可能会通过使用更好的相似性度量来解决这个问题，但在实践中，这些很难在视觉等复杂领域进行工程设计，而且如果没有指示哪些数据点彼此相似的标签，它们也很难训练。 相反，VAE 会改变采样程序以使其更快，而不会改变相似性度量，解决了这个棘手的问题。</strong></p>
<h3 id="2-1-设定目标函数"><a href="#2-1-设定目标函数" class="headerlink" title="2.1 设定目标函数"></a>2.1 设定目标函数</h3><p>在使用采样来计算公式 1 时，我们可以采取什么捷径吗？ <strong>实际上，对于大多数 $z$，$P(X|z)$ 几乎为零，因此对我们对 $P(X)$ 的估计几乎没有贡献。 变分自编码器背后的关键思想是尝试对可能产生 X 的 z 值进行采样，并仅从这些值中计算 P(X)。</strong> 这意味着我们需要一个新函数 $Q(z|X)$，它可以取 $X$ 的值，并为我们提供可能产生 $X$ 的 $z$ 值的分布。<strong>希望可能在 $Q$ 下的 $z$ 值的空间会远小于所有可能在先验 $P(z)$ 下的 $z$ 的空间。</strong> 例如，这让我们可以相对容易地计算 $E_{z∼Q}P(X|z)$。但是，如果 $z$ 是从PDF $Q(z)$的任意分布中采样的，而不是 $\mathcal{N}(0, I)$，那么这如何帮助我们优化 $P(X)$？ 我们需要做的第一件事是关联 $E_{z∼Q}P(X|z)$ 和 $P(X)$。 稍后我们将看到 $Q$ 的来源。</p>
<p>$E_{z∼Q}P(X|z)$ 和$P(X)$之间的关系是变分贝叶斯方法的基石之一。 我们首先定义 $P(z|X)$ 和 $Q(z)$ 之间的 Kullback-Leibler 散度（KL 散度或 $\mathcal{D}$），对于一些任意 $Q$（可能取决于也可能不取决于 $X$）：</p>
<script type="math/tex; mode=display">
D [Q(z)∥P(z|X)] = E_{z∼Q} [log Q(z) − log P(z|X)] .</script><p>通过对 $P(z|X)$ 应用贝叶斯规则$P(z|X)=\frac{P(X|z)\cdot P(z)}{P(X)}$，我们可以将 $P(X)$ 和 $P(X|z)$ 都放入这个方程中：</p>
<script type="math/tex; mode=display">
D [Q(z)∥P(z|X)] = E_{z∼Q} [log Q(z) − log P(X|z) − log P(z)] + log P(X).</script><p>这里，$log P(X)$ 跳出了期望$E$，因为它不依赖于 $z$。 将两边取反、重新排列，将 $E_{z∼Q}$ 的部分项总结为另一个 KL 散度项，产生：</p>
<script type="math/tex; mode=display">
log P(X)- \mathcal{D} [Q(z)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z)||P(z)]</script><p>请注意，$X$ 是固定的，并且 $Q$ 可以是<strong>任何</strong>分布，而不仅仅是将 $X$ 很好地映射到可以产生 $X$ 的 $z$ 的分布。 由于我们对推断 $P(X)$ 感兴趣，因此构造一个 $Q$ 确实取决于（does depend on） $X$，特别是使 $\mathcal{D} [Q(z)∥P(z|X)]$ 变小的一个是有意义的：</p>
<script type="math/tex; mode=display">
log P(X)- \mathcal{D} [Q(z|X)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z|X)||P(z)]</script><p><strong>这个方程是变分自编码器的核心，值得花一些时间思考它所说的内容</strong>（从历史上看，这个数学公式（尤其是公式 5）早在 VAE 之前就已为人所知。 例如， Helmholtz Machines [16]（见公式 5）使用几乎相同的数学，但有一个关键区别。 期望中的积分被 Dayan [16] 的总和 (sum) 所取代。 因为Helmholtz Machines假设潜在变量为离散分布。 这种选择可以避免一些转换使 VAE 中的梯度下降易于处理。） 在以上两个公式中，左边有我们想要最大化的量：$log P(X)$（加上一个误差项，这使得 $Q$ 产生可以重现给定 $X$ 的 $z$；如果 $Q$ 是高容量的，这个项会变小 ）。 右侧是我们可以通过随机梯度下降优化 $Q$ 的正确选择（尽管它可能还不明显）。 <strong>请注意，该框架——尤其是等式 5 的右侧——突然采用了一种看起来像自动编码器的形式，因为 $Q$ 将 $X$“编码”为 $z$，而 $P$ 正在“解码”它以重建 $X$。</strong>我们’ 稍后将更详细地探讨这种联系。</p>
<p>现在详细了解方程 5。从左侧开始，我们最大化 $log P(X)$，同时最小化 $\mathcal{D}[Q(z|X)∥P(z|X)]$。 $P(z|X)$ 不是我们可以分析计算的：它描述了 $z$ 的值，这些值可能会在我们的模型下产生像图 1 中的 $X$ 这样的样本。 然而，左边的第二项拉动 $Q( z|x)$ 匹配 $P(z|X)$。 假设我们对 $Q(z|x)$ 使用任意高容量模型，那么 $Q(z|x)$ 将有望实际匹配 $P(z|X)$，在这种情况下，这个 KL-散度项将为零，我们将 直接优化 $log P(X)$。 作为额外的奖励，我们使棘手的 $P(z|X)$ 变得易于处理：我们可以只使用 $Q(z|x)$ 来计算它。</p>
<h3 id="2-2-最优化目标函数"><a href="#2-2-最优化目标函数" class="headerlink" title="2.2 最优化目标函数"></a>2.2 最优化目标函数</h3><p>那么我们如何在等式 5 的右侧执行随机梯度下降呢？ 首先，我们需要更具体地了解 $Q(z|X)$ 将采用的形式。 通常的选择是说 $Q(z|X) = \mathcal{N} (z|μ(X; θ), Σ(X; θ))$，其中 $μ$ 和 $Σ$ 是任意确定性函数，其参数 θ 可以从数据中学习 （我们将在后面的方程中省略 θ）。 在实践中，μ 和 Σ 再次通过神经网络实现，并且 $Σ$ 被约束为对角矩阵。 这种选择的优点是计算性的，因为它们清楚地说明了如何计算右侧。 最后一项——$D [Q(z|X)∥P(z)]$——现在是两个多元高斯分布之间的 KL 散度，可以用封闭形式计算为：</p>
<script type="math/tex; mode=display">
D[\mathcal{N}(μ_0, Σ_0)∥ \mathcal{N}(μ_1, Σ_1)] = \frac{1}{2} 􏰃(tr(􏰃Σ^{-1}_1Σ_0) 􏰄+(μ_1 −μ_0 )^⊤ Σ^{-1}_1(μ_1−μ_0 )−k+log􏰃(\frac{detΣ_1}{detΣ_0}) 􏰄)</script><p>其中 $k$ 是分布的维数。 在我们的例子中，这简化为：</p>
<script type="math/tex; mode=display">
D[\mathcal{N}(μ(X), Σ(X))∥ \mathcal{N} (0, I)] = \frac{1}{2}(tr (Σ(X)) + (μ(X))⊤ (μ(X)) − k − log det (Σ(X)) 􏰄 .</script><p>公式 5 右侧的第一项有点棘手。 我们可以使用采样来估计 $E_{z∼Q} [log P(X|z)]$，但是要得到一个好的估计需要通过 $f$ 传递 $z$ 的许多样本，这将是昂贵的。 因此，作为随机梯度下降的标准，我们取 $z$ 的一个样本并将该 $z$ 的 $log P(X|z)$ 视为 $E_{z∼Q} [log P(X|z)]$ 的近似值。 毕竟，我们已经在对从数据集 $D$ 中采样的不同 $X$ 值进行随机梯度下降。因此我们想要优化的目标函数整理如下：</p>
<script type="math/tex; mode=display">
E_{X∼D} [log P(X) − \mathcal{D} [Q(z|X)∥P(z|X)]] = E_{X∼D} [E_{z∼Q} [log P(X|z)] − \mathcal{D} [Q(z|X)∥P(z)]] .</script><p>如果我们取这个方程的梯度，梯度符号可以移动到期望中。 因此，我们可以从分布 $Q(z|X)$ 中采样单个 $X$ 值和单个 $z$ 值，并计算梯度： </p>
<script type="math/tex; mode=display">
log P(X|z) − \mathcal{D} [Q(z|X)∥P(z)] .</script><p>然后我们可以在 $X$ 和 $z$ 的任意多个样本上平均这个函数的梯度，结果收敛到方程 8 的梯度。</p>
<p>然而，方程 9 存在一个重大问题。 $E_{z∼Q} [log P(X|z)]$ 不仅取决于 $P$ 的参数，还取决于 $Q$ 的参数。 然而，在方程 9 中，这种依赖性有 消失了！ 为了使 VAE 工作，必须驱动 $Q$ 为 $X$ 生成 $P$ 可以可靠解码的代码。 从不同的角度来看问题，等式 9 中描述的网络与图 4（左）所示的网络非常相似。 该网络的前向传递工作正常，如果输出对 $X$ 和 $z$ 的许多样本进行平均，则会产生正确的预期值。</p>
<p><img src="/images/vae_fig4.png" alt="vae_fig4"></p>
<p>然而，我们需要通过一个从 $Q(z|X)$ 中采样 $z$ 的层来反向传播误差，这是一个非连续的操作，没有梯度。 通过反向传播的随机梯度下降可以处理随机输入，但不能处理网络中的随机单元！ 这种方法，被称为“重新参数化技巧”，解决方案是将采样移动到输入层。 给定 $μ(X)$ 和 $Σ(X)$——$Q(z|X)$ 的均值和协方差——我们可以通过第一次采样 $ε ∼ N (0, I)$ 从 $\mathcal{N} (μ(X), Σ(X))$ 中采样 ，然后计算 $z = μ(X) + Σ^{1/2}(X) ∗ ε$。 因此，我们实际取梯度的方程是：</p>
<script type="math/tex; mode=display">
E_{X\sim D}E_{\epsilon \sim \mathcal{N}(0, I)} [log P(X|z = μ(X) + Σ^{1/2}(X) ∗ ε)] − \mathcal{D} [Q(z|X)∥P(z)]􏰊</script><p>这在图 4（右）中示意性地显示。 请注意，没有任何期望与依赖于我们模型参数的分布有关，因此我们可以安全地将梯度符号移动到它们中，同时保持相等。 也就是说，给定一个固定的 $X$ 和 $ε$，这个函数在 $P$ 和 $Q$ 的参数中是确定性和连续的，这意味着反向传播可以计算出一个适用于随机梯度下降的梯度。 值得指出的是，“重新参数化技巧”只有在我们可以通过评估函数 $h(η, X)$ 从 $Q(z|X)$ 中采样时才有效，其中 $η$ 是来自未被学习的分布的噪声。 此外，$h$ 必须在 $X$ 中连续，以便我们可以反向传播它。 这意味着 $Q(z|X)$（因此 $P(z)$）不能是离散分布！ 如果 $Q$ 是离散的，那么对于固定的 $η$，要么 $h$ 需要忽略 $X$，要么需要某个点 $h(η, X)$ 从 $Q$ 的样本空间中的一个可能值“跳跃”到另一个，即 不连续性。</p>
<h3 id="2-3-测试学习到的模型"><a href="#2-3-测试学习到的模型" class="headerlink" title="2.3 测试学习到的模型"></a>2.3 测试学习到的模型</h3><p><img src="/images/vae_fig5.png" alt="vae_fig5"></p>
<p>在测试时，当我们想要生成新样本时，我们只需将 $z ∼ N (0, I)$ 的值输入到解码器中。 也就是说，我们移除了“编码器”，包括会改变 $z$ 分布的乘法和加法运算。 这个（非常简单的）测试时间网络如图 5 所示。</p>
<p>假设我们要评估模型下测试示例的概率。 这通常是难以处理的。 但是请注意，$\mathcal{D} [Q(z|X)∥P(z|X)]$ 是正数，这意味着等式 5 的右侧是 P(X) 的下限。 由于对 z 的期望，这个下界仍然不能完全以封闭形式计算，这需要采样。 然而，从 $Q$ 中采样 $z$ 给出了期望的估计量，其收敛速度通常比第 2 节中讨论的从 $\mathcal{N} (0, I)$ 中采样 $z$ 快得多。因此，这个下界可以成为获得粗略想法的有用工具，例如我们的模型捕获特定数据点 $X$ 的效果如何。</p>
<h2 id="2-4-直观理解目标函数"><a href="#2-4-直观理解目标函数" class="headerlink" title="2.4 直观理解目标函数"></a>2.4 直观理解目标函数</h2><p>到现在为止，您希望相信 VAE 中的学习是易于处理的，并且它优化了我们整个数据集的 $log P(X)$ 之类的东西。 然而，我们并没有精确地优化 $log P(X)$，因此本节旨在更深入地了解目标函数实际在做什么。 我们讨论三个主题。 首先，我们探求通过优化 $\mathcal{D}[Q(z|X)∥P(z|X)]$，会给 $log P(X)$引入多少误差。 其次，我们描述了 VAE 框架——尤其是 r.h.s. 等式 5——在信息论方面，将其与基于最小描述长度的其他方法联系起来。 最后，我们研究了 VAE 是否具有类似于稀疏自编码器中的稀疏惩罚的“正则化参数”。</p>
<h3 id="2-4-1-来自-mathcal-D-Q-z-X-∥P-z-X-的误差"><a href="#2-4-1-来自-mathcal-D-Q-z-X-∥P-z-X-的误差" class="headerlink" title="2.4.1 来自 $ \mathcal{D}[Q(z|X)∥P(z|X)]$的误差"></a>2.4.1 来自 $ \mathcal{D}[Q(z|X)∥P(z|X)]$的误差</h3><p>该模型的易处理性依赖于我们的假设，即 $Q(z|X)$ 可以建模为具有某均值 $μ(X)$ 和方差 $Σ(X)$的高斯分布。<strong>当且仅当 $\mathcal{D}[Q(z|X)∥P(z|X)]$ 趋近于0时，$P(X)$ 收敛（在分布中）到真实分布。</strong>不幸的是，要确保这种情况发生并不容易。即使我们假设 $μ(X)$ 和 $Σ(X)$ 是任意高的容量，对于我们用来定义 $P$ 的任意 $f$ 函数，后验 $P(z|X)$ 不一定是高斯的。对于固定的 $P$，这可能意味着 $D[Q(z|X)∥P(z|X)]$ 永远不会变为零。然而，好消息是，给定足够高容量的神经网络，有许多 $f$ 函数会导致我们的模型生成任何给定的输出分布。这些函数中的任何一个都会同样好地最大化 $log P(X)$。因此，我们所需要的只是一个函数 $f$，它既最大化 $log P(X)$ 又导致 $P(z|X)$ 对于所有 $X$ 都是高斯分布的。如果是这样，$ \mathcal{D}[Q(z|X)∥P(z|X) ]$ 会将我们的模型拉向分布的参数化。那么，对于我们可能想要近似的所有分布，这样的函数是否存在？我还没有意识到有人总体上证明了这一点，但事实证明，如果 $σ$ 相对于真实分布的 CDF 的曲率很小（至少在 1D 中），则可以证明这样的函数确实存在。 （证据包含在附录 A）中。 在实践中，这么小的 $σ$ 可能会给现有的机器学习算法带来问题，因为梯度会变得很糟糕。 <strong>然而，令人欣慰的是，至少在这种情况下，VAE 的近似误差为零。 这一事实表明，未来的理论工作可能会向我们展示 VAE 在更实际的设置中具有多少近似误差。</strong> （在我看来，应该可以将附录 A 中的证明技术扩展到多个维度，但这留待以后的工作。）</p>
<h3 id="2-4-2-信息论解释"><a href="#2-4-2-信息论解释" class="headerlink" title="2.4.2 信息论解释"></a>2.4.2 信息论解释</h3><p>查看等式 5 右侧的另一个重要方式是信息论，特别是“最小描述长度”原则，它激发了许多 VAE 的前辈，如亥姆霍兹机 [16]、Wake-睡眠算法 [17]、深度信念网 [18] 和玻尔兹曼机 [19]。 - $log P(X) $可以看作是在我们的模型下使用理想编码构造给定 $X$ 所需的总位数。等式 5 的右侧将此视为构造 $X$ 的两步过程。我们首先使用一些位来构造 $z$。回想一下，KL 散度以位（或更准确地说，nats）为单位。具体来说，$\mathcal{D}[Q(z|X)||P(z)]$ 是将 $P(z)$ 中的无信息样本转换为 $Q(z|X)$ 中的样本所需的预期信息（所谓的KL 散度的“信息增益”解释）。也就是说，当 $z$ 来自 $Q(z|X)$ 而不是来自 $P(z)$ 时，它测量我们获得的关于 $X$ 的额外信息量（有关更多详细信息，请参阅 [20, 21] 的“bits back”参数） ）。在第二步中，$P(X|z)$ 测量在理想编码下从 $z$ 重建 $X$ 所需的信息量。因此，总位数 (− $log P(X)$) 是这两个步骤的总和，减去我们为 $Q$ 作为次优编码 ($\mathcal{D}[Q(z|X)||P(z) |X)]$)。</p>
<h3 id="2-4-3-VAEs和正则化参数"><a href="#2-4-3-VAEs和正则化参数" class="headerlink" title="2.4.3 VAEs和正则化参数"></a>2.4.3 VAEs和正则化参数</h3><p>查看等式 5，将 $\mathcal{D}[Q(z|X)||P(z)]$ 视为正则化项很有趣，很像稀疏自编码器中的稀疏正则化 [10]。 从这个角度来看，询问变分自编码器是否有任何“正则化参数”是很有趣的。 也就是说，在稀疏自编码器目标中，我们在目标函数中有一个 $λ$ 正则化参数，如下所示：</p>
<script type="math/tex; mode=display">
∥\phi(ψ(X)) − X∥^2 + λ∥ψ(X)∥_0</script><p>其中 $ψ$ 和 $\phi$ 分别是编码器和解码器函数，$∥·∥_0$ 是鼓励编码稀疏的 $L_0$ 范数。 这个 $λ$ 必须手动设置。</p>
<p><strong>然而，变分自编码器通常没有这样的正则化参数，这很好，因为程序员需要调整的参数少了一个。</strong> 然而，对于某些模型，我们可以让它看起来像这样一个正则化参数存在。 很容易认为这个参数可以来自改变 $z ∼N(0, I)$ 到类似 $z′ ∼ \mathcal{N}(0,λ∗I)$ 的东西，但事实证明这不会改变模型。要看到这一点，请注意我们可以将这个常数吸收到 $P$ 中 和 $Q$ 将它们写成 $f’(z’) = f(z’/λ), μ’(X) = μ(X) ∗ λ$, 并且 $Σ’(X) = Σ(X) ∗ λ^2$。 这将产生一个目标函数，其值（等式 5 的右侧）与我们在 $z ∼ \mathcal{N} (0, I)$ 中的损失相同。 此外，采样 $X$ 的模型将是相同的，因为 $z’/λ ∼ \mathcal{N} (0, I)$。</p>
<p>然而，正则化参数可以来自另一个地方。回想一下，对于我们提供的一些 $σ$，连续数据的输出分布的一个很好的选择是 $P(X|z) ∼ \mathcal{N} ( f (z), σ^2 ∗ I)$。因此，$log P(X|z) = C − \frac{1}{2} ∥X − f (z)∥^2/σ^2$（其中 $C$ 是一个不依赖于 $f$ 的常数，在优化过程中可以忽略）。当我们编写完整的优化目标时，$σ$ 出现在等式5的 r.h.s. 的第二项，但不是第一个；从这个意义上说，我们选择的 $σ$ 表现得像控制两项权重的 $λ$。但是请注意，此参数的存在取决于我们对给定 $z$ 的 $X$ 分布的选择。如果 $X$ 是二元的并且我们使用伯努利输出模型，那么这个正则化参数就会消失，唯一的方法就是使用复制 $X$ 的维度之类的技巧。从信息论的角度来看，这是有道理的：当 $X$ 是二进制，我们实际上可以计算编码 $X$ 所需的位数，公式 5 右侧的两项都使用相同的单位来计算这些位数。但是，当 $X$ 是连续的时，每个样本都包含无限的信息。我们对 $σ$ 的选择决定了我们期望模型重建 $X$ 的准确程度，这是必要的，以便信息内容可以变得有限。</p>
<h2 id="3-条件变分自编码器"><a href="#3-条件变分自编码器" class="headerlink" title="3. 条件变分自编码器"></a>3. 条件变分自编码器</h2><p>让我们回到生成手写数字的运行示例。 假设我们不只是想生成新数字，而是想将数字添加到由一个人编写的现有数字字符串中。 这类似于计算机图形学中一个真正实际的问题，称为填孔（hole filling）：给定一个现有图像，其中用户删除了不需要的对象，目标是用看似合理的像素填充孔。这两个问题的一个重要困难是似真输出的空间是多模态的：下一个数字或外推像素有很多可能性。 在这种情况下，标准回归模型将失败，因为训练目标通常会惩罚单个预测与真实情况之间的距离。 面对这样的问题，回归器可以产生的最佳解决方案是介于可能性之间的东西，因为它最小化了预期距离。在数字的情况下，这很可能看起来像一个毫无意义的模糊，它是所有可能的数字和所有可能出现的样式的“平均图像”。 我们需要的是一种算法，它接受一个字符串或一个图像，并产生一个复杂的多模态分布，我们可以从中进行采样。 进入条件变分自编码器 (CVAE)，它通过简单地调节输入的整个生成过程来修改上一节中的数学。 CVAE 允许我们解决输入到输出映射是一对多的问题，而无需我们明确指定输出分布的结构。</p>
<p>给定一个输入 $X$ 和一个输出 $Y$，我们想创建一个模型 $P(Y|X)$，这最大限度地提高了基本事实的概率（我很抱歉在这里重新定义 $X$。但是，标准机器学习符号将 $X$ 映射到 $Y$，所以我也会这样做）。 我们通过引入一个潜在变量 $z ∼ N (0, I)$ 来定义模型，使得：</p>
<script type="math/tex; mode=display">
P(Y|X) = \mathcal{N} ( f (z, X), σ^2 ∗ I)</script><p>其中 $f$ 是我们可以从数据中学习的确定性函数。 我们可以将方程 2 到 5 对 $X$ 的条件重写如下：</p>
<script type="math/tex; mode=display">
\mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] = E_{z∼Q(·|Y,X)} [log Q(z|Y, X) − log P(z|Y, X)</script><script type="math/tex; mode=display">
\mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] =
E_{z∼Q(·|Y,X)} [log Q(z|Y, X) − log P(Y|z, X) − log P(z|X)] + log P(Y|X)</script><script type="math/tex; mode=display">
log P(Y|X) − \mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] =
E_{z∼Q(·|Y,X)} [log P(Y|z, X)] − \mathcal{D} [Q(z|Y, X)∥P(z|X)]</script><p>请注意，$P(z|X)$ 仍然是 $\mathcal{N} (0, I)$，因为我们的模型假设 $z$ 在测试时独立于 $X$ 进行采样。 该模型的结构如下图6。</p>
<p><img src="/images/vae-fig6.png" alt="vae-fig6"></p>
<p>在测试时，我们可以简单地从分布 $P(Y|X)$ 中采样 $z ∼ \mathcal{N} (0, I)$。</p>
<h2 id="4-案例"><a href="#4-案例" class="headerlink" title="4. 案例"></a>4. 案例</h2><h3 id="4-1-MNIST-变分自编码器"><a href="#4-1-MNIST-变分自编码器" class="headerlink" title="4.1 MNIST 变分自编码器"></a>4.1 MNIST 变分自编码器</h3><p><img src="/images/vae-fig7.png" alt="vae-fig7"></p>
<p>为了演示 VAE 框架的分布学习能力，让我们在 MNIST 上训练一个变分自编码器。 为了表明该框架并不严重依赖于初始化或网络结构，我们不使用现有的已发布的 VAE 网络结构，而是采用 Caffe 中包含的基本 MNIST AutoEncoder 示例。 （然而，我们使用 ReLU 非线性 和 ADAM，因为两者都是加速收敛的标准技术。）虽然 MNIST 是实值的，但它被限制在 0 和 1 之间，所以我们对 $P(X|z)$ 使用 Sigmoid 交叉熵损失。 这有一个概率解释：假设我们通过将每个维度独立采样为 $Xi’ ∼ Bernoulli(Xi)$ 来创建一个新的数据点 $X’$。 交叉熵测量 $X’$ 的预期概率。 因此，我们实际上是在建模 $X’$，即 MNIST 的随机二值化版本，但我们只给出了该数据 $X$ 的 $q$ 摘要。诚然，这与 VAE 框架规定的不太一样，但在实践中运行良好，并且用于其他 VAE 文献。 尽管我们的模型比 [1] 和 [3] 深得多，但训练模型并不困难。 训练只运行了一次（尽管训练重新开始了 5 次以找到使损失下降最快的学习率）。 噪声产生的数字如图 7 所示。值得注意的是，这些样本很难评估，因为没有简单的方法来衡量这些样本与训练集的不同 [24]。 然而，失败案例很有趣：虽然大多数数字看起来很现实，但重要的数字是“中间”不同的数字。 例如，最左边一列从上数的第七个数字显然介于 7 和 9 之间。发生这种情况是因为我们正在通过平滑函数映射连续分布。</p>
<p>在实践中，模型似乎对 $z$ 的维度非常不敏感，除非 $z$ 过大或过小。 $z$ 太少意味着模型无法再捕获所有变化：少于 4 个 $z$ 维度会产生明显更差的结果。 1,000 个 $z$ 的结果很好，但 10,000 个 $z$ 的结果也下降了。 理论上，如果有 $n$个$z$ 的模型是好的，那么有 $m&gt;&gt;n$ 的模型应该不会更糟，因为模型可以简单地学会忽略额外的维度。 然而，在实践中，当 $z$ 非常大时，随机梯度下降似乎很难保持 $D[Q(z|X)||P(z)]$ 低。</p>
<p><img src="/images/vae-fig8.png" alt="vae-fig8"></p>
<h2 id="5-MNIST-条件变分自编码器"><a href="#5-MNIST-条件变分自编码器" class="headerlink" title="5. MNIST 条件变分自编码器"></a>5. MNIST 条件变分自编码器</h2><p>我原本打算展示一个条件变分自动编码器，只给定每个数字的一半来完成 MNIST 数字。虽然 CVAE 为此目的工作得很好，但不幸的是，回归器实际上也工作得很好，产生了相对清晰的样本。明显的原因是 MNIST 的大小。与 4.1 节中的网络容量相似的网络可以轻松记住整个数据集，因此回归量过拟合严重。因此，在测试时，它产生的预测表现类似于最近邻匹配，实际上非常敏锐。当给定训练示例输出不明确时，CVAE 模型最有可能胜过简单回归。因此，让我们对问题进行两次修改以使其更加模糊，但代价是使其更加人为化。首先，输入是取自数字中间的一列像素。在 MNIST 中，每个像素都有一个介于 0 和 1 之间的值，这意味着即使在这一列像素中仍然有足够的信息供网络识别特定的训练示例。因此，第二个修改是用二进制值（0 或 1）替换我们列中的每个像素，选择 1 的概率等于像素强度。每次向网络传递一个数字时，都会对这些二进制值进行重新采样。图 8 显示了结果。请注意，回归器模型通过模糊其输出来处理歧义（尽管在某些情况下，回归器在做出错误猜测时非常自信，这表明过度拟合仍然是一个问题）。回归器输出中的模糊使与可能产生输入的许多数字集的距离最小化。另一方面，CVAE 通常会选择一个特定的数字来输出并且不会模糊，从而产生更可信的图像。</p>
<h2 id="附录A-一维情况下证明给定任意强大的学习器，VAE-的近似误差为零。"><a href="#附录A-一维情况下证明给定任意强大的学习器，VAE-的近似误差为零。" class="headerlink" title="附录A 一维情况下证明给定任意强大的学习器，VAE 的近似误差为零。"></a>附录A 一维情况下证明给定任意强大的学习器，VAE 的近似误差为零。</h2><p>基本上全部是数学公式证明，隐约感觉难以理解，暂时先不做过多赘述了。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ai-class-A-best-papers</title>
    <url>/2021/07/08/ai-class-A-best-papers/</url>
    <content><![CDATA[<h1 id="AI领域A类会议最佳论文"><a href="#AI领域A类会议最佳论文" class="headerlink" title="AI领域A类会议最佳论文"></a>AI领域A类会议最佳论文</h1><h2 id="List-of-papers"><a href="#List-of-papers" class="headerlink" title="List of papers"></a>List of papers</h2><p><strong>AAAI 2021</strong></p>
<ul>
<li>Best Paper Awards</li>
</ul>
<ol>
<li><p><a href="https://arxiv.org/pdf/2012.07436.pdf"><strong><em>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</em></strong></a></p>
<p><strong>Institution(s):</strong> Beihang University, UC Berkeley, Rutgers University, Beijing Guowang Fuda Science &amp; Technology Development Company</p>
<p><strong>Authors:</strong> Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2012.03083.pdf"><strong><em>Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory</em></strong></a></p>
<p><strong>Institution(s):</strong> Singapore University of Technology and Design</p>
<p><strong>Authors:</strong> Stefanos Leonardos, Georgios Piliouras</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2012.03083.pdf"><strong><em>Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory</em></strong></a></p>
<p><strong>Institution(s):</strong> Dartmouth College, University of Texas at Austin, Google AI</p>
<p><strong>Authors:</strong> Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, and Soroush Vosoughi</p>
</li>
</ol>
<ul>
<li>Best Paper Runners Up</li>
</ul>
<ol>
<li><p><strong><em><a href="https://arxiv.org/pdf/2009.12947.pdf">Learning From Extreme Bandit Feedback</a></em></strong></p>
<p><strong>Institution(s):</strong> UC Berkeley, University of Texas at Austin</p>
<p><strong>Authors:</strong> Romain Lopez, Inderjit Dhillon, Michael I. Jordan</p>
</li>
<li><p><strong><em><a href="https://arxiv.org/pdf/2004.11207.pdf">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></em></strong></p>
<p><strong>Institution(s):</strong> Beihang University, Microsoft Research</p>
<p><strong>Authors:</strong> Yaru Hao, Li Dong, Furu Wei, Ke Xu</p>
</li>
<li><p><strong><em><a href="https://arxiv.org/pdf/2009.06560.pdf">Dual-Mandate Patrols: Multi-Armed Bandits for Green Security</a></em></strong></p>
<p><strong>Institution(s):</strong> Harvard University, Carnegie Mellon University</p>
<p><strong>Authors:</strong> Lily Xu, Elizabeth Bondi, Fei Fang, Andrew Perrault, Kai Wang, Milind Tambe</p>
</li>
</ol>
<h2 id="AAAI-2021-Informer"><a href="#AAAI-2021-Informer" class="headerlink" title="AAAI 2021 - Informer"></a>AAAI 2021 - Informer</h2>]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Awesome-bloggers</title>
    <url>/2021/07/08/awesome-bloggers/</url>
    <content><![CDATA[<h1 id="Awesome-bloggers"><a href="#Awesome-bloggers" class="headerlink" title="Awesome bloggers"></a>Awesome bloggers</h1><h2 id="TTS"><a href="#TTS" class="headerlink" title="TTS"></a>TTS</h2><p><a href="https://rayeren.github.io">https://rayeren.github.io</a>   Yi Ren (任意)   FastSpeech</p>
<p><a href="https://www.microsoft.com/en-us/research/people/xuta/">https://www.microsoft.com/en-us/research/people/xuta/</a> Xu Tan (谭旭) FastSpeech 二作</p>
<p><a href="https://liusongxiang.github.io">https://liusongxiang.github.io</a> Songxiang Liu (刘颂湘)，PhD from Helen Meng 2021</p>
<p><a href="https://entn.at">https://entn.at</a> Ewald Enzinger, </p>
<h2 id="AI"><a href="#AI" class="headerlink" title="AI"></a>AI</h2><p><a href="https://dczha.com">https://dczha.com</a>   Daochen Zha    DouZero</p>
<p><a href="https://www.alanshawn.com">https://www.alanshawn.com</a>   Alan Xiang （项子越） 中山大学勤奋博主  </p>
<p><a href="https://kexue.fm">https://kexue.fm</a> 苏剑林 93年</p>
<h2 id="Good-resources"><a href="#Good-resources" class="headerlink" title="Good resources"></a>Good resources</h2><p><a href="https://clemense.github.io">https://clemense.github.io</a>.  Clemens Eppner   Nvidia</p>
<h2 id="Acadamy’s-great-scholar-leaders"><a href="#Acadamy’s-great-scholar-leaders" class="headerlink" title="Acadamy’s great scholar leaders"></a>Acadamy’s great scholar leaders</h2><p><a href="https://dl.acm.org/profile/81350580267/publications?Role=author&amp;pageSize=20&amp;startPage=1">https://dl.acm.org/profile/81350580267/publications?Role=author&amp;pageSize=20&amp;startPage=1</a>    Tie-Yan Liu   Microsoft</p>
<h1 id="Awesome-Company’s-Bloggers"><a href="#Awesome-Company’s-Bloggers" class="headerlink" title="Awesome Company’s Bloggers"></a>Awesome Company’s Bloggers</h1><h2 id="TTS-1"><a href="#TTS-1" class="headerlink" title="TTS"></a>TTS</h2><p><a href="https://speechresearch.github.io">https://speechresearch.github.io</a></p>
<p><a href="http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/">http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/</a></p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>简单的生活法则</title>
    <url>/2021/07/08/simple-rules-for-life-decisions/</url>
    <content><![CDATA[<h1 id="简单的生活法则"><a href="#简单的生活法则" class="headerlink" title="简单的生活法则"></a>简单的生活法则</h1><ol>
<li><p>Mental Models I Find Repeatedly Useful</p>
<p><a href="https://medium.com/@yegg/mental-models-i-find-repeatedly-useful-936f1cc405d">https://medium.com/@yegg/mental-models-i-find-repeatedly-useful-936f1cc405d</a></p>
</li>
<li><p>THREAD: 15 of the most useful razors and rules I’ve found. Rules of thumb that simplify decisions.</p>
<p><a href="https://twitter.com/george__mack/status/1350513143387189248">https://twitter.com/george__mack/status/1350513143387189248</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>End-to-end TTS &amp; VC 文章总结</title>
    <url>/2021/07/20/end2end/</url>
    <content><![CDATA[<h1 id="End-to-end-TTS-amp-VC"><a href="#End-to-end-TTS-amp-VC" class="headerlink" title="End-to-end TTS &amp; VC"></a>End-to-end TTS &amp; VC</h1><h2 id="TTS"><a href="#TTS" class="headerlink" title="TTS"></a>TTS</h2><p><a href="https://arxiv.org/pdf/2006.03575.pdf">END-TO-END ADVERSARIAL TEXT-TO-SPEECH</a> — EATS ~ ICLR 2021 ~ <strong>DeepMind</strong> ~ <a href="https://github.com/yanggeng1995/EATS">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.06103.pdf">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a> — VITS ~ ICML 2021 ~ <strong>KAIST</strong> ~ <a href="https://github.com/jaywalnut310/vits">repo</a></p>
<p><a href="https://arxiv.org/pdf/2011.03568v1.pdf">WAVE-TACOTRON: SPECTROGRAM-FREE END-TO-END TEXT-TO-SPEECH SYNTHESIS</a> — Wave-Tacotron ~ ICASSP 2021 ~ <strong>Google</strong></p>
<p><a href="https://arxiv.org/pdf/2106.09660.pdf">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</a> — WaveGrad 2 ~ Interspeech 2021 ~ <strong>Google (<em>Heiga Zen</em>)</strong> ~ <a href="https://github.com/mindslab-ai/wavegrad2">repo</a></p>
<p><a href="https://arxiv.org/pdf/2012.03500.pdf">EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture</a> — EFTS-Wac ~ ICML 2021 ~ <strong>PingAn (Chenfeng Miao)</strong> ~ <a href="https://github.com/liusongxiang/efficient_tts">repo</a></p>
<p><a href="https://arxiv.org/pdf/2006.04558.pdf">FASTSPEECH 2: FAST AND HIGH-QUALITY END-TO- END TEXT TO SPEECH</a> — Fastspeech2 ~ ICLR 2021 ~ <strong>Zhejiang U &amp; Microsoft</strong> ~ <a href="https://github.com/ming024/FastSpeech2">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.02830.pdf">Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</a> — Reinforce-Aligner ~ Interspeech 2021 ~ Korea University </p>
<h2 id="VC"><a href="#VC" class="headerlink" title="VC"></a>VC</h2><p><a href="https://arxiv.org/pdf/1904.04169.pdf">Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation</a> — Parrotron ~ Interspeech 2019 ~ <strong>Google</strong> ~ <a href="https://github.com/fd873630/Parrotron">repo</a></p>
<p><a href="https://export.arxiv.org/pdf/2104.07283">Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</a> — F0-VC ~ WIP ~ <strong>Sorbonne</strong></p>
<p><a href="https://arxiv.org/pdf/2106.00992.pdf">NVC-Net: End-to-End Adversarial Voice Conversion</a> — NVC-Net ~ UR ~ <strong>Sony</strong></p>
<p><a href="https://arxiv.org/pdf/2010.14150.pdf">FRAGMENTVC: ANY-TO-ANY VOICE CONVERSION BY END-TO-END EXTRACTING AND FUSING FINE-GRAINED VOICE FRAGMENTS WITH ATTENTION</a> — FRAGMENTVC ~ ICASSP 2021 ~ Audio2Mel ~ <strong>NTU (<em>Hung-yi Lee</em>)</strong> ~ <a href="https://github.com/yistLin/FragmentVC">repo</a></p>
<p><a href="https://arxiv.org/pdf/2002.03808.pdf">Vocoder-free End-to-End Voice Conversion with Transformer Network</a> — Transformer-VC ~ WIP ~ Raw_spectrum - to - raw_spectrum ~ <strong>KNU</strong></p>
<p><a href="https://arxiv.org/pdf/2104.02901v2.pdf">S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</a> S2VC ~ Interspeech 2021 ~ self-supervised ~ <strong>NTU (<em>Hung-yi Lee</em>)</strong> ~ <a href="https://github.com/howard1337/S2VC">repo</a></p>
<h2 id="METRIC"><a href="#METRIC" class="headerlink" title="METRIC"></a>METRIC</h2><p><a href="https://arxiv.org/pdf/2107.09392.pdf">SVSNet: An End-to-end Speaker Voice Similarity Assessment Model</a> — SVSNet ~ Similarity</p>
<p><a href="https://arxiv.org/pdf/2104.03017v2.pdf">Utilizing Self-supervised Representations for MOS Prediction</a> — MOS ~ NTU (<em>Hung-yi Lee</em>) ~ <a href="https://github.com/s3prl/s3prl">repo</a></p>
<p><a href="https://arxiv.org/pdf/1904.08352v3.pdf">MOSNet: Deep Learning-based Objective Assessment for Voice Conversion</a> — MOSNet ~ IISAS ~ <a href="https://github.com/lochenchou/MOSNet">repo</a></p>
<p><a href="https://arxiv.org/pdf/2104.11673v1.pdf">Deep Learning Based Assessment of Synthetic Speech Naturalness</a> — MOS ~ QUTTUB ~ <a href="https://github.com/gabrielmittag/NISQA">repo</a></p>
<p><a href="https://arxiv.org/pdf/2103.00110.pdf">MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network</a> — MOS ~ ICASSP 2021 ~ USTC &amp; Microsoft (Xu Tan) ~ <a href="https://github.com/sky1456723/Pytorch-MBNet">repo</a></p>
<h2 id="Others"><a href="#Others" class="headerlink" title="Others"></a>Others</h2><p><a href="https://arxiv.org/pdf/2102.04040.pdf">LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search</a> — LightSpeech ~ ICASSP 2021 ~ USTC &amp; Microsoft (Xu Tan) ~ <a href="https://github.com/rishikksh20/LightSpeech">repo</a></p>
<h2 id="Inspirations-from-ASR"><a href="#Inspirations-from-ASR" class="headerlink" title="Inspirations from ASR"></a>Inspirations from ASR</h2><p><a href="https://arxiv.org/pdf/2107.09428.pdf">Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models</a> — End2end ASR ~ Interspeech 2021 ~ Hopkins &amp; Yahoo Japan &amp; CMU </p>
<p><a href="https://arxiv.org/pdf/2105.10042.pdf">A Streaming End-to-End Framework For Spoken Language Understanding</a> — StreamSLU ~  IJCAI 2021 ~ University of Waterloo &amp; Huawei Noah’s Ark Lab &amp; Tsinghua University </p>
<p><strong>Tips:</strong> </p>
<ul>
<li><p>UR: Under Review</p>
</li>
<li><p>WIP: Work in Progress</p>
</li>
<li><p>KAIST：Korea Advanced Institute of Science and Technology，韩国科学技术院</p>
</li>
<li><p>KNU：Kyungpook National University，韩国庆北大学</p>
</li>
<li><p>IISAS：Institute of Information Science, Academia Sinica, Taipei, Taiwan，台湾中央研究院信息科学研究所</p>
</li>
<li><p>QUTTUB：Quality and Usability Lab, Technische Universita ̈t Berlin, Berlin, Germany，德国柏林工业大学质量和可用性实验室</p>
</li>
</ul>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>VITS</title>
    <url>/2021/07/15/vits/</url>
    <content><![CDATA[<h1 id="Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech"><a href="#Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech" class="headerlink" title="Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"></a>Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最近提出了几种支持单阶段训练和并行采样的端到端文本到语音（TTS）模型，但它们的样本质量与两阶段 TTS 系统的样本质量不匹配。在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。我们的方法采用变分推理，并通过归一化流程和对抗性训练过程进行增强，从而提高了生成建模的表达能力。我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量和随机持续时间预测器的不确定性建模，我们的方法表达了自然的一对多关系，其中可以以多种方式以不同的音高和节奏说出文本输入。对 LJ Speech（单个说话者数据集）的主观人类评估（平均意见分数或 MOS）表明，我们的方法优于最好的公开可用的 TTS 系统，并实现了与Ground-Truth实况相当的 MOS。 </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文本到语音 (TTS) 系统从给定文本通过多个组件合成原始语音波形。 随着深度神经网络的快速发展，除了文本规范化和音素化等文本预处理之外，TTS 系统管道已被简化为两阶段生成建模。 第一阶段是从预处理的文本中生成中间语音表示，如 Melspectrograms 或语言特征，第二阶段是生成以中间表示为条件的原始波形。 每个两级管道的模型都是独立开发的。</p>
<p>基于神经网络的自回归 TTS 系统已经显示出合成真实语音的能力，但它们的顺序生成过程使得现代并行处理器难以充分利用。为了克服这个限制并提高合成速度，已经提出了几种非自回归方法。在文本到谱图生成步骤中，尝试从预训练的自回归教师网络中提取注意力图，以降低学习文本和谱图之间对齐的难度.最近，基于似然的方法通过估计或学习最大化目标梅尔谱图的可能性的比对来进一步消除对外部比对器的依赖。同时，在第二阶段模型中探索了生成对抗网络（GAN）。基于 GAN 的前馈网络具有多个鉴别器，每个鉴别器区分不同尺度或周期的样本，实现高质量的原始波形合成。</p>
<p>尽管并行 TTS 系统取得了进展，但两级管道仍然存在问题，因为它们需要顺序训练或微调才能进行高质量的生产，其中后期模块使用早期模型生成的样本进行训练。此外，它们对预定义中间特征的依赖妨碍了应用学习到的隐藏表示来获得性能的进一步改进。最近，几项工作，即 FastSpeech 2s 和 EATS ，提出了有效的端到端训练方法，例如对短音频剪辑而不是整个波形进行训练，利用梅尔谱图解码器来帮助文本表示学习，并设计专门的谱图损失来减轻目标和生成语音之间的长度不匹配问题。然而，尽管通过利用学习的表示可能提高性能，但它们的合成质量落后于两阶段系统。 </p>
<p>在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。使用变分自编码器 (VAE)，我们通过潜在变量连接 TTS 系统的两个模块，以实现高效的端到端学习。为了提高我们方法的表达能力，以便可以合成高质量的语音波形，我们将标准化流应用于我们的条件先验分布和波形域的对抗训练。除了生成细粒度的音频之外，TTS 系统表达一对多关系也很重要，在这种关系中，文本输入可以以多种方式以不同的变化（例如，音调和持续时间）说出。为了解决一对多的问题，我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量的不确定性建模和随机持续时间预测器，我们的方法可以捕获无法用文本表示的语音变化。</p>
<p>与最好的公开可用的 TTS 系统 Glow-TTS和 HiFi-GAN相比，我们的方法获得了更自然的语音和更高的采样效率。 我们公开演示页面和源代码。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p>在本节中，我们将解释我们提出的方法及其架构。 所提出的方法主要在前三个小节中描述：条件 VAE 公式； 来自变分推理的对齐估计； 用于提高合成质量的对抗性训练。 本节末尾描述了整体架构。 图 1a 和 1b 分别显示了我们方法的训练和推理过程。 从现在开始，我们将把我们的方法称为端到端文本到语音（VITS）对抗学习的变分推理。</p>
<h3 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>VITS 可以表示为条件 VAE，其目标是最大化数据 $log p_θ (x|c)$ 的难以处理的边际对数似然的变分下界，也称为证据下界 (ELBO: the evidence lower bound)：</p>
<script type="math/tex; mode=display">
log p_θ (x|c) ≥ E_{q_{φ(z|x)}}  [log p_θ (x|z)−log \frac{q_φ(z|x)} {p_θ (z|c)}]</script><p>其中 $p_θ(z|c)$​ 表示给定条件 $c$​ 的潜在变量 $z$​ 的先验分布，$p_θ(x|z)$​ 是数据点 $x$​ 的似然函数，$q_φ(z|x)$​ 是近似后验分布。 训练损失是负的 ELBO，可以看作是重建损失的总和 $- log p_θ(x|z)$​ 和 KL 散度 $log q_φ(z|x) - log p_θ(z|c)$​，其中 $z ∼ q_φ(z|x)$​​。</p>
<h4 id="重构损失"><a href="#重构损失" class="headerlink" title="重构损失"></a>重构损失</h4><p>作为重建损失中的目标数据点，我们使用梅尔谱图而不是原始波形，用 $x_{mel}$ 表示。 我们通过解码器将潜在变量 $z$ 上采样到波形域 $\hat y$，并将 $\hat y$ 变换到梅尔谱图域 $\hat x_{mel}$。 然后将预测和目标梅尔谱图之间的 $L_1$ 损失用作重建损失：</p>
<script type="math/tex; mode=display">
L_{recon} = ∥x_{mel} − \hat x_{mel}∥_1</script><p>这可以看作是最大似然估计，假设数据分布为拉普拉斯分布并忽略常数项。 我们定义了梅尔谱域中的重建损失，以通过使用近似人类听觉系统响应的梅尔尺度来提高感知质量。 请注意，来自原始波形的梅尔谱图估计不需要可训练的参数，因为它只使用 STFT 和线性投影到梅尔尺度上。 此外，估计仅在训练期间使用，而不是在推理期间使用。 在实践中，我们不会对整个潜在变量 $z$ 进行上采样，而是使用部分序列作为解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p>
<h4 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL-散度"></a>KL-散度</h4><p>先验编码器 $c$ 的输入条件由从文本中提取的音素 $c_{text}$ 和音素和潜在变量之间的对齐 $A$ 组成。 对齐是一个硬单调注意矩阵，带有 $|c_{text}| × |z|$ 维度表示每个输入音素扩展到与目标语音时间对齐的时间。 因为对齐没有真实标签，我们必须在每次训练迭代时估计对齐，我们将在第 2.2.1 节中讨论。 在我们的问题设置中，我们旨在为后验编码器提供更多高分辨率信息。 因此，我们使用目标语音 $x_<br>{lin}$ 的线性频谱图而不是梅尔频谱图作为输入。 请注意，修改后的输入不会违反变分推理的属性。 KL散度则为：</p>
<script type="math/tex; mode=display">
L_{kl} =logq_φ(z|x_{lin})−logp_θ(z|c_{text},A),</script><script type="math/tex; mode=display">
z ∼ q_φ(z|x_{lin}) = N(z; μ_φ(x_{lin}), σ_φ(x_{lin}))</script><p>分解正态分布用于参数化我们的先验和后验编码器。 我们发现增加先验分布的表现力对于生成真实样本很重要。 因此，我们应用了归一化流 $f_θ$ ，它允许在分解的正态先验基础之上，按照变量变化规则将简单分布可逆变换为更复杂的分布分配：</p>
<script type="math/tex; mode=display">
p_{\theta} (z|c) = N(f_{\theta} (z);μ_{\theta} (c),σ_{\theta} (c))􏰀􏰀􏰀 | det \frac {∂f_θ(z)} {∂ z}􏰀􏰀􏰀|,</script><script type="math/tex; mode=display">
c = [c_{text}, A]</script><h3 id="对齐估计"><a href="#对齐估计" class="headerlink" title="对齐估计"></a>对齐估计</h3><h4 id="MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索"><a href="#MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索" class="headerlink" title="MONOTONIC ALIGNMENT SEARCH 单调对齐搜索"></a>MONOTONIC ALIGNMENT SEARCH 单调对齐搜索</h4><p>为了估计输入文本和目标语音之间的对齐 A，我们采用了单调对齐搜索 (MAS)（Kim 等人，2020 年），这是一种搜索对齐的方法，该方法可以最大化由归一化流 f 参数化的数据的可能性：</p>
<script type="math/tex; mode=display">
A = arg max_{\hat A} log p(x|c_{text} , \hat A) = arg max_{\hat A} log N(f(x); μ(c_{text}, \hat A), σ(c_{text}, \hat A))</script><p>在人类按顺序阅读文本而不跳过任何单词的事实之后，候选对齐被限制为单调和非跳过。 为了找到最佳对齐方式，Kim 等人 (2020) 使用动态规划。 在我们的设置中直接应用 MAS 很困难，因为我们的目标是 ELBO，而不是确切的对数似然。 因此，我们重新定义 MAS 以找到最大化 ELBO 的对齐，这简化为找到最大化潜在变量 $z$ 的对数似然的对齐：</p>
<script type="math/tex; mode=display">
arg max_{\hat A} log p_θ(x_{mel}|z) − log \frac {q_φ(z|x_{lin})} {p_θ(z|c_{text}, \hat A)}
= arg max_{\hat A} log p_θ(z|c_{text}, \hat A)</script><script type="math/tex; mode=display">
= log N(f_θ(z); μ_θ(c_{text}, \hat A), σ_θ(c_{text}, \hat A))</script><p>由于等式 5 与等式 6 相似，我们可以不加修改地使用原始 MAS 实现。 附录 A 包括 MAS 的伪代码。</p>
<h4 id="来自文本的持续时间预测"><a href="#来自文本的持续时间预测" class="headerlink" title="来自文本的持续时间预测"></a>来自文本的持续时间预测</h4><p>我们可以通过对估计对齐$\sum_j A_{i,j}$的每一行中的所有列求和来计算每个输入标记 $d_i$ 的持续时间。 正如之前的工作（Kim 等人，2020 年）所提出的那样，持续时间可用于训练确定性持续时间预测器，但它无法表达一个人每次以不同语速说话的方式。 为了生成类似人类的语音节奏，我们设计了一个随机持续时间预测器，使其样本遵循给定音素的持续时间分布。随机持续时间预测器是基于流的生成模型，通常通过最大似然估计进行训练。 然而，最大似然估计的直接应用是困难的，因为每个输入音素的持续时间是 1) 一个离散整数，需要对其进行反量化以使用连续归一化流，以及 2) 一个标量，它阻止了高维 可逆性引起的变换。我们应用变分反量化（Ho et al., 2019）和变分数据增强（Chen et al., 2020）来解决这些问题。 具体来说，我们引入两个随机变量 $u$ 和 $ν$，它们具有与持续时间序列 $d$ 相同的时间分辨率和维数，分别用于变分去组化和变分数据增强。 我们将 $u$ 的支持度限制为 $[0, 1)$，以便差 $d - u$ 成为正实数序列，并且我们将 $ν$ 和 $d$ 通道级联以形成更高维的潜在表示。 我们通过近似后验分布 $q_φ(u, ν|d, c_{text})$ 对两个变量进行采样。 由此产生的目标是音素持续时间的对数似然的变分下界：</p>
<script type="math/tex; mode=display">
logp_θ(d|c_{text}) ≥ E_{q_φ (u,ν |d,c_{text} )} log \frac {p_θ(d-u,v|c_{text})} {q_φ (u, ν |d, c_{text} )}</script><p>训练损失 $L_{dur}$ 是负变分下界。 我们将停止梯度算子 (van den Oord et al., 2017) 应用于输入条件，以防止反向传播输入的梯度，以便持续时间预测器的训练不会影响其他模块的训练。</p>
<p>取样程序比较简单； 通过随机持续时间预测器的逆变换从随机噪声中采样音素持续时间，然后将其转换为整数。</p>
<h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器 $D$，用于区分解码器 $G$ 生成的输出和真实波形 $y$。 在这项工作中，我们使用了两种成功应用于语音合成的损失类型； 用于对抗训练的最小二乘损失函数 (Mao et al., 2017)，以及用于训练生成器的附加特征匹配损失 (Larsen et al., 2016)：</p>
<script type="math/tex; mode=display">
L_{adv}(D) = E_{(y,z)}􏰄 [(D(y) − 1)^2 + (D(G(z)))^2],</script><script type="math/tex; mode=display">
L_{adv}(G) = E_z [(D(G(z)) − 1)^2] ,</script><script type="math/tex; mode=display">
L_{fm}(G) = E_{(y,z)} [\sum_{l=1}^T \frac {1} {N_l} ∥D^l (y) − D^l (G(z))∥_1 ]</script><p>其中 $T$​ 表示鉴别器中的总层数，$D^l$​ 输出具有 $N_l$​ 个特征的鉴别器第 $l$​ 层的特征图。 值得注意的是，特征匹配损失可以看作是在鉴别器的隐藏层中测量的重建损失，建议作为 VAE 逐元素重建损失的替代方案。</p>
<h3 id="最终的损失函数"><a href="#最终的损失函数" class="headerlink" title="最终的损失函数"></a>最终的损失函数</h3><p>通过 VAE 和 GAN 训练的结合，训练我们的条件 VAE 的总损失可以表示如下：</p>
<script type="math/tex; mode=display">
L_{vae} = L_{recon} +L_{kl} +L_{dur} +L_{adv}(G)+L_{fm}(G)</script><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>所提出模型的整体架构由后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器组成。 后验编码器和鉴别器仅用于训练，不用于推理。 附录 B 中提供了架构详细信息。</p>
<h4 id="2-5-1-后验编码器"><a href="#2-5-1-后验编码器" class="headerlink" title="2.5.1 后验编码器"></a>2.5.1 后验编码器</h4><p>对于后验编码器，我们在 WaveGlow 和 Glow-TTS 中使用非因果 WaveNet 残差块。 WaveNet 残差块由带有门控激活单元 (gated activation unit) 和跳过连接 (skip connection) 的扩张卷积层 (dilated convolutions) 组成。 块上方的线性投影层产生正态后验分布的均值和方差。 对于多说话人的情况，我们在残差块中使用全局调节来添加说话人嵌入。 </p>
<h4 id="2-5-2-先验编码器"><a href="#2-5-2-先验编码器" class="headerlink" title="2.5.2 先验编码器"></a>2.5.2 先验编码器</h4><p>先验编码器由处理输入音素 $c_{text}$ 的文本编码器和提高先验分布灵活性的归一化流 $f_θ$ 组成。 文本编码器是一个转换器 (Transformer) 编码器，它使用相对位置表示而不是绝对位置编码。 我们可以通过文本编码器和文本编码器上方的线性投影层从 $c_{text}$ 中获得隐藏表示 $h_{text}$，该线性投影层产生用于构建先验分布的均值和方差。 归一化流程是一堆仿射耦合层，由一堆 WaveNet 残差块组成。 为简单起见，我们将标准化流设计为一个体积保持变换 (volume-perserving transformation) ，雅可比行列式为 1。 对于多说话人的情况，我们通过全局条件将说话人嵌入添加到归一化流程中的残差块中。</p>
<h4 id="2-5-3-解码器"><a href="#2-5-3-解码器" class="headerlink" title="2.5.3 解码器"></a>2.5.3 解码器</h4><p>解码器本质上是 HiFi-GAN V1 生成器。 它由一堆转置卷积组成，每个卷积后面跟着一个多感受野融合模块（MRF）。 MRF 的输出是具有不同感受野大小的残差块的输出之和。 对于多说话人设置，我们添加一个线性层来转换说话人嵌入并将其添加到输入潜在变量 $z$。</p>
<h4 id="2-5-4-鉴别器"><a href="#2-5-4-鉴别器" class="headerlink" title="2.5.4 鉴别器"></a>2.5.4 鉴别器</h4><p>我们遵循 HiFi-GAN中提出的多周期鉴别器的鉴别器架构。 多周期鉴别器是基于马尔可夫窗口的子鉴别器的混合，每个子鉴别器都对输入波形的不同周期模式进行操作。</p>
<h4 id="2-5-5-随机持续时间预测器"><a href="#2-5-5-随机持续时间预测器" class="headerlink" title="2.5.5 随机持续时间预测器"></a>2.5.5 随机持续时间预测器</h4><p>随机持续时间预测器根据条件输入 $h_{text}$ 估计音素持续时间的分布。 为了对随机持续时间预测器进行有效的参数化，我们用扩张的和深度可分离的卷积层堆叠残差块。 我们还将神经样条流应用于耦合层，该流通过使用单调有理二次样条采用可逆非线性变换的形式。 与常用的仿射耦合层相比，神经样条流以相似数量的参数提高了变换表达能力。 对于多说话人设置，我们添加一个线性层来转换扬声器嵌入并将其添加到输入 $h_{text}$。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在两个不同的数据集上进行了实验。我们使用 LJ Speech 数据集与其他公开可用的模型和 VCTK 数据集进行比较，以验证我们的模型是否可以学习和表达不同的语音特征。 LJ Speech 数据集由单个说话人的 13,100 个短音频剪辑组成，总长度约为 24 小时。音频格式为 16 位 PCM，采样率为 22 kHz，我们使用它时没有进行任何操作。我们将数据集随机分成训练集（12,500 个样本）、验证集（100 个样本）和测试集（500 个样本）。 VCTK 数据集包含大约 44,000 个短音频片段，由 109 位英语为母语的人发出，带有各种口音。音频剪辑的总长度约为 44 小时。音频格式为 16 位 PCM，采样率为 44 kHz。我们将采样率降低到 22 kHz。我们将数据集随机分为训练集（43,470 个样本）、验证集（100 个样本）和测试集（500 个样本）。</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>我们使用可以通过短时傅立叶变换 (STFT) 从原始波形中获得的线性频谱图作为后验编码器的输入。 变换的FFT大小、窗口大小和跳跃大小分别设置为1024、1024和256。 我们使用 80 波段梅尔尺度频谱图进行重建损失，这是通过将梅尔滤波器组应用于线性频谱图而获得的。 </p>
<p>我们使用国际音标 (IPA) 序列作为先验编码器的输入。 我们使用开源软件将文本序列转换为 IPA 音素序列，并且在实现 Glow-TTS 之后，转换后的序列中穿插着空白标记。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>网络使用 AdamW 优化器进行训练，其中 $β_1 = 0.8$、$β_2 = 0.99$ 和权重衰减 $λ = 0.01$。 学习率衰减按每个 epoch 的 $0.999^{1/8}$ 因子进行调度，初始学习率为 $2 × 10^{−4}$。 继之前的工作之后，我们采用了窗口生成器训练，这是一种仅生成一部分原始波形的方法，以减少训练期间的训练时间和内存使用量。 我们随机提取窗口大小为 32 的潜在表示片段以馈送至解码器，而不是馈送整个潜在表示，并且还从地面实况原始波形中提取相应的音频片段作为训练目标。 我们在 4 个 NVIDIA V100 GPU 上使用混合精度训练。 每个 GPU 的批量大小设置为 64，模型最多训练 800k 步。</p>
<h3 id="用于比较的实验设置"><a href="#用于比较的实验设置" class="headerlink" title="用于比较的实验设置"></a>用于比较的实验设置</h3><p>我们将我们的模型与最好的公开可用模型进行了比较。 我们使用自回归模型 Tacotron 2 和基于流的非自回归模型 Glow-TTS 作为第一阶段模型，将 HiFi-GAN 作为第二阶段模型。 我们使用了他们的公开实现和预训练的权重。 由于两阶段 TTS 系统理论上可以通过顺序训练实现更高的合成质量，我们将微调的 HiFi-GAN 包括了高达 100k 步的预测输出 第一阶段模型。 我们根据经验发现，在教师强制模式下，使用 Tacotron 2 生成的梅尔谱图对 HiFi-GAN 进行微调，与使用生成的梅尔谱图微调相比，Tacotron 2 和 Glow-TTS 的质量都会更好。 Glow-TTS，因此我们在 Tacotron 2 和 Glow-TTS 中附加了更好的微调 HiFi-GAN。</p>
<p>由于每个模型在采样过程中都有一定程度的随机性，我们在整个实验过程中固定了控制每个模型随机性的超参数。 Tactron 2 的 pre-net 中的 dropout 概率设置为 0.5。 对于 Glow-TTS，先验分布的标准偏差设置为 0.333。 对于 VITS，随机持续时间预测器的输入噪声标准差设置为 0.8，我们将比例因子 0.667 乘以先验分布的标准差。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="语音合成质量"><a href="#语音合成质量" class="headerlink" title="语音合成质量"></a>语音合成质量</h3><p>我们进行了众包 MOS 测试来评估质量。 评分者聆听随机选择的音频样本，并以 1 到 5 的 5 分制对它们的自然度进行评分。评分者被允许对每个音频样本进行一次评估，我们将所有音频片段标准化以避免幅度差异对分数的影响。 这项工作中的所有质量评估都是以这种方式进行的。</p>
<p><img src="/images/vits_result1.png" alt="vits_result1"></p>
<p>评估结果如表 1 所示。 VITS 优于其他 TTS 系统，并实现了与Ground-Truth实况相似的 MOS。 VITS (DDP) 采用 Glow-TTS 中使用的相同确定性持续时间预测器架构而不是随机持续时间预测器，在 MOS 评估中在 TTS 系统中得分第二高。 这些结果意味着 1）随机持续时间预测器比确定性持续时间预测器生成更真实的音素持续时间；2）我们的端到端训练方法是制作比其他 TTS 模型更好的样本的有效方法，即使保持相似 持续时间预测器架构。</p>
<p><img src="/images/vits_result2.png" alt="vits_result2"></p>
<p>我们进行了一项消融研究以证明我们方法的有效性，包括先验编码器中的归一化流和线性尺度频谱图后验输入。 消融研究中的所有模型都经过最多 30 万步的训练。 结果如表 2 所示。去除先前编码器中的归一化流导致 1.52 MOS 比基线降低，这表明先验分布的灵活性显着影响合成质量。 用梅尔谱图替换后验输入的线性尺度谱图导致质量下降（-0.19 MOS），表明高分辨率信息对于 VITS 提高合成质量是有效的。</p>
<h3 id="泛化至多说话人TTS"><a href="#泛化至多说话人TTS" class="headerlink" title="泛化至多说话人TTS"></a>泛化至多说话人TTS</h3><p><img src="/images/vits_tab3.png" alt="vits_tab3"></p>
<p>为了验证我们的模型可以学习和表达不同的语音特征，我们将我们的模型与 Tacotron 2、Glow-TTS 和 HiFi-GAN 进行了比较，以显示扩展到多说话人语音合成的能力。 我们在 VCTK 数据集上训练模型。 我们将说话人嵌入添加到我们的模型中，如第 2.5 节所述。 对于 Tacotron 2，我们扩展说话人嵌入并将其与编码器输出concat起来，对于 Glow-TTS，我们参照2.5，应用了全局调节global conditioning。 评估方法与第 4.1 节中描述的相同。 如表 3 所示，我们的模型实现了比其他模型更高的 MOS。 这表明我们的模型以端到端的方式学习和表达各种语音特征。</p>
<h3 id="语音多样性"><a href="#语音多样性" class="headerlink" title="语音多样性"></a>语音多样性</h3><p>我们验证了随机持续时间预测器产生了多少不同长度的语音，以及合成样本具有多少不同的语音特征。</p>
<p><img src="/images/vits_fig2.png" alt="vits_fig2"></p>
<p><img src="/images/vits_fig3.png" alt="vits_fig3"></p>
<p>类似于 Valle 等人。 （2021），这里的所有样本都是从一句话<em>How much variation is there?</em>生成的。图 2a 显示了每个模型生成的 100 个话语的长度的直方图。由于确定性的持续时间预测器，Glow-TTS 仅生成固定长度的话语，我们模型中的样本遵循与 Tacotron 2 相似的长度分布。图 2b 显示了在多说话人设置中，使用五个说话者身份，生成的 5*100 个话语的长度，以展示模型能够学习到说话人相关的音素持续时间。图 3 中使用 YIN 算法 (De Cheveigne ́ &amp; Kawahara, 2002) 提取的 10 个话语的 F0 轮廓显示我们的模型生成具有不同音高和节奏的语音，图 3d 中的每个不同说话者身份生成的五个样本证明我们的模型为每个说话者身份表达了非常不同的语音长度和音调。请注意，Glow-TTS 可以通过增加先验分布的标准偏差来增加音高的多样性，但相反，它会降低合成质量。</p>
<h3 id="合成速度"><a href="#合成速度" class="headerlink" title="合成速度"></a>合成速度</h3><p><img src="/images/vits_tab4.png" alt="vits_tab4"></p>
<p>我们将模型的合成速度与并行的两阶段 TTS 系统 Glow-TTS 和 HiFi-GAN 进行了比较。 我们测量了整个过程的同步经过时间，以从 LJ Speech 数据集的测试集中随机选择 100 个句子的音素序列生成原始波形。 我们使用了一个批处理大小为 1 的 NVIDIA V100 GPU。结果如表 4 所示。由于我们的模型不需要用于生成预定义中间表示的模块，因此其采样效率和速度得到了极大的提高。</p>
<h2 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h2><p>在这项工作中，我们提出了一个并行的 TTS 系统 VITS，它可以以端到端的方式学习和生成。我们进一步引入了随机持续时间预测器来表达不同的语音节奏。由此产生的系统直接从文本合成自然发声的语音波形，而无需经过预定义的中间语音表示。我们的实验结果表明，我们的方法优于两阶段 TTS 系统并达到接近人类的质量。我们希望所提出的方法将用于许多语音合成任务，其中使用了两阶段 TTS 系统，以实现性能提升并享受简化的训练过程。我们还想指出，即使我们的方法在 TTS 系统中集成了两个分离的生成管道，仍然存在文本预处理的问题。研究语言表征的自监督学习可能是去除文本预处理步骤的一个可能方向。我们将发布我们的源代码和预训练模型，以促进未来许多方向的研究。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>EfficientTTS An Efficient and High-Quality Text-to-Speech Architecture</title>
    <url>/2021/07/30/EFTS-wav/</url>
    <content><![CDATA[<h1 id="EfficientTTS：一种高效、高质量的文本转语音架构"><a href="#EfficientTTS：一种高效、高质量的文本转语音架构" class="headerlink" title="EfficientTTS：一种高效、高质量的文本转语音架构"></a>EfficientTTS：一种高效、高质量的文本转语音架构</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在这项工作中，我们通过提出一种称为 EfficientTTS 的非自回归架构来解决文本到语音（TTS）任务。与主要的非自回归 TTS 模型需要外部对齐器进行训练不同，EfficientTTS 使用稳定的端到端训练程序来优化其所有参数，同时允许合成高质量的语音，是一种快速有效的方式。 EfficientTTS 是由一种新的单调对齐建模（monotonic alignment modeling）方法（也在本工作中引入）推动的，该方法在几乎不增加计算量的情况下指定对序列对齐的单调约束。通过将 EfficientTTS 与不同的前馈网络结构相结合，我们开发了一系列 TTS 模型，包括 text-to-melspectrogram 和 text-to-waveform 网络。我们的实验表明，所提出的模型在语音质量、训练效率和合成速度方面明显优于 Tacotron 2 (Shen et al., 2018) 和 Glow-TTS (Kim et al., 2020) 等对应模型，同时仍然产生强大的稳健性和多样性的演讲。此外，我们证明了所提出的方法可以轻松扩展到自回归模型，例如 Tacotron 2。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文本到语音（TTS）是语音处理中的一项重要任务。随着深度学习的飞速发展，TTS 技术近年来受到了广泛关注。最流行的神经 TTS 模型是基于编码器-解码器框架的自回归模型。在这个框架中，编码器将文本序列作为输入并学习其隐藏表示，而解码器逐帧生成输出，即以自回归方式。随着自回归模型性能的大幅提升，综合效率正成为新的研究热点。</p>
<p>最近，大量努力致力于非自回归 TTS 模型的开发。然而，大多数现有的非自回归 TTS 模型都存在训练过程复杂、计算成本或训练时间成本高的问题，使其不适合实际应用。在这项工作中，我们提出了 EfficientTTS，一种高效且高质量的文本转语音架构。我们的贡献总结如下，</p>
<ul>
<li>除了几乎不增加计算量的一般注意力机制之外，我们提出了一种新方法来为序列到序列模型产生软或硬单调对齐。 最重要的是，所提出的方法可以纳入任何注意力机制，而不受网络结构的限制。</li>
<li>我们提出了 EfficientTTS，这是一种非自回归架构，可在没有额外对齐器的情况下从文本序列执行高质量语音生成。 EfficientTTS 是完全并行、完全卷积的，并且经过端到端训练，因此对于训练和推理都非常有效。</li>
<li>我们开发了一系列基于 EfficientTTS 的 TTS 模型，包括： (1) EFTS-CNN，一个卷积模型以高训练效率学习 melspectrogram； (2) EFTS-Flow，一种基于流的模型，可实现具有可控语音变化的并行 melspectrogram 生成； (3) EFTS-Wav，一个完全端到端的模型，直接从文本序列中学习波形生成。 我们通过实验表明，与对应模型 Tacotron 2 和 Glow-TTS 相比，所提出的模型在语音质量、合成速度和训练效率方面取得了显着改善。</li>
<li>我们还表明，所提出的方法可以很容易地扩展到自回归模型，如本文末尾的 Tacotron 2。</li>
</ul>
<p>本文的其余部分的结构如下。 第 2 节讨论相关工作。 我们在第 3 节中介绍了使用索引映射向量（index mapping vector）的单调对齐建模（monotonic alignment modeling）。第 4 节介绍了 EfficientTTS 架构。在第 5 节中，介绍了 EfficientTTS 模型。 第 6 节展示了实验结果和实现细节。 最后，第 7 节总结了本文。</p>
<h2 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h2><h3 id="2-1-Non-Autoregressive-TTS-models"><a href="#2-1-Non-Autoregressive-TTS-models" class="headerlink" title="2.1 Non-Autoregressive TTS models"></a>2.1 Non-Autoregressive TTS models</h3><p>在 TTS 任务中，输入文本序列 $x = \{x_0,x_1,…,x_{T_1−1}\}$​​​​ 通过编码器-解码器帧转换为输出序列 $y = \{y_0 , y_1 , …, y_{T_2 −1} \}$​​​​​。 通常情况下，$x$​​首先通过编码器 $f: h = f(x)$​ 转换为一系列隐藏状态 $h = \{h_0,h_1,…,h_{T_1−1}\}$​，然后通过解码器产生输出$y$​。 对于每个输出时间步，注意力机制允许搜索 $h$ 的整个元素以生成上下文向量 $c$：</p>
<script type="math/tex; mode=display">
c_j = \sum^{T_1-1}_{i=0}􏰂 α_{i,j} ∗h_i,</script><p>其中 $α = \{α_{i,j} \} ∈ \mathcal{R}^{(T_1 ,T_2 )}$​ 是对齐矩阵。然后将 $c$ 馈送到另一个网络 $g$ 以生成输出 $y：y = g(c)$。 $f$ 和 $g$​ 的网络可以很容易地用并行结构替换，因为它们都获得一致的输入和输出长度。<strong>因此，构建非自回归 TTS 模型的关键在于并行对齐预测。</strong>在之前的工作中，大多数非自回归 TTS 模型从外部模型或工具中学习对齐，这使得训练变得复杂。最近，提出了 Flow-TTS（Miao 等，2020）、Glow-TTS（Kim 等，2020）和 EATS（Donahue 等，2020）。 Flow-TTS 和 EATS 在训练过程中直接从文本序列的隐藏表示中学习比对，没有考虑从输出序列中提取比对，使得训练效率低下。 Glow-TTS 使用独立算法提取每个输入token的持续时间，该算法排除了使用标准反向传播。另一方面，EfficientTTS 通过单个网络以完全端到端的方式联合学习序列对齐和语音生成，同时保持稳定高效的训练。</p>
<h3 id="2-2-Monotonic-alignment-modeling"><a href="#2-2-Monotonic-alignment-modeling" class="headerlink" title="2.2 Monotonic alignment modeling"></a>2.2 Monotonic alignment modeling</h3><p>如第 2.1 节所述，通用注意力机制会在每个输出时间步检查每个输入步。 这种机制经常会遇到错位并且训练成本很高，尤其是对于长序列。 因此，如果结合了一些先验知识，它一定会有所帮助。 <strong>一般来说，单调对齐应该遵循严格的标准</strong>，如图1所示，包括：（1）<strong>单调性</strong>，在每个输出时间步，对齐位置永不倒退； (2) <strong>连续性</strong>，在每个输出时间步，对齐的位置最多向前移动一步； (3) <strong>完整性</strong>，对齐的位置必须覆盖输入标记的所有位置。 已经提出了许多先前的研究来确保正确的对齐（Li 等人，2020 年），但其中大多数需要连续的步骤，并且经常无法满足上述所有标准。 在这项工作中，我们提出了一种有效且高效地产生单调注意力的新方法。</p>
<h2 id="采用IMV进行单调对齐建模"><a href="#采用IMV进行单调对齐建模" class="headerlink" title="采用IMV进行单调对齐建模"></a>采用IMV进行单调对齐建模</h2><p>我们通过提出<strong>索引映射向量 (IMV: index mapping vector)</strong> 开始本节，然后我们在单调对齐建模中利用 IMV。 我们进一步展示了如何将 IMV 合并到一般的序列到序列模型中。</p>
<h3 id="3-1-IMV定义"><a href="#3-1-IMV定义" class="headerlink" title="3.1 IMV定义"></a>3.1 IMV定义</h3><p>设 $α∈\mathcal{R}^{(T_1,T_2)}$​​​ 为输入序列 $x∈\mathcal{R}^{(D_1,T_1)}$​​ ​和输出序列 $y∈ \mathcal{R}^{(D_2,T_2)}$​​ 之间的比对矩阵。 我们将索引映射向量 (IMV) $π$​​ 定义为索引向量 $p = \{0, 1, · · · , T_1 − 1\}$​ 的总和，由 $α$ 加权：</p>
<script type="math/tex; mode=display">
π_j = \sum^{T_1 -1}_{i=0} 􏰂 α_{i,j} ∗p_i,</script><p>其中，$0 ≤ j ≤ T_2 −1,π ∈ \mathcal{R}^{T_2},􏰂\sum^{T_1-1}_{i=0}α_{i,j} = 1$​​.</p>
<p>我们可以将 IMV 理解为每个的预期位置输出时间步长，其中期望值是从 $0$ 到 $T_1-1$​ 的所有可能输入位置。</p>
<h3 id="3-2-采用IMV进行单调对齐建模"><a href="#3-2-采用IMV进行单调对齐建模" class="headerlink" title="3.2 采用IMV进行单调对齐建模"></a>3.2 采用IMV进行单调对齐建模</h3><p><strong>连续性和单调性。</strong> 我们首先证明对齐矩阵 $α$​ 的连续性和单调性标准等价于以下约束：</p>
<script type="math/tex; mode=display">
0 ≤ ∆π_i ≤ 1</script><p>其中，$∆π_i = π_i −π_{i−1},1 ≤ i ≤ T_2 −1$​。 详细的验证见附录 A。</p>
<p><strong>完整性。</strong> 给定 $π$​ 是连续单调的，完备性等价于边界条件：</p>
<script type="math/tex; mode=display">
π_0=0,</script><script type="math/tex; mode=display">
π_{T_2−1} = T_1 − 1.</script><p>这可以从 $α_0 = \{1, 0, …, 0\}$​​​​​​ 和 $α_{T_2 −1} = \{0,0,…,1\}$​​​​​​ 推导出，其中$α_0 =\{α_{i,j} |0≤i≤T_1−1， j=0\}$​​​​​​ 和 $α_{T_2 −1} = \{α_{i,j} | 0 ≤ i ≤ T_1 − 1，j = T_2 − 1\}$​​​​​​​​。</p>
<h3 id="3-3-将IMV融合到网络中"><a href="#3-3-将IMV融合到网络中" class="headerlink" title="3.3 将IMV融合到网络中"></a>3.3 将IMV融合到网络中</h3><p>我们提出了两种将 IMV 纳入序列到序列网络的策略：软单调比对 (SMA) 和硬单调比对 (HMA)。 </p>
<p><strong>软单调对齐 (SMA)</strong>。 让序列到序列模型使用3.2节给出的3个约束条件进行训练。 一个自然的想法是将这些约束转化为训练目标。 我们将这些约束表述为 SMA 损失，计算如下：</p>
<script type="math/tex; mode=display">
L_{SMA} = λ_0∥|∆π| − ∆π∥_1 + λ_1∥|∆π − 1| + (∆π − 1)∥_1 + λ_2∥\frac {π_0} {T_1−1} ∥_2 + λ_3∥ \frac {π_{T_2−1}}{T_1 -1} − 1∥_2,</script><p>其中$∥·∥_1$和$∥·∥_2$​分别为L1范数和L2范数，$λ_0、λ_1、λ_2、λ_3$​​为正系数。 可以看出，$L_{SMA}$ 是非负的，只有当 $π$​ 满足所有约束时才为零。 $L_{SMA}$ 的计算只需要对齐矩阵 $α$（索引向量 $p$​ 总是已知的），因此，将 SMA 损失合并到序列到序列网络中而不改变它们的网络结构是很容易的。 一般来说，SMA 扮演着与引导注意（Guided Attention）相似的角色。 然而，SMA 优于引导注意，因为 SMA 理论上对对齐提供了更准确的约束。</p>
<p><strong>硬单调对齐 (HMA)</strong>。 虽然 SMA 允许序列到序列网络通过结合 SMA 损失来产生单调对齐，但这些网络的训练可能仍然很昂贵，因为网络无法在训练的开始阶段产生单调对齐。 相反，他们一步一步地学习这种能力。 为了解决这个限制，我们提出了另一种单调策略，我们称之为 HMA，用于硬单调对齐。 HMA 的核心思想是建立一个具有战略设计结构的新网络，允许在没有监督的情况下产生单调对齐。</p>
<p>首先，我们根据IMV的定义从对齐矩阵 $α$ 计算 IMV $π’$​。尽管 $π’$ 不是单调的，但它随后通过使用 ReLU 激活强制 $Δπ &gt; 0$ 被转换为 $π$，这是一个严格单调的 IMV。</p>
<script type="math/tex; mode=display">
∆π_j^′ =π_j^′ −π^′_{j−1}, 0<j≤T_2−1,</script><script type="math/tex; mode=display">
∆π_j =\text{ReLU}(∆π_j^′), 0<j≤T_2−1,</script><script type="math/tex; mode=display">
π_j= 
\begin{cases}
0, & j=0\\
\sum_{m=0}^j  ∆π_m & 0<j≤T_2−1
\end{cases}􏰁</script><p>此外，要将 $π$ 的域限制在完整性等式中给出的区间 $[0, T_1 − 1]$。我们将 $π$ 乘以一个正标量：</p>
<script type="math/tex; mode=display">
π_j^∗ = π_j ∗ \frac {T_1 − 1} {max(π)} =  π_j ∗ \frac {T_1 − 1} {π_{T_2} −1},0 ≤ j ≤ T_2 − 1.</script><p>回想一下，我们的目标是构建单调对齐。 为了实现这一点，我们引入了以下变换，通过利用以 $π^∗$为中心的高斯核来重建对齐：</p>
<script type="math/tex; mode=display">
α^′_{i,j} = \frac {exp (−σ^{−2}(p_i − π_j^∗)^2)} {􏰁\sum^{T_1−1}_{m=0} exp (−σ^{−2}(p_m − π_j^∗)^2)}</script><p>其中，$σ^2$​ 表示表示对齐变化的超参数。 $α’$​ 用作原始对齐 $α$ 的替代。 $α$ 和 $α’$ 之间的区别在于 $α’$ 保证是单调的，而 $α$ 对单调性没有约束。 HMA 降低了学习单调对齐的难度，从而提高了训练效率。 与 SMA 类似，HMA 可用于任何序列到序列网络。</p>
<h2 id="EfficientTTS-模型架构"><a href="#EfficientTTS-模型架构" class="headerlink" title="EfficientTTS 模型架构"></a>EfficientTTS 模型架构</h2><p>EfficientTTS 的整体架构设计如图 2 所示。在训练阶段，我们通过 IMV 生成器从文本序列和 melspectrogram 的隐藏表示中学习 IMV。 文本序列和梅尔谱图的隐藏表示分别从文本编码器和梅尔编码器中学习。 然后将 IMV 转换为二维对齐矩阵，该矩阵进一步用于通过对齐重建层生成时间对齐表示。 时间对齐的表示通过解码器生成输出梅尔光谱图或波形。 我们同时训练一个对齐位置预测器，它学习在每个输入文本标记的输出时间步长中预测对齐位置。 在推理阶段，我们从预测的对齐位置重建对齐矩阵。 我们在以下小节和附录 D 中每个组件的伪代码中展示了详细的实现。</p>
<p><img src="/images/fig2.png" alt="fig2"></p>
<h3 id="4-1-文本编码器和Mel谱编码器"><a href="#4-1-文本编码器和Mel谱编码器" class="headerlink" title="4.1 文本编码器和Mel谱编码器"></a>4.1 文本编码器和Mel谱编码器</h3><p>我们使用文本编码器和梅尔编码器将文本符号和梅尔谱图分别转换为强大的隐藏表示。</p>
<p>在文本编码器的实现中，我们使用学习嵌入(learned embedding)将文本序列转换为高维向量序列。 然后，高维向量通过一堆卷积，散布着权重归一化和 Leaky ReLU 激活。 我们还为每个卷积添加了一个残差连接以允许深度网络。</p>
<p>在梅尔编码器的实现中，我们首先通过线性投影将梅尔谱图转换为高维向量。 与文本编码器相同，mel-encoder 由一堆卷积组成，其中散布着权重归一化、Leaky ReLU 激活和残差连接。 请注意，mel-encoder 仅用于训练阶段。</p>
<h3 id="4-2-IMV-生成器"><a href="#4-2-IMV-生成器" class="headerlink" title="4.2 IMV 生成器"></a>4.2 IMV 生成器</h3><p>为了在训练阶段生成单调 IMV，我们首先通过等式中给出的缩放点积注意力学习输入和输出之间的对齐 $α$。 然后从$α$​计算IMV。</p>
<script type="math/tex; mode=display">
α_{i,j}= \frac {\text{exp}(−D^{−0.5}(q_j ·k_i))} {\sum^{T_1−1}_{m=0}exp(−D^{−0.5}(q_j·k_m))}</script><p>其中，$q$ 和 $k$ 是 mel-encoder 和 text-encoder 的输出，$D$ 是 $q$ 和 $k$ 的维度。</p>
<p>计算 IMV 的一种简单方法是遵循3.1中定义的等式 (2)。然而，由于缩放点积注意力对单调性没有限制，在我们的初步实验中，将 SMA 损失纳入训练。 但我们进一步发现 HMA 更有效。 我们遵循3.3中定义的方程 (7,8,9,10) 实施 HMA。 在实验中，我们比较了不同单调策略的效果。</p>
<h3 id="4-3-对齐位置预测器"><a href="#4-3-对齐位置预测器" class="headerlink" title="4.3 对齐位置预测器"></a>4.3 对齐位置预测器</h3><p>在推理阶段，模型需要从文本序列 $h$​​ 的隐藏表示中预测 IMV $π$​​，这在实践中具有挑战性。 有两个限制：（1）$π$​​​是时间对齐的，分辨率高，但是$h$​​分辨率低； (2) 由于3.3中的等式（9）中引入的累积求和运算，$π_i$ 的每个预测都会影响 $π_j (j &gt; i)$​ 的后续预测，使得并行预测 $π$​变得困难。 幸运的是，可以通过预测每个输入标记的对齐位置 $e$ 来缓解这些限制。 我们定义3.1中的方程 (2) 作为变换 $m(·): π = m(q)$​。 由于 $π$ 和 $q$ 在时间步长上都是单调连续的，这意味着变换 $m(·)$​ 是单调连续的，因此 $m(·)$ 是可逆的：</p>
<script type="math/tex; mode=display">
q = m^{−1}(π),</script><p>每个输入标记的输出时间步长中对齐的位置 $e$ 可以计算为：</p>
<script type="math/tex; mode=display">
e=m^{−1}(p), p=\{0,1,...,T_1 −1\}.</script><p><img src="/images/fig3.png" alt="fig3">我们在图 3 中说明了 $m(·)$、$e$、$π$ 的关系。为了计算 $e$，我们首先利用与3.3中的方程（11) 类似的变换来计算概率密度矩阵 $γ$。  唯一的区别是概率密度是在不同维度上计算的。 对齐位置 $e$ 是由 $γ$ 加权的输出索引向量 $q$​ 的加权和。</p>
<script type="math/tex; mode=display">
γ_{i,j} = \frac {\text{exp}(−σ^{−2}(p_i −π_j)^2)} {􏰁\sum ^{T_2−1}_{n=0} \text{exp} (−σ^{−2}(p_i − π_n)^2)}</script><script type="math/tex; mode=display">
e_i = \sum^{T_2-1}_{n=0}􏰂 γ_{i,n} ∗q_n</script><p>可以看出，$e$ 的计算是可微的，允许通过梯度方法进行训练，因此可以用于训练和推理。 此外，$e$​ 是可预测的，因为：(1) 分辨率 $e$​ 与 $h$ 相同； (2) 我们可以学习相对位置$∆e,(∆e_i =e_i−e_{i−1},1≤i≤T_1−1)$ 而不是直接学习$e$ 来克服第二个限制。</p>
<p>对齐位置预测器由 2 个卷积组成，每个卷积之后是层归一化和 ReLU 激活。 我们将根据 $π$ 计算出的 $Δe$ 作为训练目标。 估计位置 $∆ \hat e$ 和目标位置 $∆e$​ 之间的损失函数计算如下：</p>
<script type="math/tex; mode=display">
L_{ap} =∥log(∆ \hat e+ε)−log(∆e+ε)∥_1,</script><p>其中，$ε$​ 是一个小数，以避免数值不稳定性。 对数尺度损失的目标是准确拟合小值，这对于训练的后期阶段往往更为重要。 对齐位置预测器与模型的其余部分共同学习。 因为我们通过利用对齐的位置来生成对齐，作为附带好处，EfficientTTS 继承了基于持续时间的非自回归 TTS 模型的语速控制的能力。</p>
<h3 id="4-4-对齐重构"><a href="#4-4-对齐重构" class="headerlink" title="4.4 对齐重构"></a>4.4 对齐重构</h3><p>为了将输入隐藏表示 $h$ 映射到时间对齐表示，需要一个对齐矩阵，用于训练和推理。 我们也可以从 IMV 或对齐位置构建对齐。 对于大多数情况，3.3 的方程 (11) 是从 IMV 重建对齐矩阵的有效方法。 但是因为我们在推理过程中预测对齐位置而不是 IMV，为了保持一致，我们从对齐位置 $e$ 重建对齐矩阵，用于训练和推理。具体来说，我们采用从4.3中的方程 (15) 计算的对齐位置 $e$​​​​​ 用于训练，以及来自对齐位置预测器的预测用于推理。</p>
<p>我们遵循 EATS 的类似思想，通过引入以对齐位置 $e$ 为中心的高斯核来重建对齐矩阵 $α’$​。</p>
<script type="math/tex; mode=display">
α^{'}_{i,j} = \frac {\text{exp}(−σ^{−2}(e_i −q_j)^2)} {􏰁\sum^{T_1−1}_{m=0} \text{exp}(−σ^{−2}(e_m −q_j)^2)}</script><p>其中 $q = \{0, 1, …, T_2 − 1\}$ 是输出序列的索引向量。 输出序列 $T_2$ 的长度在训练中已知，在推理中从 $e$​ 计算：</p>
<script type="math/tex; mode=display">
T_2 = e_{T_1−1} + ∆e_{T_1−1}</script><p>尽管重建的对齐矩阵可能不如由3.3中的等式(11)计算的矩阵准确（由于$e$的分辨率低），对输出的影响很小，因为网络能够补偿。 因此，由于训练和推理的一致性不断提高，我们享受到语音质量的提高。</p>
<p>我们在图 4 中说明了 $π$ 和相同话术的重构 $α$​​​。可以看出，$α$ 在第一个训练步骤是对角线的，并且在第 $10k^{th}$ 个训练步骤快速收敛，这是非常快的。 我们通过使用遵循2.1中的等式(1)的 $α’$ 将文本编码器 $h$ 的输出映射到时间对齐的表示。然后将时间对齐的表示作为输入馈送到解码器。</p>
<h3 id="4-5-解码器"><a href="#4-5-解码器" class="headerlink" title="4.5 解码器"></a>4.5 解码器</h3><p>由于解码器的输入和输出都是时间对齐的，因此很容易实现具有并行结构的解码器。 在下一节中，我们开发了三种基于具有不同解码器实现的 EfficientTTS 的模型。</p>
<h2 id="EfficientTTS-Models"><a href="#EfficientTTS-Models" class="headerlink" title="EfficientTTS Models"></a>EfficientTTS Models</h2><h3 id="5-1-EFTS-CNN"><a href="#5-1-EFTS-CNN" class="headerlink" title="5.1 EFTS-CNN"></a>5.1 EFTS-CNN</h3><p>我们首先通过一堆卷积来参数化解码器。 每个卷积都穿插了权重归一化、Leaky ReLU 激活和残差连接。 我们在最后添加一个线性投影来生成melspectrogram。 均方误差 (MSE) 用作重建误差。 EFTS-CNN 的总体训练目标是 melspectrogram 的对齐位置损失和 MSE 损失的组合。</p>
<h3 id="5-2-EFTS-Flow"><a href="#5-2-EFTS-Flow" class="headerlink" title="5.2 EFTS-Flow"></a>5.2 EFTS-Flow</h3><p>为了让 TTS 模型能够控制生成语音的变化，我们实现了一个基于流的解码器。 在训练阶段，我们通过直接最大化似然，以时间对齐表示为条件，学习从melspectrogram到高维高斯分布$\mathcal{N}(0, 1)$​的变换$f$​。 $f$​ 是可逆的，具有战略设计的结构。 具体来说，它由几个流程步骤组成，每个流程步骤由两个基本的可逆变换组成：一个可逆线性层和一个仿射耦合层。 为了提高生成语音的多样性，我们在推理过程中从高斯分布 $\mathcal{N} (0, 1)$ 中采样潜在变量 $z$，并使用温度因子 $t$ 用零向量 $o$ 解释 $z$，并逆变换 $f$ 以生成梅尔频谱图 .</p>
<script type="math/tex; mode=display">
z^{'} =t∗z+o∗(1−t),0≤t≤1</script><script type="math/tex; mode=display">
x = f^{−1}(z^{'})</script><p>为简单起见，我们在实现基于流的解码器时遵循 Flow-TTS的解码器结构。 EFTS-Flow 的总体训练目标是对齐位置损失和最大似然估计 (MLE) 损失的组合。</p>
<h3 id="5-3-EFTS-Wav"><a href="#5-3-EFTS-Wav" class="headerlink" title="5.3 EFTS-Wav"></a>5.3 EFTS-Wav</h3><p>为了简化 2 阶段训练管道并以完全端到端的方式训练 TTS 模型，我们通过将 EfficientTTS 与扩张卷积对抗解码器相结合来开发文本到 wav 模型。解码器结构类似于 MelGAN，除了： (1) 生成器的输入是高维隐藏表示，而不是 80 通道的梅尔频谱图； (2) 多分辨率 STFT 损耗包含在生成器的末端。我们采用与 MelGAN 鉴别器相同的结构进行对抗训练。与 ClariNet 和 EATS 类似，我们通过调节与 1s 音频剪辑相对应的切片输入来训练 MelGAN 部分，而其他部分则在全长话语上进行训练。我们在全长解码器输入上添加线性投影以同时生成melspectrogram，这允许EFTS-Wav学习每个训练步骤的全长对齐。 <strong>EFTS-Wav 生成器的总体训练目标是 melspectrogram 的重建损失、MelGAN 生成器损失、多分辨率 STFT 损失和对齐位置损失的线性组合。</strong></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><p>在本节中，我们首先在语音保真度、训练和推理效率方面将提出的模型与其对应模型进行比较。 然后，我们分析了所提出的单调方法在 EFTS-CNN 和 Tacotron 2 上的有效性。我们还证明了所提出的模型可以在本节末尾生成非常多样化的语音。</p>
<h3 id="6-1-实验设置"><a href="#6-1-实验设置" class="headerlink" title="6.1 实验设置"></a>6.1 实验设置</h3><p><strong>数据集。</strong> 我们在来自 DataBaker 的开源标准普通话数据集上进行了大部分实验，该数据集包含来自单个女性演讲者的 10,000 个中文片段，采样率为 22.05kHZ。 剪辑的长度从 1 秒到 10 秒不等，剪辑的总长度约为 12 小时。 我们遵循Tacotron将波形转换为 80 通道 melspectrogram，FFT 大小为 1024，跳长为 256，窗口大小为 1024。我们还使用 LJ-Speech 数据集进行了一些实验，这是一个由 13100条音频片段组成的单个女性演讲者的 24 小时波形音频集，采样率22.05kHZ。</p>
<p><strong>实施细则。</strong> 我们的 EfficientTTS 实现由文本编码器（ 5 个卷积）和梅尔编码器（3 个卷积）组成，所有卷积的内核大小和维度大小分别设置为 5 和 512。 我们在具有相同卷积配置的 EFTS-CNN 解码器中使用 6 层卷积堆栈。 EFTS-Flow 的解码器由 8 个流程步骤组成，我们在实现多尺度架构时每 3 个流程步骤就提前输出 20 个通道。 我们遵循 MelGAN 的配置来实现 EFTS-wav。 我们使用 HiFi-GAN 声码器从 EFTS-CNN 和 EFTS-Flow 生成的梅尔谱图生成波形。 我们使用具有 HiFi-GAN-V1 配置的 <a href="https://github.com/jik876/hifi-gan">HiFi-GAN</a>的开放实现。</p>
<p><strong>对照模型。</strong> 我们在以下实验中将提出的模型与自回归 Tacotron 2 和非自回归 Glow-TTS 进行比较。 我们直接使用带有默认配置的 <a href="https://github.com/NVIDIA/tacotron2">Tacotron 2</a> 和 <a href="https://github.com/jaywalnut310/glow-tts">Glow-TTS</a> 的开源实现。</p>
<p><strong>训练。</strong> 我们在单个 Tesla V100 GPU 上训练所有模型。对于 EFTS-CNN 和 EFTS-Flow，我们使用 Adam 优化器，批量大小为 96，学习率为 $1 × 10^{−4}$。 对于 EFTS-Wav，我们使用批次大小为 48 的 Adam 优化器。我们对 EFTS-Wav 生成器使用 $1 × 10^{−4}$ 的学习率，对EFTS-Wav 鉴别器使用 $5 × 10^{−5}$​的学习率。 在 DataBaker 上训练 EFTS-CNN 需要 270k 步直到收敛，EFTS-Flow 需要 400k 训练步骤，EFTS-Wav 需要 560k 训练步骤。</p>
<h3 id="6-2-与对照模型对比"><a href="#6-2-与对照模型对比" class="headerlink" title="6.2. 与对照模型对比"></a>6.2. 与对照模型对比</h3><p><img src="/images/efts-table2.png" alt="efts-table2"></p>
<p><img src="/images/efts-table3.png" alt="efts-table3"></p>
<p><strong>语音质量。</strong>我们对 DataBaker 数据集进行了 5 级平均意见得分 (MOS) 评估，以衡量合成音频的质量。每个音频至少由 15 名测试人员收听，他们都是母语人士。我们将 EfficientTTS 系列生成的音频样本的 MOS 与真实音频以及对应模型生成的音频样本进行比较。具有 95% 置信区间的 MOS 结果显示在上表2中。 我们观察到 EfficientTTS 系列优于对应模型。 Tacotron 2 由于教师强制训练和自回归推理之间的不一致导致语音质量下降，而 Glow-TTS 复制了文本序列的隐藏表示，这破坏了隐藏表示的连续性。 EfficientTTS 使用 IMV 重建对齐，这比令牌持续时间更具表现力，因此实现了更好的语音质量。此外，EfficientTTS 的对齐部分与模型的其余部分一起训练，进一步提高了语音质量。由于我们的训练设置可能与对应模型的原始设置不同，我们进一步将我们的模型 EFTS-CNN 与 LJ-Speech 数据集上的 Tacotron 2 和 Glow-TTS 的相关模型进行比较。如表3所示。 EFTS-CNN 在 LJ-Speech 上的表现也明显优于对应模型。</p>
<p><img src="/images/efts-table1.png" alt="efts-table1"></p>
<p><strong>训练和推理速度。</strong>由于非自回归和完全卷积，所提出的模型对于训练和推理都非常有效。 训练时间和推理延迟的定量结果显示在表1中。可以看出，<strong>EFTS-CNN 需要最少的训练时间</strong>。 尽管 EFTS-Flow 需要与 Tacotron 2 相当的训练时间，但它比 Glow-TTS 快得多。 至于推理延迟，EfficientTTS 模型比 Tacotron 2 和 Glow-TTS 更快。 特别是，EFTS-CNN 的推理延迟为 6ms，比 Tacotron 2 快 130 倍，并且比 Glow-TTS 快得多。 由于去除了 melspectrogram 生成，<strong>EFTS-Wav 明显快于 2-staged 模型，从文本序列合成测试音频仅需 16 毫秒，比 Tacotron 2 快 54 倍。</strong></p>
<h3 id="6-3-单调对齐方法的评估"><a href="#6-3-单调对齐方法的评估" class="headerlink" title="6.3 单调对齐方法的评估"></a>6.3 单调对齐方法的评估</h3><p>为了评估所提出的单调方法的行为，我们在 EFTS-CNN 和 Tacotron 2 上进行了多次实验。我们首先比较了 EFTS-CNN 上的训练效率，然后对 Tacotron 2 和 EFTS-CNN 进行了稳健性测试。</p>
<p><img src="/images/efts-fig5.png" alt="efts-fig5"></p>
<p><img src="/images/efts-table4.png" alt="efts-table4"></p>
<p><strong>EFTS-CNN 上的实验。</strong>我们用不同的设置训练 EFTS-CNN，包括：（1）EFTS-HMA，EFTS-CNN 的默认实现，带有硬单调 IMV 生成器。 (2) EFTS-SMA，一个带有软单调 IMV 生成器的 EFTS-CNN 模型。 (3) EFTS-NM，一个没有单调性常数的EFTS-CNN模型。 EFTS-NM 的网络结构与 EFTS-SMA 相同，不同之处在于 EFTS-SMA 是在 SMA 损失的情况下训练的，而 EFTS-NM 是在没有 SMA 损失的情况下训练的。我们首先发现EFTS-NM根本不收敛，它的对齐矩阵不是对角的，而EFTS-SMA和EFTS-HMA都能够产生合理的对齐。我们在图 5 中绘制了 EFTS-SMA 和 EFTS-HMA 的melspectrogram 损失曲线。<strong>可以看出，EFTS-HMA 比 EFTS-SMA 实现了显着的加速</strong>。因此，我们可以得出结论，单调对齐对于所提出的模型非常重要。我们的方法，对于 SMA 和 HMA，成功地学习了单调对齐，而原始注意力机制失败了。由于严格的单调对齐，EFTS-HMA 显着提高了模型性能。更多训练细节显示在表4中。</p>
<p><img src="/images/efts-table5.png" alt="efts-table5"></p>
<p><strong>稳健性。</strong>许多 TTS 模型在合成时遇到错位，特别是对于自回归模型。我们在本小节中分析了 EfficientTTS 的注意力错误，错误包括：重复单词、跳过单词和错误发音。我们对 50 个句子的测试集进行了稳健性评估，其中包括对 TTS 系统特别具有挑战性的案例，例如特别长的句子、重复的字母等。我们将 EFTS-CNN 与 Tacotron 2 进行比较。我们还将 SMA 和 HMA 合并到Tacotron 2 进行更详细的比较（Tacotron2-SMA 和 Tacotron2-HMA 的详细实现以及更多实验结果显示在附录 C 中）。稳健性测试的实验结果见表 5，可以看出EFTS-CNN在Tacotron 2遇到很多错误的情况下，有效地消除了重复错误和跳过错误。然而，通过利用 SMA 或 HMA，Tacotron 2 的合成错误显着减少，这表明所提出的单调方法可以提高 TTS 模型的鲁棒性。</p>
<h3 id="6-4-多样性"><a href="#6-4-多样性" class="headerlink" title="6.4 多样性"></a>6.4 多样性</h3><p>为了合成多样化的语音样本，大多数 TTS 模型利用外部条件，例如样式嵌入或说话者嵌入，或者在推理过程中仅依靠 drop-out。 然而，EfficientTTS 能够以多种方式合成各种语音样本，包括：（1）用不同的对齐方案合成语音。 对齐方案可以是由mel编码器和IMV生成器从现有音频中提取的IMV，也可以是持续时间序列或对齐位置序列； (2) 通过在预测的对齐位置上乘以一个标量来合成不同语速的语音，这类似于其他基于时长的非自回归模型； (3) 通过在推理过程中改变潜在变量 $z$ 的温度 $t$​，为 EFTS-Flow 合成具有不同语音变体的语音。 我们在附录 B 中绘制了从相同文本序列生成的各种melspectrograms。</p>
<h2 id="结论和未来工作"><a href="#结论和未来工作" class="headerlink" title="结论和未来工作"></a>结论和未来工作</h2><p>通常假设模型效率与语音质量之间存在不可避免的权衡。自回归模型，例如 Tacotron 2 和 TransformerTTS，可实现类似人类的语音质量，但由于其自回归结构，通常合成速度较慢。非自回归模型可以快速合成，但无法有效训练。在这项工作中，我们提出了一种非自回归架构，可以实现高质量的语音生成以及高效的训练和合成。我们开发了一系列基于 EfficientTTS 的模型，涵盖文本到光谱图和文本到波形的生成。通过大量实验，我们观察到改进的定量结果，包括训练效率、合成速度、鲁棒性以及语音质量。我们表明，与现有的 TTS 模型相比，所提出的模型非常具有竞争力。</p>
<p>未来的工作有很多可能的方向。 EfficientTTS 不仅可以在给定的对齐方式下生成语音，还可以从给定的语音中提取对齐方式，使其成为语音转换和歌唱合成的绝佳选择。将所提出的单调方法应用于其他单调对齐很重要的序列到序列任务也是一个不错的选择，例如自动语音识别 (ASR)、神经机器翻译 (NMT) 和光学字符识别 (OCR) 。此外，我们也对 IMV 的进一步研究非常感兴趣，包括与对齐矩阵相比的优缺点。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
</search>
