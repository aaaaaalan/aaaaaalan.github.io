<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ICASSP 2020 Overview 思维导图</title>
    <url>/2021/06/16/ICASSP-2020-XMind-Overview/</url>
    <content><![CDATA[<p><img src="/images/ICASSP_2020.png" alt="ICASSP 2020 Overview"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>玩过的21种运动</title>
    <url>/2021/06/16/21-sports/</url>
    <content><![CDATA[<ul>
<li>跳绳</li>
<li>跳皮筋</li>
<li>丢沙包</li>
<li>跳房子</li>
<li>蹦床</li>
<li>爬健身器材</li>
<li>花样轮滑</li>
<li>游龙板</li>
<li>短跑</li>
<li>跳远</li>
<li>游泳</li>
<li>跆拳道</li>
<li>太极拳</li>
<li>健美操</li>
<li>舞狮</li>
<li>龙舟</li>
<li>瑜伽</li>
<li>攀岩</li>
<li>滑雪</li>
<li>滑冰</li>
<li>射箭</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>金载勋跆拳道- 2020年总结</title>
    <url>/2020/12/31/2020-jhk/</url>
    <content><![CDATA[<p>被两个极喜欢拖着我一起立flag折腾自己的狐朋狗友又一次立了一个flag，本想做一个视频，想到社会蓝的故事编辑技能树需要发光发亮，那我也来记录下这一年多的金载勋生涯吧。</p>
<p>2010年的暑假是我第一次接触tkd并深爱上跆拳道，似乎是2012年初二的暑假结束了跆拳道课程，止步于蓝带。印象中结束的最后一节课只有我一个学员上课，在我十分敬佩的葛教练指导下进行十分难得的1v1教学，上的课是当年我最爱的反应踢（教练随意出腿法脚靶，然后迅速反应）。当年留有贝克汉姆发型的我在葛教练的逼迫和鼓励下，顺利学会了俯卧撑、握拳俯卧撑和三指俯卧撑，也在一次次地习练中打开了横竖叉任督二脉。那几年的我，似乎达到了身体素质的局部巅峰。也是这种一次次突破的感觉，让我深爱习练tkd，深信自己也会成为一名优秀的跆拳道选手。然而，同伴的退出和教练的不停更换和面临初三中考的压力，我最后也选择了退出。尽管早已偷偷学会太极七章，尽管再没有机会弥补上痛失金牌的愿望，最终仍止步于蓝带。</p>
<p>中考后的我，还曾经和朋友一起回到道馆，尝试找回曾经的回忆，可笨重的自己和课堂上灵活的小朋友们一起上课后的感受只有“没有对比，就没有伤害”。即便还是曾经的那个道馆，年龄的增长还是劝退了我。</p>
<p>本想大学过后找到tkd社团就继续完成我的黑带梦想，可itf的发声方式，教练要改口为师范，以及从头再来的打击又一次将我劝退，充满棱角的我选择不愿意变更流派。虽然大学四年间每次路过道馆都会观察是不是wtf，有没有成人训练。可奈何都没有合适之选。我也逐渐转移了注意力到其他的运动上，从连续一个月每日中午晚上的舞狮训练上又找回了那种走火入魔的快乐。</p>
<p>直到2019年，工作上稍微稳定些的我继续开始寻觅自己的黑带梦想能够在哪里得以实现。在对比了解了不下十家道馆的我，被国际化的金载勋道馆所吸引了，于是预约了第一次的体验课。</p>
<p>尽管初次体验迟到了大概十五分钟，但耐心又漂亮温柔的前台小姐姐，1对1的体验，小阁楼式的装修，和隔板间的换衣房让我对这国际化道馆的好感印象不断上升。柴师范是我的试课师范，尽管距离当年的意气风发已久隔近十年，但当年对跆拳道的热爱使我还没有忘记腿法的基本动作，已学过的腿法都能够顺利完成。至于拳法的话，对于wtf的我就是一团糟了，只能在师范的指导下尝试模仿。</p>
<p>课上师范的鼓励和表扬和更加专业体系化的指导让我似乎找回了当年的快乐，也对这里的课程逐渐产生了信任。课后与丁师范近一个小时的聊天也让我在残酷的社会和异地漂泊的寒冷中找到了一丝温暖。唯一的顾虑还是itf or wtf。</p>
<p>我记得我有问是哪一种，dsf的回答十分巧妙而又深刻，两种都不是，但两种都会学，并且作为崔洪熙将军直系弟子的金载勋师范传授下来的课程，与市面上更流行普适的跆拳道课程不同，是更加地道和专业的。</p>
<p>另外印象深刻的一点是，我说为什么以前的课程都是一个半小时，现在一个小时感觉时间太短了，无法得到充分的练习。丁师范的回答十分坦诚，说“我们也不希望课程里面穿插很多无用的热身活动，比如跑步就跑了半个小时，如果排成一个半小时，那需要排的课更少了，对我们来说反而是轻松了。大家时间都很宝贵，我们还是希望能在课程里教给学员们更多的专业技能”。这一番言论在dsf绵绵细语下格外真诚，让我逐渐放下了心中对常规销售套路的防备心。</p>
<p>介意从头再来的我仍然问了那个问题，是否可以从以前的级别继续习练。dsf对此也是包容和支持，希望腰间的带色能够勉励我们自己不停努力，加紧训练，尽快追赶上自己腰间的色带。最终，csf对我天赋的鼓励和dsf对道馆体系化教学的讲解双面夹击，我放下了对教练称呼和训练体系改变的排斥，当场报名了半年的课程，希望能一展宏图。</p>
<p>然而，后面一个月内的课程大多是不习惯，不习惯课前没有充分的热身就开始进行腿法练习，不习惯每一次课前热身动作的重复无趣，不习惯不练习俯卧撑，不习惯打拳的时候抬手准备，不习惯做动作的时候要垫脚，不习惯要自己摸索动作结束的标准位置，不习惯很多腿法的动作细节，不习惯分散练习时无人指导的茫然，不习惯韧带没有拉开仍需要勉强完成一些大幅度的腿法动作。这一个月的习练甚至让我开始怀疑自己报名的冲动，最难受的是怀疑自己是不是并不适合这项运动，乃至开始咨询深圳内的其他成人道馆。</p>
<p>这一切直至遇到成长背景都十分相似，同龄又同样有执念的黑带胡哲才算得到一些开解。胡哲是我在跆拳道生涯中遇到过最让我敬佩的学员，尽管已经身为黑带还是在课后刻苦习练，尽管大学时候查有膝盖积水，仍旧在朗朗诗声中咬牙完成自己的横叉梦想，即便动作都已经非常熟练还是一遍又一遍的熟悉，同为wtf转方向，仍旧能够耐下心来完成新的体系的学习。这一切令我感到有些找回当年热血的感觉。课后我开始向她讨教腿法，品势的细节动作，曾经也是wtf的她帮助我开始逐渐熟悉了金载勋的训练体系。</p>
<p>说来很神奇，陌生人面前比较孤冷的我见到她之后话匣子似乎被打开了，她给我讲述大学中的一件件训练故事，分享给我大学的训练日记，文武双全的她让我开始感到崇拜。有她在的每一节课变得不再那么的孤单和艰辛，她不在的日子我都似乎又关闭了自己的说话功能，选择不去接触更多的人，把这一份热情和温暖唯独留给我欣赏的人。</p>
<p>除了同伴的收获，还有一次是杨师范的训练课也令我印象深刻。丁师范的课程还是比较偏向金载勋的训练体系，我还没有从幼时游戏式的热身，更多体能、全方面身体素质方面训练的热身中走出来，尤其在分散练习独自一人无人指导，也无熟悉同伴共同训练的时候倍感孤单，甚至觉得这样好似报名了一个健身房，得不到一些及时的指导和团队协作和竞争上的训练。可是ysf的课程是让排成两队，做一些交叉步，青蛙跳等素质练习的动作，逐渐让我找回了舒适圈，一些动作也得到了杨师范的认可让我逐渐找回了自信。课上的腿法练习中，杨师范总是可以观察一会，很准确的找出动作的问题所在，并给出改进方案，在尝试了师范的方式后，自我感觉也得到了显著的提升，那种成就感令我找回了一些快乐。那节课是我一个月以来最快乐的一次，从那以后开始甚至有点期待杨师范的课程。然而半年来都没有期盼到。</p>
<p>所幸的是，胡哲也与杨师范交情甚好，课后总是会在杨师范的指导下进行1对1柔韧度练习。我深知柔韧度对跆拳道训练的重要性，柔韧度很大程度上决定了腿法动作的幅度、标准度和灵活度，对挑战横叉的胡哲我眼中尽是钦佩和羡慕，所以选择留下来帮助胡哲按住一条腿。可长久以来都无法鼓起勇气说自己也想要完成这个柔韧度的训练。最终，选择报名了一个舞蹈软开度课程。课程结束也没有达到一些显著的提升。</p>
<p>在胡哲的横叉飞速进步中，很快就迎接来了过年，随着本命年的到来，居然是动荡全球的疫情风云。很早回到深圳的我还是不能够回到道馆继续训练。在家里的一个月时间中，我时不时会跟着keep进行一些简单的软开度训练，但都是2周没有见到效果就放弃了。</p>
<p>疫情期间一次公司聚餐中，意外接到了熟悉的dsf打来的电话，听到熟悉而亲切的声音的我激动但却不得不继续公司里的工作，因此和dsf解释说是否可以晚点联系。聚餐后匆忙给dsf打回去的我，打过去却是金载勋的总机电话，那一天时间里似乎都在期盼着dsf再次拨回来。又一次接到电话的我赶忙问何时可以开课，已经迫不及待了，dsf说会随政府安排，尽快，但是叮嘱说在家也要勤加练习哦。真是一个有爱有人情味的道馆。</p>
<p>终于熬到了四月底，又可以回到道馆了。之后的一个月就认识了肖宝杰小妹妹。对bj的印象是，总会有一个男孩子在楼上等着她😂好像不是很多话。</p>
<p>后面就迎来了开新的分馆，一直打探分馆主教练的我其实内心早就在想这样是不是就可以上更多的杨师范的课了，说不定还可以找杨师范帮忙拉开韧带。心里想着，似乎黑带梦想又重燃了希望。</p>
<p>分馆如期开张，杨师范也如期升职为分馆主教练，更开心的是，本舍不得在杨师范和丁师范中做个选择，海岸城的课程居然是两位主教练轮换教课，这也解除了我的一部分顾虑，不会因为总是上一个师范的课而感到略显重复枯燥。</p>
<p>宝杰也转到了分馆，并且在我第一次去的时候，发现宝杰也开始了自己的横叉磨练。杨师范似乎从我眼中看出了我的心思，便问我要不要也一起来，终于稍微鼓起勇气的我开始尝试。也在杨师范的“狠绝”下第二天就达到了一个小目标。</p>
<p>于是就这样开始了两个人的横叉之旅。印象很深的是一次我已经满头大汗了，杨师范还是心里一坚持，又加了一些强度，那一刻感觉两边的腿内侧的筋似乎开始燃烧了，然而神奇的是，从那以后两侧的筋就再也不痛了。但是，让我略生退却心思的是，有时候拉完韧带回到家，会导致前半夜腿痛，影响睡眠，甚至一次需要敷上止痛膏药才可以稍微缓解一些这种疼痛，并且有时候的腰痛也开始让我怀疑自己这把老骨头是不是回笼胯，是不是天生就是这么硬，因为小的时候也是竖叉很快就拉开了，横叉花费了多一倍的时间才拉开。</p>
<p>尽管心生疑虑，但是还是忍住不敢讲出口，怕师范会过于担心我的身体，我就更加放弃自己了。</p>
<p>这其中，印象很深的一次是，那次是隔了四五天没有压韧带，我心里很害怕退步了，太过紧张所以导致肌肉紧绷，大师兄也在一旁说“世上无难事，只要肯放弃”，丁师范也在一旁劝我不要勉强，最后就选择放弃了。回家路上我很严肃，责怪自己，又不知道怎样开解自己。只是心里充斥了恨铁不成钢。第二天都没有颜面去上杨师范的课了，感觉似乎辜负了师范。然而还是厚着脸皮去上了晚课，那一天，又到了一天一度的“地狱”时刻了，我心里仍旧是害怕，身体上还是诚实地去多热了热身，希望这样能缓解一些。宝杰已经完成了自己的每日任务，昨天就放弃的我，开始犹豫。杨师范给我喊了过去，一只很有力气的脚推在我的小腿上，说“奥兰你今天就别想跑了”。听了这话的我心里也想，不能再辜负师范的努力和期望了，眼镜一扔，眼睛一闭，就想豁出去了，一定要完成。哎，很神奇，很快就到达了目标了啊，没有想象中的那么痛啊？甚至比一周前还要好很多？？那天我真的十分感激杨师范帮我克服心中的魔鬼，终究是没有中断和放弃。</p>
<p>后面坚持和宝杰一起开横叉的时光艰辛而又互相勉励。柔韧度进步后在一些腿法上感觉更加收放自如，自我都感觉来到海岸城的这段时间实则进步了一大截。终于迎来了红蓝带考试，我也很骄傲的到后海馆给许久未见的其他同学和老师们看在杨师范指导半年下的飞速进步。</p>
<p>可惜后面宝杰韧带拉伤，我自己虽然心里不想放弃，但是也找不到主动去找杨师范的理由，所以也就随着宝杰的韧带拉伤中止了每天的撕心裂肺。</p>
<p>下半年的击破表演和红带考试是2020后半年的重点，两次都有各自的遗憾，但我似乎也开始慢慢学会开解自己的遗憾，不再懊恼和自责，而且在训练中多加补足，争取下一次能弥补遗憾。下半年的周六也增加了更多的体能训练，自我感觉在体能上也得到了长足进步，不再那么的担心考场中体力不支而无法呈现出最好的表现了。</p>
<p>宝杰应该是我跆拳道生涯里第二敬佩的学员。半年来的习练无论是在带色还是技术上的飞速进步是她每日坚持的成果，多次的韧带拉伤也没有打消她的韧带梦想，再加上实战时候的勇敢和坚定，生活中的不服输和挑战自我都让我这个废柴似乎又找回了一点年轻时的热血。</p>
<p>说到实战，胡姐姐头脑清晰，技术全面，实战经验丰富却仍谦虚勤练，宝杰呢，勇猛坚定，时机紧握，不畏级别高低，实战场上的自信令哪怕是较她级别高许多的学员都心生几分畏惧。这两个人不仅能打，还好打，每逢周六晚上都已经不想去练习枯燥的基本动作了，直接是一通暴打。红带时没有实战压力的我丝毫不想卷入这两个强者之间的战争。然而考完红带后，考试的压力让我不得不去面对自己畏惧的对抗互捶。</p>
<p>第一次参与其中的我还是一团混乱，找不到节奏，除了打不着别人就是被别人狂打头，僵硬的颈椎都放松了好几分。但是鼓起勇气的我在两个强者的陪伴下，再加上有事没事就开始在视频上学习对打技巧，几次习练后似乎找到了一些门道，克服了心中的恐惧，找到了适合自己的一些技巧，在和丁师范对打时丁师范也夸奖到进步了。</p>
<p>另一件让我感到十分温暖的事情是，在考黑红带前大约半个月的时间，偶然问到师范多久可以考黑带，当时大概是10月底，师范说，大概明年2月考黑红带，明年10月考黑带，当时已经学完考试内容的我略感疑惑，感觉自己每日的坚持没有了动力，习惯了临时抱佛脚的我觉得那为何不等快考试的几个月再加紧习练呢。于是是一个月的停练和心中放弃的魔鬼又跑了出来。偶然间和宝杰胡哲提到心中想法后，两位同伴一个苦口婆心的劝导，另一个还特地打来电话慰问。甚至二位还帮我将内心说不出口的想法讲给了师范，看看有没有解决办法。于是乎，师范通知，可以尽快准备考试了。说来也为自己的功利感到羞愧，本身想要踏实完成黑带考试，成为一名优秀黑带选手的我在面对选择的时候，还是选择了更加功利的早些得到黑带。但我很感激同伴的热情帮我争取这次机会，我也很感激师范的信任让我能够拥有这一次机会，因此也是倍加珍惜，尽量争取能够多加习练，争取能够多多出勤，不辜负同伴和师范的期望。也在持证教练肖教练的帮助下逐步跟上了品势内容。说到这里，不得不cue，肖教练真的是有教练的天赋，恩威并施，耐心又严厉，我猜她如果有此志向的话，一定会成为优秀的女教练。</p>
<p>世上无难事，只怕有心人。一句俗语，可也一直在陪伴我的成人跆拳道生涯。好汉不提当年勇，年轻的灵活敏捷早已该放到过去，对现实的困难发起挑战是要从几位师范、同伴身上学习的，“礼义廉耻，忍耐克己，百折不屈”曾经陪伴过我俯卧撑从0-n的突破，陪伴过我冬季3000m长跑的咬牙坚持，和许多运动生涯的艰辛瞬间。年长的我虽然心智在逐渐成熟，但对跆拳道的意志力却与幼时的自己不增反减。很感谢遇到的两位鸡血同伴挽救了一个废柴跆拳道选手，很感谢这一个又一个被迫立下的flag目前还没有轰然倒塌，很感谢我们即便面对质疑与世俗的不解，仍然坚持在自己热爱的道路上。</p>
<p>对成年后仍在这条道路的我，不期待三天打鱼，两天晒网的短暂热情，期待能够习得自律的习惯和稳步进步，期待能够像几位师范和同伴一样，热情不减，勇敢更多。许愿自己2021flag不倒，心愿达成。祝愿师范事业继续蒸蒸日上，生活温馨顺利。祝愿两位伙伴不忘初心，永远热忱。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>新心愿达成，新身份新快乐收获</title>
    <url>/2021/03/13/jhk-teaching-assistant-1st-time/</url>
    <content><![CDATA[<p>纪念一下收获了爱与能量的一天！</p>
<p>早上8点多起来，就是为了能不放同学鸽子，毕竟已经放了道友们好多次鸽子了，再放鸽子感觉我人品有问题，必须得去！所以就这样开启了神奇的一天。</p>
<p>早上8-9就起床了去看表演，自己做的很烂，朋友们都比我现场看的好很多哈哈哈哈，dsf剪辑也有点厉害啊，（就像胡哲说的，了解了一个人的人物传记之后，就会对这个人很有好感），我今天又成为dsf的迷妹了，感觉又在发着光。</p>
<p>但印象很深刻的是马玲的冲木前面几个动作巨好看，完美，养眼，赏心悦目以及自愧不如（可能这就是宝杰说的那种感受吧，她觉得很完美，但或许当事人不觉得）。然后就看到我的后踢又驼背了，是三个人里最差的，腰背部力量不够，身体没撑起来（难道这也是ysf晚上突然又cue我们做后踢辅助动作的原因之一吗？难道他也发现问题了？哈哈哈哈）</p>
<p>其他道馆，尽量发现的话还是有几个精品，但是普遍水平感觉没差很多，高级别很帅，看来tkd也是吃经验的。</p>
<p>10:15，换了便装的上身卫衣，匆匆赶到南山馆，还可以继续看一会，海岸城馆气势很大，哈哈哈哈，感觉果真什么师范带出什么样的学生。</p>
<p>10:50，收拾道具，迷茫，还傻乎乎的问ksf需不需要帮忙，没眼力见，在dsf问“你俩谁可以拍照”的时候，终于又勇敢了一把，舒坦爽快，也就开启了勇敢的一天。</p>
<p>11:00，开始30来个学员的儿童班课程，级别从白带到黑带，年纪从三四岁，到十来岁，看得头都大了，无法想象怎么上课，但是作为大头兵的我就哪里需要哪里搬就好了。</p>
<p>11:05，ksf整队敬礼，带队热身活动，迷茫，不知道该站哪，不知道该不该一起热身，不知道该不该帮助小朋友，这时的马师范已经开始自觉热身了，还得到了dsf的表扬，🍋，后面才知道原来msf之前有过这种经验，一回生二回熟哈哈哈</p>
<p>11:10，30个小朋友按级别分成了五队，五个助教，一人带一队，当场感叹师范简直是人才，这么简单的除法就解决了30个熵混乱的问题哈哈哈哈。尴尬的是，ksf喊我们三个过去的时候，作为打头的我愣住了，不知道该从哪个路径走过去，恍惚了几秒，一向超级超级super-nice温柔的ksf突然很严肃的说，助教要快一点啊！妈呀被嫌弃了有点尴尬，还是要硬着头皮迎接接下来的任务啊。</p>
<p>11:15，ksf做师范，助教帮忙拿脚把，妈呀第一脚就被踢，第二脚又被踢了，都怪我脚把拿多高我都不知道，而且小朋友身高一会高一会低，脚把位置一直在变动，我的天，为啥有的小孩踢左脚，有的右脚，有的正面，有的背面，有的格斗式，有的并脚，刚才ksf怎么做的？？？脑子一面空白，dsf过来旁边告诉我，你放松一点，不要脚把抓的那么紧，dsf也帮助小朋友指正动作，这时候我终于只需要做个不需要思考的拿把机器就好了，舒坦，但是dsf走了之后又陷入了慌张和迷茫。终于ksf喊停了，第一组动作，带小朋友一起划水结束哈哈哈哈</p>
<p>11:20，这一次的示范动作我很认真的观察ksf怎样站位，怎样准备的，怎样起腿，怎样结束动作，脚把位置多高，甚至关键点是什么。到我这边实践后，小朋友们还是乱七八糟，我开始学会说，换脚，面向我，背对我，左腿，右腿等简单的方位动作，终于捋顺了大家的完整动作，从一个方向出腿了。开始也去观察主要的问题点，并且提出合理的建议，或者做出简单的解决方案，才到我大腿个子的小朋友（看似也就三四岁），居然可以听懂我的简单指令？？？成功建立沟通连接了之后，感觉开始上手了。居然也可以听懂我的解决方案？？小朋友真厉害！！真的进步了一点点点！好棒！！！（虽然内心疯狂喜悦，肾上腺激素疯狂上飙，但是嘴巴上的我还是一个敷衍的表扬机器），“很好”，“还不错”，我自己都感觉自己的表扬十分敷衍，但是又学不会dsf的那种“对iiiiii了！”，“对iiiii，就是这样做嘛！”太厉害了，佩服的五体投地了。</p>
<p>渐入佳境的我完成了几组动作指导后，发现队伍有点乱，不知道该怎么管理，发现后面小朋友在自己玩，不知道该咋办，不敢凶他们，发现小朋友东张西望了，不知道该怎么勾回他们的注意力，只能先注意到在做动作的这个人身上了。</p>
<p>小朋友们的性格，脾气，也各式各样，各有千秋，蓝带高个子朋友打头，最高级别，年龄也相对较大一些，所以能够沟通，动作上也是最好的。所以多是表扬，和给他更多的一些挑战。比如旁边两组高级别都是跳后旋，他在我们这组里级别最高，我看他也可以尝试，就叫他尝试了一下，发现果真还不错、他的表情令我感觉他自己做了高级别的动作，也有点小满足，小骄傲，小自豪，哈哈哈，他的满足也是我的快乐，我也接收到快乐了。</p>
<p>绿带也是大腿高度的小男孩，从一开始就一直笑笑的，笑的人都要融化掉了，僵硬冰冷了这么多年的我（3年）感觉被暖到了，所以我也卸掉了社会中的面具，变得笑笑的，微笑也给我带来了满足和快乐。这个小男孩更逗的一点是，后面练拳法，我不知道为啥他总要连续快速做，本来只要做一个就好，我说你这咋还变成连环拳了，他笑笑继续他的连环拳。柴师范过来说，哎，你不可以这样对待姐姐哦，要好好表现，他果真听话了？？？开始一拳一拳做。原来他刚才不是没听懂或者不会做？柴师范一说他咋就懂了？？所以刚才是故意的？引起我的注意？？被小孩的聪明才智折服也骗到了，怪不得老师都喜欢偏爱坏学生，原来坏学生总是能很聪明的挑战到他的底线，让她觉得教育这件事都变有趣了，更想要管教好她，所以才会选择偏爱更有趣和更有挑战性的学生。哈哈哈哈哈神奇。</p>
<p>绿黄带小孩给我印象也很深刻，几次勾踢和后旋踢有点惊艳到了，感觉不是他这个级别应该领悟到的东西啊。哈哈哈，或许这就是低级别的快乐。总是会被夸奖。哈哈哈哈，反正黑红带以前我也总是被夸奖，自从要考黑带了就每日被diss。说回绿黄带，本来期望绿黄带应该不太能做这两个动作吧，居然甩了几下有模有样的？？？ksf也予以了多次的表扬和认可，原来不止我觉得这个有天赋，是个苗子。更逗乐的是，他的每次出拳之前，别人都是ha之后就出拳了，我不知道为啥，这个小孩看起来只有三四岁，他好像思考了有一两秒，攥紧了拳头，好像在蓄力的样子，打出的拳头，虽然那么小只，感觉也很有力气啊我的天，或许这就是传说中的天赋？？这个蓄力的动作太神奇了，感觉一个三四岁小孩都充满了神秘感，不知道他的小脑袋瓜在想什么。（哈哈哈这里有个插曲，就是，dsf跑过来，说，让姐姐看到你的力气哦！我心想，dsf真尊重我们，真社会，真客气，真乖，哈哈哈哈我都不好意思了，也不知道小宝贝心里怎么想的。）在思考出拳方向？还是在蓄力？不知道，但是打出的拳头还是不错的</p>
<p>还有多次dsf跑过来说，一开始说，你放松一点，你不要拿脚把那么紧张，你不要扎着马步，这一天腰都断了，放松自然一点。你要适时有感染性的鼓励一下小朋友，你要带有情绪的，“哇！真棒！”，“好棒啊”，做得好的甚至可以“give me five”。这个时候我感觉自己才是道场里的学生，dsf和小朋友都是我的老师，在教化着我怎么去应对这个不太擅长的局面，dsf是我的指路人，用他的经验告诉我怎样是对的，小朋友们是我的助教，陪我实践，给我反馈，一次又一次表情上的反馈告诉我这样做是不是对的。大多数都会是肯定我，我这样做确实是对的，他们很开心，他们有快乐，有成就感。天呐，没想到，我才是这堂课收获最多的人。</p>
<p>好了说回我队的小宝贝，绿带高个子男孩，看起来有四五年级了，很成熟稳重，容易沟通，还能在我搞不了小朋友的时候，帮我指导一下，只不过就是有点好像循规蹈矩，很认真做，但感觉还是不太灵动，所以会感觉有点死板，再加上最明显的问题就是速度慢了，所以对他的要求主要是提高速度，也是希望能提高灵动性，他一开始没有讲过话，后面中间一次他好像问了一句，是不是这样，那个时候我感觉好像互相产生信任了，我给他的回复也给了他安全感。他也更加卖力去做了。也很明显感觉速度快了动作舒服很多。</p>
<p>蓝绿带软萌小女孩，妈呀太可爱了，头发好多，软萌软萌的，不忍心碰她，哎，没想到协调性贼好，转身动作极其完美酷炫，又超乎想象了。小拳头不是打过来，好像是砸过来的，太可爱了，都不忍心纠正她可爱的动作了，哈哈哈哈。萌化。</p>
<p>最后好像还有一个绿带还是蓝绿带的小宝贝，他应该是做的都还不错，所以没有花心思给他改正动作的话，印象似乎没有太深刻，这可能就是那种平凡的孩子？自己很乖，很优秀，很听话，动作也很标准了，但是就是不容易给人留下印象了，突然有点代入一向“普普通通”“平平凡凡”的自己。</p>
<p>总而言之，这次助教课收获颇多！这种能量的突变确实力量巨大，从去之前的恐惧，担心是熊孩子，从小觉得自己没有孩子缘，担心自己管不住小孩，等等，居然全都得到了救赎，师范们把小朋友们带的很好，该听话的时候很听话不闹，小朋友们像小天使一般，用他们的笑容，单纯，善良，进取，努力，甚至调皮等都带给了我巨大能量，让我这一天都充满了能量，（也导致我现在话匣子打开都停不下来了），有点上头，有点快乐。</p>
<p>课后dsf暗示我说，我在楼下坐，你有什么技术上的交流可以来找我，我温习了今天的课程之后，留了十分钟，本想找dsf看一步对打，没想到坐下就开始聊教课体验，哈哈哈。我真诚的表达了自己的一些小想法，他也很快乐，似乎觉得给我的能量似乎也反馈给他了一部分快乐，我能从表情里看到他对于这件事安排的满意程度，主要是因为他觉得我乐在其中，很有收获。我说这个事情，有点有成就感啊，dsf说，我们做师范最大的就是成就感，blablabla说了一堆，总而言之就是表达我这种小想法的认可，有成绩感是正常的。应该要有成就感。然后我说我没有经验，很紧张，dsf说以后还可以找你来，我很开心，也爽快的答应了。甚至还说，如果你以后，敲代码敲烦了，不想敲了，就给我打电话，（略带哭腔）地说，师范，我代码敲的不快乐了，我马上给你安排助教课，给你找找成就感，然后你再回去快乐的敲代码。天啊啊啊啊啊啊啊啊，听到这句话的我真的彻底融化，在这个诺大的城市里，受了委屈，有师范给我兜底，我真的心怀感激，备感荣幸，也感受到了深深的安全感，就算捅了篓子，也有人有事情能救回我的感觉，这也太适合我这个时不时低沉但是内心始终温暖热血的表面丧人了。听完这话的我，虽然内心知道这一把年纪了，肯定是不会好意思这样做的，最多也就是师范需要我，我会很乐意去帮忙，但是内心也是着实被暖到了，感觉似乎在深圳这个城市里伪装的冰冷，今天被小朋友和师范们全都融化了。表达完我有点快乐有点上头的情绪后，师范似乎也放宽了心，说以后还会找我来，让我慢慢接触尝试，说不定就转师范了哈哈哈哈哈，这种职业道路的变更，虽然想过，想过无数次、很多次，数万次，但是太需要勇气了，风险极大，落差极大，自己的专业性和天赋能力也远远不足，只能慢慢来，先享受小天使们带给我的快乐和满足吧。我现在算是体会到dsf所说的大师兄是自己享受指导和帮助人的过程，我现在算是理解了，之前可能是我们理解错了吧。到现在也算又圆了一个小时候的跆拳道助教梦想哈哈哈哈，人生真的太神奇了，儿时的梦想居然一个一个接一个的实现了，这种肾上腺素飙升的感觉也太快乐了。</p>
<p>至此，先鞠躬敬礼感谢丁师范的信任给我这一次机会，感谢dsf在看我迷茫的时候一直耐心的指导我该怎样做，哪里不对，（比指导我技术的时候耐心多了哈哈哈哈），感谢小朋友们包容我这个新手助教，给我信任、微笑、力量和快乐，感谢杨师范包容我中途跑掉，还不回来上课了，晚上还跑过来蹭场地的同时，晚上还不计前嫌地帮我个人抠动作了有三四十分钟，感谢柴师范还是用他的一如既往的温暖温暖着我，感谢自己珍惜这次机会才得偿所愿，感谢心中有梦的自己这么多年还没有忘记最初的梦想，人间又值得了！！！！！</p>
<p>最后上升一些生活哲学，</p>
<ol>
<li>六个小朋友不同的性格，似乎都让我看到了社会和人生。</li>
<li>你带的学生，最后就会变成你自己的一面镜子。dsf这句话真的绝了。联想到现实更绝。ksf和dsf，dsf和cj，ysf和bj，hjj和我算是dsf和ysf的混交吧，hjj偏dsf，我算是混的比较平均的。哈哈哈哈也算是这种结合方式的首次尝试，也挺特别和快乐的。</li>
<li>tkd快乐今日double、乃至triple，hundredable，突然感觉被赋予了使命，感受到了教育的快乐</li>
<li>又一次感受到了几个师范的温柔，简直温柔了深漂的岁月，今天繁师范问我为什么成人还要练tkd，之前一个小朋友的爸爸也问过我，我第一次回答对黑带的执念（或许是我最初的梦想），今天回答工作之余的发泄和对现实生活的逃避，感觉有点官方现实，但也有点映射哲学了。现在总结一下，于我而言，最开始是对黑带的执念，小时候不断突破自己的回忆让我埋下种子，在更有时间精力的时候完成小时候未竟的梦想，在参加了之后完全是被金载勋吸引了，道馆师范对自己要求太高，包括技术上，心理上，各种综合素质都相对于其他成人道馆要成熟太多太多了。对深漂的自己是一种解脱，感觉像是在深圳的一个家，朋友之间没有利益冲突，可以很坦率的交朋友，师范燃烧自己，照亮他人，最关键的是，师范自身的经历，相近的年龄，能给自己带来很多人生的启迪，有时候工作不顺训练上身体解压，朋友上互相开导，师范也会给出建议以及甚至给你兜底，懂得教育心理学的师范们，即便你受欺负了，也能帮你解脱出来。女孩子独自一人在外的护身防卫，不要以为是花架子，金载勋不一样，不仅仅玩运动竞技场上的规则，尤其是成人班很强调实用性的，很多动作你是真实可以应用到实际情景的，上海两例女性自我保护事件，这个东西会给自己在外漂泊更多安全感。价格不贵的， 挠痒痒一样，对道馆来说，成人班只是他们积累更多经验，基本不赚钱的，你说这种性价比这么高的活动，不值得吗？时间也不会耽误很多，但是需要坚持，但是培养任何一项业务爱好和特长，都需要坚持。最后才是，减肥，运动，保持身材，柔韧性，协调性，力量等等很多问题。如果想要自己的生活，不仅仅是工作，家庭，没有别的味道了，那还是非常值得推荐的。总而言之，工作之外的生活，变得更加丰富多彩，快乐，充实和解压。</li>
</ol>
<p>对自己黑带的要求是：小于3次失误，击破、一步对打和品势因为只有一次机会，所以要0失误。横叉flag不要倒啊加油啊不要倒啊。</p>
<p>好了，放过自己，今天的输入太多，终于输出完毕，今天可以圆满结束了。晚安蓝蓝。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>BC challenge 2019 Top5队伍 技术分析</title>
    <url>/2020/03/09/bc2019top5/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>Frontend</strong></th>
<th><strong>Duration Modelling</strong></th>
<th><strong>Spectrogram modelling</strong></th>
<th><strong>Vocoder</strong></th>
<th><strong>Features</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>USTC-iflytek</strong> <strong>科大讯飞</strong></td>
<td>Tasks:  special marks procession, polyphones classification, breaks prediction focuses prediction.  Methodoly: Bidirectional Encoders Representations from Transformers (BERT)-based multi-task models</td>
<td>LSTM-RNN models autoregressive model structure,</td>
<td>A statistical parametric speech system (SPSS) GAN-based multi- task acoustic modeling Fundamental frequency (F0), 41 dimensional mel-cepstra (M- CEP), band aperiodicity (BAP) were adopted as the acoustic features</td>
<td>Wavenet   The acoustic feature used was the joint feature vector of Mel-cepstrum, F0 and the u/v decision. Multi-speaker dataset for argumentation</td>
<td>Text-side: Manual annotations: Pinyin(with tone), PW, PP, and focus position Speech-side: Frame-level acoustic features:</td>
</tr>
<tr>
<td><strong>DeepSound</strong> <strong>深声科技</strong></td>
<td>Tasks: text normalization, qingsheng, sandhi and erhua, : rule-based G2P: Bi-LSTM prosody prediction, PW, PPH, IPH: Bi-LSTM BiLSTM-based recurrent network (RNN) is used in the G2P module for polyphone and prosody prediction.</td>
<td>/</td>
<td>VQVAE. + a embedding+prenet oper- ation + GAN based postfiltering     (robust on the unclean dataset )</td>
<td>robust multi-speaker neural vocoder conditioned on the mel spectrograms</td>
<td>manual and auto- matic tagging operations: phoneme, tone, prosody and pause duration</td>
</tr>
<tr>
<td><strong>腾讯</strong></td>
<td>Festival front-end to predict phoneme, tone and other linguistic features   +   BERT sentence embeddings are generated by a pre-trained Bert model.</td>
<td>/</td>
<td>A multi-speaker model is trained first.</td>
<td>multi-speaker model trained first. Wavenet</td>
<td>linguistic feature (The HTS full-context label) and sentence embedding mel spectrograms + channel embedding</td>
</tr>
<tr>
<td><strong>灵伴</strong></td>
<td>text normalization, word segmentation, part-of-speech tagging, phonetic disambiguation word segmentation of the sentence, Part-of-Speeches (POS) of this word sequence and prosodic hierarchy</td>
<td>/</td>
<td>DNN-LSTM</td>
<td>Wavenet   ground-truth mel-spectrograms plus F0</td>
<td>spectral envelope, fundamental frequency (F0), contextual labels (phone-related and word-related features)</td>
</tr>
<tr>
<td><strong>Horizon</strong> <strong>南京团队</strong></td>
<td>The corresponding texts were manually embedded into 476-dimensional vectors using our own text an- alyzing system. The embedded vectors consisted of one-hot encoded phonemes, tones, part-of-speech, prosodic boundaries and the position information.   Prosody boundary: phoneme boundaries, syllable boundaries, phrase boundaries, secondary phrase boundaries</td>
<td></td>
<td>DCTTS[14] and Deep Voice 3[13]</td>
<td>WaveRNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>韵律建模思维导图</title>
    <url>/2020/09/20/prosody-modelling/</url>
    <content><![CDATA[<p><img src="/images/%E6%83%85%E6%84%9F%E5%BB%BA%E6%A8%A1.png" alt="韵律建模论文思维导图"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>语音顶会论文集总结</title>
    <url>/2020/12/03/speech-papers/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>会议地点</strong></th>
<th><strong>离线论文包</strong></th>
<th><strong>论文集网址</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Icassp 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Interspeech 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td><a href="https://www.isca-speech.org/archive/Interspeech_2020/">https://www.isca-speech.org/archive/Interspeech_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Neurips</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://proceedings.neurips.cc/">https://proceedings.neurips.cc</a></td>
<td></td>
</tr>
<tr>
<td><strong>BC 2020 &amp; VC 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://www.isca-speech.org/archive/VCC_BC_2020/">https://www.isca-speech.org/archive/VCC_BC_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Iclr 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech">https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>论文合集网址</strong></td>
<td>-</td>
<td>-</td>
<td><a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive">https://www.isca-speech.org/iscaweb/index.php/archive/online-archive</a></td>
<td>含interspeech、ssw等论文合集</td>
</tr>
<tr>
<td><a href="https://openreview.net/">https://openreview.net</a></td>
<td>含iclr、icml、acm等论文集</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Vocoders 模型总结</title>
    <url>/2020/12/03/vocoders/</url>
    <content><![CDATA[<p>语音合成声码器脉络总结如下，持续更新ing</p>
<table>
<thead>
<tr>
<th><strong>Order</strong></th>
<th><strong>Model</strong></th>
<th><strong>Year</strong></th>
<th><strong>Institution</strong></th>
<th><strong>Conference</strong></th>
<th><strong>Inherited Model (Base model)</strong></th>
<th><strong>Corresponding Author (Team leader)</strong></th>
<th><strong>URL</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>1</strong></td>
<td>WaveNet</td>
<td>2016.9</td>
<td>Google DeepMind</td>
<td>SSW 2016</td>
<td>CNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1609.03499.pdf">https://arxiv.org/pdf/1609.03499.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>WaveRNN</td>
<td>2018.6</td>
<td>DeepMind &amp; Google Brain</td>
<td>ICML 2018</td>
<td>RNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1802.08435.pdf">https://arxiv.org/pdf/1802.08435.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>WaveGlow</td>
<td>2018.10</td>
<td>Nvidia</td>
<td>ICASSP 2019</td>
<td>WaveNet</td>
<td>Rafael Valle</td>
<td><a href="https://arxiv.org/pdf/1811.00002.pdf">https://arxiv.org/pdf/1811.00002.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>LPCNet</td>
<td>2019.2</td>
<td>Mozilla, Google</td>
<td>ICASSP 2019</td>
<td>WaveRNN</td>
<td>Jean-Marc Valin</td>
<td><a href="https://arxiv.org/pdf/1810.11846.pdf">https://arxiv.org/pdf/1810.11846.pdf</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WaveGAN</td>
<td>2019.2</td>
<td>UC San Diego</td>
<td>ICLR 2019</td>
<td>GAN</td>
<td>Miller Puckette</td>
<td><a href="https://arxiv.org/pdf/1802.04208.pdf">https://arxiv.org/pdf/1802.04208.pdf</a></td>
</tr>
<tr>
<td><strong>6</strong></td>
<td>Multi-band WaveRNN</td>
<td>2019.4</td>
<td>Tecent AI Lab</td>
<td>Interspeech 2020</td>
<td>DurIAN, WaveRNN</td>
<td>Dong Yu</td>
<td><a href="https://arxiv.org/pdf/1909.01700.pdf">https://arxiv.org/pdf/1909.01700.pdf</a></td>
</tr>
<tr>
<td><strong>7</strong></td>
<td>MelGAN</td>
<td>2019.12</td>
<td>University of Montreal, Mila, Lyrebird AI</td>
<td>NeurIPS 2019</td>
<td>GAN</td>
<td>Yoshua Bengio</td>
<td><a href="https://arxiv.org/pdf/1910.06711.pdf">https://arxiv.org/pdf/1910.06711.pdf</a></td>
</tr>
<tr>
<td><strong>8</strong></td>
<td>SqueezeWave</td>
<td>2020.1</td>
<td>UC Berkeley</td>
<td></td>
<td>WaveGlow</td>
<td>Bichen Wu</td>
<td><a href="https://arxiv.org/pdf/2001.05685.pdf">https://arxiv.org/pdf/2001.05685.pdf</a></td>
</tr>
<tr>
<td><strong>9</strong></td>
<td>Parallel WaveGAN (PWG)</td>
<td>2020.2</td>
<td>LINE Corp., NAVER Corp.</td>
<td></td>
<td>GAN</td>
<td>Ryuichi Yamamoto</td>
<td><a href="https://arxiv.org/pdf/1910.11480.pdf">https://arxiv.org/pdf/1910.11480.pdf</a></td>
</tr>
<tr>
<td><strong>10</strong></td>
<td>Multi-band MelGAN</td>
<td>2020.5</td>
<td>西北工业大学，sogou</td>
<td></td>
<td>melgan, multi-band</td>
<td>Xielei</td>
<td><a href="https://arxiv.org/pdf/2005.05106.pdf">https://arxiv.org/pdf/2005.05106.pdf</a></td>
</tr>
<tr>
<td><strong>11</strong></td>
<td>FeatherWave</td>
<td>2020.10</td>
<td>Tecent</td>
<td>Interspeech 2020</td>
<td>MB LP, WaveRNN</td>
<td>Shan Liu</td>
<td><a href="https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf">https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf</a></td>
</tr>
<tr>
<td><strong>12</strong></td>
<td>WaveGrad</td>
<td>2020.10</td>
<td>Johns Hopkins University, Google Brain</td>
<td></td>
<td>CNN</td>
<td>Heiga Zen</td>
<td><a href="https://arxiv.org/pdf/2009.00713.pdf">https://arxiv.org/pdf/2009.00713.pdf</a></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ASRU 2019 语音合成相关论文</title>
    <url>/2021/06/17/asru-2019-papers/</url>
    <content><![CDATA[<div class="table-container">
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>论文题目</strong></th>
<th><strong>作者</strong></th>
<th><strong>单位</strong></th>
<th><strong>摘要</strong></th>
<th><strong>关键词</strong></th>
<th><strong>论文链接</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</td>
<td><em>Xuankai Chang</em>1,2<em>, Wangyou Zhang</em>2<em>, Yanmin Qian</em>2†<em>, Jonathan Le Roux</em>3<em>, Shinji Watanabe</em>1†</td>
<td>1Center for Language and Speech Processing, Johns Hopkins University, USA 2<strong>SpeechLab</strong>, Department of Computer Science and Engineering, <strong>Shanghai Jiao Tong University</strong>, China 3Mitsubishi Electric Research Laboratories (MERL), USA</td>
<td>MIMO-Speech, which extends the original seq2seq to deal with <strong>multi-channel input and multi-channel output</strong> so that it can <strong>fully model multi-channel multi-speaker speech separation and recognition</strong>. MIMO-Speech is a fully neural end-to- end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model.</td>
<td><strong>Overlapped speech recognition</strong>, end-to-end, neural beamforming, <strong>speech separation</strong>, curriculum learning.</td>
<td><a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</td>
<td><em>Fengyu Yang</em>1<em>, Shan Yang</em>1<em>, Pengcheng Zhu</em>2<em>, Pengju Yan</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1∗</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University</strong>, Xian, China 2Tongdun AI Lab</td>
<td>We introduce a novel self-attention based encoder with learnable Gaussian bias in Tacotron. The proposed approach has the ability to generate stable and natural speech with minimum language-dependent front-end modules.</td>
<td>Tacotron, end-to-end, speech synthesis, <strong>self-attention, Gaussian bias</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>LEARNING HIERARCHICAL REPRESENTATIONS FOR EXPRESSIVE SPEAKING STYLE IN END-TO-END SPEECH SYNTHESIS</td>
<td><em>Xiaochun An</em>1†<em>, Yuxuan Wang</em>2<em>, Shan Yang</em>1,2<em>, Zejun Ma</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1⇤</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University,</strong> Xi’an, China 2 ByteDance AI Lab</td>
<td>we introduce a hierarchical GST archi- tecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech.</td>
<td>Speaking style, disentangled representations, <strong>hierarchical GST,</strong> style transfer</td>
<td><a href="http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf">http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>BOOTSTRAPPING NON-PARALLEL VOICE CONVERSION FROM SPEAKER-ADAPTIVE TEXT-TO-SPEECH</td>
<td><em>Hieu-Thi Luong<strong>1,2</strong>, Junichi Yamagishi**1,2,3</em></td>
<td>1SOKENDAI (The Graduate University for Advanced Studies), Kanagawa, Japan 2National Institute of Informatics, Tokyo, Japan 3<strong>The University of Edinburgh</strong>, Edinburgh, UK</td>
<td>Bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.</td>
<td><strong>voice conversion,</strong> cross-lingual, speaker adaptation, transfer learning, text-to-speech</td>
<td><a href="https://export.arxiv.org/pdf/1909.06532">https://export.arxiv.org/pdf/1909.06532</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WAVENET FACTORIZATION WITH SINGULAR VALUE DECOMPOSITION FOR VOICE</td>
<td><em>Hongqiang Du<strong>1,2</strong>, Xiaohai Tian<strong>2</strong>,</em> <strong><em>Lei Xie***</em></strong>1*<strong>**</strong>, Haizhou Li*<strong>*</strong>2***</td>
<td>1School of Computer Science, <strong>Northwestern Polytechnical University</strong>, xi’an, China 2Department of Electrical and Computer Engineering, National University of Singapore, Singapore <a href="mailto:hongqiang.du@u.nus.edu">hongqiang.du@u.nus.edu</a>, <a href="mailto:eletia@nus.edu.sg">eletia@nus.edu.sg</a>, <a href="mailto:lxie@nwpu.edu.cn">lxie@nwpu.edu.cn</a>, <a href="mailto:haizhou.li@nus.edu.sg">haizhou.li@nus.edu.sg</a></td>
<td>We propose to use singular value decomposition (SVD) to reduce WaveNet parame- ters while maintaining its output voice quality. Specifically, we apply SVD on dilated convolution layers, and impose semi-orthogonal constraint to improve the performance.</td>
<td>Voice Conversion (VC), <strong>WaveNet, Sin- gular Value Decomposition (SVD)</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf</a></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION"><a href="#Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION" class="headerlink" title="Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION"></a>Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</h1><h2 id="MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf"><a href="#MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf" class="headerlink" title="MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）https://arxiv.org/pdf/1910.06522.pdf"></a>MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）<a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>MIMO-Speech，采用“多通道-输入”和“多通道-输出”，以建模 多通道 多说话人 情景下的语音分离和语音识别。</p>
<ul>
<li><p>模型结构由三部分组成：</p>
<p>1）单声道masking网络；</p>
<p>2）多源神经波束形成器；</p>
<p>3）多输出-语音识别模型；</p>
</li>
<li><p>学习策略：a curriculum learning strategy</p>
</li>
<li><p>实验结果：60% WER reduction</p>
</li>
</ul>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><ul>
<li><p>待解决问题：鸡尾酒聚会课题，分为 单通道 / 多通道 语音识别问题</p>
<ul>
<li><p>单通道多说话人语音分离：</p>
<p>1） Deep clustering (DPCL), 将时域单元映射到嵌入向量，再采用聚类算法将每个单元聚类到说话人源。此方法之后被嵌入到端到端训练框架中。</p>
<p>2）Permutation-invariant training (PIT)：用一个permutation-free目标函数来最小化重构损失。PIT之后被应用于多说话人ASR，在一个DNN-HMM混合ASR框架下。</p>
</li>
<li><p>多通道多说话人语音分离：</p>
<p>1）PIT，语音分离</p>
<p>2）unmixing transducer (a mask-based beamformer)，语音分离</p>
<p>3）DPCL：将inter-channel differences作为空间特征，和单通道频谱特征，语音分离</p>
</li>
</ul>
</li>
<li><p>本文贡献：多通道-多说话人-语音识别，输入multi-channel input (MI), 输出 multiple output (MO) text sequences, one for each speaker, 所以称为 MIMO-Speech。</p>
</li>
<li><p>可行性：最近的单说话人-远场-语音识别展示了 “神经波束”技巧对于去噪的价值，并且一些研究证实了end-to-end的可行性。[27] 进一步证实了神经波束方法在多通道端到端系统能够增强信号。</p>
</li>
</ul>
<h3 id="MIMO-Speech-2-pages"><a href="#MIMO-Speech-2-pages" class="headerlink" title="MIMO-Speech (2 pages)"></a>MIMO-Speech (2 pages)</h3><p><img src="/images/MIMO-Speech.png" alt="MIMO-Speech"></p>
<h4 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h4><ul>
<li>Stage 1: a single-channel masking network，通过预测多说话人和各通道噪音masks来实现<strong>预分离</strong></li>
<li>Stage 2: 多源 “神经波束” 来<strong>空间上分离多说话人的源头</strong></li>
<li>Stage 3: 端到端ASR来实现<strong>多说话人语音识别</strong></li>
</ul>
<p>创新点：masking network + neural beamformer，单目标函数进行模型训练。</p>
<p>Stage1 (Monaural masking network) 可以预分离开噪音和多说话人音源；Stage 2 生成多个beamforming filters $g^{i}(f)$ 用来分离和降噪输入的多声道信号；Stage3有多个说话人的encoder，和一个attention decoder组成，来生成多个说话人的文本序列输出。</p>
<h4 id="Data-scheduling-and-curriculum-learning"><a href="#Data-scheduling-and-curriculum-learning" class="headerlink" title="Data scheduling and curriculum learning"></a>Data scheduling and curriculum learning</h4><ul>
<li><p>问题痛点：端到端训练难以收敛</p>
</li>
<li><p>解决方案<strong>（Data scheduling）</strong>：随机从以下两个数据集中选择一个batch</p>
<p>1） 不仅采用多空间域的多说话人数据集</p>
<p>2） 也采用单说话人的数据集</p>
</li>
<li><p>细节<strong>（Curriculum learning）</strong> 配Algorithm：</p>
<p>1） 当选择到单说话人的数据集的时候，数据不经过 masking network 和 neural beamformer 模型，以加强end-to-end ASR模型的训练。</p>
<p>2） 计算出最大声和最小声说话人声音之间的信噪比SNR，然后按“升序”排列，从SNR=1的数据集开始训练</p>
<p>3） 将单说话人的数据集从短到长进行排序，让seq2seq模型首先学习短语句。</p>
</li>
</ul>
<h3 id="Experiments-3-pages"><a href="#Experiments-3-pages" class="headerlink" title="Experiments (3 pages)"></a>Experiments (3 pages)</h3><h4 id="3-1-Configurations"><a href="#3-1-Configurations" class="headerlink" title="3.1 Configurations"></a>3.1 Configurations</h4><h5 id="3-1-1-Neural-Beamformer"><a href="#3-1-1-Neural-Beamformer" class="headerlink" title="3.1.1 Neural Beamformer"></a>3.1.1 Neural Beamformer</h5><h5 id="3-1-2-Encoder-Decoder-Network"><a href="#3-1-2-Encoder-Decoder-Network" class="headerlink" title="3.1.2 Encoder-Decoder Network"></a>3.1.2 Encoder-Decoder Network</h5><h4 id="3-2-Performance-on-ASR"><a href="#3-2-Performance-on-ASR" class="headerlink" title="3.2 Performance on ASR"></a>3.2 Performance on ASR</h4><p>Motivation: 验证提出的模型好于baselines</p>
<p>Baselines / Ours</p>
<h4 id="3-3-Performance-on-Speech-seperation"><a href="#3-3-Performance-on-Speech-seperation" class="headerlink" title="3.3 Performance on Speech seperation"></a>3.3 Performance on Speech seperation</h4><p>Motivation: 验证 neural beamformer 学习了一个波束行为，能够用于语音分离。</p>
<h4 id="3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验"><a href="#3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验" class="headerlink" title="3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)"></a>3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)</h4><p>Motivation: 验证在实际情况下的模型性能。</p>
<h1 id="Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS"><a href="#Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS" class="headerlink" title="Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS"></a>Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</h1><h2 id="Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf"><a href="#Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf" class="headerlink" title="Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf"></a>Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 <a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></h2><h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>问题痛点：虽然对于英文来讲，现有的如Tacotron等模型已经能够实现端到端的语音合成过程，即从英文字母直接转换至语音。但是对于如中文这样的语言，仍旧需要繁复的前处理过程（如词边界、韵律边界等），使得这个文本处理前端的过程和传统方法一样复杂。</p>
<p>解决方案：为了保持生成语音的自然度、以及摒弃特定语言的特殊性，普通话语音合成过程中，我们引入了一个创新性的自注意力机制作为编码器，并且引入可学习的高斯bias到Tacotron中</p>
<p>实验结果：我们评估了不同的系统（在 有/无 韵律信息的情况下），结果显示提出的方法能够在最小的语言-依赖的前端模块的情况下，生成稳定和自然的语音。</p>
<h3 id="Introduction-1页"><a href="#Introduction-1页" class="headerlink" title="Introduction (1页)"></a>Introduction (1页)</h3><ul>
<li><p>待解决问题</p>
<p>传统的端到端方法包含复杂的特征提取过程，如：Part-of-speech tagging, pronunciation prediction, prosody labelling. 即便如Tacotron的端到端语音合成系统被提出，但是单纯的输入音素也无法使得语音合成模型得到良好的效果，所以学者提出嵌入PW、PPH、IPH，来进行韵律边界的特征建模。但是这使得违背了端到端语音合成的初衷，使得这个系统再次变得更加复杂。</p>
</li>
<li><p>本文贡献</p>
<p>1）<strong>全局韵律建模：</strong>由于self-attention被证明对于简单的音素序列进行全局建模时有良好的效果，所以本文尝试采用自-注意力机制作为编码器来获取全局的韵律信息。</p>
<p>2）<strong>局部韵律建模：</strong>至于局部的韵律信息，我们采用了一个可学习的高斯bias引入到自注意力机制中，因为Gaussian分布更加集中于当前位置的局部关系。</p>
</li>
</ul>
<h3 id="Proposed-SAG-Tacotron-1-5页"><a href="#Proposed-SAG-Tacotron-1-5页" class="headerlink" title="Proposed SAG-Tacotron (1.5页)"></a>Proposed SAG-Tacotron (1.5页)</h3><h4 id="3-1-Motivation"><a href="#3-1-Motivation" class="headerlink" title="3.1 Motivation"></a>3.1 Motivation</h4><ul>
<li><p>目的：为了采用最少的文本分析模块，所以引入自-注意力学习机制，来进行全局依赖建模。</p>
</li>
<li><p>方案：1）将Encoder的CBHG模块，用自-注意力机制替换；2）可学习的高斯bias来提升局部建模。</p>
</li>
</ul>
<p><img src="/images/SAG-Tacotron.png" alt="SAG-Tacotron"></p>
<h4 id="3-2-基于-自注意力机制-的-编码器"><a href="#3-2-基于-自注意力机制-的-编码器" class="headerlink" title="3.2 基于 自注意力机制 的 编码器"></a>3.2 基于 自注意力机制 的 编码器</h4><p>Encoder 的 Pre-net是一个3层-CNN + Batch Norm + ReLU，尽管自-注意力机制不包含序列信息，我们注入类似于Transformer的位置信息。</p>
<script type="math/tex; mode=display">
PE_{pos,2i}=sin(pos/10000^{2i/d})</script><script type="math/tex; mode=display">
PE_{pos, 2i+1} = cos(pos/10000^{2i/d})</script><p>其中，$pos$是当前位置，$d$是特征维度，$i$是当前维度。PE也被输入到自-注意力模块。自注意力模块包含了一个自注意力层+全连接层+tanh激活函数。残差连接被应用于上述层。</p>
<p>对于多头注意力机制的每一个$head_i$, 对于一个有$n$个元素的序列$x$，我们想要获得有相同长度n的隐状态向量$head_i$, 这里采用scale-product注意力机制。</p>
<script type="math/tex; mode=display">
Head_{i}=\sum_{j=1}^{n}ATT(Q,K)V</script><script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy)​</script><script type="math/tex; mode=display">
energy = \frac {QK^{T}} {\sqrt{d}}</script><p>最终的多头注意力机制为：</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_{1}, ..., head_{h})W^{O}</script><p>其中的$W^{O}$是最后一层线性层的参数矩阵。</p>
<h4 id="3-3-可学习的Gaussian-bias"><a href="#3-3-可学习的Gaussian-bias" class="headerlink" title="3.3 可学习的Gaussian bias"></a>3.3 可学习的Gaussian bias</h4><p>在序列-序列模型中，对于中文来讲，相近的位置是十分重要的。在这种情况下，我们想要为注意力机制提升编码器对于临近状态的局部贡献。</p>
<p><img src="/images/Gaussian-bias.png" alt="Gaussian-bias"></p>
<p>如上图所示，首先，假设一个以e5为中心的高斯bias，窗长为3（实际上，窗长是一个可学习的参数）。然后将注意力机制的分布通过高斯bias来进行正则化，以生成最终的分布。如图3所示，最终的分布是会在e5附近有更多的权重的。</p>
<p>具体来讲，Gaussian bias $G$被mask到energy上，即</p>
<script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy + G)</script><p>其中$G \in R^{N\times N}$，$G \in (-1;0]$ 度量了当前的query $x_i$与position $j$ 之间的关系：</p>
<script type="math/tex; mode=display">
G_{ij}=-\frac {(j - P_{i})^2}{2\sigma_i^2}</script><p>其中的$P_i$是$x_i$的中心位置，当给定输入序列 $x=(x_1, x_2, …, x_n)$，$\sigma_i$是标准差。如何选择合适的$P_i$和$\sigma_i$是关键。</p>
<script type="math/tex; mode=display">
P_i = N\cdot sigmoid(v_p^{T}tanh(W_{p}x_i))</script><script type="math/tex; mode=display">
D_i = N\cdot sigmoid(v_d^{T}tanh(W_{d}x_i))</script><p>其中$\sigma_i = \frac {D_i}{2}$, $W_p$ 和$W_d$是模型参数矩阵。</p>
<h3 id="Experiments-2-5页"><a href="#Experiments-2-5页" class="headerlink" title="Experiments (2.5页)"></a>Experiments (2.5页)</h3><h4 id="4-1-Basic-setups"><a href="#4-1-Basic-setups" class="headerlink" title="4.1 Basic setups"></a>4.1 Basic setups</h4><h4 id="4-2-System-comparison"><a href="#4-2-System-comparison" class="headerlink" title="4.2 System comparison"></a>4.2 System comparison</h4><ul>
<li>Baseline: Tacotron1</li>
<li>Baseline-prosody: Tacotron1 with complex inputs</li>
<li>SAE-Tacotron: Self-attention as encoder without Gaussian bias with simply inputs</li>
<li>SAG-Tacotron: Self-attention as encoder with Gaussian bias with simple inputs</li>
<li>Transformer with simple inputs</li>
</ul>
<p><img src="/images/simple_inputs.png" alt="simple_inputs"></p>
<p><img src="/images/complex_inputs.png" alt="complex_inputs"></p>
<h4 id="4-3-Model-details"><a href="#4-3-Model-details" class="headerlink" title="4.3 Model details"></a>4.3 Model details</h4><h4 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h4><h5 id="4-4-1-Robustness-test"><a href="#4-4-1-Robustness-test" class="headerlink" title="4.4.1 Robustness test"></a>4.4.1 Robustness test</h5><p>Motivation: 评估attention对齐（Repeats / Skips）的鲁棒性</p>
<h5 id="4-4-2-Prosody-analysis"><a href="#4-4-2-Prosody-analysis" class="headerlink" title="4.4.2 Prosody analysis"></a>4.4.2 Prosody analysis</h5><p>Motivation: 评估重读音节的pitch以及trajectory pattern of F0</p>
<h5 id="4-4-3-Objective-test"><a href="#4-4-3-Objective-test" class="headerlink" title="4.4.3 Objective test"></a>4.4.3 Objective test</h5><p>Motivation: 采用MCD评估学习到的频谱的质量，MCD越低越好。</p>
<h5 id="4-4-4-Subjective-test"><a href="#4-4-4-Subjective-test" class="headerlink" title="4.4.4 Subjective test"></a>4.4.4 Subjective test</h5><p>Motivation: 评估模型主管听测效果</p>
<p>评估方式：20个人，30句随机抽取的语音</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages</title>
    <url>/2021/06/21/mlms/</url>
    <content><![CDATA[<h1 id="Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017"><a href="#Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017" class="headerlink" title="Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)"></a>Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)</h1><h2 id="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK"><a href="#论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK" class="headerlink" title="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK"></a>论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>痛点：对于低资源语种来说，获取大量的训练数据是昂贵且困难的，通常是仅仅能获取到一小部分 / 或者没有数据集。</p>
<p>解决方案：本文提出了一种利用长短期循环神经网络的声学模型，目的是解决小语种语言数据缺失的问题。“说话人自适应”系统目的在于在多种语言间保持说话人的相似度，而本方法的突出特征是，模型构建成功后，系统不需要再重新训练以解决集外的语种，这是由于<strong>语言和说话人-不可知的建模方法和通用的语言特征集。</strong></p>
<p>实验结果：1）在12种语言上的实验结果显示，对于集外语种，系统仍能生成智能、自然的声音。2）当提供了少量训练数据的情况下，pooling the data有时能够提高整体的智能性和自然度。3）有时，构建一个zero-shot的多语种系统好于few-shot 单说话人单语种系统。</p>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><p>近期发展：近些年，统计参数语音合成的方法，从HMM转向了神经网络系统，2013年 Heiga Zen 发布了第一个采用前向DNN网络的语音合成系统（Google，ICASSP 2013），且合成效果优于HMM系统。之后的LSTM-RNN模型提升了语音合成的效果，并且最近的PCM生成模型（WaveNet）近一步提升了模型效果。</p>
<p>挑战：1）语音数据的获取。当从先验收集到少量数据时，说话人-自适应方法可以被采用，这时候，需要将模型在新说话人的少量数据集上进行fine-tune。但是这种方法不能用于zero-shot的情境。2）获取多说话人的广泛数据集，并且构建一个平均音色模型。但是这个方法不能应用于缺少足够语言信息的语种上。</p>
<p>本文课题：对于指定的低资源语种数据，有最小的语言表示信息。</p>
<p>解决方法：一个多语种声学模型被训练，其中目标语种的数据集未包含在训练数据集集内。</p>
<p>本文贡献：一个通用的MLMS（multi-lingual multi-speaker）模型被训练，并且是采用语言和说话人-不可知的方法。</p>
<h3 id="Multilingual-Architecture-1-page"><a href="#Multilingual-Architecture-1-page" class="headerlink" title="Multilingual Architecture (1 page)"></a>Multilingual Architecture (1 page)</h3><p>本文的优势：1）一个具象的输入特征空间，不需要在新语种上fine-tune；2）一个类似于单说话人的简单模型架构。</p>
<h4 id="2-1-文本特征"><a href="#2-1-文本特征" class="headerlink" title="2.1 文本特征"></a>2.1 文本特征</h4><h5 id="2-1-1-典型语言表示"><a href="#2-1-1-典型语言表示" class="headerlink" title="2.1.1 典型语言表示"></a>2.1.1 典型语言表示</h5><p>训练数据集是包含多种语言和口音的。首先将多语种全部转换至IPA。尽管这个转换过程有一些困难，如1）需要专家知识来做相应的转换，2）不能直接的转换。但是这个IPA还是能够为语言空间提供具象的特征。</p>
<h5 id="2-1-2-系统发育语言特征"><a href="#2-1-2-系统发育语言特征" class="headerlink" title="2.1.2 系统发育语言特征"></a>2.1.2 系统发育语言特征</h5><p>基于BCP-47标注，我们采用语言和边界识别特征来建模同语种的不同口音。+ 一个系统语言分类树</p>
<h4 id="2-2-LSTM-RNN-声学模型"><a href="#2-2-LSTM-RNN-声学模型" class="headerlink" title="2.2 LSTM-RNN 声学模型"></a>2.2 LSTM-RNN 声学模型</h4><p>给定语言特征后，LSTM-RNN时长模型的作用是预测每个音素的发音时长。然后再将这个时长和语言特征一同输入到声学模型。以预测音频波形。音频波形的平滑性，是采用RNN的循环单元来建模的。</p>
<p>由于本文需要处理更大数量的数据集和更加多样的语言特征，所以本文的模型与baseline的区别在于ReLU的单元数量和LSTM的层数，以及声学模型输出层的循环单元的个数。</p>
<h3 id="Experiments-2-page"><a href="#Experiments-2-page" class="headerlink" title="Experiments (2 page)"></a>Experiments (2 page)</h3><p>用于训练声学模型的数据集语料有超过800小时的语音，包含了37种不同的语言种类。这些语言属于原始的59组语言/地区对，一些语种，如英语，有不同的说话人数据集，对应不同的地域口音。对于一些口音（如EN-US）有多个说话人。一些音频是在消声室（anechoic chambers）录制的，而一些就是常规的录音室录制的（a regular recording studio）</p>
<h4 id="3-1-方法论：系统细节"><a href="#3-1-方法论：系统细节" class="headerlink" title="3.1 方法论：系统细节"></a>3.1 方法论：系统细节</h4><p>语音数据采用22.05KHz的数据集，LSTM-RNN模型输出的特征是音素的发音时长</p>
<h4 id="3-2-模型参数和评估"><a href="#3-2-模型参数和评估" class="headerlink" title="3.2 模型参数和评估"></a>3.2 模型参数和评估</h4><p>实验被设计为两种情景：</p>
<ul>
<li>模型被在除去12种语言的语料上训练（其中有6种，是毫无语料的情况）。但每一种被排除的语种（除了其中2种）都有“亲戚”语种在训练数据集中。在对这些被排除在外的目标语种进行语音合成。其中的模型称为H</li>
<li>用所有语种的数据集来训练模型。其中的模型称为I</li>
</ul>
<p>因为声学模型可以被speaker和gender identifying特征控制，所以以下实验被设计来观察如何影响合成质量。</p>
<ul>
<li>speaker和gender特征 unset （default，D），set to the highest quality female speaker (EN-US, F), highest quality male speaker (EN-GB, M), speaker of the closet language (C).</li>
<li>Setting the speaker and gender features for this speaker (S)</li>
</ul>
<p>实验评估：100句集外话术，每个人最多听100句话。每句话有1min的评估时间。每一种语言有8个评分者。</p>
<h4 id="3-3-实验结果和讨论"><a href="#3-3-实验结果和讨论" class="headerlink" title="3.3 实验结果和讨论"></a>3.3 实验结果和讨论</h4>]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>考古两篇TTS with NN/DNN的开山之作</title>
    <url>/2021/06/21/1st-dnn-tts/</url>
    <content><![CDATA[<h1 id="Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013"><a href="#Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013" class="headerlink" title="Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)"></a>Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)</h1><h2 id="https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf"><a href="#https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf" class="headerlink" title="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf"></a><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf</a></h2><h2 id="总结以及感想"><a href="#总结以及感想" class="headerlink" title="总结以及感想"></a>总结以及感想</h2><p>看了考古文对科研又产生了新的理解，目前的论文大多修修补补，灌水严重，实验部分不是很充分，无法印证论文的可复现性和实验结论的可靠性。这篇论文虽然采用的DNN技术还是最早期的神经网络系统架构，但是对于系统设计的每一个细小的结构都进行了充分的实验对比验证，得到了可靠的实验结论。在结论处也给同行留下了更多想象和探索的空间。强烈建议TTS从业者逐字逐行阅读本文，学习论文构思和写作的思想，并且了解深度学习在TTS领域应用的起源，至少从实验部分的objective /subjective evaluation可以学习到客观评估TTS合成效果的方法，使得自己的TTS研究更加扎实可靠。</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>痛点：传统的统计参数语音合成方式通常采用决策树，上下文依赖的HMM模型来表示给定文本的语音的概率密度。其中，语音参数是从概率密度中生成的来最大化它们的输出概率，然后再用生成的语音参数，重构语音波形。这种方法的缺点是，决策树对于建模复杂的上下文依赖关系是比较无效的。</p>
<p>解决方案：本文基于深度神经网络（DNN）。输入文本和声学表示的关系通过一个DNN来建模。DNN的使用能够解决许多传统方法的局限性。</p>
<p>实验结果：DNN效果优于HMM</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>基于HMM的参数方法在过去十年间盛行，相对于波形拼接算法来讲，它的优势在于能够灵活地替换音色，小的踪迹和鲁棒性。然而，它的主要局限性是合成语音的质量。Zen等提出了合成质量的三个主要特征：声码器，声学模型的准确度，和过拟合。本文的方法主要在于解决声学模型的准确度。</p>
<p>影响语音的一定数量的上下文特征包含音素、语言和语法特征，会在统计参数合成过程中，参与建模。典型系统包含50种上下文。因此，这些复杂的上下文依赖的有效建模是统计参数语音合成的关键点。为了解决这种上下文问题的标准方法是基于HMM的统计参数语音合成算法。对于每一种独立的上下文组合，都采用一个独立的HMM模型，作为一个上下文相关的HMM模型。通常来讲，对于这种全部上下文依赖的HMM模型来说，训练数据是不充分的，难以学习到一个稳定的模型，能够覆盖到所有所需的上下文组合。</p>
<p>为了解决这种问题，基于自上而下的决策树算法的上下文聚类被广泛使用。在这种方法中，上下文-依赖的HMM的状态被区分为多个“簇”，并且每个“簇”的分布参数是共享的。HMM模型的任务是通过二分类决策树，检验每一个HMM的上下文组合，其中一个上下文相关的二分类问题是与每一个 非叶子结点 相关的。“簇”的数量，也是 叶子结点 的数量，决定了模型的复杂程度。决策树通过序列化挑选能够在训练数据集上产生最高mle的分数来挑选问题。树的大小是通过一个预定义的mle阈值，一个模型复杂度惩罚，以及交叉验证来决定的。采用了上下文相关的问题和状态参数共享后，未知的上下文和数据稀疏性问题得到了有效解决。就像在语音识别中所成功解决的，基于HMM的方法自然地对于有丰富数据的上下文有较好的效果。</p>
<p>尽管基于上下文决策树的HMM模型在统计参数语音合成方法中是有效的。但是，有以下局限性：</p>
<ul>
<li>对于复杂上下文依赖如“XOR”，奇偶校验和复用问题，这种方法是无效的；</li>
<li>这种方法将输入的空间区分开，并且对于每个区域都采用了独立的参数，每个区域对应着一个决策树的叶子结点。这导致了分裂训练数据集，并且在聚类和估计分布的时候，使得每个簇的数据不充分。</li>
</ul>
<p>有一个相对大的决策树，并且分裂训练数据集都会导致过拟合，损害合成语音的质量。</p>
<p>为了解决上述局限性，本文采用基于DNN的结构。上述基于决策树的方案，建模了从 文本中抽取的语言上下文到语音参数的映射。在这里的决策树被一个DNN模型所替代。值得注意的是，自从90年代开始，NN就尝试被应用于TTS中。</p>
<h2 id="DNN-VS-Decision-tree-DT"><a href="#DNN-VS-Decision-tree-DT" class="headerlink" title="DNN VS Decision tree (DT)"></a>DNN VS Decision tree (DT)</h2><ul>
<li>DT 在表达输入特征的复杂关系时无效，如XOR、d 位奇偶校验函数、或者多路复用的问题。为了表达上述情境下的问题，决策树可能会十分巨大。然而，这些关系能够被DNN模型来具象表示</li>
<li>决策树致力于分割输入空间，对每个空间采用一组独立的参数和一个叶子结点。这样会导致在每个区域的数据数量少和较差的泛化性能。Yu et al 证明了在采用决策树建模时，一些较弱的输入特征如语音中词级别的重读会被丢失。由于DNN的权重是从整体的训练数据得到的，所以DNN会得到更好的泛化性能。DNN也提供了输入高维、多种输入特征的可能性。</li>
<li>相较于决策树，通过反向传播来训练一个DNN模型通常需要大量的计算过程。在预测过程中，DNN需要在每一层都有一个矩阵乘法，但是决策树仅仅需要通过一个输入特征的子集从根结点遍历树直至叶子结点。</li>
<li>决策树的推理是更加可解释的，DNN中的权重很难在直观上获得解释。</li>
</ul>
<h2 id="基于DNN的语音合成"><a href="#基于DNN的语音合成" class="headerlink" title="基于DNN的语音合成"></a>基于DNN的语音合成</h2><p>由于人类的发声系统是多层级的，才能够将文本信息转换为语音波形，所以本文尝试采用深度神经网络来进行语音建模。</p>
<p><img src="/images/DNN-based-tts.png" alt="DNN-based-tts"></p>
<p>上图展示了一个基于DNN的语音合成框架。输入文本首先被转换为输入特征序列$\{x^t_n\}$，其中的$x^t_n$表示在第$t$帧的第$n$维输入特征。输入的特征是对于文本上下文关系的二分类问题，包含如（e.g is-current-phoneme-aa?）和数值（e.g. 在短语中的单词数量，在当前音素序列的当前帧的相对位置，和当前音素的发音时长）</p>
<p>然后输入特征通过一个训练好的DNN，采用前向传播的方法被映射到输出特征$\{y^t_m\}$，其中$y^t_m$表示在第$t$帧的第$m$个输出特征。输出特征包含频谱和激励参数以及他们的时间导数（动态特征）。DNN的权重能够采用从训练数据集中抽取的成对的输入和输出特征来进行训练。类似于HMM的方法，这样是可以生成语音参数的。通过从DNN中设定预测的输出特征作为均值向量，再加上从所有训练数据预先计算的方差作为协方差矩阵，语音参数的生成算法能够生成平滑的语音参数特征轨迹，满足了静态和动态特征的统计情况。最终，一个语音合成模块通过得到的语音参数来生成语音波形。</p>
<p>注意到，文本分析，语音参数生成，和波形生成模块可以与HMM模型共享，<strong>即仅仅从上下文依赖关系的标签生成统计参数的过程需要被替换。</strong></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="4-1-实验条件"><a href="#4-1-实验条件" class="headerlink" title="4.1 实验条件"></a>4.1 实验条件</h3><p>实验数据：US-EN 女性语音数据，约33000条。语音分析的条件和方法论类似于Nitech-HTS2005系统的方法。语音数据首先从48KHz降采样到16KHz，然后每5ms抽取一次40维的Mel倒谱系数，$log F_0$，和5段非周期性系数。每一个观察向量包含40维的mel倒谱系数，$log F_0$，和5段非周期性系数，以及他们的delta和delta-delta特征。从左至右，包含5个状态的无跳过隐藏半马尔可夫模型 (HSMM)被采用。为了建模 $log F_0$序列包含了声学和非声学观察序列。一个多空间的密度分布被使用（multi-space probability distribution (MSD)）。基于决策树的上下文聚类的问题数量是2554个。在HMM系统中的决策树大小是通过改变模型复杂度惩罚因子$\alpha$来控制的（最小描述长度标准（MDL）是（$\alpha=16,8,4,2,1,0.5,0.375,or 0.25$）。当$\alpha=1$时，Mel频谱，$log F_0$和频带非周期性的叶子结点的数量分别是12342, 26209, 和401（总共有3209991个参数）。</p>
<p>基于DNN系统的输入特征包含表征类别语言上下文（例如音素身份、重音标记）的342个二分类特征和表征数字语言上下文（例如，单词中的音节数、短语中当前音节的位置）的25个数值特征。除去文本上下文相关的输入特征，还包含了3个用于粗略编码当前音素序列中当前帧位置的数值特征，以及一个用于估计当前音节时长的数值特征。输出特征与HMM系统基本一致。为了通过DNN模型建模$log F_0$序列，我们采用了显式发声建模（explicit voicing modeling）的方法来获取连续$F_0$ ，发声/不发声的二分类特征值被用于添加到输出特征，并且在不发声值中的 $log F_0$ 通过插值得到。为了降低计算成本，80%的静音段从训练数据中移除。DNN的权重被随机初始化，然后在最小化MSE的目标函数下得到最优化。优化策略为基于小批次的随机梯度下降（SGD）的后向传播算法。DNN的输入和输出特征均被正则化，其中输入特征被正则化至(0,1)分布，然后输出特征根据训练数据中的最大最小值被正则化至0.01-0.99隐藏层采用sigmoid激活函数。建模频谱和激励特征参数的DNN神经网络被训练。</p>
<p>在评估的语句中，语音参数通过语音参数生成算法被生成。在倒谱域采用了基于后过滤的频谱增强算法。语音波形通过source-filter模型来重构语音波形。</p>
<p><strong>为了客观评估HMM和DNN模型系统，MCD（mel-cepstral distortion）(dB)，Linear aperiodicity distortion (dB), 发声/不发声错误率（%），和$log F_0$的RMSE被使用。</strong>音素发音时长在后面被使用，我们挑选了173句训练集外的语句用于模型评估。</p>
<h3 id="4-2-客观评估"><a href="#4-2-客观评估" class="headerlink" title="4.2 客观评估"></a>4.2 客观评估</h3><p><img src="/images/5th-mcep-comparison.png" alt="5th-mcep-comparison"></p>
<p>上图绘制了Ground-Truth、HMM预测值和DNN预测值的第五个mel倒谱系数，从图中可以观察到，三个模型都可以产生合理的语音参数轨迹。</p>
<p>在客观评估中，我们调查了预测结果和DNN结构（1，2，3，4，5层）的关系，以及与每层神经元个数（256，512，1024，2048）的关系，下图展示了实验结果。</p>
<p><img src="/images/dnn-tts-exp-res.png" alt="dnn-tts-exp-res"></p>
<p>基于DNN的系统一直都比HMM系统要好在 “voiced/unvoiced error rate”和”aperiodicity prediction”。在MCD中，有多层的DNN模型要相似于或者好于HMM模型。然而，在$log F_0$的预测中，HMM在大多数情况下要好于DNN模型。其中，所有的不发音帧都被插值作为发音帧来建模。我们认为这种方法会降低$log F_0$的预测效果，因为这些插值的$F_0$对于DNN模型来说是一个bias。对于MCD和aperiodicity预测中，模型深度的提升比在每一层上增加神经元的个数更加有效。</p>
<p>以上的客观指标并不能评估合成语音的自然度，但是可以作为评估声学模型准确率的指标。</p>
<h3 id="4-3-主观评估"><a href="#4-3-主观评估" class="headerlink" title="4.3 主观评估"></a>4.3 主观评估</h3><p>173句话被评估，每个评估人最多评估30句话，这些话术是随机打乱的。每一对语音被5个人评估。评估人有带耳机。在听完一对语音后，评估人需要选择一个更喜欢的语音，如果觉得两个语音很相似的话，可以选择“中立”，在这个评估过程中，HMM系统和DNN系统采用相似的模型数量来被评估。DNN模型采用了4个隐藏层，神经元的个数也进行了多组实验（256，512，1024个神经元）</p>
<p><img src="/images/dnn-tts-mos.png" alt="dnn-tts-mos"></p>
<p>上表展示了实验结果，可以从以上结果看出，在相似的参数数量配置的情况下，DNN模型要远优于HMM模型。我们认为较好的MCD代表了更佳的效果。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章examined the use of the DNNs to perform speech synthesis. DNN模型有潜力解决传统DT-HMM模型的局限性。主观评估和客观评估均显示DNN能够实现较好的效果。HMM的一个优势在于模型参数较少，计算开销较小。在合成时，HMM模型便利决策树来找到每一个状态的参数。然而，本文提出的DNN算法是在每一帧进行输入到输出的预测，接下来的工作可以在如何降低DNN模型的计算开销，添加更多的输入特征包括一些弱特征如重读，并且可以探索如果获得一个更好的 $log F_0$建模方案。</p>
<h1 id="Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper"><a href="#Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper" class="headerlink" title="Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)"></a>Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)</h1><h2 id="https-arxiv-org-pdf-cs-9811031-pdf"><a href="#https-arxiv-org-pdf-cs-9811031-pdf" class="headerlink" title="https://arxiv.org/pdf/cs/9811031.pdf"></a><a href="https://arxiv.org/pdf/cs/9811031.pdf">https://arxiv.org/pdf/cs/9811031.pdf</a></h2><h2 id="Abstact"><a href="#Abstact" class="headerlink" title="Abstact"></a>Abstact</h2><p>传统的文本-语音转换通过拼接短的语音单元或者采用基于规则的系统来将语音的音素表示转换为声学表示形式，然后被转换为语音。本文描述了一种采用时延神经网络（time-delay neural network TDNN）的方案来进行音素-声学特征的建模，不需要额外的神经网络来控制生成语音的timing。这个神经网络系统相较于拼接算法可以降低对于系统存储资源的需求，对比于其他的商业系统表现良好。</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="1-1-Description-of-Problem"><a href="#1-1-Description-of-Problem" class="headerlink" title="1.1 Description of Problem"></a>1.1 Description of Problem</h3><p>文语转换通常包含了先将文本转换为语音参数，再将语音参数转换为语音波形。计算机交互可以采用对话沟通交互方式，也可以配置到移动端。拼接系统首先制作拼接数据库，然后再拼接时，调整音素的发音时长，平滑转接点来生成语音参数。拼接系统的主要问题是存储成本高昂。基于规则的合成方法将每一种可能的音素表示存储好目标声学参数。然后根据衔接点的情况来根据规则选择语音参数，主要问题是：拼接点不自然，因为转接规则倾向于只生成少量的转接风格。另外，大量的转接规则需要进行存储，会造成合成的机械音。</p>
<h3 id="1-2-Discussion-of-the-use-of-networks"><a href="#1-2-Discussion-of-the-use-of-networks" class="headerlink" title="1.2 Discussion of the use of networks"></a>1.2 Discussion of the use of networks</h3><ul>
<li><p>先前提到的两种方法都是语言-依赖的，而NN方法是语言-不相关的；</p>
</li>
<li><p>拼接系统的高昂存储成本导致了难以配置到移动端，而NN通过生成具象的表示方式，能够降低拼接系统的冗余性；</p>
</li>
</ul>
<p>下图展示了TTS的流程图（终于找到了现有的TTS流程设计的出处）</p>
<p><img src="/images/tts-diagram.png" alt="tts-diagram"></p>
<h2 id="系统描述"><a href="#系统描述" class="headerlink" title="系统描述"></a>系统描述</h2><h3 id="2-1-数据库"><a href="#2-1-数据库" class="headerlink" title="2.1 数据库"></a>2.1 数据库</h3><p>38岁男性，居住在Florida和Chicago。录制时采用了类似于近距离麦克风的DAT录制起。文本包含480个音素均衡的语句，是从Harvard sentence list中筛选出来的。除此之外，160句在其他情境下的话术也被录制了下来。录音音频通过数字方式转移到计算机，每句话对应一个音频。每句话被归一化，以使得每句话都在非静音段有相同的平均信号能量。文本信息被标记为音素、节奏和音调信息。</p>
<p>标记方式采用了类似于TIMIT数据库的标记方式（是ARPABET的变种），停止标记为关闭和释放作为单独的音素。这使得模型能够有效预测到停顿，以及开始。精准的对齐对于帧级的损失函数是有效的。</p>
<p>音素不是唯一的输入特征，音素时长，F0曲线（通常被音节重读和语法边界影响）。语法边界（syntactic labelling）标记了音节，词，短语，从句和句子的开始和结束时间。语法重读（lexical stress：primary, secondary, or none）被应用到词汇的每一个字母中。词性(function word (article, pronoun, conjunction or preposition) or content word)；每个词语都有一个层级(level)，基于生成F0的rule-based系统。</p>
<p>尽管语法（syntactic）和重读（lexical stress）对于语音的韵律变化很重要，但是这些信息没有完全决定了这些韵律变化。<strong>说话者对于语句的重读可能取决于句子中的对比度，比如在遇到陌生词汇的时候，可能会不自觉的重读。</strong>因此标记如此音调重读的实际位置到字幕上，或者词语间的强对比性是有效的。在英文中的标准是ToBI（Tone and Break Index）系统。</p>
<h3 id="2-2-从音素表示上生成片段时长"><a href="#2-2-从音素表示上生成片段时长" class="headerlink" title="2.2 从音素表示上生成片段时长"></a>2.2 从音素表示上生成片段时长</h3><p>神经网络的两个任务之一是去决策，从音素顺序和语法和韵律信息上，每一个音素的发音时长。</p>
<p>网络的输入大多数采用二分类数值，分类数值代表了采用1-out-of-n codes和一些通过bar codes表示的小的整数值。表示音素片段的输入数据包含音素片段，它的发音特点，字幕凸起的描述和包含片段的词语，以及片段接近的任何语法边界。网络结构被训练来生成时长的log。</p>
<p>时长预测网络结构如图2所示，网络有两个输入值（2和3），通过I/O block 1 和 2 输入。（Stream 2 包含了通过shift register来给一个音素提供上下文描述），stream 3 包含了仅仅用于一个特定音素时长的生成过程。当神经网络被用于生成时长的过程中，I/O block 6写入输出的数据流。在训练过程中，Block 6 读取目标值并且生成error value。Block 3、4和5是单层的神经网络模块，模块7、8和recurrent buffer控制了循环生成的机制。</p>
<p><img src="/images/duration-prediction.png" alt="duration-prediction"></p>
<h3 id="2-3-从音素和时长信息生成声学信息"><a href="#2-3-从音素和时长信息生成声学信息" class="headerlink" title="2.3 从音素和时长信息生成声学信息"></a>2.3 从音素和时长信息生成声学信息</h3><p>系统中使用的第二个神经网络从音素、语法和时长信息来生成语音参数信息。更精准地来说，网络从一个帧级的音素上下文信息生成语音10-ms帧的声学表示。</p>
<h4 id="2-3-1-网络输出-—-Coder"><a href="#2-3-1-网络输出-—-Coder" class="headerlink" title="2.3.1 网络输出 — Coder"></a>2.3.1 网络输出 — Coder</h4><p>神经网络不会直接生成语音，这个的计算资源十分昂贵，并且不太可能生成好的结果。该网络为声码器的分析-合成风格的合成部分生成数据帧。许多语音编码的研究致力于数据压缩的问题；然而，神经网络对于coder的需求没有被大多数的数据压缩技术所满足。具体来讲，将语音编码成每帧的数值向量是有价值的，这样的话，向量的每个元素对于每一帧都会有一个定义好的数值，因此用于训练的神经网络的错误度量是合适的。（例如，如果神经网络生成向量，并且错误度量相对于训练向量是较小的话，生成语音的质量，即通过running these vectors通过coder的合成部分的话，将得到较好的语音质量。）加权Euclidean距离被用作error criterion使得coder没有使用二分类输出值是明智的，并且根据其他的向量元素，向量元素的含义没有改变。</p>
<p>coder是LPC声码器的形式，采用线性频谱（line spectral frequencies）来表示filter coefficients和一个2-band的激励模型（不同的filter coefficients的表示形式被测试，模型对于线性频谱表现良好）。2-band激励模型是一个multi-band激励模型的变种，包含一个低频带的voiced band，和一个高频带的unvoiced band。两个bands之间的边界是coder之一的参数。F0 和 power of the voice signam是剩下的参数。F0在不发音的帧级，被插值为一个高频的数值。</p>
<h4 id="2-3-2-网络输入"><a href="#2-3-2-网络输入" class="headerlink" title="2.3.2 网络输入"></a>2.3.2 网络输入</h4><p>音素网络的输入包含了时长网络的所有输入，和时长网络输出的timing information。网络采用了一定数量的不同的输入coding技术。blocks 5，6，20和21采用了300毫秒的TDNN的风格输入窗口。窗口的采样不是均匀的，最优的采样区间是通过分析神经网络从TDNN窗口的不同部分来决策神经网络对信息的使用。Blocks 6 和20 处理了一组与输入音素相关的特征，Blocks 7和8为音素和语法边界编码时长和距离信息。网络的输入数据是二分类数据的混合，1-out-of-codes和bar codes</p>
<h4 id="2-3-3-网络结构"><a href="#2-3-3-网络结构" class="headerlink" title="2.3.3 网络结构"></a>2.3.3 网络结构</h4><p>决定好的网络结构需要大量的实验，也就需要大量的计算资源，然而本课题的复杂程度和数据集的大小使得训练时间成为了主要瓶颈。因此，一个 in house neural network simulator被开发来降低训练时间（多个月-&gt;几天），并且可以同时验证多个方法。一些神经网络的技术和理论通过这种方式被pass掉了。</p>
<p>最终的网络结构整合了TDNN、recurrent、和modular网络，和一些实验过程中演化的技巧。下图是当前方法的图示，其中六边形模块是I/O或者用户写入的子程序，方块是神经网络的模块。神经网络的模块采用后向传播来训练。网络的模块化是通过专家知识来手动调整的。</p>
<p>网络通过逐渐降低的学习率和momentum方法来训练，以一种新型的顺序和随机混合的训练模式来训练。训练的网络需要&lt;100 Kilobyters of 8-bit quantized weights ，对比于拼接算法，得到了显著降低。</p>
<p><img src="/images/phonetic-network.png" alt="phonetic-network"></p>
<h2 id="系统效果"><a href="#系统效果" class="headerlink" title="系统效果"></a>系统效果</h2><h3 id="3-1-语音质量和自然度"><a href="#3-1-语音质量和自然度" class="headerlink" title="3.1 语音质量和自然度"></a>3.1 语音质量和自然度</h3><p><img src="/images/tts-nn-exp-results.png" alt="tts-nn-exp-results"></p>
<p>上图展示了GT语音频谱和系统生成的语音频谱（生成的频谱没有采用ToBI标注系统）。为了对比更加清晰，有两种合成语音频谱被展示出来。第一种，phonetic 特征是网络预测的，而duration是真实的，为了仅仅展示phonetic network的效果。第二种，duration和phonetic都是预测的。对比实验发现，在语音接受度（Acceptability）上，本方法生成的质量远好于其他的系统。在片段的拟人度方面（Segmental Intelligibility），本方法仍旧有提升空间，而本次试验中的较差的数据可能是由于缺少单字语音样本所导致的。</p>
<p><img src="/images/tts-nn-table1.png" alt="tts-nn-table1"></p>
<h3 id="实时合成"><a href="#实时合成" class="headerlink" title="实时合成"></a>实时合成</h3><p>最开始模型是在Sun SPARCstation平台来通过ANSI C语言实现的。最近这个被插入到Power Macintosh 8500/120，PowerPC快速的乘法和加法使得合成器能够实时合成。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>本方法从Acceptability角度来看，是优于传统算法的，但是仍旧可以有一些提升，如数据库可以扩充，来获得更多的音调变化，包含更多的音素上下文特征，数据库可以包含更多的短语，单字，和长段的语句。在coder、network architecture、和训练方法上也可以做出一些提升。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
</search>
