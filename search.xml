<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ICASSP 2020 Overview 思维导图</title>
    <url>/2021/06/16/ICASSP-2020-XMind-Overview/</url>
    <content><![CDATA[<p><img src="/images/ICASSP_2020.png" alt="ICASSP 2020 Overview"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>玩过的n=22种运动</title>
    <url>/2021/06/16/21-sports/</url>
    <content><![CDATA[<p>目前玩过的22种运动如下，更多运动探索中……</p>
<ul>
<li>跳绳</li>
<li>跳皮筋</li>
<li>丢沙包</li>
<li>跳房子</li>
<li>蹦床</li>
<li>爬健身器材</li>
<li>花样轮滑</li>
<li>游龙板</li>
<li>短跑</li>
<li>跳远</li>
<li>游泳</li>
<li>跆拳道</li>
<li>太极拳</li>
<li>健美操</li>
<li>舞狮</li>
<li>龙舟</li>
<li>瑜伽</li>
<li>攀岩</li>
<li>滑雪</li>
<li>滑冰</li>
<li>射箭</li>
<li>乒乓球</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>金载勋跆拳道- 2020年总结</title>
    <url>/2020/12/31/2020-jhk/</url>
    <content><![CDATA[<p>被两个极喜欢拖着我一起立flag折腾自己的狐朋狗友又一次立了一个flag，本想做一个视频，想到社会蓝的故事编辑技能树需要发光发亮，那我也来记录下这一年多的金载勋生涯吧。</p>
<p>2010年的暑假是我第一次接触tkd并深爱上跆拳道，似乎是2012年初二的暑假结束了跆拳道课程，止步于蓝带。印象中结束的最后一节课只有我一个学员上课，在我十分敬佩的葛教练指导下进行十分难得的1v1教学，上的课是当年我最爱的反应踢（教练随意出腿法脚靶，然后迅速反应）。当年留有贝克汉姆发型的我在葛教练的逼迫和鼓励下，顺利学会了俯卧撑、握拳俯卧撑和三指俯卧撑，也在一次次地习练中打开了横竖叉任督二脉。那几年的我，似乎达到了身体素质的局部巅峰。也是这种一次次突破的感觉，让我深爱习练tkd，深信自己也会成为一名优秀的跆拳道选手。然而，同伴的退出和教练的不停更换和面临初三中考的压力，我最后也选择了退出。尽管早已偷偷学会太极七章，尽管再没有机会弥补上痛失金牌的愿望，最终仍止步于蓝带。</p>
<p>中考后的我，还曾经和朋友一起回到道馆，尝试找回曾经的回忆，可笨重的自己和课堂上灵活的小朋友们一起上课后的感受只有“没有对比，就没有伤害”。即便还是曾经的那个道馆，年龄的增长还是劝退了我。</p>
<p>本想大学过后找到tkd社团就继续完成我的黑带梦想，可itf的发声方式，教练要改口为师范，以及从头再来的打击又一次将我劝退，充满棱角的我选择不愿意变更流派。虽然大学四年间每次路过道馆都会观察是不是wtf，有没有成人训练。可奈何都没有合适之选。我也逐渐转移了注意力到其他的运动上，从连续一个月每日中午晚上的舞狮训练上又找回了那种走火入魔的快乐。</p>
<p>直到2019年，工作上稍微稳定些的我继续开始寻觅自己的黑带梦想能够在哪里得以实现。在对比了解了不下十家道馆的我，被国际化的金载勋道馆所吸引了，于是预约了第一次的体验课。</p>
<p>尽管初次体验迟到了大概十五分钟，但耐心又漂亮温柔的前台小姐姐，1对1的体验，小阁楼式的装修，和隔板间的换衣房让我对这国际化道馆的好感印象不断上升。柴师范是我的试课师范，尽管距离当年的意气风发已久隔近十年，但当年对跆拳道的热爱使我还没有忘记腿法的基本动作，已学过的腿法都能够顺利完成。至于拳法的话，对于wtf的我就是一团糟了，只能在师范的指导下尝试模仿。</p>
<p>课上师范的鼓励和表扬和更加专业体系化的指导让我似乎找回了当年的快乐，也对这里的课程逐渐产生了信任。课后与丁师范近一个小时的聊天也让我在残酷的社会和异地漂泊的寒冷中找到了一丝温暖。唯一的顾虑还是itf or wtf。</p>
<p>我记得我有问是哪一种，dsf的回答十分巧妙而又深刻，两种都不是，但两种都会学，并且作为崔洪熙将军直系弟子的金载勋师范传授下来的课程，与市面上更流行普适的跆拳道课程不同，是更加地道和专业的。</p>
<p>另外印象深刻的一点是，我说为什么以前的课程都是一个半小时，现在一个小时感觉时间太短了，无法得到充分的练习。丁师范的回答十分坦诚，说“我们也不希望课程里面穿插很多无用的热身活动，比如跑步就跑了半个小时，如果排成一个半小时，那需要排的课更少了，对我们来说反而是轻松了。大家时间都很宝贵，我们还是希望能在课程里教给学员们更多的专业技能”。这一番言论在dsf绵绵细语下格外真诚，让我逐渐放下了心中对常规销售套路的防备心。</p>
<p>介意从头再来的我仍然问了那个问题，是否可以从以前的级别继续习练。dsf对此也是包容和支持，希望腰间的带色能够勉励我们自己不停努力，加紧训练，尽快追赶上自己腰间的色带。最终，csf对我天赋的鼓励和dsf对道馆体系化教学的讲解双面夹击，我放下了对教练称呼和训练体系改变的排斥，当场报名了半年的课程，希望能一展宏图。</p>
<p>然而，后面一个月内的课程大多是不习惯，不习惯课前没有充分的热身就开始进行腿法练习，不习惯每一次课前热身动作的重复无趣，不习惯不练习俯卧撑，不习惯打拳的时候抬手准备，不习惯做动作的时候要垫脚，不习惯要自己摸索动作结束的标准位置，不习惯很多腿法的动作细节，不习惯分散练习时无人指导的茫然，不习惯韧带没有拉开仍需要勉强完成一些大幅度的腿法动作。这一个月的习练甚至让我开始怀疑自己报名的冲动，最难受的是怀疑自己是不是并不适合这项运动，乃至开始咨询深圳内的其他成人道馆。</p>
<p>这一切直至遇到成长背景都十分相似，同龄又同样有执念的黑带胡哲才算得到一些开解。胡哲是我在跆拳道生涯中遇到过最让我敬佩的学员，尽管已经身为黑带还是在课后刻苦习练，尽管大学时候查有膝盖积水，仍旧在朗朗诗声中咬牙完成自己的横叉梦想，即便动作都已经非常熟练还是一遍又一遍的熟悉，同为wtf转方向，仍旧能够耐下心来完成新的体系的学习。这一切令我感到有些找回当年热血的感觉。课后我开始向她讨教腿法，品势的细节动作，曾经也是wtf的她帮助我开始逐渐熟悉了金载勋的训练体系。</p>
<p>说来很神奇，陌生人面前比较孤冷的我见到她之后话匣子似乎被打开了，她给我讲述大学中的一件件训练故事，分享给我大学的训练日记，文武双全的她让我开始感到崇拜。有她在的每一节课变得不再那么的孤单和艰辛，她不在的日子我都似乎又关闭了自己的说话功能，选择不去接触更多的人，把这一份热情和温暖唯独留给我欣赏的人。</p>
<p>除了同伴的收获，还有一次是杨师范的训练课也令我印象深刻。丁师范的课程还是比较偏向金载勋的训练体系，我还没有从幼时游戏式的热身，更多体能、全方面身体素质方面训练的热身中走出来，尤其在分散练习独自一人无人指导，也无熟悉同伴共同训练的时候倍感孤单，甚至觉得这样好似报名了一个健身房，得不到一些及时的指导和团队协作和竞争上的训练。可是ysf的课程是让排成两队，做一些交叉步，青蛙跳等素质练习的动作，逐渐让我找回了舒适圈，一些动作也得到了杨师范的认可让我逐渐找回了自信。课上的腿法练习中，杨师范总是可以观察一会，很准确的找出动作的问题所在，并给出改进方案，在尝试了师范的方式后，自我感觉也得到了显著的提升，那种成就感令我找回了一些快乐。那节课是我一个月以来最快乐的一次，从那以后开始甚至有点期待杨师范的课程。然而半年来都没有期盼到。</p>
<p>所幸的是，胡哲也与杨师范交情甚好，课后总是会在杨师范的指导下进行1对1柔韧度练习。我深知柔韧度对跆拳道训练的重要性，柔韧度很大程度上决定了腿法动作的幅度、标准度和灵活度，对挑战横叉的胡哲我眼中尽是钦佩和羡慕，所以选择留下来帮助胡哲按住一条腿。可长久以来都无法鼓起勇气说自己也想要完成这个柔韧度的训练。最终，选择报名了一个舞蹈软开度课程。课程结束也没有达到一些显著的提升。</p>
<p>在胡哲的横叉飞速进步中，很快就迎接来了过年，随着本命年的到来，居然是动荡全球的疫情风云。很早回到深圳的我还是不能够回到道馆继续训练。在家里的一个月时间中，我时不时会跟着keep进行一些简单的软开度训练，但都是2周没有见到效果就放弃了。</p>
<p>疫情期间一次公司聚餐中，意外接到了熟悉的dsf打来的电话，听到熟悉而亲切的声音的我激动但却不得不继续公司里的工作，因此和dsf解释说是否可以晚点联系。聚餐后匆忙给dsf打回去的我，打过去却是金载勋的总机电话，那一天时间里似乎都在期盼着dsf再次拨回来。又一次接到电话的我赶忙问何时可以开课，已经迫不及待了，dsf说会随政府安排，尽快，但是叮嘱说在家也要勤加练习哦。真是一个有爱有人情味的道馆。</p>
<p>终于熬到了四月底，又可以回到道馆了。之后的一个月就认识了肖宝杰小妹妹。对bj的印象是，总会有一个男孩子在楼上等着她😂好像不是很多话。</p>
<p>后面就迎来了开新的分馆，一直打探分馆主教练的我其实内心早就在想这样是不是就可以上更多的杨师范的课了，说不定还可以找杨师范帮忙拉开韧带。心里想着，似乎黑带梦想又重燃了希望。</p>
<p>分馆如期开张，杨师范也如期升职为分馆主教练，更开心的是，本舍不得在杨师范和丁师范中做个选择，海岸城的课程居然是两位主教练轮换教课，这也解除了我的一部分顾虑，不会因为总是上一个师范的课而感到略显重复枯燥。</p>
<p>宝杰也转到了分馆，并且在我第一次去的时候，发现宝杰也开始了自己的横叉磨练。杨师范似乎从我眼中看出了我的心思，便问我要不要也一起来，终于稍微鼓起勇气的我开始尝试。也在杨师范的“狠绝”下第二天就达到了一个小目标。</p>
<p>于是就这样开始了两个人的横叉之旅。印象很深的是一次我已经满头大汗了，杨师范还是心里一坚持，又加了一些强度，那一刻感觉两边的腿内侧的筋似乎开始燃烧了，然而神奇的是，从那以后两侧的筋就再也不痛了。但是，让我略生退却心思的是，有时候拉完韧带回到家，会导致前半夜腿痛，影响睡眠，甚至一次需要敷上止痛膏药才可以稍微缓解一些这种疼痛，并且有时候的腰痛也开始让我怀疑自己这把老骨头是不是回笼胯，是不是天生就是这么硬，因为小的时候也是竖叉很快就拉开了，横叉花费了多一倍的时间才拉开。</p>
<p>尽管心生疑虑，但是还是忍住不敢讲出口，怕师范会过于担心我的身体，我就更加放弃自己了。</p>
<p>这其中，印象很深的一次是，那次是隔了四五天没有压韧带，我心里很害怕退步了，太过紧张所以导致肌肉紧绷，大师兄也在一旁说“世上无难事，只要肯放弃”，丁师范也在一旁劝我不要勉强，最后就选择放弃了。回家路上我很严肃，责怪自己，又不知道怎样开解自己。只是心里充斥了恨铁不成钢。第二天都没有颜面去上杨师范的课了，感觉似乎辜负了师范。然而还是厚着脸皮去上了晚课，那一天，又到了一天一度的“地狱”时刻了，我心里仍旧是害怕，身体上还是诚实地去多热了热身，希望这样能缓解一些。宝杰已经完成了自己的每日任务，昨天就放弃的我，开始犹豫。杨师范给我喊了过去，一只很有力气的脚推在我的小腿上，说“奥兰你今天就别想跑了”。听了这话的我心里也想，不能再辜负师范的努力和期望了，眼镜一扔，眼睛一闭，就想豁出去了，一定要完成。哎，很神奇，很快就到达了目标了啊，没有想象中的那么痛啊？甚至比一周前还要好很多？？那天我真的十分感激杨师范帮我克服心中的魔鬼，终究是没有中断和放弃。</p>
<p>后面坚持和宝杰一起开横叉的时光艰辛而又互相勉励。柔韧度进步后在一些腿法上感觉更加收放自如，自我都感觉来到海岸城的这段时间实则进步了一大截。终于迎来了红蓝带考试，我也很骄傲的到后海馆给许久未见的其他同学和老师们看在杨师范指导半年下的飞速进步。</p>
<p>可惜后面宝杰韧带拉伤，我自己虽然心里不想放弃，但是也找不到主动去找杨师范的理由，所以也就随着宝杰的韧带拉伤中止了每天的撕心裂肺。</p>
<p>下半年的击破表演和红带考试是2020后半年的重点，两次都有各自的遗憾，但我似乎也开始慢慢学会开解自己的遗憾，不再懊恼和自责，而且在训练中多加补足，争取下一次能弥补遗憾。下半年的周六也增加了更多的体能训练，自我感觉在体能上也得到了长足进步，不再那么的担心考场中体力不支而无法呈现出最好的表现了。</p>
<p>宝杰应该是我跆拳道生涯里第二敬佩的学员。半年来的习练无论是在带色还是技术上的飞速进步是她每日坚持的成果，多次的韧带拉伤也没有打消她的韧带梦想，再加上实战时候的勇敢和坚定，生活中的不服输和挑战自我都让我这个废柴似乎又找回了一点年轻时的热血。</p>
<p>说到实战，胡姐姐头脑清晰，技术全面，实战经验丰富却仍谦虚勤练，宝杰呢，勇猛坚定，时机紧握，不畏级别高低，实战场上的自信令哪怕是较她级别高许多的学员都心生几分畏惧。这两个人不仅能打，还好打，每逢周六晚上都已经不想去练习枯燥的基本动作了，直接是一通暴打。红带时没有实战压力的我丝毫不想卷入这两个强者之间的战争。然而考完红带后，考试的压力让我不得不去面对自己畏惧的对抗互捶。</p>
<p>第一次参与其中的我还是一团混乱，找不到节奏，除了打不着别人就是被别人狂打头，僵硬的颈椎都放松了好几分。但是鼓起勇气的我在两个强者的陪伴下，再加上有事没事就开始在视频上学习对打技巧，几次习练后似乎找到了一些门道，克服了心中的恐惧，找到了适合自己的一些技巧，在和丁师范对打时丁师范也夸奖到进步了。</p>
<p>另一件让我感到十分温暖的事情是，在考黑红带前大约半个月的时间，偶然问到师范多久可以考黑带，当时大概是10月底，师范说，大概明年2月考黑红带，明年10月考黑带，当时已经学完考试内容的我略感疑惑，感觉自己每日的坚持没有了动力，习惯了临时抱佛脚的我觉得那为何不等快考试的几个月再加紧习练呢。于是是一个月的停练和心中放弃的魔鬼又跑了出来。偶然间和宝杰胡哲提到心中想法后，两位同伴一个苦口婆心的劝导，另一个还特地打来电话慰问。甚至二位还帮我将内心说不出口的想法讲给了师范，看看有没有解决办法。于是乎，师范通知，可以尽快准备考试了。说来也为自己的功利感到羞愧，本身想要踏实完成黑带考试，成为一名优秀黑带选手的我在面对选择的时候，还是选择了更加功利的早些得到黑带。但我很感激同伴的热情帮我争取这次机会，我也很感激师范的信任让我能够拥有这一次机会，因此也是倍加珍惜，尽量争取能够多加习练，争取能够多多出勤，不辜负同伴和师范的期望。也在持证教练肖教练的帮助下逐步跟上了品势内容。说到这里，不得不cue，肖教练真的是有教练的天赋，恩威并施，耐心又严厉，我猜她如果有此志向的话，一定会成为优秀的女教练。</p>
<p>世上无难事，只怕有心人。一句俗语，可也一直在陪伴我的成人跆拳道生涯。好汉不提当年勇，年轻的灵活敏捷早已该放到过去，对现实的困难发起挑战是要从几位师范、同伴身上学习的，“礼义廉耻，忍耐克己，百折不屈”曾经陪伴过我俯卧撑从0-n的突破，陪伴过我冬季3000m长跑的咬牙坚持，和许多运动生涯的艰辛瞬间。年长的我虽然心智在逐渐成熟，但对跆拳道的意志力却与幼时的自己不增反减。很感谢遇到的两位鸡血同伴挽救了一个废柴跆拳道选手，很感谢这一个又一个被迫立下的flag目前还没有轰然倒塌，很感谢我们即便面对质疑与世俗的不解，仍然坚持在自己热爱的道路上。</p>
<p>对成年后仍在这条道路的我，不期待三天打鱼，两天晒网的短暂热情，期待能够习得自律的习惯和稳步进步，期待能够像几位师范和同伴一样，热情不减，勇敢更多。许愿自己2021flag不倒，心愿达成。祝愿师范事业继续蒸蒸日上，生活温馨顺利。祝愿两位伙伴不忘初心，永远热忱。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>新心愿达成，新身份新快乐收获</title>
    <url>/2021/03/13/jhk-teaching-assistant-1st-time/</url>
    <content><![CDATA[<p>纪念一下收获了爱与能量的一天！</p>
<p>早上8点多起来，就是为了能不放同学鸽子，毕竟已经放了道友们好多次鸽子了，再放鸽子感觉我人品有问题，必须得去！所以就这样开启了神奇的一天。</p>
<p>早上8-9就起床了去看表演，自己做的很烂，朋友们都比我现场看的好很多哈哈哈哈，dsf剪辑也有点厉害啊，（就像胡哲说的，了解了一个人的人物传记之后，就会对这个人很有好感），我今天又成为dsf的迷妹了，感觉又在发着光。</p>
<p>但印象很深刻的是马玲的冲木前面几个动作巨好看，完美，养眼，赏心悦目以及自愧不如（可能这就是宝杰说的那种感受吧，她觉得很完美，但或许当事人不觉得）。然后就看到我的后踢又驼背了，是三个人里最差的，腰背部力量不够，身体没撑起来（难道这也是ysf晚上突然又cue我们做后踢辅助动作的原因之一吗？难道他也发现问题了？哈哈哈哈）</p>
<p>其他道馆，尽量发现的话还是有几个精品，但是普遍水平感觉没差很多，高级别很帅，看来tkd也是吃经验的。</p>
<p>10:15，换了便装的上身卫衣，匆匆赶到南山馆，还可以继续看一会，海岸城馆气势很大，哈哈哈哈，感觉果真什么师范带出什么样的学生。</p>
<p>10:50，收拾道具，迷茫，还傻乎乎的问ksf需不需要帮忙，没眼力见，在dsf问“你俩谁可以拍照”的时候，终于又勇敢了一把，舒坦爽快，也就开启了勇敢的一天。</p>
<p>11:00，开始30来个学员的儿童班课程，级别从白带到黑带，年纪从三四岁，到十来岁，看得头都大了，无法想象怎么上课，但是作为大头兵的我就哪里需要哪里搬就好了。</p>
<p>11:05，ksf整队敬礼，带队热身活动，迷茫，不知道该站哪，不知道该不该一起热身，不知道该不该帮助小朋友，这时的马师范已经开始自觉热身了，还得到了dsf的表扬，🍋，后面才知道原来msf之前有过这种经验，一回生二回熟哈哈哈</p>
<p>11:10，30个小朋友按级别分成了五队，五个助教，一人带一队，当场感叹师范简直是人才，这么简单的除法就解决了30个熵混乱的问题哈哈哈哈。尴尬的是，ksf喊我们三个过去的时候，作为打头的我愣住了，不知道该从哪个路径走过去，恍惚了几秒，一向超级超级super-nice温柔的ksf突然很严肃的说，助教要快一点啊！妈呀被嫌弃了有点尴尬，还是要硬着头皮迎接接下来的任务啊。</p>
<p>11:15，ksf做师范，助教帮忙拿脚把，妈呀第一脚就被踢，第二脚又被踢了，都怪我脚把拿多高我都不知道，而且小朋友身高一会高一会低，脚把位置一直在变动，我的天，为啥有的小孩踢左脚，有的右脚，有的正面，有的背面，有的格斗式，有的并脚，刚才ksf怎么做的？？？脑子一面空白，dsf过来旁边告诉我，你放松一点，不要脚把抓的那么紧，dsf也帮助小朋友指正动作，这时候我终于只需要做个不需要思考的拿把机器就好了，舒坦，但是dsf走了之后又陷入了慌张和迷茫。终于ksf喊停了，第一组动作，带小朋友一起划水结束哈哈哈哈</p>
<p>11:20，这一次的示范动作我很认真的观察ksf怎样站位，怎样准备的，怎样起腿，怎样结束动作，脚把位置多高，甚至关键点是什么。到我这边实践后，小朋友们还是乱七八糟，我开始学会说，换脚，面向我，背对我，左腿，右腿等简单的方位动作，终于捋顺了大家的完整动作，从一个方向出腿了。开始也去观察主要的问题点，并且提出合理的建议，或者做出简单的解决方案，才到我大腿个子的小朋友（看似也就三四岁），居然可以听懂我的简单指令？？？成功建立沟通连接了之后，感觉开始上手了。居然也可以听懂我的解决方案？？小朋友真厉害！！真的进步了一点点点！好棒！！！（虽然内心疯狂喜悦，肾上腺激素疯狂上飙，但是嘴巴上的我还是一个敷衍的表扬机器），“很好”，“还不错”，我自己都感觉自己的表扬十分敷衍，但是又学不会dsf的那种“对iiiiii了！”，“对iiiii，就是这样做嘛！”太厉害了，佩服的五体投地了。</p>
<p>渐入佳境的我完成了几组动作指导后，发现队伍有点乱，不知道该怎么管理，发现后面小朋友在自己玩，不知道该咋办，不敢凶他们，发现小朋友东张西望了，不知道该怎么勾回他们的注意力，只能先注意到在做动作的这个人身上了。</p>
<p>小朋友们的性格，脾气，也各式各样，各有千秋，蓝带高个子朋友打头，最高级别，年龄也相对较大一些，所以能够沟通，动作上也是最好的。所以多是表扬，和给他更多的一些挑战。比如旁边两组高级别都是跳后旋，他在我们这组里级别最高，我看他也可以尝试，就叫他尝试了一下，发现果真还不错、他的表情令我感觉他自己做了高级别的动作，也有点小满足，小骄傲，小自豪，哈哈哈，他的满足也是我的快乐，我也接收到快乐了。</p>
<p>绿带也是大腿高度的小男孩，从一开始就一直笑笑的，笑的人都要融化掉了，僵硬冰冷了这么多年的我（3年）感觉被暖到了，所以我也卸掉了社会中的面具，变得笑笑的，微笑也给我带来了满足和快乐。这个小男孩更逗的一点是，后面练拳法，我不知道为啥他总要连续快速做，本来只要做一个就好，我说你这咋还变成连环拳了，他笑笑继续他的连环拳。柴师范过来说，哎，你不可以这样对待姐姐哦，要好好表现，他果真听话了？？？开始一拳一拳做。原来他刚才不是没听懂或者不会做？柴师范一说他咋就懂了？？所以刚才是故意的？引起我的注意？？被小孩的聪明才智折服也骗到了，怪不得老师都喜欢偏爱坏学生，原来坏学生总是能很聪明的挑战到他的底线，让她觉得教育这件事都变有趣了，更想要管教好她，所以才会选择偏爱更有趣和更有挑战性的学生。哈哈哈哈哈神奇。</p>
<p>绿黄带小孩给我印象也很深刻，几次勾踢和后旋踢有点惊艳到了，感觉不是他这个级别应该领悟到的东西啊。哈哈哈，或许这就是低级别的快乐。总是会被夸奖。哈哈哈哈，反正黑红带以前我也总是被夸奖，自从要考黑带了就每日被diss。说回绿黄带，本来期望绿黄带应该不太能做这两个动作吧，居然甩了几下有模有样的？？？ksf也予以了多次的表扬和认可，原来不止我觉得这个有天赋，是个苗子。更逗乐的是，他的每次出拳之前，别人都是ha之后就出拳了，我不知道为啥，这个小孩看起来只有三四岁，他好像思考了有一两秒，攥紧了拳头，好像在蓄力的样子，打出的拳头，虽然那么小只，感觉也很有力气啊我的天，或许这就是传说中的天赋？？这个蓄力的动作太神奇了，感觉一个三四岁小孩都充满了神秘感，不知道他的小脑袋瓜在想什么。（哈哈哈这里有个插曲，就是，dsf跑过来，说，让姐姐看到你的力气哦！我心想，dsf真尊重我们，真社会，真客气，真乖，哈哈哈哈我都不好意思了，也不知道小宝贝心里怎么想的。）在思考出拳方向？还是在蓄力？不知道，但是打出的拳头还是不错的</p>
<p>还有多次dsf跑过来说，一开始说，你放松一点，你不要拿脚把那么紧张，你不要扎着马步，这一天腰都断了，放松自然一点。你要适时有感染性的鼓励一下小朋友，你要带有情绪的，“哇！真棒！”，“好棒啊”，做得好的甚至可以“give me five”。这个时候我感觉自己才是道场里的学生，dsf和小朋友都是我的老师，在教化着我怎么去应对这个不太擅长的局面，dsf是我的指路人，用他的经验告诉我怎样是对的，小朋友们是我的助教，陪我实践，给我反馈，一次又一次表情上的反馈告诉我这样做是不是对的。大多数都会是肯定我，我这样做确实是对的，他们很开心，他们有快乐，有成就感。天呐，没想到，我才是这堂课收获最多的人。</p>
<p>好了说回我队的小宝贝，绿带高个子男孩，看起来有四五年级了，很成熟稳重，容易沟通，还能在我搞不了小朋友的时候，帮我指导一下，只不过就是有点好像循规蹈矩，很认真做，但感觉还是不太灵动，所以会感觉有点死板，再加上最明显的问题就是速度慢了，所以对他的要求主要是提高速度，也是希望能提高灵动性，他一开始没有讲过话，后面中间一次他好像问了一句，是不是这样，那个时候我感觉好像互相产生信任了，我给他的回复也给了他安全感。他也更加卖力去做了。也很明显感觉速度快了动作舒服很多。</p>
<p>蓝绿带软萌小女孩，妈呀太可爱了，头发好多，软萌软萌的，不忍心碰她，哎，没想到协调性贼好，转身动作极其完美酷炫，又超乎想象了。小拳头不是打过来，好像是砸过来的，太可爱了，都不忍心纠正她可爱的动作了，哈哈哈哈。萌化。</p>
<p>最后好像还有一个绿带还是蓝绿带的小宝贝，他应该是做的都还不错，所以没有花心思给他改正动作的话，印象似乎没有太深刻，这可能就是那种平凡的孩子？自己很乖，很优秀，很听话，动作也很标准了，但是就是不容易给人留下印象了，突然有点代入一向“普普通通”“平平凡凡”的自己。</p>
<p>总而言之，这次助教课收获颇多！这种能量的突变确实力量巨大，从去之前的恐惧，担心是熊孩子，从小觉得自己没有孩子缘，担心自己管不住小孩，等等，居然全都得到了救赎，师范们把小朋友们带的很好，该听话的时候很听话不闹，小朋友们像小天使一般，用他们的笑容，单纯，善良，进取，努力，甚至调皮等都带给了我巨大能量，让我这一天都充满了能量，（也导致我现在话匣子打开都停不下来了），有点上头，有点快乐。</p>
<p>课后dsf暗示我说，我在楼下坐，你有什么技术上的交流可以来找我，我温习了今天的课程之后，留了十分钟，本想找dsf看一步对打，没想到坐下就开始聊教课体验，哈哈哈。我真诚的表达了自己的一些小想法，他也很快乐，似乎觉得给我的能量似乎也反馈给他了一部分快乐，我能从表情里看到他对于这件事安排的满意程度，主要是因为他觉得我乐在其中，很有收获。我说这个事情，有点有成就感啊，dsf说，我们做师范最大的就是成就感，blablabla说了一堆，总而言之就是表达我这种小想法的认可，有成绩感是正常的。应该要有成就感。然后我说我没有经验，很紧张，dsf说以后还可以找你来，我很开心，也爽快的答应了。甚至还说，如果你以后，敲代码敲烦了，不想敲了，就给我打电话，（略带哭腔）地说，师范，我代码敲的不快乐了，我马上给你安排助教课，给你找找成就感，然后你再回去快乐的敲代码。天啊啊啊啊啊啊啊啊，听到这句话的我真的彻底融化，在这个诺大的城市里，受了委屈，有师范给我兜底，我真的心怀感激，备感荣幸，也感受到了深深的安全感，就算捅了篓子，也有人有事情能救回我的感觉，这也太适合我这个时不时低沉但是内心始终温暖热血的表面丧人了。听完这话的我，虽然内心知道这一把年纪了，肯定是不会好意思这样做的，最多也就是师范需要我，我会很乐意去帮忙，但是内心也是着实被暖到了，感觉似乎在深圳这个城市里伪装的冰冷，今天被小朋友和师范们全都融化了。表达完我有点快乐有点上头的情绪后，师范似乎也放宽了心，说以后还会找我来，让我慢慢接触尝试，说不定就转师范了哈哈哈哈哈，这种职业道路的变更，虽然想过，想过无数次、很多次，数万次，但是太需要勇气了，风险极大，落差极大，自己的专业性和天赋能力也远远不足，只能慢慢来，先享受小天使们带给我的快乐和满足吧。我现在算是体会到dsf所说的大师兄是自己享受指导和帮助人的过程，我现在算是理解了，之前可能是我们理解错了吧。到现在也算又圆了一个小时候的跆拳道助教梦想哈哈哈哈，人生真的太神奇了，儿时的梦想居然一个一个接一个的实现了，这种肾上腺素飙升的感觉也太快乐了。</p>
<p>至此，先鞠躬敬礼感谢丁师范的信任给我这一次机会，感谢dsf在看我迷茫的时候一直耐心的指导我该怎样做，哪里不对，（比指导我技术的时候耐心多了哈哈哈哈），感谢小朋友们包容我这个新手助教，给我信任、微笑、力量和快乐，感谢杨师范包容我中途跑掉，还不回来上课了，晚上还跑过来蹭场地的同时，晚上还不计前嫌地帮我个人抠动作了有三四十分钟，感谢柴师范还是用他的一如既往的温暖温暖着我，感谢自己珍惜这次机会才得偿所愿，感谢心中有梦的自己这么多年还没有忘记最初的梦想，人间又值得了！！！！！</p>
<p>最后上升一些生活哲学，</p>
<ol>
<li>六个小朋友不同的性格，似乎都让我看到了社会和人生。</li>
<li>你带的学生，最后就会变成你自己的一面镜子。dsf这句话真的绝了。联想到现实更绝。ksf和dsf，dsf和cj，ysf和bj，hjj和我算是dsf和ysf的混交吧，hjj偏dsf，我算是混的比较平均的。哈哈哈哈也算是这种结合方式的首次尝试，也挺特别和快乐的。</li>
<li>tkd快乐今日double、乃至triple，hundredable，突然感觉被赋予了使命，感受到了教育的快乐</li>
<li>又一次感受到了几个师范的温柔，简直温柔了深漂的岁月，今天繁师范问我为什么成人还要练tkd，之前一个小朋友的爸爸也问过我，我第一次回答对黑带的执念（或许是我最初的梦想），今天回答工作之余的发泄和对现实生活的逃避，感觉有点官方现实，但也有点映射哲学了。现在总结一下，于我而言，最开始是对黑带的执念，小时候不断突破自己的回忆让我埋下种子，在更有时间精力的时候完成小时候未竟的梦想，在参加了之后完全是被金载勋吸引了，道馆师范对自己要求太高，包括技术上，心理上，各种综合素质都相对于其他成人道馆要成熟太多太多了。对深漂的自己是一种解脱，感觉像是在深圳的一个家，朋友之间没有利益冲突，可以很坦率的交朋友，师范燃烧自己，照亮他人，最关键的是，师范自身的经历，相近的年龄，能给自己带来很多人生的启迪，有时候工作不顺训练上身体解压，朋友上互相开导，师范也会给出建议以及甚至给你兜底，懂得教育心理学的师范们，即便你受欺负了，也能帮你解脱出来。女孩子独自一人在外的护身防卫，不要以为是花架子，金载勋不一样，不仅仅玩运动竞技场上的规则，尤其是成人班很强调实用性的，很多动作你是真实可以应用到实际情景的，上海两例女性自我保护事件，这个东西会给自己在外漂泊更多安全感。价格不贵的， 挠痒痒一样，对道馆来说，成人班只是他们积累更多经验，基本不赚钱的，你说这种性价比这么高的活动，不值得吗？时间也不会耽误很多，但是需要坚持，但是培养任何一项业务爱好和特长，都需要坚持。最后才是，减肥，运动，保持身材，柔韧性，协调性，力量等等很多问题。如果想要自己的生活，不仅仅是工作，家庭，没有别的味道了，那还是非常值得推荐的。总而言之，工作之外的生活，变得更加丰富多彩，快乐，充实和解压。</li>
</ol>
<p>对自己黑带的要求是：小于3次失误，击破、一步对打和品势因为只有一次机会，所以要0失误。横叉flag不要倒啊加油啊不要倒啊。</p>
<p>好了，放过自己，今天的输入太多，终于输出完毕，今天可以圆满结束了。晚安蓝蓝。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>BC challenge 2019 Top5队伍 技术分析</title>
    <url>/2020/03/09/bc2019top5/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>Frontend</strong></th>
<th><strong>Duration Modelling</strong></th>
<th><strong>Spectrogram modelling</strong></th>
<th><strong>Vocoder</strong></th>
<th><strong>Features</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>USTC-iflytek</strong> <strong>科大讯飞</strong></td>
<td>Tasks:  special marks procession, polyphones classification, breaks prediction focuses prediction.  Methodoly: Bidirectional Encoders Representations from Transformers (BERT)-based multi-task models</td>
<td>LSTM-RNN models autoregressive model structure,</td>
<td>A statistical parametric speech system (SPSS) GAN-based multi- task acoustic modeling Fundamental frequency (F0), 41 dimensional mel-cepstra (M- CEP), band aperiodicity (BAP) were adopted as the acoustic features</td>
<td>Wavenet   The acoustic feature used was the joint feature vector of Mel-cepstrum, F0 and the u/v decision. Multi-speaker dataset for argumentation</td>
<td>Text-side: Manual annotations: Pinyin(with tone), PW, PP, and focus position Speech-side: Frame-level acoustic features:</td>
</tr>
<tr>
<td><strong>DeepSound</strong> <strong>深声科技</strong></td>
<td>Tasks: text normalization, qingsheng, sandhi and erhua, : rule-based G2P: Bi-LSTM prosody prediction, PW, PPH, IPH: Bi-LSTM BiLSTM-based recurrent network (RNN) is used in the G2P module for polyphone and prosody prediction.</td>
<td>/</td>
<td>VQVAE. + a embedding+prenet oper- ation + GAN based postfiltering     (robust on the unclean dataset )</td>
<td>robust multi-speaker neural vocoder conditioned on the mel spectrograms</td>
<td>manual and auto- matic tagging operations: phoneme, tone, prosody and pause duration</td>
</tr>
<tr>
<td><strong>腾讯</strong></td>
<td>Festival front-end to predict phoneme, tone and other linguistic features   +   BERT sentence embeddings are generated by a pre-trained Bert model.</td>
<td>/</td>
<td>A multi-speaker model is trained first.</td>
<td>multi-speaker model trained first. Wavenet</td>
<td>linguistic feature (The HTS full-context label) and sentence embedding mel spectrograms + channel embedding</td>
</tr>
<tr>
<td><strong>灵伴</strong></td>
<td>text normalization, word segmentation, part-of-speech tagging, phonetic disambiguation word segmentation of the sentence, Part-of-Speeches (POS) of this word sequence and prosodic hierarchy</td>
<td>/</td>
<td>DNN-LSTM</td>
<td>Wavenet   ground-truth mel-spectrograms plus F0</td>
<td>spectral envelope, fundamental frequency (F0), contextual labels (phone-related and word-related features)</td>
</tr>
<tr>
<td><strong>Horizon</strong> <strong>南京团队</strong></td>
<td>The corresponding texts were manually embedded into 476-dimensional vectors using our own text an- alyzing system. The embedded vectors consisted of one-hot encoded phonemes, tones, part-of-speech, prosodic boundaries and the position information.   Prosody boundary: phoneme boundaries, syllable boundaries, phrase boundaries, secondary phrase boundaries</td>
<td></td>
<td>DCTTS[14] and Deep Voice 3[13]</td>
<td>WaveRNN</td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>韵律建模思维导图</title>
    <url>/2020/09/20/prosody-modelling/</url>
    <content><![CDATA[<p><img src="/images/%E6%83%85%E6%84%9F%E5%BB%BA%E6%A8%A1.png" alt="韵律建模论文思维导图"></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>语音顶会论文集总结</title>
    <url>/2020/12/03/speech-papers/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th></th>
<th><strong>会议地点</strong></th>
<th><strong>离线论文包</strong></th>
<th><strong>论文集网址</strong></th>
<th></th>
</tr>
</thead>
<tbody><tr>
<td><strong>Icassp 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>Interspeech 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td><a href="https://www.isca-speech.org/archive/Interspeech_2020/">https://www.isca-speech.org/archive/Interspeech_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Neurips</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://proceedings.neurips.cc/">https://proceedings.neurips.cc</a></td>
<td></td>
</tr>
<tr>
<td><strong>BC 2020 &amp; VC 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://www.isca-speech.org/archive/VCC_BC_2020/">https://www.isca-speech.org/archive/VCC_BC_2020/</a></td>
<td></td>
</tr>
<tr>
<td><strong>Iclr 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td><a href="https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech">https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech</a></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td><strong>论文合集网址</strong></td>
<td>-</td>
<td>-</td>
<td><a href="https://www.isca-speech.org/iscaweb/index.php/archive/online-archive">https://www.isca-speech.org/iscaweb/index.php/archive/online-archive</a></td>
<td>含interspeech、ssw等论文合集</td>
</tr>
<tr>
<td><a href="https://openreview.net/">https://openreview.net</a></td>
<td>含iclr、icml、acm等论文集</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Vocoders 模型总结</title>
    <url>/2020/12/03/vocoders/</url>
    <content><![CDATA[<p>语音合成声码器脉络总结如下，持续更新ing</p>
<table>
<thead>
<tr>
<th><strong>Order</strong></th>
<th><strong>Model</strong></th>
<th><strong>Year</strong></th>
<th><strong>Institution</strong></th>
<th><strong>Conference</strong></th>
<th><strong>Inherited Model (Base model)</strong></th>
<th><strong>Corresponding Author (Team leader)</strong></th>
<th><strong>URL</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>1</strong></td>
<td>WaveNet</td>
<td>2016.9</td>
<td>Google DeepMind</td>
<td>SSW 2016</td>
<td>CNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1609.03499.pdf">https://arxiv.org/pdf/1609.03499.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>WaveRNN</td>
<td>2018.6</td>
<td>DeepMind &amp; Google Brain</td>
<td>ICML 2018</td>
<td>RNN</td>
<td>Nal Kalchbrenner</td>
<td><a href="https://arxiv.org/pdf/1802.08435.pdf">https://arxiv.org/pdf/1802.08435.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>WaveGlow</td>
<td>2018.10</td>
<td>Nvidia</td>
<td>ICASSP 2019</td>
<td>WaveNet</td>
<td>Rafael Valle</td>
<td><a href="https://arxiv.org/pdf/1811.00002.pdf">https://arxiv.org/pdf/1811.00002.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>LPCNet</td>
<td>2019.2</td>
<td>Mozilla, Google</td>
<td>ICASSP 2019</td>
<td>WaveRNN</td>
<td>Jean-Marc Valin</td>
<td><a href="https://arxiv.org/pdf/1810.11846.pdf">https://arxiv.org/pdf/1810.11846.pdf</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WaveGAN</td>
<td>2019.2</td>
<td>UC San Diego</td>
<td>ICLR 2019</td>
<td>GAN</td>
<td>Miller Puckette</td>
<td><a href="https://arxiv.org/pdf/1802.04208.pdf">https://arxiv.org/pdf/1802.04208.pdf</a></td>
</tr>
<tr>
<td><strong>6</strong></td>
<td>Multi-band WaveRNN</td>
<td>2019.4</td>
<td>Tecent AI Lab</td>
<td>Interspeech 2020</td>
<td>DurIAN, WaveRNN</td>
<td>Dong Yu</td>
<td><a href="https://arxiv.org/pdf/1909.01700.pdf">https://arxiv.org/pdf/1909.01700.pdf</a></td>
</tr>
<tr>
<td><strong>7</strong></td>
<td>MelGAN</td>
<td>2019.12</td>
<td>University of Montreal, Mila, Lyrebird AI</td>
<td>NeurIPS 2019</td>
<td>GAN</td>
<td>Yoshua Bengio</td>
<td><a href="https://arxiv.org/pdf/1910.06711.pdf">https://arxiv.org/pdf/1910.06711.pdf</a></td>
</tr>
<tr>
<td><strong>8</strong></td>
<td>SqueezeWave</td>
<td>2020.1</td>
<td>UC Berkeley</td>
<td></td>
<td>WaveGlow</td>
<td>Bichen Wu</td>
<td><a href="https://arxiv.org/pdf/2001.05685.pdf">https://arxiv.org/pdf/2001.05685.pdf</a></td>
</tr>
<tr>
<td><strong>9</strong></td>
<td>Parallel WaveGAN (PWG)</td>
<td>2020.2</td>
<td>LINE Corp., NAVER Corp.</td>
<td></td>
<td>GAN</td>
<td>Ryuichi Yamamoto</td>
<td><a href="https://arxiv.org/pdf/1910.11480.pdf">https://arxiv.org/pdf/1910.11480.pdf</a></td>
</tr>
<tr>
<td><strong>10</strong></td>
<td>Multi-band MelGAN</td>
<td>2020.5</td>
<td>西北工业大学，sogou</td>
<td></td>
<td>melgan, multi-band</td>
<td>Xielei</td>
<td><a href="https://arxiv.org/pdf/2005.05106.pdf">https://arxiv.org/pdf/2005.05106.pdf</a></td>
</tr>
<tr>
<td><strong>11</strong></td>
<td>FeatherWave</td>
<td>2020.10</td>
<td>Tecent</td>
<td>Interspeech 2020</td>
<td>MB LP, WaveRNN</td>
<td>Shan Liu</td>
<td><a href="https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf">https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf</a></td>
</tr>
<tr>
<td><strong>12</strong></td>
<td>WaveGrad</td>
<td>2020.10</td>
<td>Johns Hopkins University, Google Brain</td>
<td></td>
<td>CNN</td>
<td>Heiga Zen</td>
<td><a href="https://arxiv.org/pdf/2009.00713.pdf">https://arxiv.org/pdf/2009.00713.pdf</a></td>
</tr>
</tbody></table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ASRU 2019 语音合成相关论文</title>
    <url>/2021/06/17/asru-2019-papers/</url>
    <content><![CDATA[<div class="table-container">
<table>
<thead>
<tr>
<th><strong>序号</strong></th>
<th><strong>论文题目</strong></th>
<th><strong>作者</strong></th>
<th><strong>单位</strong></th>
<th><strong>摘要</strong></th>
<th><strong>关键词</strong></th>
<th><strong>论文链接</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>1</strong></td>
<td>MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</td>
<td><em>Xuankai Chang</em>1,2<em>, Wangyou Zhang</em>2<em>, Yanmin Qian</em>2†<em>, Jonathan Le Roux</em>3<em>, Shinji Watanabe</em>1†</td>
<td>1Center for Language and Speech Processing, Johns Hopkins University, USA 2<strong>SpeechLab</strong>, Department of Computer Science and Engineering, <strong>Shanghai Jiao Tong University</strong>, China 3Mitsubishi Electric Research Laboratories (MERL), USA</td>
<td>MIMO-Speech, which extends the original seq2seq to deal with <strong>multi-channel input and multi-channel output</strong> so that it can <strong>fully model multi-channel multi-speaker speech separation and recognition</strong>. MIMO-Speech is a fully neural end-to- end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model.</td>
<td><strong>Overlapped speech recognition</strong>, end-to-end, neural beamforming, <strong>speech separation</strong>, curriculum learning.</td>
<td><a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></td>
</tr>
<tr>
<td><strong>2</strong></td>
<td>IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</td>
<td><em>Fengyu Yang</em>1<em>, Shan Yang</em>1<em>, Pengcheng Zhu</em>2<em>, Pengju Yan</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1∗</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University</strong>, Xian, China 2Tongdun AI Lab</td>
<td>We introduce a novel self-attention based encoder with learnable Gaussian bias in Tacotron. The proposed approach has the ability to generate stable and natural speech with minimum language-dependent front-end modules.</td>
<td>Tacotron, end-to-end, speech synthesis, <strong>self-attention, Gaussian bias</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></td>
</tr>
<tr>
<td><strong>3</strong></td>
<td>LEARNING HIERARCHICAL REPRESENTATIONS FOR EXPRESSIVE SPEAKING STYLE IN END-TO-END SPEECH SYNTHESIS</td>
<td><em>Xiaochun An</em>1†<em>, Yuxuan Wang</em>2<em>, Shan Yang</em>1,2<em>, Zejun Ma</em>2<em>,</em> <strong><em>Lei Xie\</em></strong>1⇤</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University,</strong> Xi’an, China 2 ByteDance AI Lab</td>
<td>we introduce a hierarchical GST archi- tecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech.</td>
<td>Speaking style, disentangled representations, <strong>hierarchical GST,</strong> style transfer</td>
<td><a href="http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf">http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf</a></td>
</tr>
<tr>
<td><strong>4</strong></td>
<td>BOOTSTRAPPING NON-PARALLEL VOICE CONVERSION FROM SPEAKER-ADAPTIVE TEXT-TO-SPEECH</td>
<td><em>Hieu-Thi Luong<strong>1,2</strong>, Junichi Yamagishi**1,2,3</em></td>
<td>1SOKENDAI (The Graduate University for Advanced Studies), Kanagawa, Japan 2National Institute of Informatics, Tokyo, Japan 3<strong>The University of Edinburgh</strong>, Edinburgh, UK</td>
<td>Bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.</td>
<td><strong>voice conversion,</strong> cross-lingual, speaker adaptation, transfer learning, text-to-speech</td>
<td><a href="https://export.arxiv.org/pdf/1909.06532">https://export.arxiv.org/pdf/1909.06532</a></td>
</tr>
<tr>
<td><strong>5</strong></td>
<td>WAVENET FACTORIZATION WITH SINGULAR VALUE DECOMPOSITION FOR VOICE</td>
<td><em>Hongqiang Du<strong>1,2</strong>, Xiaohai Tian<strong>2</strong>,</em> <strong><em>Lei Xie***</em></strong>1*<strong>**</strong>, Haizhou Li*<strong>*</strong>2***</td>
<td>1School of Computer Science, <strong>Northwestern Polytechnical University</strong>, xi’an, China 2Department of Electrical and Computer Engineering, National University of Singapore, Singapore <a href="mailto:hongqiang.du@u.nus.edu">hongqiang.du@u.nus.edu</a>, <a href="mailto:eletia@nus.edu.sg">eletia@nus.edu.sg</a>, <a href="mailto:lxie@nwpu.edu.cn">lxie@nwpu.edu.cn</a>, <a href="mailto:haizhou.li@nus.edu.sg">haizhou.li@nus.edu.sg</a></td>
<td>We propose to use singular value decomposition (SVD) to reduce WaveNet parame- ters while maintaining its output voice quality. Specifically, we apply SVD on dilated convolution layers, and impose semi-orthogonal constraint to improve the performance.</td>
<td>Voice Conversion (VC), <strong>WaveNet, Sin- gular Value Decomposition (SVD)</strong></td>
<td><a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf</a></td>
</tr>
</tbody>
</table>
</div>
<h1 id="Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION"><a href="#Paper-1-MIMO-SPEECH-END-TO-END-MULTI-CHANNEL-MULTI-SPEAKER-SPEECH-RECOGNITION" class="headerlink" title="Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION"></a>Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</h1><h2 id="MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf"><a href="#MIMO-Speech：端到端多通道多说话人语音识别（ASRU-2019-Best-paper）https-arxiv-org-pdf-1910-06522-pdf" class="headerlink" title="MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）https://arxiv.org/pdf/1910.06522.pdf"></a>MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）<a href="https://arxiv.org/pdf/1910.06522.pdf">https://arxiv.org/pdf/1910.06522.pdf</a></h2><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>ASRU的best paper思路是挺清晰的，但是与其他会议发表的classic论文相比还是感觉有一些些差距，有一些模型的细节点来说，会感觉有些晦涩难懂，打分：🌟🌟🌟</p>
<h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>MIMO-Speech，采用“多通道-输入”和“多通道-输出”，以建模 多通道 多说话人 情景下的语音分离和语音识别。</p>
<ul>
<li><p>模型结构由三部分组成：</p>
<p>1）单声道masking网络；</p>
<p>2）多源神经波束形成器；</p>
<p>3）多输出-语音识别模型；</p>
</li>
<li><p>学习策略：a curriculum learning strategy</p>
</li>
<li><p>实验结果：60% WER reduction</p>
</li>
</ul>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><ul>
<li><p>待解决问题：鸡尾酒聚会课题，分为 单通道 / 多通道 语音识别问题</p>
<ul>
<li><p>单通道多说话人语音分离：</p>
<p>1） Deep clustering (DPCL), 将时域单元映射到嵌入向量，再采用聚类算法将每个单元聚类到说话人源。此方法之后被嵌入到端到端训练框架中。</p>
<p>2）Permutation-invariant training (PIT)：用一个permutation-free目标函数来最小化重构损失。PIT之后被应用于多说话人ASR，在一个DNN-HMM混合ASR框架下。</p>
</li>
<li><p>多通道多说话人语音分离：</p>
<p>1）PIT，语音分离</p>
<p>2）unmixing transducer (a mask-based beamformer)，语音分离</p>
<p>3）DPCL：将inter-channel differences作为空间特征，和单通道频谱特征，语音分离</p>
</li>
</ul>
</li>
<li><p>本文贡献：多通道-多说话人-语音识别，输入multi-channel input (MI), 输出 multiple output (MO) text sequences, one for each speaker, 所以称为 MIMO-Speech。</p>
</li>
<li><p>可行性：最近的单说话人-远场-语音识别展示了 “神经波束”技巧对于去噪的价值，并且一些研究证实了end-to-end的可行性。[27] 进一步证实了神经波束方法在多通道端到端系统能够增强信号。</p>
</li>
</ul>
<h3 id="MIMO-Speech-2-pages"><a href="#MIMO-Speech-2-pages" class="headerlink" title="MIMO-Speech (2 pages)"></a>MIMO-Speech (2 pages)</h3><p><img src="/images/MIMO-Speech.png" alt="MIMO-Speech"></p>
<h4 id="Model-architecture"><a href="#Model-architecture" class="headerlink" title="Model architecture"></a>Model architecture</h4><ul>
<li>Stage 1: a single-channel masking network，通过预测多说话人和各通道噪音masks来实现<strong>预分离</strong></li>
<li>Stage 2: 多源 “神经波束” 来<strong>空间上分离多说话人的源头</strong></li>
<li>Stage 3: 端到端ASR来实现<strong>多说话人语音识别</strong></li>
</ul>
<p>创新点：masking network + neural beamformer，单目标函数进行模型训练。</p>
<p>Stage1 (Monaural masking network) 可以预分离开噪音和多说话人音源；Stage 2 生成多个beamforming filters $g^{i}(f)$ 用来分离和降噪输入的多声道信号；Stage3有多个说话人的encoder，和一个attention decoder组成，来生成多个说话人的文本序列输出。</p>
<h4 id="Data-scheduling-and-curriculum-learning"><a href="#Data-scheduling-and-curriculum-learning" class="headerlink" title="Data scheduling and curriculum learning"></a>Data scheduling and curriculum learning</h4><ul>
<li><p>问题痛点：端到端训练难以收敛</p>
</li>
<li><p>解决方案<strong>（Data scheduling）</strong>：随机从以下两个数据集中选择一个batch</p>
<p>1） 不仅采用多空间域的多说话人数据集</p>
<p>2） 也采用单说话人的数据集</p>
</li>
<li><p>细节<strong>（Curriculum learning）</strong> 配Algorithm：</p>
<p>1） 当选择到单说话人的数据集的时候，数据不经过 masking network 和 neural beamformer 模型，以加强end-to-end ASR模型的训练。</p>
<p>2） 计算出最大声和最小声说话人声音之间的信噪比SNR，然后按“升序”排列，从SNR=1的数据集开始训练</p>
<p>3） 将单说话人的数据集从短到长进行排序，让seq2seq模型首先学习短语句。</p>
</li>
</ul>
<h3 id="Experiments-3-pages"><a href="#Experiments-3-pages" class="headerlink" title="Experiments (3 pages)"></a>Experiments (3 pages)</h3><h4 id="3-1-Configurations"><a href="#3-1-Configurations" class="headerlink" title="3.1 Configurations"></a>3.1 Configurations</h4><h5 id="3-1-1-Neural-Beamformer"><a href="#3-1-1-Neural-Beamformer" class="headerlink" title="3.1.1 Neural Beamformer"></a>3.1.1 Neural Beamformer</h5><h5 id="3-1-2-Encoder-Decoder-Network"><a href="#3-1-2-Encoder-Decoder-Network" class="headerlink" title="3.1.2 Encoder-Decoder Network"></a>3.1.2 Encoder-Decoder Network</h5><h4 id="3-2-Performance-on-ASR"><a href="#3-2-Performance-on-ASR" class="headerlink" title="3.2 Performance on ASR"></a>3.2 Performance on ASR</h4><p>Motivation: 验证提出的模型好于baselines</p>
<p>Baselines / Ours</p>
<h4 id="3-3-Performance-on-Speech-seperation"><a href="#3-3-Performance-on-Speech-seperation" class="headerlink" title="3.3 Performance on Speech seperation"></a>3.3 Performance on Speech seperation</h4><p>Motivation: 验证 neural beamformer 学习了一个波束行为，能够用于语音分离。</p>
<h4 id="3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验"><a href="#3-4-Evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验" class="headerlink" title="3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)"></a>3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)</h4><p>Motivation: 验证在实际情况下的模型性能。</p>
<h1 id="Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS"><a href="#Paper-2-IMPROVING-MANDARIN-END-TO-END-SPEECH-SYNTHESIS-BY-SELF-ATTENTION-AND-LEARNABLE-GAUSSIAN-BIAS" class="headerlink" title="Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS"></a>Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</h1><h2 id="Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf"><a href="#Paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-http-lxie-nwpu-aslp-org-papers-2019ASRU-YFY-pdf" class="headerlink" title="Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf"></a>Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 <a href="http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf">http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</a></h2><h2 id="感想-1"><a href="#感想-1" class="headerlink" title="感想"></a>感想</h2><p>有些论文读起来会觉得高深莫测，但是有有部分价值可以吸取，打分：🌟🌟</p>
<h3 id="Abstract-1"><a href="#Abstract-1" class="headerlink" title="Abstract"></a>Abstract</h3><p>问题痛点：虽然对于英文来讲，现有的如Tacotron等模型已经能够实现端到端的语音合成过程，即从英文字母直接转换至语音。但是对于如中文这样的语言，仍旧需要繁复的前处理过程（如词边界、韵律边界等），使得这个文本处理前端的过程和传统方法一样复杂。</p>
<p>解决方案：为了保持生成语音的自然度、以及摒弃特定语言的特殊性，普通话语音合成过程中，我们引入了一个创新性的自注意力机制作为编码器，并且引入可学习的高斯bias到Tacotron中</p>
<p>实验结果：我们评估了不同的系统（在 有/无 韵律信息的情况下），结果显示提出的方法能够在最小的语言-依赖的前端模块的情况下，生成稳定和自然的语音。</p>
<h3 id="Introduction-1页"><a href="#Introduction-1页" class="headerlink" title="Introduction (1页)"></a>Introduction (1页)</h3><ul>
<li><p>待解决问题</p>
<p>传统的端到端方法包含复杂的特征提取过程，如：Part-of-speech tagging, pronunciation prediction, prosody labelling. 即便如Tacotron的端到端语音合成系统被提出，但是单纯的输入音素也无法使得语音合成模型得到良好的效果，所以学者提出嵌入PW、PPH、IPH，来进行韵律边界的特征建模。但是这使得违背了端到端语音合成的初衷，使得这个系统再次变得更加复杂。</p>
</li>
<li><p>本文贡献</p>
<p>1）<strong>全局韵律建模：</strong>由于self-attention被证明对于简单的音素序列进行全局建模时有良好的效果，所以本文尝试采用自-注意力机制作为编码器来获取全局的韵律信息。</p>
<p>2）<strong>局部韵律建模：</strong>至于局部的韵律信息，我们采用了一个可学习的高斯bias引入到自注意力机制中，因为Gaussian分布更加集中于当前位置的局部关系。</p>
</li>
</ul>
<h3 id="Proposed-SAG-Tacotron-1-5页"><a href="#Proposed-SAG-Tacotron-1-5页" class="headerlink" title="Proposed SAG-Tacotron (1.5页)"></a>Proposed SAG-Tacotron (1.5页)</h3><h4 id="3-1-Motivation"><a href="#3-1-Motivation" class="headerlink" title="3.1 Motivation"></a>3.1 Motivation</h4><ul>
<li><p>目的：为了采用最少的文本分析模块，所以引入自-注意力学习机制，来进行全局依赖建模。</p>
</li>
<li><p>方案：1）将Encoder的CBHG模块，用自-注意力机制替换；2）可学习的高斯bias来提升局部建模。</p>
</li>
</ul>
<p><img src="/images/SAG-Tacotron.png" alt="SAG-Tacotron"></p>
<h4 id="3-2-基于-自注意力机制-的-编码器"><a href="#3-2-基于-自注意力机制-的-编码器" class="headerlink" title="3.2 基于 自注意力机制 的 编码器"></a>3.2 基于 自注意力机制 的 编码器</h4><p>Encoder 的 Pre-net是一个3层-CNN + Batch Norm + ReLU，尽管自-注意力机制不包含序列信息，我们注入类似于Transformer的位置信息。</p>
<script type="math/tex; mode=display">
PE_{pos,2i}=sin(pos/10000^{2i/d})</script><script type="math/tex; mode=display">
PE_{pos, 2i+1} = cos(pos/10000^{2i/d})</script><p>其中，$pos$是当前位置，$d$是特征维度，$i$是当前维度。PE也被输入到自-注意力模块。自注意力模块包含了一个自注意力层+全连接层+tanh激活函数。残差连接被应用于上述层。</p>
<p>对于多头注意力机制的每一个$head_i$, 对于一个有$n$个元素的序列$x$，我们想要获得有相同长度n的隐状态向量$head_i$, 这里采用scale-product注意力机制。</p>
<script type="math/tex; mode=display">
Head_{i}=\sum_{j=1}^{n}ATT(Q,K)V</script><script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy)​</script><script type="math/tex; mode=display">
energy = \frac {QK^{T}} {\sqrt{d}}</script><p>最终的多头注意力机制为：</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_{1}, ..., head_{h})W^{O}</script><p>其中的$W^{O}$是最后一层线性层的参数矩阵。</p>
<h4 id="3-3-可学习的Gaussian-bias"><a href="#3-3-可学习的Gaussian-bias" class="headerlink" title="3.3 可学习的Gaussian bias"></a>3.3 可学习的Gaussian bias</h4><p>在序列-序列模型中，对于中文来讲，相近的位置是十分重要的。在这种情况下，我们想要为注意力机制提升编码器对于临近状态的局部贡献。</p>
<p><img src="/images/Gaussian-bias.png" alt="Gaussian-bias"></p>
<p>如上图所示，首先，假设一个以e5为中心的高斯bias，窗长为3（实际上，窗长是一个可学习的参数）。然后将注意力机制的分布通过高斯bias来进行正则化，以生成最终的分布。如图3所示，最终的分布是会在e5附近有更多的权重的。</p>
<p>具体来讲，Gaussian bias $G$被mask到energy上，即</p>
<script type="math/tex; mode=display">
ATT(Q, K)=softmax(energy + G)</script><p>其中$G \in R^{N\times N}$，$G \in (-1;0]$ 度量了当前的query $x_i$与position $j$ 之间的关系：</p>
<script type="math/tex; mode=display">
G_{ij}=-\frac {(j - P_{i})^2}{2\sigma_i^2}</script><p>其中的$P_i$是$x_i$的中心位置，当给定输入序列 $x=(x_1, x_2, …, x_n)$，$\sigma_i$是标准差。如何选择合适的$P_i$和$\sigma_i$是关键。</p>
<script type="math/tex; mode=display">
P_i = N\cdot sigmoid(v_p^{T}tanh(W_{p}x_i))</script><script type="math/tex; mode=display">
D_i = N\cdot sigmoid(v_d^{T}tanh(W_{d}x_i))</script><p>其中$\sigma_i = \frac {D_i}{2}$, $W_p$ 和$W_d$是模型参数矩阵。</p>
<h3 id="Experiments-2-5页"><a href="#Experiments-2-5页" class="headerlink" title="Experiments (2.5页)"></a>Experiments (2.5页)</h3><h4 id="4-1-Basic-setups"><a href="#4-1-Basic-setups" class="headerlink" title="4.1 Basic setups"></a>4.1 Basic setups</h4><h4 id="4-2-System-comparison"><a href="#4-2-System-comparison" class="headerlink" title="4.2 System comparison"></a>4.2 System comparison</h4><ul>
<li>Baseline: Tacotron1</li>
<li>Baseline-prosody: Tacotron1 with complex inputs</li>
<li>SAE-Tacotron: Self-attention as encoder without Gaussian bias with simply inputs</li>
<li>SAG-Tacotron: Self-attention as encoder with Gaussian bias with simple inputs</li>
<li>Transformer with simple inputs</li>
</ul>
<p><img src="/images/simple_inputs.png" alt="simple_inputs"></p>
<p><img src="/images/complex_inputs.png" alt="complex_inputs"></p>
<h4 id="4-3-Model-details"><a href="#4-3-Model-details" class="headerlink" title="4.3 Model details"></a>4.3 Model details</h4><h4 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h4><h5 id="4-4-1-Robustness-test"><a href="#4-4-1-Robustness-test" class="headerlink" title="4.4.1 Robustness test"></a>4.4.1 Robustness test</h5><p>Motivation: 评估attention对齐（Repeats / Skips）的鲁棒性</p>
<h5 id="4-4-2-Prosody-analysis"><a href="#4-4-2-Prosody-analysis" class="headerlink" title="4.4.2 Prosody analysis"></a>4.4.2 Prosody analysis</h5><p>Motivation: 评估重读音节的pitch以及trajectory pattern of F0</p>
<h5 id="4-4-3-Objective-test"><a href="#4-4-3-Objective-test" class="headerlink" title="4.4.3 Objective test"></a>4.4.3 Objective test</h5><p>Motivation: 采用MCD评估学习到的频谱的质量，MCD越低越好。</p>
<h5 id="4-4-4-Subjective-test"><a href="#4-4-4-Subjective-test" class="headerlink" title="4.4.4 Subjective test"></a>4.4.4 Subjective test</h5><p>Motivation: 评估模型主管听测效果</p>
<p>评估方式：20个人，30句随机抽取的语音</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages</title>
    <url>/2021/06/21/mlms/</url>
    <content><![CDATA[<h1 id="Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017"><a href="#Paper-title-Uniform-Multilingual-Multi-Speaker-Acoustic-Model-for-Statistical-Parametric-Speech-Synthesis-of-Low-Resourced-Languages-——-Google-UK-Interspeech-2017" class="headerlink" title="Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)"></a>Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages —— Google UK (Interspeech 2017)</h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>这篇Google UK的论文，采用了一个MLMS的想法来解决低资源语种合成的问题，思想感觉上与采用IPA来统一音素特征是非常相似的，论文结构清晰，实验部分翔实充分，打分：🌟🌟🌟</p>
<h2 id="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK"><a href="#论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型-——-谷歌UK" class="headerlink" title="论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK"></a>论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 —— 谷歌UK</h2><h3 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h3><p>痛点：对于低资源语种来说，获取大量的训练数据是昂贵且困难的，通常是仅仅能获取到一小部分 / 或者没有数据集。</p>
<p>解决方案：本文提出了一种利用长短期循环神经网络的声学模型，目的是解决小语种语言数据缺失的问题。“说话人自适应”系统目的在于在多种语言间保持说话人的相似度，而本方法的突出特征是，模型构建成功后，系统不需要再重新训练以解决集外的语种，这是由于<strong>语言和说话人-不可知的建模方法和通用的语言特征集。</strong></p>
<p>实验结果：1）在12种语言上的实验结果显示，对于集外语种，系统仍能生成智能、自然的声音。2）当提供了少量训练数据的情况下，pooling the data有时能够提高整体的智能性和自然度。3）有时，构建一个zero-shot的多语种系统好于few-shot 单说话人单语种系统。</p>
<h3 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h3><p>近期发展：近些年，统计参数语音合成的方法，从HMM转向了神经网络系统，2013年 Heiga Zen 发布了第一个采用前向DNN网络的语音合成系统（Google，ICASSP 2013），且合成效果优于HMM系统。之后的LSTM-RNN模型提升了语音合成的效果，并且最近的PCM生成模型（WaveNet）近一步提升了模型效果。</p>
<p>挑战：1）语音数据的获取。当从先验收集到少量数据时，说话人-自适应方法可以被采用，这时候，需要将模型在新说话人的少量数据集上进行fine-tune。但是这种方法不能用于zero-shot的情境。2）获取多说话人的广泛数据集，并且构建一个平均音色模型。但是这个方法不能应用于缺少足够语言信息的语种上。</p>
<p>本文课题：对于指定的低资源语种数据，有最小的语言表示信息。</p>
<p>解决方法：一个多语种声学模型被训练，其中目标语种的数据集未包含在训练数据集集内。</p>
<p>本文贡献：一个通用的MLMS（multi-lingual multi-speaker）模型被训练，并且是采用语言和说话人-不可知的方法。</p>
<h3 id="Multilingual-Architecture-1-page"><a href="#Multilingual-Architecture-1-page" class="headerlink" title="Multilingual Architecture (1 page)"></a>Multilingual Architecture (1 page)</h3><p>本文的优势：1）一个具象的输入特征空间，不需要在新语种上fine-tune；2）一个类似于单说话人的简单模型架构。</p>
<h4 id="2-1-文本特征"><a href="#2-1-文本特征" class="headerlink" title="2.1 文本特征"></a>2.1 文本特征</h4><h5 id="2-1-1-典型语言表示"><a href="#2-1-1-典型语言表示" class="headerlink" title="2.1.1 典型语言表示"></a>2.1.1 典型语言表示</h5><p>训练数据集是包含多种语言和口音的。首先将多语种全部转换至IPA。尽管这个转换过程有一些困难，如1）需要专家知识来做相应的转换，2）不能直接的转换。但是这个IPA还是能够为语言空间提供具象的特征。</p>
<h5 id="2-1-2-系统发育语言特征"><a href="#2-1-2-系统发育语言特征" class="headerlink" title="2.1.2 系统发育语言特征"></a>2.1.2 系统发育语言特征</h5><p>基于BCP-47标注，我们采用语言和边界识别特征来建模同语种的不同口音。+ 一个系统语言分类树</p>
<h4 id="2-2-LSTM-RNN-声学模型"><a href="#2-2-LSTM-RNN-声学模型" class="headerlink" title="2.2 LSTM-RNN 声学模型"></a>2.2 LSTM-RNN 声学模型</h4><p>给定语言特征后，LSTM-RNN时长模型的作用是预测每个音素的发音时长。然后再将这个时长和语言特征一同输入到声学模型。以预测音频波形。音频波形的平滑性，是采用RNN的循环单元来建模的。</p>
<p>由于本文需要处理更大数量的数据集和更加多样的语言特征，所以本文的模型与baseline的区别在于ReLU的单元数量和LSTM的层数，以及声学模型输出层的循环单元的个数。</p>
<h3 id="Experiments-2-page"><a href="#Experiments-2-page" class="headerlink" title="Experiments (2 page)"></a>Experiments (2 page)</h3><p>用于训练声学模型的数据集语料有超过800小时的语音，包含了37种不同的语言种类。这些语言属于原始的59组语言/地区对，一些语种，如英语，有不同的说话人数据集，对应不同的地域口音。对于一些口音（如EN-US）有多个说话人。一些音频是在消声室（anechoic chambers）录制的，而一些就是常规的录音室录制的（a regular recording studio）</p>
<h4 id="3-1-方法论：系统细节"><a href="#3-1-方法论：系统细节" class="headerlink" title="3.1 方法论：系统细节"></a>3.1 方法论：系统细节</h4><p>语音数据采用22.05KHz的数据集，LSTM-RNN模型输出的特征是音素的发音时长</p>
<h4 id="3-2-模型参数和评估"><a href="#3-2-模型参数和评估" class="headerlink" title="3.2 模型参数和评估"></a>3.2 模型参数和评估</h4><p>实验被设计为两种情景：</p>
<ul>
<li>模型被在除去12种语言的语料上训练（其中有6种，是毫无语料的情况）。但每一种被排除的语种（除了其中2种）都有“亲戚”语种在训练数据集中。在对这些被排除在外的目标语种进行语音合成。其中的模型称为H</li>
<li>用所有语种的数据集来训练模型。其中的模型称为I</li>
</ul>
<p>因为声学模型可以被speaker和gender identifying特征控制，所以以下实验被设计来观察如何影响合成质量。</p>
<ul>
<li>speaker和gender特征 unset （default，D），set to the highest quality female speaker (EN-US, F), highest quality male speaker (EN-GB, M), speaker of the closet language (C).</li>
<li>Setting the speaker and gender features for this speaker (S)</li>
</ul>
<p>实验评估：100句集外话术，每个人最多听100句话。每句话有1min的评估时间。每一种语言有8个评分者。</p>
<h4 id="3-3-实验结果和讨论"><a href="#3-3-实验结果和讨论" class="headerlink" title="3.3 实验结果和讨论"></a>3.3 实验结果和讨论</h4>]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>考古两篇TTS with NN/DNN的开山之作</title>
    <url>/2021/06/21/1st-dnn-tts/</url>
    <content><![CDATA[<h1 id="Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013"><a href="#Paper-1-STATISTICAL-PARAMETRIC-SPEECH-SYNTHESIS-USING-DEEP-NEURAL-NETWORKS-——-Google-Heiga-Zen-ICASSP-2013" class="headerlink" title="Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)"></a>Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS —— Google, Heiga Zen (ICASSP 2013)</h1><h2 id="https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf"><a href="#https-storage-googleapis-com-pub-tools-public-publication-data-pdf-40837-pdf" class="headerlink" title="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf"></a><a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf</a></h2><h2 id="总结以及感想"><a href="#总结以及感想" class="headerlink" title="总结以及感想"></a>总结以及感想</h2><p>看了考古文对科研又产生了新的理解，目前的论文大多修修补补，灌水严重，实验部分不是很充分，无法印证论文的可复现性和实验结论的可靠性。这篇论文虽然采用的DNN技术还是最早期的神经网络系统架构，但是对于系统设计的每一个细小的结构都进行了充分的实验对比验证，得到了可靠的实验结论。在结论处也给同行留下了更多想象和探索的空间。强烈建议TTS从业者逐字逐行阅读本文，学习论文构思和写作的思想，并且了解深度学习在TTS领域应用的起源，至少从实验部分的objective /subjective evaluation可以学习到客观评估TTS合成效果的方法，使得自己的TTS研究更加扎实可靠。打分：🌟🌟🌟🌟🌟</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>痛点：传统的统计参数语音合成方式通常采用决策树，上下文依赖的HMM模型来表示给定文本的语音的概率密度。其中，语音参数是从概率密度中生成的来最大化它们的输出概率，然后再用生成的语音参数，重构语音波形。这种方法的缺点是，决策树对于建模复杂的上下文依赖关系是比较无效的。</p>
<p>解决方案：本文基于深度神经网络（DNN）。输入文本和声学表示的关系通过一个DNN来建模。DNN的使用能够解决许多传统方法的局限性。</p>
<p>实验结果：DNN效果优于HMM</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>基于HMM的参数方法在过去十年间盛行，相对于波形拼接算法来讲，它的优势在于能够灵活地替换音色，小的踪迹和鲁棒性。然而，它的主要局限性是合成语音的质量。Zen等提出了合成质量的三个主要特征：声码器，声学模型的准确度，和过拟合。本文的方法主要在于解决声学模型的准确度。</p>
<p>影响语音的一定数量的上下文特征包含音素、语言和语法特征，会在统计参数合成过程中，参与建模。典型系统包含50种上下文。因此，这些复杂的上下文依赖的有效建模是统计参数语音合成的关键点。为了解决这种上下文问题的标准方法是基于HMM的统计参数语音合成算法。对于每一种独立的上下文组合，都采用一个独立的HMM模型，作为一个上下文相关的HMM模型。通常来讲，对于这种全部上下文依赖的HMM模型来说，训练数据是不充分的，难以学习到一个稳定的模型，能够覆盖到所有所需的上下文组合。</p>
<p>为了解决这种问题，基于自上而下的决策树算法的上下文聚类被广泛使用。在这种方法中，上下文-依赖的HMM的状态被区分为多个“簇”，并且每个“簇”的分布参数是共享的。HMM模型的任务是通过二分类决策树，检验每一个HMM的上下文组合，其中一个上下文相关的二分类问题是与每一个 非叶子结点 相关的。“簇”的数量，也是 叶子结点 的数量，决定了模型的复杂程度。决策树通过序列化挑选能够在训练数据集上产生最高mle的分数来挑选问题。树的大小是通过一个预定义的mle阈值，一个模型复杂度惩罚，以及交叉验证来决定的。采用了上下文相关的问题和状态参数共享后，未知的上下文和数据稀疏性问题得到了有效解决。就像在语音识别中所成功解决的，基于HMM的方法自然地对于有丰富数据的上下文有较好的效果。</p>
<p>尽管基于上下文决策树的HMM模型在统计参数语音合成方法中是有效的。但是，有以下局限性：</p>
<ul>
<li>对于复杂上下文依赖如“XOR”，奇偶校验和复用问题，这种方法是无效的；</li>
<li>这种方法将输入的空间区分开，并且对于每个区域都采用了独立的参数，每个区域对应着一个决策树的叶子结点。这导致了分裂训练数据集，并且在聚类和估计分布的时候，使得每个簇的数据不充分。</li>
</ul>
<p>有一个相对大的决策树，并且分裂训练数据集都会导致过拟合，损害合成语音的质量。</p>
<p>为了解决上述局限性，本文采用基于DNN的结构。上述基于决策树的方案，建模了从 文本中抽取的语言上下文到语音参数的映射。在这里的决策树被一个DNN模型所替代。值得注意的是，自从90年代开始，NN就尝试被应用于TTS中。</p>
<h2 id="DNN-VS-Decision-tree-DT"><a href="#DNN-VS-Decision-tree-DT" class="headerlink" title="DNN VS Decision tree (DT)"></a>DNN VS Decision tree (DT)</h2><ul>
<li>DT 在表达输入特征的复杂关系时无效，如XOR、d 位奇偶校验函数、或者多路复用的问题。为了表达上述情境下的问题，决策树可能会十分巨大。然而，这些关系能够被DNN模型来具象表示</li>
<li>决策树致力于分割输入空间，对每个空间采用一组独立的参数和一个叶子结点。这样会导致在每个区域的数据数量少和较差的泛化性能。Yu et al 证明了在采用决策树建模时，一些较弱的输入特征如语音中词级别的重读会被丢失。由于DNN的权重是从整体的训练数据得到的，所以DNN会得到更好的泛化性能。DNN也提供了输入高维、多种输入特征的可能性。</li>
<li>相较于决策树，通过反向传播来训练一个DNN模型通常需要大量的计算过程。在预测过程中，DNN需要在每一层都有一个矩阵乘法，但是决策树仅仅需要通过一个输入特征的子集从根结点遍历树直至叶子结点。</li>
<li>决策树的推理是更加可解释的，DNN中的权重很难在直观上获得解释。</li>
</ul>
<h2 id="基于DNN的语音合成"><a href="#基于DNN的语音合成" class="headerlink" title="基于DNN的语音合成"></a>基于DNN的语音合成</h2><p>由于人类的发声系统是多层级的，才能够将文本信息转换为语音波形，所以本文尝试采用深度神经网络来进行语音建模。</p>
<p><img src="/images/DNN-based-tts.png" alt="DNN-based-tts"></p>
<p>上图展示了一个基于DNN的语音合成框架。输入文本首先被转换为输入特征序列$\{x^t_n\}$，其中的$x^t_n$表示在第$t$帧的第$n$维输入特征。输入的特征是对于文本上下文关系的二分类问题，包含如（e.g is-current-phoneme-aa?）和数值（e.g. 在短语中的单词数量，在当前音素序列的当前帧的相对位置，和当前音素的发音时长）</p>
<p>然后输入特征通过一个训练好的DNN，采用前向传播的方法被映射到输出特征$\{y^t_m\}$，其中$y^t_m$表示在第$t$帧的第$m$个输出特征。输出特征包含频谱和激励参数以及他们的时间导数（动态特征）。DNN的权重能够采用从训练数据集中抽取的成对的输入和输出特征来进行训练。类似于HMM的方法，这样是可以生成语音参数的。通过从DNN中设定预测的输出特征作为均值向量，再加上从所有训练数据预先计算的方差作为协方差矩阵，语音参数的生成算法能够生成平滑的语音参数特征轨迹，满足了静态和动态特征的统计情况。最终，一个语音合成模块通过得到的语音参数来生成语音波形。</p>
<p>注意到，文本分析，语音参数生成，和波形生成模块可以与HMM模型共享，<strong>即仅仅从上下文依赖关系的标签生成统计参数的过程需要被替换。</strong></p>
<h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="4-1-实验条件"><a href="#4-1-实验条件" class="headerlink" title="4.1 实验条件"></a>4.1 实验条件</h3><p>实验数据：US-EN 女性语音数据，约33000条。语音分析的条件和方法论类似于Nitech-HTS2005系统的方法。语音数据首先从48KHz降采样到16KHz，然后每5ms抽取一次40维的Mel倒谱系数，$log F_0$，和5段非周期性系数。每一个观察向量包含40维的mel倒谱系数，$log F_0$，和5段非周期性系数，以及他们的delta和delta-delta特征。从左至右，包含5个状态的无跳过隐藏半马尔可夫模型 (HSMM)被采用。为了建模 $log F_0$序列包含了声学和非声学观察序列。一个多空间的密度分布被使用（multi-space probability distribution (MSD)）。基于决策树的上下文聚类的问题数量是2554个。在HMM系统中的决策树大小是通过改变模型复杂度惩罚因子$\alpha$来控制的（最小描述长度标准（MDL）是（$\alpha=16,8,4,2,1,0.5,0.375,or 0.25$）。当$\alpha=1$时，Mel频谱，$log F_0$和频带非周期性的叶子结点的数量分别是12342, 26209, 和401（总共有3209991个参数）。</p>
<p>基于DNN系统的输入特征包含表征类别语言上下文（例如音素身份、重音标记）的342个二分类特征和表征数字语言上下文（例如，单词中的音节数、短语中当前音节的位置）的25个数值特征。除去文本上下文相关的输入特征，还包含了3个用于粗略编码当前音素序列中当前帧位置的数值特征，以及一个用于估计当前音节时长的数值特征。输出特征与HMM系统基本一致。为了通过DNN模型建模$log F_0$序列，我们采用了显式发声建模（explicit voicing modeling）的方法来获取连续$F_0$ ，发声/不发声的二分类特征值被用于添加到输出特征，并且在不发声值中的 $log F_0$ 通过插值得到。为了降低计算成本，80%的静音段从训练数据中移除。DNN的权重被随机初始化，然后在最小化MSE的目标函数下得到最优化。优化策略为基于小批次的随机梯度下降（SGD）的后向传播算法。DNN的输入和输出特征均被正则化，其中输入特征被正则化至(0,1)分布，然后输出特征根据训练数据中的最大最小值被正则化至0.01-0.99隐藏层采用sigmoid激活函数。建模频谱和激励特征参数的DNN神经网络被训练。</p>
<p>在评估的语句中，语音参数通过语音参数生成算法被生成。在倒谱域采用了基于后过滤的频谱增强算法。语音波形通过source-filter模型来重构语音波形。</p>
<p><strong>为了客观评估HMM和DNN模型系统，MCD（mel-cepstral distortion）(dB)，Linear aperiodicity distortion (dB), 发声/不发声错误率（%），和$log F_0$的RMSE被使用。</strong>音素发音时长在后面被使用，我们挑选了173句训练集外的语句用于模型评估。</p>
<h3 id="4-2-客观评估"><a href="#4-2-客观评估" class="headerlink" title="4.2 客观评估"></a>4.2 客观评估</h3><p><img src="/images/5th-mcep-comparison.png" alt="5th-mcep-comparison"></p>
<p>上图绘制了Ground-Truth、HMM预测值和DNN预测值的第五个mel倒谱系数，从图中可以观察到，三个模型都可以产生合理的语音参数轨迹。</p>
<p>在客观评估中，我们调查了预测结果和DNN结构（1，2，3，4，5层）的关系，以及与每层神经元个数（256，512，1024，2048）的关系，下图展示了实验结果。</p>
<p><img src="/images/dnn-tts-exp-res.png" alt="dnn-tts-exp-res"></p>
<p>基于DNN的系统一直都比HMM系统要好在 “voiced/unvoiced error rate”和”aperiodicity prediction”。在MCD中，有多层的DNN模型要相似于或者好于HMM模型。然而，在$log F_0$的预测中，HMM在大多数情况下要好于DNN模型。其中，所有的不发音帧都被插值作为发音帧来建模。我们认为这种方法会降低$log F_0$的预测效果，因为这些插值的$F_0$对于DNN模型来说是一个bias。对于MCD和aperiodicity预测中，模型深度的提升比在每一层上增加神经元的个数更加有效。</p>
<p>以上的客观指标并不能评估合成语音的自然度，但是可以作为评估声学模型准确率的指标。</p>
<h3 id="4-3-主观评估"><a href="#4-3-主观评估" class="headerlink" title="4.3 主观评估"></a>4.3 主观评估</h3><p>173句话被评估，每个评估人最多评估30句话，这些话术是随机打乱的。每一对语音被5个人评估。评估人有带耳机。在听完一对语音后，评估人需要选择一个更喜欢的语音，如果觉得两个语音很相似的话，可以选择“中立”，在这个评估过程中，HMM系统和DNN系统采用相似的模型数量来被评估。DNN模型采用了4个隐藏层，神经元的个数也进行了多组实验（256，512，1024个神经元）</p>
<p><img src="/images/dnn-tts-mos.png" alt="dnn-tts-mos"></p>
<p>上表展示了实验结果，可以从以上结果看出，在相似的参数数量配置的情况下，DNN模型要远优于HMM模型。我们认为较好的MCD代表了更佳的效果。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章examined the use of the DNNs to perform speech synthesis. DNN模型有潜力解决传统DT-HMM模型的局限性。主观评估和客观评估均显示DNN能够实现较好的效果。HMM的一个优势在于模型参数较少，计算开销较小。在合成时，HMM模型便利决策树来找到每一个状态的参数。然而，本文提出的DNN算法是在每一帧进行输入到输出的预测，接下来的工作可以在如何降低DNN模型的计算开销，添加更多的输入特征包括一些弱特征如重读，并且可以探索如果获得一个更好的 $log F_0$建模方案。</p>
<h1 id="Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper"><a href="#Paper-2-Speech-Synthesis-with-Neural-Networks-——-Motorola-Orhan-Karaali-1996-Sep-World-Congress-on-Neural-Networks-Invited-Paper" class="headerlink" title="Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)"></a>Paper 2: Speech Synthesis with Neural Networks —— Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)</h1><h2 id="https-arxiv-org-pdf-cs-9811031-pdf"><a href="#https-arxiv-org-pdf-cs-9811031-pdf" class="headerlink" title="https://arxiv.org/pdf/cs/9811031.pdf"></a><a href="https://arxiv.org/pdf/cs/9811031.pdf">https://arxiv.org/pdf/cs/9811031.pdf</a></h2><h2 id="感想与总结"><a href="#感想与总结" class="headerlink" title="感想与总结"></a>感想与总结</h2><p>这篇1996年的文章算是nn-tts的创世之作，文章采用了一个duration mode来预测音素的发音时长，以及一个phonetic network来预测每一个音素的声学特征，这个idea让我不由得想到当前的如Fastspeech等与这个想法如出一辙，同样的也是需要预测duration和音素的声学特征，但本文的行文思路尤其是实验部分，感觉没有上一篇论文更加翔实充分，所以打分的话我会给：🌟🌟🌟🌟</p>
<h2 id="Abstact"><a href="#Abstact" class="headerlink" title="Abstact"></a>Abstact</h2><p>传统的文本-语音转换通过拼接短的语音单元或者采用基于规则的系统来将语音的音素表示转换为声学表示形式，然后被转换为语音。本文描述了一种采用时延神经网络（time-delay neural network TDNN）的方案来进行音素-声学特征的建模，不需要额外的神经网络来控制生成语音的timing。这个神经网络系统相较于拼接算法可以降低对于系统存储资源的需求，对比于其他的商业系统表现良好。</p>
<h2 id="Introduction-1"><a href="#Introduction-1" class="headerlink" title="Introduction"></a>Introduction</h2><h3 id="1-1-Description-of-Problem"><a href="#1-1-Description-of-Problem" class="headerlink" title="1.1 Description of Problem"></a>1.1 Description of Problem</h3><p>文语转换通常包含了先将文本转换为语音参数，再将语音参数转换为语音波形。计算机交互可以采用对话沟通交互方式，也可以配置到移动端。拼接系统首先制作拼接数据库，然后再拼接时，调整音素的发音时长，平滑转接点来生成语音参数。拼接系统的主要问题是存储成本高昂。基于规则的合成方法将每一种可能的音素表示存储好目标声学参数。然后根据衔接点的情况来根据规则选择语音参数，主要问题是：拼接点不自然，因为转接规则倾向于只生成少量的转接风格。另外，大量的转接规则需要进行存储，会造成合成的机械音。</p>
<h3 id="1-2-Discussion-of-the-use-of-networks"><a href="#1-2-Discussion-of-the-use-of-networks" class="headerlink" title="1.2 Discussion of the use of networks"></a>1.2 Discussion of the use of networks</h3><ul>
<li><p>先前提到的两种方法都是语言-依赖的，而NN方法是语言-不相关的；</p>
</li>
<li><p>拼接系统的高昂存储成本导致了难以配置到移动端，而NN通过生成具象的表示方式，能够降低拼接系统的冗余性；</p>
</li>
</ul>
<p>下图展示了TTS的流程图（终于找到了现有的TTS流程设计的出处）</p>
<p><img src="/images/tts-diagram.png" alt="tts-diagram"></p>
<h2 id="系统描述"><a href="#系统描述" class="headerlink" title="系统描述"></a>系统描述</h2><h3 id="2-1-数据库"><a href="#2-1-数据库" class="headerlink" title="2.1 数据库"></a>2.1 数据库</h3><p>38岁男性，居住在Florida和Chicago。录制时采用了类似于近距离麦克风的DAT录制起。文本包含480个音素均衡的语句，是从Harvard sentence list中筛选出来的。除此之外，160句在其他情境下的话术也被录制了下来。录音音频通过数字方式转移到计算机，每句话对应一个音频。每句话被归一化，以使得每句话都在非静音段有相同的平均信号能量。文本信息被标记为音素、节奏和音调信息。</p>
<p>标记方式采用了类似于TIMIT数据库的标记方式（是ARPABET的变种），停止标记为关闭和释放作为单独的音素。这使得模型能够有效预测到停顿，以及开始。精准的对齐对于帧级的损失函数是有效的。</p>
<p>音素不是唯一的输入特征，音素时长，F0曲线（通常被音节重读和语法边界影响）。语法边界（syntactic labelling）标记了音节，词，短语，从句和句子的开始和结束时间。语法重读（lexical stress：primary, secondary, or none）被应用到词汇的每一个字母中。词性(function word (article, pronoun, conjunction or preposition) or content word)；每个词语都有一个层级(level)，基于生成F0的rule-based系统。</p>
<p>尽管语法（syntactic）和重读（lexical stress）对于语音的韵律变化很重要，但是这些信息没有完全决定了这些韵律变化。<strong>说话者对于语句的重读可能取决于句子中的对比度，比如在遇到陌生词汇的时候，可能会不自觉的重读。</strong>因此标记如此音调重读的实际位置到字幕上，或者词语间的强对比性是有效的。在英文中的标准是ToBI（Tone and Break Index）系统。</p>
<h3 id="2-2-从音素表示上生成片段时长"><a href="#2-2-从音素表示上生成片段时长" class="headerlink" title="2.2 从音素表示上生成片段时长"></a>2.2 从音素表示上生成片段时长</h3><p>神经网络的两个任务之一是去决策，从音素顺序和语法和韵律信息上，每一个音素的发音时长。</p>
<p>网络的输入大多数采用二分类数值，分类数值代表了采用1-out-of-n codes和一些通过bar codes表示的小的整数值。表示音素片段的输入数据包含音素片段，它的发音特点，字幕凸起的描述和包含片段的词语，以及片段接近的任何语法边界。网络结构被训练来生成时长的log。</p>
<p>时长预测网络结构如图2所示，网络有两个输入值（2和3），通过I/O block 1 和 2 输入。（Stream 2 包含了通过shift register来给一个音素提供上下文描述），stream 3 包含了仅仅用于一个特定音素时长的生成过程。当神经网络被用于生成时长的过程中，I/O block 6写入输出的数据流。在训练过程中，Block 6 读取目标值并且生成error value。Block 3、4和5是单层的神经网络模块，模块7、8和recurrent buffer控制了循环生成的机制。</p>
<p><img src="/images/duration-prediction.png" alt="duration-prediction"></p>
<h3 id="2-3-从音素和时长信息生成声学信息"><a href="#2-3-从音素和时长信息生成声学信息" class="headerlink" title="2.3 从音素和时长信息生成声学信息"></a>2.3 从音素和时长信息生成声学信息</h3><p>系统中使用的第二个神经网络从音素、语法和时长信息来生成语音参数信息。更精准地来说，网络从一个帧级的音素上下文信息生成语音10-ms帧的声学表示。</p>
<h4 id="2-3-1-网络输出-—-Coder"><a href="#2-3-1-网络输出-—-Coder" class="headerlink" title="2.3.1 网络输出 — Coder"></a>2.3.1 网络输出 — Coder</h4><p>神经网络不会直接生成语音，这个的计算资源十分昂贵，并且不太可能生成好的结果。该网络为声码器的分析-合成风格的合成部分生成数据帧。许多语音编码的研究致力于数据压缩的问题；然而，神经网络对于coder的需求没有被大多数的数据压缩技术所满足。具体来讲，将语音编码成每帧的数值向量是有价值的，这样的话，向量的每个元素对于每一帧都会有一个定义好的数值，因此用于训练的神经网络的错误度量是合适的。（例如，如果神经网络生成向量，并且错误度量相对于训练向量是较小的话，生成语音的质量，即通过running these vectors通过coder的合成部分的话，将得到较好的语音质量。）加权Euclidean距离被用作error criterion使得coder没有使用二分类输出值是明智的，并且根据其他的向量元素，向量元素的含义没有改变。</p>
<p>coder是LPC声码器的形式，采用线性频谱（line spectral frequencies）来表示filter coefficients和一个2-band的激励模型（不同的filter coefficients的表示形式被测试，模型对于线性频谱表现良好）。2-band激励模型是一个multi-band激励模型的变种，包含一个低频带的voiced band，和一个高频带的unvoiced band。两个bands之间的边界是coder之一的参数。F0 和 power of the voice signam是剩下的参数。F0在不发音的帧级，被插值为一个高频的数值。</p>
<h4 id="2-3-2-网络输入"><a href="#2-3-2-网络输入" class="headerlink" title="2.3.2 网络输入"></a>2.3.2 网络输入</h4><p>音素网络的输入包含了时长网络的所有输入，和时长网络输出的timing information。网络采用了一定数量的不同的输入coding技术。blocks 5，6，20和21采用了300毫秒的TDNN的风格输入窗口。窗口的采样不是均匀的，最优的采样区间是通过分析神经网络从TDNN窗口的不同部分来决策神经网络对信息的使用。Blocks 6 和20 处理了一组与输入音素相关的特征，Blocks 7和8为音素和语法边界编码时长和距离信息。网络的输入数据是二分类数据的混合，1-out-of-codes和bar codes</p>
<h4 id="2-3-3-网络结构"><a href="#2-3-3-网络结构" class="headerlink" title="2.3.3 网络结构"></a>2.3.3 网络结构</h4><p>决定好的网络结构需要大量的实验，也就需要大量的计算资源，然而本课题的复杂程度和数据集的大小使得训练时间成为了主要瓶颈。因此，一个 in house neural network simulator被开发来降低训练时间（多个月-&gt;几天），并且可以同时验证多个方法。一些神经网络的技术和理论通过这种方式被pass掉了。</p>
<p>最终的网络结构整合了TDNN、recurrent、和modular网络，和一些实验过程中演化的技巧。下图是当前方法的图示，其中六边形模块是I/O或者用户写入的子程序，方块是神经网络的模块。神经网络的模块采用后向传播来训练。网络的模块化是通过专家知识来手动调整的。</p>
<p>网络通过逐渐降低的学习率和momentum方法来训练，以一种新型的顺序和随机混合的训练模式来训练。训练的网络需要&lt;100 Kilobyters of 8-bit quantized weights ，对比于拼接算法，得到了显著降低。</p>
<p><img src="/images/phonetic-network.png" alt="phonetic-network"></p>
<h2 id="系统效果"><a href="#系统效果" class="headerlink" title="系统效果"></a>系统效果</h2><h3 id="3-1-语音质量和自然度"><a href="#3-1-语音质量和自然度" class="headerlink" title="3.1 语音质量和自然度"></a>3.1 语音质量和自然度</h3><p><img src="/images/tts-nn-exp-results.png" alt="tts-nn-exp-results"></p>
<p>上图展示了GT语音频谱和系统生成的语音频谱（生成的频谱没有采用ToBI标注系统）。为了对比更加清晰，有两种合成语音频谱被展示出来。第一种，phonetic 特征是网络预测的，而duration是真实的，为了仅仅展示phonetic network的效果。第二种，duration和phonetic都是预测的。对比实验发现，在语音接受度（Acceptability）上，本方法生成的质量远好于其他的系统。在片段的拟人度方面（Segmental Intelligibility），本方法仍旧有提升空间，而本次试验中的较差的数据可能是由于缺少单字语音样本所导致的。</p>
<p><img src="/images/tts-nn-table1.png" alt="tts-nn-table1"></p>
<h3 id="实时合成"><a href="#实时合成" class="headerlink" title="实时合成"></a>实时合成</h3><p>最开始模型是在Sun SPARCstation平台来通过ANSI C语言实现的。最近这个被插入到Power Macintosh 8500/120，PowerPC快速的乘法和加法使得合成器能够实时合成。</p>
<h2 id="结论-1"><a href="#结论-1" class="headerlink" title="结论"></a>结论</h2><p>本方法从Acceptability角度来看，是优于传统算法的，但是仍旧可以有一些提升，如数据库可以扩充，来获得更多的音调变化，包含更多的音素上下文特征，数据库可以包含更多的短语，单字，和长段的语句。在coder、network architecture、和训练方法上也可以做出一些提升。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages</title>
    <url>/2021/06/27/vqvae/</url>
    <content><![CDATA[<h1 id="低资源语种序列-序列语音合成无监督学习方法"><a href="#低资源语种序列-序列语音合成无监督学习方法" class="headerlink" title="低资源语种序列-序列语音合成无监督学习方法"></a>低资源语种序列-序列语音合成无监督学习方法</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>待解决问题：近些年，包含attention机制的序列-到-序列在TTS领域获得了广泛的成功。这些模型能够以一个大的标注的语料库来生成近似人的语音。</p>
<p>解决方案：然而，准备这样一个大的数据集是昂贵且耗费人力的，为了解决数据依赖的问题，我们提出了一种创新性的无监督预训练机制。具体来讲，首先，我们采用VQVAE模型来从大规模公开发表的，未标注的数据中抽取无监督语言单元。然后，我们采用&lt;无监督语言单元，语音&gt;对来预训练序列-序列TTS模型。最终，我们采用目标说话人的小数据量的<text, audio>数据，来fine-tune模型。</p>
<p>实验结果：主观和客观实验结果均显示，我们提出的方案可以采用相同数量的成对训练数据，合成更加智能化和自然的语音。除此之外，我们将我们提出的方案延伸到假设的低资源语言中，采用客观评估方法，验证模型的有效性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>序列到序列的TTS模型由一个编码器-解码器-attention的框架组成，能够生成自然语音。然而，训练这些S2S TTS模型需要成百上千的标注语音来生成近似人声的语音。尽管少量的数据需要被用来生成类人的语音，它限制了整体的自然度，并且模型容易造成不希望的错误。</p>
<p>尽管收集一个这样一个大型的标注语音语料库是昂贵的，耗费成本的，研究者开始调研TTS中数据的有效性。一些学者集中于采用少量数据集迁移TTS模型到新说话人。一些调研采用Speaker embeddings来在TTS中建模speaker identities。一些也探索使用一个speaker embeddings的组合并且fine-tune。一些人甚至致力于zero-shot 说话人自适应研究。</p>
<p>其他的一些学者探索通过通用数据来建模TTS模型。一些人尝试采用传统TTS的技术，将分布式的文本或语言信息引入TTS。一些人采用ASR数据或者通过数据筛选或分析找到的已有数据数据来训练TTS模型。最近，有学者提出了一个简单但是有效的半监督学习方法来仅仅采用语音预训练端到端TTS中的decoder。</p>
<p>目前有一些工作在研究低资源语种TTS的数据有效性，并且显示，训练一个多语种的统计参数语音合成方式，能够将adaption迁移到有小数据量的新语言。最近的工作调查显示从高资源的语种语言，迁移到低资源的语言也是有效的。</p>
<p>本文目的在于通过利用大量的，公开的，未标注的语音数据，来减轻S2S TTS训练中的数据需求量。我们提出了一个训练Tacotron2的无监督学习框架。具体来讲，我们首先通过VQ-VAE模型来从未标注语音中抽取无监督语言单元。然后通过使用&lt;无监督语言单元，语音&gt;对来预训练Tacotron。最终，我们采用目标说话人少量的<text, audio>文本语音对来fine-tune 模型。</p>
<p>我们的工作与[20]Y.-A Chung (ICASSP 2019) “Semi-supervised training for improving data efficiency in end-to-end speech synthesis”相关。然而，区别如下：</p>
<ul>
<li>我们的方法采用无监督学习方法抽取类似于音素的语言单元，使得有可能预训练整个TTS模型，然而[20]分开预训练模型的几个部分；</li>
<li>我们也在假设的低资源语言上，证实了方法的可行性；</li>
<li>最后，我们在实验中主要采用公开数据集，因此能够很容易被复现。</li>
</ul>
<h2 id="提出的方案"><a href="#提出的方案" class="headerlink" title="提出的方案"></a>提出的方案</h2><p>我们采用一个baseline Tacotron的模型架构，其中采用location-sensitive attention 和 从文本中生成的音素序列。为了将预测的频谱转换为语音，我们采用Griffin-Lim算法for fast experiment cycles，因为我们是关注于数据有效性的问题，而不是生成高保真的语音。在baseline模型中，模型是从0开始训练的，意外着模型的所有parameters全都是被paired data来训练的。</p>
<p><img src="/images/vqvae-algorithm.png" alt="vqvae-algorithm"></p>
<h3 id="2-1-半监督预训练"><a href="#2-1-半监督预训练" class="headerlink" title="2.1 半监督预训练"></a>2.1 半监督预训练</h3><p>在baseline Tacotron model中，模型应该同时学习到文本声学表示和他们之间的对齐。[20] 提出了两种模型预训练的方法来利用外部的文本和声学信息。对于文本表示来说，他们通过外部的word-vectors预训练了Tacotron的encoder，对于声学表示，他们通过未标注的语音，预训练了decoder。</p>
<p>[20]然后采用paired data来fine模型，在这一步，模型集中于学习textual representations和acoustic ones之间的对齐。</p>
<h3 id="2-2-无监督学习—-预训练"><a href="#2-2-无监督学习—-预训练" class="headerlink" title="2.2 无监督学习—-预训练"></a>2.2 无监督学习—-预训练</h3><p>尽管[13] 展示出提出的半监督预训练的方法能够合成更加智能的语音，但是它也发现同时分开训练编码器和解码器不会相较于仅仅预训练解码器带来更多提升。然而，仅预训练解码器和fine-tune整个模型有一个不匹配。为了避免这种不匹配带来的潜在损失，并且进一步通过仅仅使用语音来提高数据有效性，我们提出从未标注语音中抽取无监督语言单元来预训练整个模型。</p>
<p>我们提出的方法提供在了算法1中。整体的框架包含两个模型：一个无监督模型，用来抽取类似于音素的语言特征，和Tacotron模型。</p>
<h4 id="2-2-1-无监督语言单元"><a href="#2-2-1-无监督语言单元" class="headerlink" title="2.2.1 无监督语言单元"></a>2.2.1 无监督语言单元</h4><p>无监督语言表示在表示学习和特征解耦两个方面都带来了很大的提升。在它们之间，离散表示在语言和语音社区是较为流行的，因为直观上来看，语言和语音都是由有限的离散单元来组成的，例如文本中的字母和语音中的音素。本文利用VQ-VAE模型作为离散语言单元的抽取器。</p>
<p>在这种情况下，VQ-VAE模型作为一个类似于ASR的识别模型。然而，VQVAE模型和ASR模型的主要区别是VQVAE模型以一种无监督的方式来训练，然而ASR是采用一种有监督的方式。这种区别只要考虑到低资源语种的问题就会有所影响。当我们没有一个用于低资源语种的ASR模型时，这种提出的无监督方法对于提取低资源语种的语言表示单元是有意义的。</p>
<p>VQ-VAE模型有采用了一个encoder-decoder的框架，和一个码书（codebook dictionary）$e = C \times D $，其中$C$是字典中隐状态嵌入的数量，$D$是每一个嵌入的维度。编码器$E$输入原始语音波形 $x_{1:T}=x_1, x_2, …, x_T$作为输入，并且生成编码状态$z_{1:N}=E(x_{1:T})$，其中$N$依赖于文本时间长度$T$和编码器中下采样层的数量。然后，连续的隐状态表示$z_{1:N}$能够被映射到$\hat z_{1:N}$通过在字典中找到最近的预定义的离散嵌入$\hat z = e_k$，其中$k=argmin_j||z-e_j||$，$e_j$是在码书中的第$j$个嵌入，并且$j\in 1,2,…,C$。最终，隐嵌入$\hat z_{1:N}$和说话人嵌入$s$被一同输入到解码器$D$来重构语音波形$\hat x = D(\hat z, s)$。</p>
<p>因为模型输入和输出是相同的，模型能够以一种auto-encoder的方式来训练。然而，梯度不能够通过$argmin$计算来获取，因此直接采用梯度估计来近似。因此模型的整体loss为：</p>
<script type="math/tex; mode=display">
L = -log(x|\hat z(x), s) + ||sg(z(x))-e_j||^2_2 + \beta \ast ||z(x) - sg(e_j)||^2_2</script><p>其中第一项是negative log-likelihood用来更新整体模型的；第二项更新码书字典，其中<br>$sg$指代stop-gradient计算；第三项，指代承诺损失，鼓励编码器输出$z$来接近于码书嵌入，超参数$\beta$是用于给第三项增加一个权重。</p>
<p><img src="/images/vqvae-t2.png" alt="vqvae-t2"></p>
<h4 id="2-2-2-Tacotron预训练和fine-tune"><a href="#2-2-2-Tacotron预训练和fine-tune" class="headerlink" title="2.2.2 Tacotron预训练和fine-tune"></a>2.2.2 Tacotron预训练和fine-tune</h4><p>在VQVAE被训练后，我们抽取每句话的无监督语言单元。然后给无监督语言单元随机处理化一个嵌入表，通过查表得到的语言嵌入序列被用作Tacotron的输入。因此，我们能够通过<linguistic embedding, audio> 对来预训练Tacotron。</p>
<p>在模型被预训练后，我们采用一些paired语言数据来fine-tune模型。在这一步中，模型的输入是从文本中得到的音素序列。</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p>数据集：LJSpeech</p>
<p>VQ-VAE：与“Unsupervised speech representation learning using WaveNet autoencoders”相似的网络结构</p>
<p>当训练VQ-VAE的时候，我们采用39维MFCC作为模型输入。在我们调研学习后，码书的大小是256，并且每一个码书embedding的嵌入是64维。jitter rate 和$\beta$是0.12和0.25。</p>
<p>[20]发现<strong>24-分钟语音是刚好不能构建一个Tacotron系统的语音的最小最大值</strong>。因此，我们集中于对比不同的仅仅采用24分钟paired数据训练的模型。</p>
<h3 id="3-2-在24-min数据上的结果"><a href="#3-2-在24-min数据上的结果" class="headerlink" title="3.2 在24-min数据上的结果"></a>3.2 在24-min数据上的结果</h3><p>模型如下：</p>
<ul>
<li>Tac：仅仅采用LJSpeech训练的Tacotron</li>
<li>T-Dec: 以半监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-VQ: 以本文提出的学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-phone: 以监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune，意指模型的上限。</li>
</ul>
<p>外部数据集：VCTK</p>
<p>其中在预训练T-Dec和T-VQ的时候，仅仅采用语音数据，对于T-Phone，采用VCTK的文本语音对数据。</p>
<p>本文采用了客观和主观两种方式，来评估实验结果。对于客观方式，我们采用了Dynamic-time-warping Mel-cepstral Distortion(DTW MCD)，度量了合成语音和真实语音之间的距离，越小越好。我们采用了<strong>大约20分钟的unseen数据作为评估数据</strong>。对于主观评估方式，我们采用20条unseen utterances执行了一系列的AB tests。20 raters(10男10女)是地道的普通话中文者，英文熟练。</p>
<h4 id="3-2-1-MCD客观评估"><a href="#3-2-1-MCD客观评估" class="headerlink" title="3.2.1 MCD客观评估"></a>3.2.1 MCD客观评估</h4><p><img src="/images/vqvae-results.png" alt="vqvae-results"></p>
<p> MCD结果如上表1所示，如[20]中所描述的，仅仅预训练decoder能够降低MCD。然而，提出的方法实现了最好的效果，MCD相较于baselineTacotron低了14.3%。我们也发现了T-VQ的结果十分接近Upper bound（T-Phone）</p>
<h4 id="3-2-2-AB偏好主观测试"><a href="#3-2-2-AB偏好主观测试" class="headerlink" title="3.2.2 AB偏好主观测试"></a>3.2.2 AB偏好主观测试</h4><p>AB tests的结果如表2所示，可以清晰看出预训练的技巧能够帮助提升模型性能。在Tacotron和预训练模型（T-Dec / T-VQ）中有一个较大的表现差距。我们发现采用LJSpeech数据从0开始训练模型能够很难得到智能数据，部分原因是LJSpeech的数据集质量也不是足够高。</p>
<p>在T-Dec和T-VQ的AB Test中，T-VQ获取了更好的表现，从不正式的听测中，我们注意到T-Dec合成的语音在智能性上更加中庸，T-VQ的智能性会更好。这显示通过无监督语言单元和语音来进行预训练能够进一步提升模型性能。原因是在提出的预训练的配置中，模型不仅仅能够学习到声学表示，也能够学习到对齐信息。尽管无监督的语言单元没有在fine-tune中使用，提出的预训练的方法对于textual representation learning也是有效的，因为这些无监督的语言单元被证明很像音素。</p>
<p>在Tac-VQ和T-phone的对比中，大多数的raters没有选择，但其中还是有20%的人在二者之中选择T-Phone。</p>
<h3 id="3-3-在其他数量数据上的实验结果"><a href="#3-3-在其他数量数据上的实验结果" class="headerlink" title="3.3 在其他数量数据上的实验结果"></a>3.3 在其他数量数据上的实验结果</h3><p>实验结果表示：</p>
<ul>
<li>在使用24min数据时，Tacotron与其他三个模型有很大差异</li>
<li>随着数据量增大，差异缩小，证明了pre-training的作用在降低</li>
<li>T-VQ和T-Phone始终比Tac和T-Dec的方法效果要好</li>
</ul>
<h3 id="3-4-低资源语种的实验结果"><a href="#3-4-低资源语种的实验结果" class="headerlink" title="3.4 低资源语种的实验结果"></a>3.4 低资源语种的实验结果</h3><p>本节验证提出的方法在2种低资源语种的实验效果。假设English和Mandarin Chinese是两种低资源语种。主要为了解决以下两个问题：</p>
<ul>
<li>此种方法能否在这种情境下提升数据有效性？</li>
<li>那些预训练的语种对于提出的方法更有效？与目标语种音素相近的还是不相关的？</li>
</ul>
<p>目标语种，英语的语料是LJSpeech，中文是内部数据集Xiaomin，新闻风格，女性。</p>
<p>训练VQ-VAE和预训练Tacotron的语言包含以下几种：韩语，日语，西班牙语，法语，德语。我们仅仅利用语音数据来训练VQ-VAE和预训练Tacotron。在训练VQ-VAE的时候，仅做了一个改动：码书的大小从256改变到512，因为这里采用了多语种的数据集。三种模型衍生如下：</p>
<ul>
<li>Tac：通过LJSpeech或者Xiaomin训练的Tacotron；</li>
<li>T-VQ-A：以本文提出的学习方式，采用亚洲数据集（韩语、日语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
<li>T-VQ-E：以本文提出的学习方式，采用欧洲数据集（西班牙语，韩语，德语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
</ul>
<p><img src="/images/low-resource-languages.png" alt="low-resource-languages"></p>
<p>To alleviate the burden of raters，我们仅仅评估MCD值。如表3和表4所示。我们提出的预训练的方式，能够有效提升合成语音的质量，对于低资源语种来说很有价值，因为语音的收集成本十分高。</p>
<p>除此之外，在English TTS中T-VQ-E结果好于T-VQ-A，在普通话实验中，T-VQ-A slightly out-performs T-VQ-E。这个结果展示出了，采用音素相近的语言来进行模型预训练，是更加有效的。此外，我们发现了随着fine-tune数据的增多，MCD的下降，这个结果与上一节结果是相似的。最后，通过对比English TTS中最好的模型T-VQ-E模型，和上一节中的T-VQ模型，我们发现这里尽管使用音素相似的语言，仍然会存在一个不可忽视的gap。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>本文提出了一种在序列-序列TTS中，提升数据有效性的无监督学习方案。本方案首先从大规模未转译外部数据中，抽取文本和声学特征表示。然后在采用S2S模型来训练。具体来讲，采用了此种预训练的方式，Tacotron能够采用较少的数据，生成较好的语音。尽管我们是采用Tacotron来进行试验的，我们坚信我们的方法在其他的sequence-to-sequence模型中，也应该有效。我们也证实了此种方法在低资源语种上的有效性，这样的话，不需要目标语种的语音，我们的方法能够提供一个显著的效果提升。尽管，我们假设了两种低资源语种，但我们相信此种方法能够泛化到真实的低资源语种。这个结果给单语种和多语种TTS系统增加了曙光。</p>
<p>未来前进方向：可以尝试其他的无监督学习方式；采用小数据量来adaptation neural vocoders也同样需要被调研。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised speech representation learning using WaveNet autoencoders</title>
    <url>/2021/06/28/unsupervised-representation-learning/</url>
    <content><![CDATA[<h1 id="Unsupervised-speech-representation-learning-using-WaveNet-autoencoders-https-export-arxiv-org-pdf-1901-08810"><a href="#Unsupervised-speech-representation-learning-using-WaveNet-autoencoders-https-export-arxiv-org-pdf-1901-08810" class="headerlink" title="Unsupervised speech representation learning using WaveNet autoencoders https://export.arxiv.org/pdf/1901.08810"></a>Unsupervised speech representation learning using WaveNet autoencoders <a href="https://export.arxiv.org/pdf/1901.08810">https://export.arxiv.org/pdf/1901.08810</a></h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>第一次完整的精读完一篇期刊文章，一开始还是自己逐字句翻译的，但后面发现这样太耗费时间了（主要是突然解锁了typora的copy-paste功能），所以这篇文章从模型部分开始主要是Google翻译的。通篇来说论文Idea很容易理解，模型结构主要展示在了图1的模型结构里面。但是后面采用了6页的科学实验来验证这个想法，并且分析这个想法中的每一步细节。总体来讲，与MelGAN这种A类的长篇论文来对比的话，我更青睐于MelGAN的这种论文，是由于读到本文的一些实验细节，并且我尝试从图中去分析实验现象的时候，个人感觉实验现象都相对来说有一些牵强，尽管实验内容很详细，但是没有很好的控制变量，以得到有效的实验结果，并且很多实验结果有点晦涩难懂。（并且这篇文章采用了4个TPU训练了一周，没有资源还是不要轻易去re-train了，或许这个方向也像BERT一样，需要投入大量的资金和精力才能够实现宇宙第一的效果，所以还是建议轻易不要挑战）。所以本次打分：🌟🌟🌟</p>
<h2 id="Abastract"><a href="#Abastract" class="headerlink" title="Abastract"></a>Abastract</h2><p>现状：无监督的语音表示学习可以通过对语音进行自编码来实现。目标是从语音中抽取出高层级语义特征，例如，音素id，与语音信号低层级的混淆信息如音高轮廓或者背景噪声无关。</p>
<p>解决方案：因为learned representation 被tuned来仅仅包含音素内容，我们借助于一个高容量的WaveNet decoder来从之前的样本点中推理encoder丢失的信息。除此之外，自编码器模型的行为依赖于被应用到隐状态表示的约束条件。我们将三种variants进行对比：一个简单的dimensionality reduction瓶颈特征，一个高斯VAE，和一个离散的VQ-VAE。</p>
<p>实验分析：我们通过speaker independence，预测音素内容的能力，准确重构独立频谱帧的能力三个方面来评估learned representation的质量。除此之外，对于采用VQ-VAE模型提取的离散编码，我们度量从它们映射到phonemes的难度。我们引入了一个“正则化”的机制，来强迫representations集中于语句的音素内容的建模，并且报告出效果是与Zerospeech 2017 无监督声学单元发现任务的前几名能够匹敌的。</p>
<p>关键字：autoencoder，speech representation learning，unsupervised learning, acoustic unit discovery</p>
<h2 id="Introduction-1-page"><a href="#Introduction-1-page" class="headerlink" title="Introduction (1 page)"></a>Introduction (1 page)</h2><p>Deep learning被高层级表示学习算法所触发，如stacked Restricted Boltzman Machines和Denoising Autoencoders. 然而，近期在计算机视觉，机器翻译，语音识别和语言理解的突破，依赖于大规模的标记数据集，而几乎没有利用无监督表示学习。这样有两点缺陷：1）大规模人工标注的数据集的需求经常使得deep learning models的发展十分昂贵。2）尽管一个深度模型能够擅长于解决一个given task，但是这种方法对于问题域产生有限的启发，主要的启发是从显著输入特征的可视化中组成的，一种仅仅能够应用于问题域的策略，很容易被人类所解释。</p>
<p>本文集中于评估和提高无监督语音表示学习。具体来讲，我们集中于从音素内容中学习能够区分说话人特征（特别是说话人性别和id）的表示，这种特性类似于从语音识别器学习到的内部表示特征。这样的特征在多个任务中都有价值，例如低资源语音识别（当仅仅小数据量的标签训练数据被提供）。在这种情景下，有限的数据可能足够基于无监督发现的表征训练一个声学模型，但是对于训练一个声学模型或者以一种有监督的方式来学习数据表征是不充足的。</p>
<p>我们集中于将自编码器学习到的表征应用到原始语音和频谱特征，并且采用LibriSpeech来调研learned representations的质量。我们tune learned 隐状态表征来仅仅编码phonetic content并且移除其他的混淆特征。然而，为了做信号重构，我们采用了一个自回归的WaveNet模型作为解码器， 来推理被编码器拒绝掉的信息。这个解码器的作用是一个inductive bias，使得编码器无需使用它的capacity来表示低层级细节，而是允许编码器集中于高层级的语义特征。我们发现当使用ASR特征，如MFCC作为输入，原始语音作为decoder targets的时候，能够得到最好的representations. 这强迫系统学习生成在特征抽取的过程中，被移除的sample level的细节。此外，我们注意到VQ-VAE模型可以生成在声学内容和说话人信息之间最好的分割。我们调研VQ-VAE的可视化直观性，通过将它们映射到phonemes，展示了模型超参数在可视化上的影响，并且提出了一个新的正则化机制，能够提高latent representation -&gt; phonetic content 的质量。最终，我们展示了模型在ZeroSpeech 2017声学单元发现任务上的有效性，度量了当一句话的音素发生了最小改变的时候，learned representation的区分度。</p>
<h2 id="Representation-learning-with-neural-networks-1-5-page"><a href="#Representation-learning-with-neural-networks-1-5-page" class="headerlink" title="Representation learning with neural networks (1.5 page)"></a>Representation learning with neural networks (1.5 page)</h2><p>神经网络是高层级信息处理的模型，常用多层计算单元来进行实现。每一层可以被理解为是一个特征抽取器，它的输出被传递到上游单元。特别是在图像领域，神经网络学习到的特征可以被展示出一个可视化原子的层级结构，能够与可视化的脑皮层组织在一定程度上匹配。相似地，当应用到语音领域时，神经网络倾向于在音乐，语音的低层级上学习到听觉的频谱特征。</p>
<p>A. 有监督特征学习</p>
<p>神经网络能够采用有监督或者无监督的方式学习到数据表征。在有监督的情况下，从大规模数据集中学习的特征是能直接被使用的，除了data-poor tasks。例如，在视觉领域，ImageNet中发现的特征也会被用作其他计算机视觉任务的输入表示。相似的，语音社区采用从在音素预测任务的网络中抽取的瓶颈特征也可以作为ASR的特征表示。相似的，在NLP，可以从机器翻译或者语言推理任务训练的网络抽取通用文本特征。</p>
<p>B. 无监督特征学习</p>
<p>在这篇文章中，我们关注于无监督特征学习。因为没有训练标签，所以我们采用自编码器，i.e. 网络任务是重构她们的输入。自编码器采用一个encoding network来抽取隐状态表示，然后这个隐状态表示被输入到一个解码器网络来恢复原始数据。理想情况下，隐状态表示保留了原始数据的显著特征，是容易分析和解决的，e.g. 通过解耦数据中不同因子的变化，并且摒弃虚假特征（噪音）。这些渴望的性质典型是通过一个包含正则化技巧和contraints or bottlenecks的明智的应用。因此，自动编码器学习到的表示会受到两种相互竞争的力量的影响。一方面，它会提供给解码器以必要信息为了完美的重构，因此会在隐状态提取尽可能多的输入数据特征模式。另一方面，contraints限制了一些信息需要被摒弃，避免了latent representation颠倒到微不足道。因此，瓶颈特征对于强迫网络学习到一个非-微不足道的数据转换是必要的。</p>
<p>降低隐表示的维度是应用到隐向量的最基本的约束，autoencoder扮演作为一个非线性的variant of 线性低维数据映射，如PCA和SVD。然而，这样的表示是很难直观解释的，因为输入的重构完全依赖于所有的隐状态特征。对比来讲，字典学习的技巧(dictionary learning techniques)，例如sparse和non-negative decompositions，从一个大池子里使用了小数据量的挑选特征来表示每一种输入特征，能够帮助他们的可解释性。基于VQ的离散特征学习能够被视为sparseness的一种极端形式，其中，重构仅需使用字典中唯一的一个元素。</p>
<p>VAE提出了一种特征学习的不同的表示方式，follows 概率框架。自编码网络结构是从一个latent variable generative model衍生的。首先，一个latent vector $z$ 被从一个先验概率$p(z)$ 采样（典型情况下，是一个多维的正态分布）。然后，数据采样点$x$是采用一个深度解码器神经网络来生成的，神经网络的参数是$\theta$, $x \sim p(x|z;\theta)$。然而，在最大似然估计训练中，计算精准的后验概率分布$p(z|x)$是很困难的。取而代之的是，VAE给后验概率引入了一个变分估计，$q(z|x;\phi)$，这个变分估计是通过一个参数为$\phi$的编码器神经网络建模的。因此VAE模拟了一个传统的自编码器的过程，其中，其中，编码器生成隐状态的分布，而不是deterministic encodings，然而解码器是从这个分布生成的样本上进行训练。编码和解码网络是联合训练的，以最大化一个lower bound on the log-likelihood of data point x:</p>
<script type="math/tex; mode=display">
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - \beta D_{KL}(q(z|x;\phi)||p(z))</script><p>以上公式中的两个terms分别是autoencoder的重构损失，以及一个对隐状态应用了penalty term。特别地，KL散度表示了网络中的信息数量，即隐表示携带的有关数据样本的信息数量。因此它作为一个隐表示的一个信息瓶颈，其中$\beta$控制了重构质量和表示相似度之间的trade-off。</p>
<p>VAE目标函数的另一种表示方式明确的限制了隐表示的信息：</p>
<script type="math/tex; mode=display">
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - max(B, D_{KL}(q(z|x;\phi)||p(z)))</script><p>其中常数$B$是指$q$中自由信息的数量，因为模型仅仅会被惩罚，如果它比隐状态的先验损失传递了超过B的信息。</p>
<p>近期提出的一种VQ-VAE的方式，用确定的量化特征，取代了连续和随机的隐向量。VQ-VAE维护了一些原型向量$e_i, i=1,…,K$。在前向计算的过程中，编码器生成的representations被原型向量中最近的一个替代。Formally, 让$z_e(x)$是编码器在量化之前的输出。VQ-VAE通过 $q(x)=argmin_i||z_e(x)-e_i||^2_2$找到最近的原型，并且将其使用作隐状态表征$z_q(x)=e_{q(x)}$输入到解码器中。当模型应用到downstream tasks时，learned representation 能够被当作一个distributed representation处理（其中的每一个采样点都被一个连续vector表示），或者一个离散的representation（每个sample都被一个原型ID / token ID表示）。</p>
<p>在反向传播时，loss对于预先量化向量的梯度通过straight-through估计器被估计。</p>
<script type="math/tex; mode=display">
\frac{\partial \mathcal{L}}{\partial z_e(x)} \approx \frac {\partial \mathcal{L}}{\partial z_q(x)}</script><p>In TensorFlow, 可以通过如下公式来实现：</p>
<script type="math/tex; mode=display">
z_q(x)=z_e(x)+stop\_gradient(e_{q(x)} - z_e(x))</script><p>这些原型，是通过延伸learning objective with terms 来最优化quantization来训练的。原型被强迫与它们所替换的vectors近一些，被称为承诺损失(commitment loss)，用于鼓励编码器生成靠近原型的vectors。如果没有承诺损失，VQ-VAE 训练可以通过生成unbounded magnitude来发散。因此，<strong>VQ-VAE模型的loss包含如下三项，NLL(negative log-likelihood) of the reconstruction, (use the straight-through estimator来反向传播), 2个VQ-相关的loss, 2) prototype 与 assigned vectors 之间的距离，3）commitment cost</strong></p>
<script type="math/tex; mode=display">
\mathcal{L} = log p(x|z_q(x)) + ||sg(z_e(x)) -e_{q(x)} ||^2_2 + \gamma|| z_e(x) - sg(e_{q(x)})||^2_2</script><p>其中，$sg(\cdot)$是<em>stop gradient</em>,来在反向传播的时候，清空gradient。</p>
<p>VQ-VAE中的量化是一个information bottleneck。编码器可以被理解为一个概率模型，puts all probability mass on the selected discrete token (prototype ID). 假设K个tokens服从均匀先验分布(uniform prior distribution)，KL 散度是常数，并且等于$log K$。因此KL项不需要被包含在VQ-VAE训练标准中，取而代之的，成为了一个与prototype inventory大小相关的超参数。</p>
<p>VQ-VAE被定性地展示出能够学习到一个区分开文本内容和说话人id的representation。除此之外，discovered tokens能够在有限的设置下被映射到phonemes。</p>
<p>C. 序列数据的自编码</p>
<p>序列数据，如语音或文本，经常包含了能够被生成模型探索的局部依赖信息。事实上，序列数据的纯自回归模型，基于历史数据预测下一个观测点，是非常成功的。对于文本来说，很容易想到与n-gram模型相关的卷积神经语言模型。相似地，WaveNet是一个时域采样点的自回归模型。</p>
<p>这样的自回归模型的缺点是，它们不能够生成数据的隐空间表示。然而，可以将如此的生成模型与一个隐表示抽取器来结合使用。目前有两种解决方案，1是编码器能够处理整句话术，生成一个隐状态向量，然后输入到自回归解码器中，2是编码器能够周期性的生成隐状态特征来输入到解码器中。我们采用方案2。</p>
<p>训练这样的latent variable和自回归的混合模型容易造成隐空间坍塌，其中，解码器学习时，忽略受限隐表示，仅仅使用自回归的无约束信号。对于VAE来说，可以给KL项增加一个权重来阻止这种隐空间坍塌，并且使用free-information formulation。VQ-VAE 自然地对潜在崩溃具有弹性，因为 KL 项是一个超参数，它没有使用给定模型的梯度训练进行优化。</p>
<h2 id="Model-description-2-pages"><a href="#Model-description-2-pages" class="headerlink" title="Model description (2 pages)"></a>Model description (2 pages)</h2><p><img src="/images/VQVAE-model.png" alt="VQVAE-model"></p>
<p>模型结构如图1所示。编码器（为了keep the autoencoder viewpoint， encoder可以理解为一层固定的信号处理层）读取原始语音采样点序列，或者是语音特征, 抽取出一个隐向量序列(a sequence of hidden vectors)，被输入到一个bottleneck来成为一个隐表示序列(a sequence of latent representations)。latent vectors的哪一个frequency 被抽取是通过number of strided convolutions applied by the encoder来决定的。</p>
<p>解码器通过采用WaveNet来条件于encoder抽取的latent representation和一个speaker embedding来重构语音。解码器显式条件于speaker identity使得encoder不需要在latent representation中捕捉说话人信息。具体来说，<strong>解码器1) 输入encoder outputs，2）选择性地应用随机正则化到latent vectors. 3）使用卷积函数来将neighboring time steps抽取的latent vectors 整合， 4）上采样至目标采样率。</strong> 语音采样点是采用WaveNet来进行重构的，整合了所有的条件信息，包括：<strong>autoregressive information</strong> about past samples, <strong>global information about</strong> <strong>the speaker</strong>, <strong>latent information</strong> about past and future samples extracted by the encoder. 我们发现encoder’s bottleneck和提出的regularization在提取数据表示时是关键的。如果没有bottleneck, 该模型倾向于学习一种简单的重建策略，该策略可以逐字复制未来的样本。我们还注意到编码器与说话人无关，只需要语音数据，而解码器也需要说话人信息。</p>
<p>我们考虑了三种bottleneck：1）简单的降维，2）具有不同潜在表示维数和不同容量的高斯 VAE，3）具有不同数量量化原型的 VQ-VAE。所有bottleneck都可选地跟随下面描述的 dropout 启发的时间抖动正则化(time-jitter regularization)。此外，我们使用原始波形、对数梅尔滤波器组（log-mel filterbank）和梅尔频率倒谱系数 (MFCC) 特征对不同的输入和输出表示进行试验，这些特征丢弃了频谱图中存在的音高信息。</p>
<p>A. Time-jitter regularization</p>
<p>是一个类似于RNN中的Zoneout dropout的本文自主创新的正则化的方法，用于防止过拟合，详情不作描述。</p>
<h2 id="实验-6页"><a href="#实验-6页" class="headerlink" title="实验 (6页)"></a>实验 (6页)</h2><p>我们在两个数据集上评估模型：LibriSpeech(clean subset)和ZeroSpeech 2017 Contest Track 1 data. 两个数据集的共性是：多说话人，清晰，的read speech(sourced from audio books)以16 kHz录制。除此之外，ZeroSpeech Challenge控制了每个说话人的数量，主要的数据是被其中的几个说话人来讲述的。</p>
<p>第 IV-B 节中介绍的初始实验比较了不同的bottleneck变体，并确定模型在图 1 所示的四个不同探测点处生成的连续潜在表示中保留了来自输入音频的哪些类型的信息。使用在每个探测点计算的表示，我们测量几个预测任务的性能：音素预测（每帧准确度）、说话人身份和性别预测准确度，以及频谱图帧的 L2 重建误差。我们确定 VQ-VAE 学习了在语音内容和说话者身份之间具有最强解缠结的潜在表示，并在以下实验中关注该架构。</p>
<p>在第 IV-C 节中，我们通过将每个离散标记映射到小标记数据集 (LibriSpeech dev) 强制对齐中最常见的对应音素来分析 VQ-VAE 标记的可解释性，并报告单独集上映射的准确性 （LibriSpeech 测试）。 直观地说，这捕获了单个令牌的可解释性。</p>
<p>然后，我们将 VQ-VAE 应用于第 IV-D 节中的 ZeroSpeech 2017 声学单元发现任务 [20]。 此任务评估表示相对于语音类的判别力。 最后，在第 IV-E 节中，我们测量了不同超参数对性能的影响。</p>
<p>A. 默认的模型超参数</p>
<p>我们最好的模型使用 <strong>MFCC 作为编码器输入</strong>，但在<strong>解码器输出处重建原始波形</strong>。 我们使用每 10 毫秒（即以 100 Hz 的速率）提取的标准 13 个 MFCC 特征，并用它们的时间一阶和二阶导数进行扩充。 这些特征最初是为语音识别而设计的，并且大多对音频信号中的音调和类似的混淆细节是不变的。 编码器有 9 个层，每层使用 768 个单元和 ReLU 激活，组织成以下组：2 个预处理卷积层，过滤器长度为 3 和残差连接，1 个步幅卷积长度减少层，过滤器长度为 4，步幅为 2（对信号进行下采样 因子二），然后是 2 个长度为 3 的卷积层和残差连接，最后是 4 个具有残差连接的前馈 ReLU 层。 产生的潜在向量以 50 Hz（即每隔一帧）提取，每个潜在向量取决于 16 个输入帧的感受野。我们还使用了一个具有两个长度缩减层的替代编码器，它以 25 Hz 的频率提取潜在表示，接受场为 30 帧。</p>
<p>当未指定时，潜在表示为 64 维，适用时限制为 14 位。 此外，对于 VQ-VAE，我们使用推荐的 γ = 0.25 [19]。</p>
<p>解码器应用了随机时间抖动正则化（参见第 III-A 部分）。 在训练期间，每个潜在向量被替换为它的任何一个邻居，概率为 0.12。 抖动的潜在序列通过具有滤波器长度 3 和 128 个隐藏单元的单个卷积层，以混合相邻时间步长的信息。 然后对该表示进行 320 次上采样（以匹配 16kHz 音频采样率），并与表示当前说话者的单热向量连接以形成自回归 WaveNet 的调节输入。 WaveNet 由 20 个因果扩张卷积层组成，每个层使用 368 个带有残差连接的门控单元，组织成两个“循环”，每层 10 层，扩张率为 1,2,4,…,29。 调节信号分别传递到每一层。 使用跳过连接将来自 WaveNet 每一层的信号传递到输出。 最后，信号通过 2 个 ReLU 层，256 个单元。 应用 Softmax 来计算下一个样本概率。 我们在 mu-law 压扩后使用了 256 个量化级别。</p>
<p>所有模型都在从训练数据集中均匀采样的 64 个序列的小批量上训练，长度为 5120 个时域样本（320 毫秒）。 <strong>在 4 个 Google Cloud TPU（16 个芯片）上训练一个模型需要一周时间。</strong> 我们使用 Adam 优化器，初始学习率为 4 × 10−4，在 400k、600k 和 800k 步后减半。 Polyak 平均应用于所有用于模型评估的检查点。</p>
<p>B. Bottleneck comparison</p>
<p>我们在 LibriSpeech 上训练模型，并分析在自动编码器瓶颈周围的隐藏表示中捕获的信息，如图 1 所示：</p>
<ul>
<li>$p_{enc}$ (768 dim) encoder output prior to the bottleneck,</li>
<li>$p_{proj}$ (64 dim) within the bottleneck after projecting to lower dimension,</li>
<li>$p_{bn}$ (64 dim) bottleneck output, corresponding to the quantized representation in VQ-VAE, or a random sample from the variational posterior in VAE, and</li>
<li>$p_{cond}$ (128 dim) after passing $p_{bn}$ through a convolution layer which captures a larger receptive field over the latent encoding.</li>
</ul>
<p>在每个探测点，我们在四个任务中的每一个上训练具有 2048 个隐藏单元的单独 MLP 网络：对整个片段的说话人性别和身份进行分类（在整个信号中平均汇集潜在向量之后），预测每一帧的音素类（每个潜在向量都制作几个预测），并在每个帧中重建对数梅尔滤波器组特征（再次从每个潜在向量预测几个连续帧）。从信号中捕获高层级语义内容的表示，同时对令人讨厌的低级信号细节保持不变，将具有高音素预测精度和高频谱重建误差。解开的表示还应该具有较低的说话人预测精度，因为该信息明确地提供给解码器调节网络，因此不需要在latent encoding中保留。由于我们主要对发现构建的表示中存在哪些信息感兴趣，因此我们报告了训练性能并且不调整探测网络以进行泛化。</p>
<p>图 2 展示了使用具有不同超参数（latent dimensionality和bottleneck bitrate）的三个瓶颈中的每一个的模型比较，说明了信息通过网络传播的程度。此外，图 3 突出显示了使用不同配置获得的语音内容和说话者身份的分离。</p>
<p>图 2 显示，每种瓶颈类型始终会丢弃 $p_{enc}$ 和 $p_{bn}$ 探针位置之间的信息，这可以从每个任务的性能降低中得到证明。 瓶颈还会影响前面层的信息内容。 特别是对于简单地降低维数的 vanilla 自动编码器 (AE)，$p_{enc}$ 的说话人预测精度和滤波器组重建损失取决于瓶颈的宽度，更窄的宽度会导致更多的信息在编码器的较低层被丢弃。 同样，与维数和比特率匹配的 VQ-VAE 相比，VQ-VAE 和 AE 在 $p_{enc}$ 上产生了更好的滤波器组重建和说话人身份预测，这对应于 VQ-VAE 的令牌数量的对数，以及与先验的 KL 散度 VAE，我们通过设置允许的空闲位数来控制。</p>
<p><strong>正如预期的那样，AE 丢弃的信息最少。 在 $p_{cond}$ 上，表示仍然对说话者和音素具有高度预测性，并且其滤波器组重建是所有配置中最好的。 然而，从无监督学习的角度来看，AE 潜在表示不太有用，因为它混合了源信号的所有属性。</strong></p>
<p>相比之下，VQ-VAE 模型产生的表示可以高度预测信号的语音内容，同时有效地丢弃说话者身份和性别信息。 在更高的比特率下，音素预测与 AE 一样准确。 滤波器组重建也不太准确。 我们观察到说话者信息主要在 $p_{proj}$ 和 $p_{bn}$ 之间的量化步骤中被丢弃。 在 $p_{cond}$ 表示中组合几个潜在向量会产生更准确的音素预测，但额外的上下文无助于恢复说话者信息。 这种现象在图 3 中突出显示。请注意，VQ-VAE 模型对瓶颈维度的依赖性很小，因此我们以默认设置 64 呈现结果。</p>
<p>最后，VAE 模型比简单的降维更好地分离说话人和语音信息，但不如 VQ-VAE。 VAE 比 VQ-VAE 更一致地丢弃语音和说话者信息：在 $p_{bn}$ 处，VAE 的音素预测不太准确，而其性别预测更准确。 此外，在 $p_{cond}$ 上结合更广泛的感受野的信息并不能像 VQ-VAE 模型那样提高音素识别。 对瓶颈维度的敏感性（如图 2 所示）也令人惊讶，与较宽的 VAE 瓶颈相比，较窄的 VAE 瓶颈丢弃的信息更少。 这可能是由于 VAE 的随机操作：为了提供与低瓶颈维度相同的 KL 散度，需要在高维度添加更多噪声。 这种噪声可能会掩盖表示中存在的信息。</p>
<p><strong>基于这些结果，我们得出结论，VQ-VAE 瓶颈最适合于学习潜在表示，这些表示捕获语音内容同时对底层说话者身份保持不变。</strong></p>
<p>C. VQ-VAE token interpretability</p>
<p>到目前为止，我们已经使用 VQ-VAE 作为量化潜在向量的瓶颈。在本节中，我们寻求对离散原型 ID 的解释，评估 VQ-VAE 令牌是否可以映射到音素，即语音的潜在离散成分。示例令牌 ID 显示在图 4 的中间窗格中，我们可以看到令牌 11 始终与瞬态“T”音素相关联。为了评估其他标记是否有类似的解释，我们测量了逐帧音素识别准确度，其中每个标记被映射到 41 个音素中的一个。我们使用 460 小时干净的 LibriSpeech 训练集进行无监督训练，并使用来自干净的开发子集的标签将每个标记与最可能的音素相关联。我们通过在干净的测试集上以 100 Hz 的帧速率计算逐帧电话识别准确度来评估映射。使用来自 s5 LibriSpeech 配方 [55] 的 Kaldi tri6b 模型从强制对齐中获得真实音素边界。</p>
<p>表 I 显示了在 LibriSpeech 上获得 VQ-VAE 令牌到音素的最佳准确度的配置的性能。 在两个时间点给出识别精度：在 200k 梯度下降步骤之后，当可以评估模型的相对性能时，以及在模型收敛后 900k 步之后。 我们没有观察到训练时间较长的过度拟合。 为所有帧预测最常见的静音音素将准确度下限设置为 16%。 在完整的 460 小时训练集上有区别地训练以预测具有与 25 Hz 编码器相同架构的音素的模型实现了 80% 的逐帧音素识别准确率，而没有时间减少层的模型将上限设置为 88%。</p>
<p>表 I 表明映射精度随着标记数量的增加而提高，使用 32768 个标记的最佳模型达到 64.5% 的精度。 然而，最大的准确度增益出现在 4096 个令牌时，随着令牌数量的进一步增加，收益递减。 该结果与 Kaldi tri6b 模型中使用的 5760 个绑定三音素状态大致对应。</p>
<p>我们还注意到，增加令牌的数量并不会轻易提高准确性，因为我们衡量的是泛化，而不是集群纯度。 在为每个帧分配不同标记的限制下，由于对我们建立映射的小型开发集过度拟合，准确性会很差。 然而，在我们的实验中，我们始终观察到提高的准确性。</p>
<p>D. 无监督ZeroSpeech 2017声学单元发现任务</p>
<p>ZeroSpeech 2017 语音单元发现任务评估representation区分不同声音的能力，而不是将representation映射到预定义语音单元的难易程度。因此，它是对上一节中使用的音素分类准确度度量的补充。 ZeroSpeech 评估方案使用最小对 ABX 测试，该测试评估模型区分三个音素长语音段对的能力，这些语音仅在中间音素（例如“get”和“得到了”）。我们在提供的训练数据（英语 45 小时，法语 24 小时，普通话 2.5 小时）上训练模型，并使用官方评估脚本对测试数据进行评估。为了确保我们不会过拟合 ZeroSpeech 任务，我们只考虑了在LibriSpeech 上找到的最佳超参数设置（参见第 IV-E 部分）。此外，为了最大限度地遵守 ZeroSpeech 约定，我们对所有语言使用了相同的超参数，在表 II 中表示为 VQ-VAE（每语言、MFCC、$p_{cond}$）。</p>
<p>在带有足够大训练数据集的英语和法语上，尽管使用了独立于说话人的编码器，但我们取得的结果比顶级参赛者更好。</p>
<p>结果与我们对 VQ-VAE 瓶颈执行的信息分离的分析一致：在更具挑战性的跨说话者评估中，最佳性能使用 $p_{cond}$ 表示，它结合了瓶颈表示（VQ-VAE）的几个相邻帧 ,（表 II 中的每个 lang、MFCC、$p_{cond}$））。 比较说话人内部和说话人之间的结果同样与第 IV-B 部分中的观察结果一致。 在说话人内部的情况下，没有必要从语音内容中分离说话人身份，因此 $p_{proj}$ 和 $p_{bn}$ 探测点之间的量化会损害性能（尽管在英语中，通过考虑 $p_{cond}$ 的更广泛的上下文来纠正这一点）。 在跨说话人的情况下，量化提高了英语和法语的分数，因为丢弃混杂说话人信息的收益抵消了一些语音细节的损失。 此外，丢弃的语音信息可以通过在 $p_{cond}$ 处混合相邻的时间步长来恢复。</p>
<p><strong>VQ-VAE 在普通话上的表现更差</strong>，我们可以将其归因于三个主要原因。 首先，训练数据集仅包含 2.4 小时或语音，导致过度拟合（参见第 IV-E7 节）。 这可以通过多语言训练得到部分改善，如 VQ-VAE，（所有语言，MFCC，$p_{cond}$）。 <strong>其次，普通话是一种声调语言，而默认输入特征 (MFCC) 会丢弃音高信息。</strong> 我们注意到在 mel filterbank 特征（VQ-VAE，（所有 lang，$f_{bank}$，$p_{proj}$））上训练的多语言模型略有改进。 第三，VQ-VAE 被证明不会在潜在表示中编码韵律。 比较各个探测点的结果，我们发现普通话是唯一一种 VQ 瓶颈会丢弃信息并降低跨说话者测试制度中性能的语言。 尽管如此，多语言预量化特征产生的精度与相当。</p>
<p>我们不认为需要更多无监督训练数据是一个问题。 未标记的数据非常丰富。 我们认为，需要并且可以更好地利用大量未标记训练数据的更强大的模型比性能在小数据集上饱和的更简单的模型更可取。 然而，增加训练数据量是否有助于普通话 VQ-VAE 学会丢弃更少的音调信息还有待验证（多语言模型可能已经学会这样做以适应法语和英语）。</p>
<p>E. Hyperparameter impact</p>
<p>所有 VQ-VAE 自动编码器超参数都使用多组网格搜索 (grid-search) 在 LibriSpeech 任务上进行了调整，优化了最高的音素识别精度。 我们还在 ZeroSpeech 挑战任务的英语部分验证了这些设计选择。 事实上，我们发现所提出的时间抖动正则化提高了所有输入表示的 ZeroSpeech ABX 分数。 使用 MFCC 或滤波器组特征会产生比使用波形更好的分数，并且当使用更多令牌时，模型始终会获得更好的分数。</p>
<p>1) 时间抖动正则化：在表 III 中，我们分析了时间抖动正则化对 VQ-VAE 编码的有效性，并将其与两种 dropout 变体进行比较：常规 dropout 应用于编码的各个维度和 dropout 随机应用于整个 在各个时间步进行编码。 常规 dropout 不会强制模型在相邻的时间步长中分离信息。 Step-wise dropout 促进了跨时间步独立的编码，并且性能比时间抖动略差6。</p>
<p>所提出的时间抖动正则化大大提高了令牌映射的准确性，并扩展了性能良好的令牌帧速率范围，包括 50 Hz。 虽然 LibriSpeech 令牌精度在 25 Hz 和 50 Hz 下相当，但更高的令牌发射频率对于 ZeroSpeech AUD 任务很重要，50 Hz 模型在该任务上明显更好。 这种行为是由于 25 Hz 模型容易忽略短音（第 IV-E6 节），这会影响 ABX 在 ZeroSpeech 任务上的结果。</p>
<p>我们还分析了 VQ-VAE、VAE 和简单降维 AE 瓶颈的四个探测点的信息内容，如图 5 所示。对于所有瓶颈机制，正则化限制了滤波器组重建的质量并提高了音素识别精度 在约束表示中。 然而，在 $p_{cond}$ 探测点中组合了相邻的时间步之后，这种好处就变小了。 此外，对于 VQ-VAE 和 VAE，正则化会降低性别预测的准确性，并使表示对说话者的敏感度略低。</p>
<p>2) 输入表示：在这组实验中，我们使用不同的输入表示来比较性能：原始波形、log-mel 频谱图或 MFCC。 原始波形编码器使用 9 个跨步卷积层，这导致令牌提取频率为 30 Hz。 然后，我们用常规的 ASR 数据管道替换了波形：每 10 毫秒从 25 毫秒长的窗口中提取 80 个对数梅尔滤波器组特征，从梅尔滤波器组输出中提取 13 个 MFCC 特征，两者都增加了它们的一阶和二阶时间导数。 在编码器中使用两个跨步卷积层导致这些模型的令牌速率为 25 Hz。 </p>
<p>结果报告在表III的底部。 高级特征，尤其是 MFCC，比波形表现更好，因为按照设计，它们会丢弃有关音高的信息并提供一定程度的说话人不变性。 使用这种简化的表示迫使编码器向解码器传输更少的信息，作为对更多说话者不变潜在编码的归纳偏置。</p>
<p>3）输出表示：我们构建了一个自回归解码器网络，重建滤波器组特征而不是原始波形样本。 受文本到语音系统最近进展的启发，我们实现了一个类似 Tacotron 2 的解码器，在自回归信息流上有一个内置的信息瓶颈，这被发现在 TTS 应用程序中至关重要。 与 Tacotron 2 类似，滤波器组特征首先由一个小的“预网络”处理，我们应用了大量的 dropout 并将解码器配置为并行预测多达 4 帧。 然而，这些修改最多产生 42% 的音素识别准确率，明显低于本文中描述的其他架构。 然而，该模型的训练速度要快一个数量级。</p>
<p>最后，我们分析了解码 WaveNet 的大小对 VQ-VAE 提取的表示的影响。 我们发现整体感受野 (RF) 的影响大于 WaveNet 的深度或宽度。 特别是，当解码器的感受野跨越大约 10 毫秒时，潜在表示的属性会发生很大的变化。 如图 6 所示，对于较小的 RF，调节信号包含更多说话人信息：性别预测接近 80%，而逐帧音素预测准确度仅为 55%。 对于较大的 RF，性别预测准确率约为 60%，而音素预测的峰值接近 65%。 最后，虽然重建对数似然随着 WaveNet 深度提高到 30 层，但音素识别准确度稳定在 20 层。 由于 WaveNet 的计算成本最大，我们决定保留 20 层配置。</p>
<p>4) Decoder speaker conditioning：WaveNet 解码器基于三个信息源生成样本：先前发出的样本（通过自回归连接）、对说话者或其他时间固定的信息的全局调节以及提取的时变表示 从编码器。 我们发现禁用全局speaker调节会使音素分类准确度降低 3 个百分点。 这进一步证实了我们关于 VQ-VAE 瓶颈引起的解缠结的发现，这使模型偏向于丢弃以更明确形式提供的信息。 在我们的实验中，我们使用了独立于speaker的编码器。 但是，使编码器适应speaker可能会进一步改善结果。 事实上，展示了使用说话人自适应方法对 ZeroSpeech 任务的改进。</p>
<p>5) 编码器超参数：我们尝试调整编码器卷积层的数量、滤波器的数量和滤波器长度。 一般来说，使用更大的编码器会提高性能，但是我们确定必须仔细控制编码器的感受野，性能最好的编码器对于每个生成的令牌可以看到大约 0.3 秒的输入信号。</p>
<p>可以使用两种机制控制有效的感受野：通过仔细调整编码器架构，或通过设计具有宽感受野的编码器，但将训练期间看到的信号段的持续时间限制为所需的感受野。 通过这种方式，模型永远不会学会使用其全部容量。 当模型在 2.5s 长段上训练时，感受野为 0.3s 的编码器的帧音素识别准确率为 56.5%，而感受野为 0.8s 的编码器的得分仅为 54.3%。 当在 0.3 秒的片段上训练时，两个模型的表现相似。</p>
<p>6) 瓶颈比特率：语音 VQ-VAE 编码器可以看作是使用非常低的比特率对信号进行编码。 为了达到预定的目标比特率，可以控制令牌率（即通过控制编码器跨步卷积中的下采样程度）和每一步提取的令牌数（或等效的比特数）。 我们发现标记率是一个必须谨慎选择的关键参数，在 50 Hz（56.0% 音素识别准确率）和 25 Hz（56.3%）下获得 200k 训练步骤后获得最佳结果。 准确率在较高的令牌率（100 Hz 时为 49.3%）时突然下降，而较低的令牌率会错过非常短的电话（12.5 Hz 时为 53% 的准确率）。 </p>
<p>与令牌的数量相比，VQ-VAE 嵌入的维度对表示质量具有次要影响。 我们发现 64 是一个很好的设置，小得多的维度会降低具有少量标记的模型的性能，而更高的维度会对具有大量标记的模型的性能产生负面影响。</p>
<p>为完整起见，我们观察到即使对于具有最大令牌库存的模型，整体编码器比特率也很低：50 Hz 时为 14 位 = 700 bps，这与经典语音编解码器的最低比特率相当。</p>
<p>7) 训练语料库大小：我们在 LibriSpeech 训练集的子集上试验了训练模型，大小从 4.6 小时 (1%) 到 460 小时 (100%) 不等。 对 4.6 小时的数据进行训练，音素识别准确率在 100k 步时达到 50.5% 的峰值，然后下降。 9 小时的训练在 180k 集上达到了 52.5% 的峰值准确率。 当训练集的大小增加超过 23 小时时，音素识别率在大约 90 万步后达到 54%。 通过对完整 460 小时的数据进行训练，没有发现进一步的改进。 我们没有观察到任何过度拟合，并且为了获得最佳结果，训练模型直到达到 900k 步而没有提前停止。未来一个有趣的研究领域将是研究增加模型容量以更好地利用大量未标记数据的方法。</p>
<p>数据集大小的影响在 ZeroSpeech Challenge 结果（表二）中也可见：VQ-VAE 模型在英语（45 小时的训练数据）和法语（24 小时）上表现良好，但在普通话上表现不佳（ 2.5 小时）。 此外，在英语和法语上，我们使用在单语数据上训练的模型获得了最好的结果。 使用对所有语言的数据联合训练的模型在普通话上获得了稍好的结果。 </p>
<h2 id="Related-Work-1页"><a href="#Related-Work-1页" class="headerlink" title="Related Work (1页)"></a>Related Work (1页)</h2><p>序列数据的 VAE 在 [49] 中被引入。 该模型使用 LSTM 编码器和解码器，而潜在表示由编码器的最后一个隐藏状态形成。 该模型被证明对自然语言处理任务很有用。 然而，它也证明了潜在表示崩溃的问题：当一个强大的自回归解码器与潜在编码的惩罚同时使用时，比如 KL 先验，VAE 倾向于忽略先验并表现得好像它是一个 纯自回归序列模型。 这个问题可以通过改变 KL 项的权重来缓解，并通过使用 word dropout 限制自回归路径上的信息量 [49]。 在确定性自动编码器中也可以避免潜在崩溃，例如 [64]，它将卷积编码器耦合到强大的自回归 WaveNet 解码器 [18]，以学习由来自各种乐器的孤立音符组成的音乐音频的潜在表示。</p>
<p>我们凭经验验证，根据说话人信息调节解码器会导致编码更具有说话人不变性。[54] 给出了一个严格的证明，这种方法产生的表示对于明确提供的信息是不变的，并将其与域对抗训练相关联，这是另一种旨在对已知干扰因素实施不变性的技术 [65]。</p>
<p>当应用于音频时，VQ-VAE 使用 WaveNet 解码器从建模信息中释放潜在表示，这些信息很容易从最近的过去 [19] 中恢复。 它通过使用具有统一先验的离散潜在代码来避免后折叠问题，从而导致恒定的 KL 惩罚。 我们采用相同的策略来设计潜在表示正则化器：我们没有使用可能导致潜在空间崩溃的惩罚项来扩展成本函数，而是依靠潜在变量的随机副本来防止它们的共同适应并促进稳定性 时间。</p>
<p>本文中引入的随机时间抖动正则化受到数据的缓慢表示 [48] 和 dropout 的启发，dropout 在训练神经元期间随机删除以防止它们的协同适应 [50]。 它也与 Zoneout [51] 非常相似，后者依赖于所选神经元的随机时间副本来规范循环神经网络。</p>
<p>几位作者最近提议使用使用变量层次结构的 VAE 对序列进行建模。 [66] 探索了一个分层潜在空间，它将序列相关变量与序列无关变量分开。 他们的模型被证明可以执行说话人转换并在存在域不匹配的情况下提高自动语音识别 (ASR) 性能。 [67] 为序列数据引入了一个随机潜在变量模型，该模型还可以产生解开的表示，并允许在生成的序列之间进行内容交换。 这些其他方法可能会受益于规范潜在表示以实现进一步的信息解开。</p>
<p>声学单位发现系统旨在将声学信号转换成一系列类似于音素的可解释单位。 它们通常涉及声学帧、MFCC 或神经网络瓶颈特征的聚类，使用概率先验进行正则化。 DP-GMM [68] 在高斯混合模型上强加了狄利克雷过程先验。 使用 HMM 时间结构为子语音单元扩展它会导致 DP-HMM 和 HDP-HMM [69]、[70]、[71]。 HMM-VAE 建议使用深度神经网络代替 GMM [72]、[73]。 这些方法通过 HMM 时间平滑和时间建模来强制执行自上而下的约束。 语言单元发现模型在类似单词的级别检测重复出现的语音模式，找到具有约束动态时间扭曲的常见重复段 [74]。</p>
<p>在分段无监督语音识别框架中，神经自动编码器用于将可变长度的语音段嵌入到一个公共向量空间中，在那里它们可以被聚类为单词类型 [75]。 [76] 用一个模型代替分段自动编码器，该模型可以预测附近的语音片段，并证明该表示与词嵌入共享许多属性。 结合无监督的分词算法和在单独的语料库 [77] 上发现的词嵌入的无监督映射，该方法产生了一个在不成对的语音和文本数据上训练的 ASR 系统 [78]。</p>
<p>ZeroSpeech 2017 挑战赛的几个条目依赖于神经网络来发现语音单元。 [61] 在使用无监督术语发现系统 [79] 找到的语音段对上训练自动编码器。 [59]首先对语音帧进行聚类，然后训练神经网络来预测聚类 ID，并将其隐藏表示用作特征。 [60] 使用在 MFCC 上训练的自动编码器发现的特征扩展了该方案。</p>
<h2 id="Conclusion-半页"><a href="#Conclusion-半页" class="headerlink" title="Conclusion (半页)"></a>Conclusion (半页)</h2><p>我们将序列自动编码器应用于语音建模并比较了不同的<strong>信息瓶颈，包括 VAE 和 VQ-VAE</strong>。 我们使用可解释性标准以及区分相似语音的能力仔细评估了诱导的潜在表示。 <strong>瓶颈的比较表明，使用 VQ-VAE 获得的离散表示保留了最多的语音信息，同时也是最大的说话人不变性。</strong> 提取的表示允许将提取的符号准确映射到音素，并在 ZeroSpeech 2017 声学单元发现任务中获得有竞争力的表现。 Cho 等人的 VQ-VAE 编码器和 WaveNet 解码器的类似组合。 在 ZeroSpeech 2019 [80] 中具有最佳的声学单元发现性能。</p>
<p>我们确定模型需要一个信息瓶颈来学习将内容与说话者特征分开的表示。 此外，我们观察到，<strong>通过使瓶颈强度成为模型超参数，或者完全去除它（如在 VQ-VAE 中）</strong>，或者通过使用自由信息 VAE 目标，可以<strong>避免由太强的瓶颈引起的潜在崩溃问题 .</strong></p>
<p>为了进一步提高表示质量，我们引入了一种<strong>时间抖动正则化方案，该方案限制了潜在代码的容量，但不会导致潜在空间的崩溃。</strong> 我们希望这可以类似地提高与其他问题域中的自回归解码器一起使用的潜在变量模型的性能。</p>
<p>VAE 和 VQ-VAE 都限制了潜在表示(latent representation)的信息带宽(information bandwidth)。 然而，VQ-VAE 使用量化机制，它确定性地强制编码等于原型，而 VAE 通过注入噪声来限制信息量。 <strong>在我们的研究中，VQ-VAE 比 VAE 产生了更好的信息分离。</strong> 然而，需要进一步的实验来充分理解这种影响。 <strong>特别是，这是量化的结果还是确定性操作的结果？</strong></p>
<p>我们还观察到，虽然 VQ-VAE 产生离散表示，但为了获得最佳结果，它使用了一个如此大的标记集，以至于为每个标记分配一个单独的含义是不切实际的。 特别是，在我们的 ZeroSpeech 实验中，<strong>我们使用了每个令牌的密集嵌入表示，这提供了比简单地使用令牌标识更细微的令牌相似性度量。 也许需要更结构化的潜在表示，其中可以以连续方式调制一小组单元。</strong></p>
<p>广泛的超参数评估表明，优化编码器和解码器网络的感受野大小对于良好的模型性能很重要。 多尺度建模方法可以进一步分离韵律信息。 我们的自动编码方法还可以与更专门用于语音处理的惩罚相结合。 在 [73] 中引入 HMM 先验可以促进潜在表示，从而更好地模仿语音的时间语音结构。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
</search>
