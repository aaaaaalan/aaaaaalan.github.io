<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>玩过的n=23种运动</title>
    <url>/2021/06/16/21-sports/</url>
    <content><![CDATA[<p>目前玩过的23种运动如下，更多运动探索中……</p>
<ul>
<li>跳绳</li>
<li>跳皮筋</li>
<li>丢沙包</li>
<li>跳房子</li>
<li>蹦床</li>
<li>爬健身器材</li>
<li>花样轮滑</li>
<li>游龙板</li>
<li>短跑</li>
<li>跳远</li>
<li>游泳</li>
<li>跆拳道</li>
<li>太极拳</li>
<li>健美操</li>
<li>舞狮</li>
<li>龙舟</li>
<li>瑜伽</li>
<li>攀岩</li>
<li>滑雪</li>
<li>滑冰</li>
<li>射箭</li>
<li>乒乓球</li>
<li>滑板</li>
</ul>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>ai-class-A-best-papers</title>
    <url>/2021/07/08/ai-class-A-best-papers/</url>
    <content><![CDATA[<h1 id="ai领域a类会议最佳论文">AI领域A类会议最佳论文</h1>
<h2 id="list-of-papers">List of papers</h2>
<p><strong>AAAI 2021</strong></p>
<ul>
<li>Best Paper Awards</li>
</ul>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/2012.07436.pdf"><strong><em>Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting</em></strong></a></li>
</ol>
<p><strong>Institution(s):</strong> Beihang University, UC Berkeley, Rutgers University, Beijing Guowang Fuda Science &amp; Technology Development Company</p>
<p><strong>Authors:</strong> Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, Wancai Zhang</p>
<ol start="2" style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/2012.03083.pdf"><strong><em>Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory</em></strong></a></li>
</ol>
<p><strong>Institution(s):</strong> Singapore University of Technology and Design</p>
<p><strong>Authors:</strong> Stefanos Leonardos, Georgios Piliouras</p>
<ol start="3" style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/2012.03083.pdf"><strong><em>Exploration-Exploitation in Multi-Agent Learning: Catastrophe Theory Meets Game Theory</em></strong></a></li>
</ol>
<p><strong>Institution(s):</strong> Dartmouth College, University of Texas at Austin, Google AI</p>
<p><strong>Authors:</strong> Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, and Soroush Vosoughi</p>
<ul>
<li>Best Paper Runners Up</li>
</ul>
<ol style="list-style-type: decimal">
<li><strong><em><a href="https://arxiv.org/pdf/2009.12947.pdf">Learning From Extreme Bandit Feedback</a></em></strong></li>
</ol>
<p><strong>Institution(s):</strong> UC Berkeley, University of Texas at Austin</p>
<p><strong>Authors:</strong> Romain Lopez, Inderjit Dhillon, Michael I. Jordan</p>
<ol start="2" style="list-style-type: decimal">
<li><strong><em><a href="https://arxiv.org/pdf/2004.11207.pdf">Self-Attention Attribution: Interpreting Information Interactions Inside Transformer</a></em></strong></li>
</ol>
<p><strong>Institution(s):</strong> Beihang University, Microsoft Research</p>
<p><strong>Authors:</strong> Yaru Hao, Li Dong, Furu Wei, Ke Xu</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em><a href="https://arxiv.org/pdf/2009.06560.pdf">Dual-Mandate Patrols: Multi-Armed Bandits for Green Security</a></em></strong></li>
</ol>
<p><strong>Institution(s):</strong> Harvard University, Carnegie Mellon University</p>
<p><strong>Authors:</strong> Lily Xu, Elizabeth Bondi, Fei Fang, Andrew Perrault, Kai Wang, Milind Tambe</p>
<h2 id="aaai-2021---informer">AAAI 2021 - Informer</h2>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ICASSP 2020 Overview 思维导图</title>
    <url>/2021/06/16/ICASSP-2020-XMind-Overview/</url>
    <content><![CDATA[<div class="figure">
<img src="/images/ICASSP_2020.png" alt="ICASSP 2020 Overview" />
<p class="caption">ICASSP 2020 Overview</p>
</div>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ASRU 2019 语音合成相关论文</title>
    <url>/2021/06/17/asru-2019-papers/</url>
    <content><![CDATA[<table style="width:100%;">
<colgroup>
<col width="2%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="16%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>序号</strong></th>
<th><strong>论文题目</strong></th>
<th><strong>作者</strong></th>
<th><strong>单位</strong></th>
<th><strong>摘要</strong></th>
<th><strong>关键词</strong></th>
<th><strong>论文链接</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</td>
<td><em>Xuankai Chang</em>1,2<em>, Wangyou Zhang</em>2<em>, Yanmin Qian</em>2†<em>, Jonathan Le Roux</em>3<em>, Shinji Watanabe</em>1†</td>
<td>1Center for Language and Speech Processing, Johns Hopkins University, USA 2<strong>SpeechLab</strong>, Department of Computer Science and Engineering, <strong>Shanghai Jiao Tong University</strong>, China 3Mitsubishi Electric Research Laboratories (MERL), USA</td>
<td>MIMO-Speech, which extends the original seq2seq to deal with <strong>multi-channel input and multi-channel output</strong> so that it can <strong>fully model multi-channel multi-speaker speech separation and recognition</strong>. MIMO-Speech is a fully neural end-to- end framework, which is optimized only via an ASR criterion. It is comprised of: 1) a monaural masking network, 2) a multi-source neural beamformer, and 3) a multi-output speech recognition model.</td>
<td><strong>Overlapped speech recognition</strong>, end-to-end, neural beamforming, <strong>speech separation</strong>, curriculum learning.</td>
<td>https://arxiv.org/pdf/1910.06522.pdf</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td>IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</td>
<td><em>Fengyu Yang</em>1<em>, Shan Yang</em>1<em>, Pengcheng Zhu</em>2<em>, Pengju Yan</em>2<em>,</em> *<strong>Lei Xie*</strong>1∗</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University</strong>, Xian, China 2Tongdun AI Lab</td>
<td>We introduce a novel self-attention based encoder with learnable Gaussian bias in Tacotron. The proposed approach has the ability to generate stable and natural speech with minimum language-dependent front-end modules.</td>
<td>Tacotron, end-to-end, speech synthesis, <strong>self-attention, Gaussian bias</strong></td>
<td>http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td>LEARNING HIERARCHICAL REPRESENTATIONS FOR EXPRESSIVE SPEAKING STYLE IN END-TO-END SPEECH SYNTHESIS</td>
<td><em>Xiaochun An</em>1†<em>, Yuxuan Wang</em>2<em>, Shan Yang</em>1,2<em>, Zejun Ma</em>2<em>,</em> *<strong>Lei Xie*</strong>1⇤</td>
<td>1Shaanxi Provincial Key Laboratory of Speech and Image Information Processing, School of Computer Science, <strong>Northwestern Polytechnical University,</strong> Xi’an, China 2 ByteDance AI Lab</td>
<td>we introduce a hierarchical GST archi- tecture with residuals to Tacotron, which learns multiple-level disentangled representations to model and control different style granularities in synthesized speech.</td>
<td>Speaking style, disentangled representations, <strong>hierarchical GST,</strong> style transfer</td>
<td>http://lxie.npu-aslp.org/papers/2019ASRU_AXC.pdf</td>
</tr>
<tr class="even">
<td><strong>4</strong></td>
<td>BOOTSTRAPPING NON-PARALLEL VOICE CONVERSION FROM SPEAKER-ADAPTIVE TEXT-TO-SPEECH</td>
<td>*Hieu-Thi Luong<strong>1,2</strong>, Junichi Yamagishi**1,2,3*</td>
<td>1SOKENDAI (The Graduate University for Advanced Studies), Kanagawa, Japan 2National Institute of Informatics, Tokyo, Japan 3<strong>The University of Edinburgh</strong>, Edinburgh, UK</td>
<td>Bootstrap a VC system from a pretrained speaker-adaptive TTS model and unify the techniques as well as the interpretations of these two tasks. Our subjective evaluations show that the proposed framework is able to not only achieve competitive performance in the standard intra-language scenario but also adapt and convert using speech utterances in an unseen language.</td>
<td><strong>voice conversion,</strong> cross-lingual, speaker adaptation, transfer learning, text-to-speech</td>
<td>https://export.arxiv.org/pdf/1909.06532</td>
</tr>
<tr class="odd">
<td><strong>5</strong></td>
<td>WAVENET FACTORIZATION WITH SINGULAR VALUE DECOMPOSITION FOR VOICE</td>
<td><em>Hongqiang Du<strong>1,2</strong>, Xiaohai Tian<strong>2</strong>,</em> <strong><em>Lei Xie*</em>1*</strong>****, Haizhou Li******2***</td>
<td>1School of Computer Science, <strong>Northwestern Polytechnical University</strong>, xi’an, China 2Department of Electrical and Computer Engineering, National University of Singapore, Singapore <a href="mailto:hongqiang.du@u.nus.edu">hongqiang.du@u.nus.edu</a>, <a href="mailto:eletia@nus.edu.sg">eletia@nus.edu.sg</a>, <a href="mailto:lxie@nwpu.edu.cn">lxie@nwpu.edu.cn</a>, <a href="mailto:haizhou.li@nus.edu.sg">haizhou.li@nus.edu.sg</a></td>
<td>We propose to use singular value decomposition (SVD) to reduce WaveNet parame- ters while maintaining its output voice quality. Specifically, we apply SVD on dilated convolution layers, and impose semi-orthogonal constraint to improve the performance.</td>
<td>Voice Conversion (VC), <strong>WaveNet, Sin- gular Value Decomposition (SVD)</strong></td>
<td>http://lxie.nwpu-aslp.org/papers/2019ASRU_DHQ.pdf</td>
</tr>
</tbody>
</table>
<h1 id="paper-1-mimo-speech-end-to-end-multi-channel-multi-speaker-speech-recognition">Paper 1: MIMO-SPEECH: END-TO-END MULTI-CHANNEL MULTI-SPEAKER SPEECH RECOGNITION</h1>
<h2 id="mimo-speech端到端多通道多说话人语音识别asru-2019-best-paperhttpsarxiv.orgpdf1910.06522.pdf">MIMO-Speech：端到端多通道多说话人语音识别（ASRU 2019 Best paper）https://arxiv.org/pdf/1910.06522.pdf</h2>
<h2 id="感想">感想</h2>
<p>ASRU的best paper思路是挺清晰的，但是与其他会议发表的classic论文相比还是感觉有一些些差距，有一些模型的细节点来说，会感觉有些晦涩难懂，打分：🌟🌟🌟</p>
<h3 id="abstract">Abstract</h3>
<p>MIMO-Speech，采用“多通道-输入”和“多通道-输出”，以建模 多通道 多说话人 情景下的语音分离和语音识别。</p>
<ul>
<li>模型结构由三部分组成：</li>
</ul>
<p>1）单声道masking网络；</p>
<p>2）多源神经波束形成器；</p>
<p>3）多输出-语音识别模型；</p>
<ul>
<li><p>学习策略：a curriculum learning strategy</p></li>
<li><p>实验结果：60% WER reduction</p></li>
</ul>
<h3 id="introduction-1-page">Introduction (1 page)</h3>
<ul>
<li><p>待解决问题：鸡尾酒聚会课题，分为 单通道 / 多通道 语音识别问题</p></li>
<li><p>单通道多说话人语音分离：</p>
<p>1） Deep clustering (DPCL), 将时域单元映射到嵌入向量，再采用聚类算法将每个单元聚类到说话人源。此方法之后被嵌入到端到端训练框架中。</p>
<p>2）Permutation-invariant training (PIT)：用一个permutation-free目标函数来最小化重构损失。PIT之后被应用于多说话人ASR，在一个DNN-HMM混合ASR框架下。</p></li>
<li><p>多通道多说话人语音分离：</p>
<p>1）PIT，语音分离</p>
<p>2）unmixing transducer (a mask-based beamformer)，语音分离</p>
<p>3）DPCL：将inter-channel differences作为空间特征，和单通道频谱特征，语音分离</p></li>
<li><p>本文贡献：多通道-多说话人-语音识别，输入multi-channel input (MI), 输出 multiple output (MO) text sequences, one for each speaker, 所以称为 MIMO-Speech。</p></li>
<li><p>可行性：最近的单说话人-远场-语音识别展示了 “神经波束”技巧对于去噪的价值，并且一些研究证实了end-to-end的可行性。[27] 进一步证实了神经波束方法在多通道端到端系统能够增强信号。</p></li>
</ul>
<h3 id="mimo-speech-2-pages">MIMO-Speech (2 pages)</h3>
<div class="figure">
<img src="/images/MIMO-Speech.png" alt="MIMO-Speech" />
<p class="caption">MIMO-Speech</p>
</div>
<h4 id="model-architecture">Model architecture</h4>
<ul>
<li>Stage 1: a single-channel masking network，通过预测多说话人和各通道噪音masks来实现<strong>预分离</strong></li>
<li>Stage 2: 多源 “神经波束” 来<strong>空间上分离多说话人的源头</strong></li>
<li>Stage 3: 端到端ASR来实现<strong>多说话人语音识别</strong></li>
</ul>
<p>创新点：masking network + neural beamformer，单目标函数进行模型训练。</p>
<p>Stage1 (Monaural masking network) 可以预分离开噪音和多说话人音源；Stage 2 生成多个beamforming filters <span class="math inline">\(g^{i}(f)\)</span> 用来分离和降噪输入的多声道信号；Stage3有多个说话人的encoder，和一个attention decoder组成，来生成多个说话人的文本序列输出。</p>
<h4 id="data-scheduling-and-curriculum-learning">Data scheduling and curriculum learning</h4>
<ul>
<li><p>问题痛点：端到端训练难以收敛</p></li>
<li><p>解决方案<strong>（Data scheduling）</strong>：随机从以下两个数据集中选择一个batch</p></li>
</ul>
<p>1） 不仅采用多空间域的多说话人数据集</p>
<p>2） 也采用单说话人的数据集</p>
<ul>
<li>细节<strong>（Curriculum learning）</strong> 配Algorithm：</li>
</ul>
<p>1） 当选择到单说话人的数据集的时候，数据不经过 masking network 和 neural beamformer 模型，以加强end-to-end ASR模型的训练。</p>
<p>2） 计算出最大声和最小声说话人声音之间的信噪比SNR，然后按“升序”排列，从SNR=1的数据集开始训练</p>
<p>3） 将单说话人的数据集从短到长进行排序，让seq2seq模型首先学习短语句。</p>
<h3 id="experiments-3-pages">Experiments (3 pages)</h3>
<h4 id="configurations">3.1 Configurations</h4>
<h5 id="neural-beamformer">3.1.1 Neural Beamformer</h5>
<h5 id="encoder-decoder-network">3.1.2 Encoder-Decoder Network</h5>
<h4 id="performance-on-asr">3.2 Performance on ASR</h4>
<p>Motivation: 验证提出的模型好于baselines</p>
<p>Baselines / Ours</p>
<h4 id="performance-on-speech-seperation">3.3 Performance on Speech seperation</h4>
<p>Motivation: 验证 neural beamformer 学习了一个波束行为，能够用于语音分离。</p>
<h4 id="evaluation-on-spatialized-reverberant-data-在空间混响数据上的实验">3.4 Evaluation on spatialized reverberant data (在空间混响数据上的实验)</h4>
<p>Motivation: 验证在实际情况下的模型性能。</p>
<h1 id="paper-2-improving-mandarin-end-to-end-speech-synthesis-by-self-attention-and-learnable-gaussian-bias">Paper 2: IMPROVING MANDARIN END-TO-END SPEECH SYNTHESIS BY SELF-ATTENTION AND LEARNABLE GAUSSIAN BIAS</h1>
<h2 id="paper-2-通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统-httplxie.nwpu-aslp.orgpapers2019asru_yfy.pdf">Paper 2: 通过自注意力机制和学习高斯bias来提升中文普通话端到端语音合成系统 http://lxie.nwpu-aslp.org/papers/2019ASRU_YFY.pdf</h2>
<h2 id="感想-1">感想</h2>
<p>有些论文读起来会觉得高深莫测，但是有有部分价值可以吸取，打分：🌟🌟</p>
<h3 id="abstract-1">Abstract</h3>
<p>问题痛点：虽然对于英文来讲，现有的如Tacotron等模型已经能够实现端到端的语音合成过程，即从英文字母直接转换至语音。但是对于如中文这样的语言，仍旧需要繁复的前处理过程（如词边界、韵律边界等），使得这个文本处理前端的过程和传统方法一样复杂。</p>
<p>解决方案：为了保持生成语音的自然度、以及摒弃特定语言的特殊性，普通话语音合成过程中，我们引入了一个创新性的自注意力机制作为编码器，并且引入可学习的高斯bias到Tacotron中</p>
<p>实验结果：我们评估了不同的系统（在 有/无 韵律信息的情况下），结果显示提出的方法能够在最小的语言-依赖的前端模块的情况下，生成稳定和自然的语音。</p>
<h3 id="introduction-1页">Introduction (1页)</h3>
<ul>
<li>待解决问题</li>
</ul>
<p>传统的端到端方法包含复杂的特征提取过程，如：Part-of-speech tagging, pronunciation prediction, prosody labelling. 即便如Tacotron的端到端语音合成系统被提出，但是单纯的输入音素也无法使得语音合成模型得到良好的效果，所以学者提出嵌入PW、PPH、IPH，来进行韵律边界的特征建模。但是这使得违背了端到端语音合成的初衷，使得这个系统再次变得更加复杂。</p>
<ul>
<li>本文贡献</li>
</ul>
<p>1）<strong>全局韵律建模：</strong>由于self-attention被证明对于简单的音素序列进行全局建模时有良好的效果，所以本文尝试采用自-注意力机制作为编码器来获取全局的韵律信息。</p>
<p>2）<strong>局部韵律建模：</strong>至于局部的韵律信息，我们采用了一个可学习的高斯bias引入到自注意力机制中，因为Gaussian分布更加集中于当前位置的局部关系。</p>
<h3 id="proposed-sag-tacotron-1.5页">Proposed SAG-Tacotron (1.5页)</h3>
<h4 id="motivation">3.1 Motivation</h4>
<ul>
<li><p>目的：为了采用最少的文本分析模块，所以引入自-注意力学习机制，来进行全局依赖建模。</p></li>
<li><p>方案：1）将Encoder的CBHG模块，用自-注意力机制替换；2）可学习的高斯bias来提升局部建模。</p></li>
</ul>
<div class="figure">
<img src="/images/SAG-Tacotron.png" alt="SAG-Tacotron" />
<p class="caption">SAG-Tacotron</p>
</div>
<h4 id="基于-自注意力机制-的-编码器">3.2 基于 自注意力机制 的 编码器</h4>
<p>Encoder 的 Pre-net是一个3层-CNN + Batch Norm + ReLU，尽管自-注意力机制不包含序列信息，我们注入类似于Transformer的位置信息。 <span class="math display">\[
PE_{pos,2i}=sin(pos/10000^{2i/d})
\]</span></p>
<p><span class="math display">\[
PE_{pos, 2i+1} = cos(pos/10000^{2i/d})
\]</span></p>
<p>其中，<span class="math inline">\(pos\)</span>是当前位置，<span class="math inline">\(d\)</span>是特征维度，<span class="math inline">\(i\)</span>是当前维度。PE也被输入到自-注意力模块。自注意力模块包含了一个自注意力层+全连接层+tanh激活函数。残差连接被应用于上述层。</p>
<p>对于多头注意力机制的每一个<span class="math inline">\(head_i\)</span>, 对于一个有<span class="math inline">\(n\)</span>个元素的序列<span class="math inline">\(x\)</span>，我们想要获得有相同长度n的隐状态向量<span class="math inline">\(head_i\)</span>, 这里采用scale-product注意力机制。 <span class="math display">\[
Head_{i}=\sum_{j=1}^{n}ATT(Q,K)V
\]</span></p>
<p><span class="math display">\[
ATT(Q, K)=softmax(energy)​
\]</span></p>
<p><span class="math display">\[
energy = \frac {QK^{T}} {\sqrt{d}}
\]</span></p>
<p>最终的多头注意力机制为： <span class="math display">\[
MultiHead(Q,K,V)=Concat(head_{1}, ..., head_{h})W^{O}
\]</span> 其中的<span class="math inline">\(W^{O}\)</span>是最后一层线性层的参数矩阵。</p>
<h4 id="可学习的gaussian-bias">3.3 可学习的Gaussian bias</h4>
<p>在序列-序列模型中，对于中文来讲，相近的位置是十分重要的。在这种情况下，我们想要为注意力机制提升编码器对于临近状态的局部贡献。</p>
<div class="figure">
<img src="/images/Gaussian-bias.png" alt="Gaussian-bias" />
<p class="caption">Gaussian-bias</p>
</div>
<p>如上图所示，首先，假设一个以e5为中心的高斯bias，窗长为3（实际上，窗长是一个可学习的参数）。然后将注意力机制的分布通过高斯bias来进行正则化，以生成最终的分布。如图3所示，最终的分布是会在e5附近有更多的权重的。</p>
<p>具体来讲，Gaussian bias <span class="math inline">\(G\)</span>被mask到energy上，即 <span class="math display">\[
ATT(Q, K)=softmax(energy + G)
\]</span></p>
<p>其中<span class="math inline">\(G \in R^{N\times N}\)</span>，<span class="math inline">\(G \in (-1;0]\)</span> 度量了当前的query <span class="math inline">\(x_i\)</span>与position <span class="math inline">\(j\)</span> 之间的关系： <span class="math display">\[
G_{ij}=-\frac {(j - P_{i})^2}{2\sigma_i^2}
\]</span></p>
<p>其中的<span class="math inline">\(P_i\)</span>是<span class="math inline">\(x_i\)</span>的中心位置，当给定输入序列 <span class="math inline">\(x=(x_1, x_2, ..., x_n)\)</span>，<span class="math inline">\(\sigma_i\)</span>是标准差。如何选择合适的<span class="math inline">\(P_i\)</span>和<span class="math inline">\(\sigma_i\)</span>是关键。 <span class="math display">\[
P_i = N\cdot sigmoid(v_p^{T}tanh(W_{p}x_i))
\]</span></p>
<p><span class="math display">\[
D_i = N\cdot sigmoid(v_d^{T}tanh(W_{d}x_i))
\]</span></p>
<p>其中<span class="math inline">\(\sigma_i = \frac {D_i}{2}\)</span>, <span class="math inline">\(W_p\)</span> 和<span class="math inline">\(W_d\)</span>是模型参数矩阵。</p>
<h3 id="experiments-2.5页">Experiments (2.5页)</h3>
<h4 id="basic-setups">4.1 Basic setups</h4>
<h4 id="system-comparison">4.2 System comparison</h4>
<ul>
<li>Baseline: Tacotron1</li>
<li>Baseline-prosody: Tacotron1 with complex inputs</li>
<li>SAE-Tacotron: Self-attention as encoder without Gaussian bias with simply inputs</li>
<li>SAG-Tacotron: Self-attention as encoder with Gaussian bias with simple inputs</li>
<li>Transformer with simple inputs</li>
</ul>
<div class="figure">
<img src="/images/simple_inputs.png" alt="simple_inputs" />
<p class="caption">simple_inputs</p>
</div>
<div class="figure">
<img src="/images/complex_inputs.png" alt="complex_inputs" />
<p class="caption">complex_inputs</p>
</div>
<h4 id="model-details">4.3 Model details</h4>
<h4 id="results">4.4 Results</h4>
<h5 id="robustness-test">4.4.1 Robustness test</h5>
<p>Motivation: 评估attention对齐（Repeats / Skips）的鲁棒性</p>
<h5 id="prosody-analysis">4.4.2 Prosody analysis</h5>
<p>Motivation: 评估重读音节的pitch以及trajectory pattern of F0</p>
<h5 id="objective-test">4.4.3 Objective test</h5>
<p>Motivation: 采用MCD评估学习到的频谱的质量，MCD越低越好。</p>
<h5 id="subjective-test">4.4.4 Subjective test</h5>
<p>Motivation: 评估模型主管听测效果</p>
<p>评估方式：20个人，30句随机抽取的语音</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Awesome-bloggers</title>
    <url>/2021/07/08/awesome-bloggers/</url>
    <content><![CDATA[<h1 id="Awesome-bloggers"><a href="#Awesome-bloggers" class="headerlink" title="Awesome bloggers"></a>Awesome bloggers</h1><h2 id="TTS"><a href="#TTS" class="headerlink" title="TTS"></a>TTS</h2><p><a href="https://rayeren.github.io">https://rayeren.github.io</a>   Yi Ren (任意)   FastSpeech</p>
<p><a href="https://tan-xu.github.io">https://tan-xu.github.io</a> 谭旭 微软亚洲研究院机器学习组的主管研究员 Fastspeech 二作</p>
<p><a href="https://liusongxiang.github.io">https://liusongxiang.github.io</a> Songxiang Liu (刘颂湘)，PhD from Helen Meng 2021</p>
<p><a href="https://entn.at">https://entn.at</a> Ewald Enzinger, </p>
<p><a href="https://jaywalnut310.github.io/#/about">https://jaywalnut310.github.io/#/about</a> Jaehyeon Kim (KAIST), VITS(一作)  / Glow-TTS（一作） / Hifi-GAN（二作） / FloWaveNet, KaKao</p>
<p><a href="https://robin1001.github.io/">https://robin1001.github.io/</a> BInbin Zhang， WeNET 开发者，Xielei团队学生</p>
<p><a href="http://tonywangx.github.io/research.html，https://scholar.google.com/citations?hl=en&amp;user=uMZhUHcAAAAJ&amp;view_op=list_works&amp;alert_preview_top_rm=2&amp;sortby=pubdate，https://researchmap.jp/wangxin?lang=en。Xin">http://tonywangx.github.io/research.html，https://scholar.google.com/citations?hl=en&amp;user=uMZhUHcAAAAJ&amp;view_op=list_works&amp;alert_preview_top_rm=2&amp;sortby=pubdate，https://researchmap.jp/wangxin?lang=en。Xin</a> Wang，王鑫，NII Yamagishi Lab组 博士后</p>
<h2 id="AI"><a href="#AI" class="headerlink" title="AI"></a>AI</h2><p><a href="https://dczha.com">https://dczha.com</a>   Daochen Zha    DouZero</p>
<p><a href="https://www.alanshawn.com">https://www.alanshawn.com</a>   Alan Xiang （项子越） 中山大学勤奋博主  </p>
<p><a href="https://kexue.fm">https://kexue.fm</a> 苏剑林 93年</p>
<p><a href="http://blog.tsuai.cn">http://blog.tsuai.cn</a> 金天 神力算法开发者 </p>
<p><a href="https://dreamhomes.top/posts/202004170915/">https://dreamhomes.top/posts/202004170915/</a> 梦家 机器学习算法工程师</p>
<p><a href="https://www.tyleryep.com">https://www.tyleryep.com</a> Tyler Torchinfo作者</p>
<p><a href="https://sites.google.com/a/email.wm.edu/teddy-lfwu/home">https://sites.google.com/a/email.wm.edu/teddy-lfwu/home</a> Teddy Wu 京东硅谷研究院首席科学家 Graph4NLP 负责人</p>
<p><a href="https://sites.google.com/view/xiaojie-guo-personal-site">https://sites.google.com/view/xiaojie-guo-personal-site</a> Xiaojie Guo 京东硅谷研究院应用科学家 Graph4NLP 第二负责人</p>
<p><a href="http://nirvacana.com/thoughts/2013/07/08/becoming-a-data-scientist/">http://nirvacana.com/thoughts/2013/07/08/becoming-a-data-scientist/</a> Swami Chandrasekaran IBM CTO / Waston</p>
<p><a href="https://jermainewang.github.io">https://jermainewang.github.io</a> Minjie Wang Amazon AI Lab Shanghai DGL-GAT, MXNET作者</p>
<p><a href="https://www.guofei.site/pages/about.html">https://www.guofei.site/pages/about.html</a> 郭飞 阿里巴巴 算法/安全工程师</p>
<p><a href="https://www.mi1k7ea.com">https://www.mi1k7ea.com</a> 安全大佬</p>
<h2 id="Good-resources"><a href="#Good-resources" class="headerlink" title="Good resources"></a>Good resources</h2><p><a href="https://clemense.github.io">https://clemense.github.io</a>.  Clemens Eppner   Nvidia</p>
<h2 id="Acadamy’s-great-scholar-leaders"><a href="#Acadamy’s-great-scholar-leaders" class="headerlink" title="Acadamy’s great scholar leaders"></a>Acadamy’s great scholar leaders</h2><p><a href="https://dl.acm.org/profile/81350580267/publications?Role=author&amp;pageSize=20&amp;startPage=1">https://dl.acm.org/profile/81350580267/publications?Role=author&amp;pageSize=20&amp;startPage=1</a>    Tie-Yan Liu   Microsoft</p>
<h1 id="Awesome-Company’s-Bloggers"><a href="#Awesome-Company’s-Bloggers" class="headerlink" title="Awesome Company’s Bloggers"></a>Awesome Company’s Bloggers</h1><h2 id="TTS-1"><a href="#TTS-1" class="headerlink" title="TTS"></a>TTS</h2><p><a href="https://speechresearch.github.io">https://speechresearch.github.io</a></p>
<p><a href="http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/">http://www.kecl.ntt.co.jp/people/kameoka.hirokazu/Demos/</a></p>
<p><a href="https://google.github.io/tacotron/index.html">https://google.github.io/tacotron/index.html</a> Google Tacotron team</p>
<p><a href="https://thuhcsi.github.io">https://thuhcsi.github.io</a> 清华大学人机语音交互实验室，吴致勇教授团队</p>
<p><a href="http://lxie.npu-aslp.org/index.htm#Publications">http://lxie.npu-aslp.org/index.htm#Publications</a> 西北工业大学谢磊团队</p>
<p><a href="https://graphdeeplearning.github.io/post/benchmarking-gnns/">https://graphdeeplearning.github.io/post/benchmarking-gnns/</a>  NTU Graph Deep Learning Lab</p>
<p><a href="https://nii-yamagishilab.github.io/voicepersonae/downloads/">https://nii-yamagishilab.github.io/voicepersonae/downloads/</a> NII Yamagishi Lab VoicePersonae</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>BC challenge 2019 Top5队伍 技术分析</title>
    <url>/2020/03/09/bc2019top5/</url>
    <content><![CDATA[<table style="width:100%;">
<colgroup>
<col width="9%" />
<col width="18%" />
<col width="14%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>Frontend</strong></th>
<th><strong>Duration Modelling</strong></th>
<th><strong>Spectrogram modelling</strong></th>
<th><strong>Vocoder</strong></th>
<th><strong>Features</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>USTC-iflytek</strong> <strong>科大讯飞</strong></td>
<td>Tasks: special marks procession, polyphones classification, breaks prediction focuses prediction. Methodoly: Bidirectional Encoders Representations from Transformers (BERT)-based multi-task models</td>
<td>LSTM-RNN models autoregressive model structure,</td>
<td>A statistical parametric speech system (SPSS) GAN-based multi- task acoustic modeling Fundamental frequency (F0), 41 dimensional mel-cepstra (M- CEP), band aperiodicity (BAP) were adopted as the acoustic features</td>
<td>Wavenet The acoustic feature used was the joint feature vector of Mel-cepstrum, F0 and the u/v decision. Multi-speaker dataset for argumentation</td>
<td>Text-side: Manual annotations: Pinyin(with tone), PW, PP, and focus position Speech-side: Frame-level acoustic features:</td>
</tr>
<tr class="even">
<td><strong>DeepSound</strong> <strong>深声科技</strong></td>
<td>Tasks: text normalization, qingsheng, sandhi and erhua, : rule-based G2P: Bi-LSTM prosody prediction, PW, PPH, IPH: Bi-LSTM BiLSTM-based recurrent network (RNN) is used in the G2P module for polyphone and prosody prediction.</td>
<td>/</td>
<td>VQVAE. + a embedding+prenet oper- ation + GAN based postfiltering (robust on the unclean dataset )</td>
<td>robust multi-speaker neural vocoder conditioned on the mel spectrograms</td>
<td>manual and auto- matic tagging operations: phoneme, tone, prosody and pause duration</td>
</tr>
<tr class="odd">
<td><strong>腾讯</strong></td>
<td>Festival front-end to predict phoneme, tone and other linguistic features + BERT sentence embeddings are generated by a pre-trained Bert model.</td>
<td>/</td>
<td>A multi-speaker model is trained first.</td>
<td>multi-speaker model trained first. Wavenet</td>
<td>linguistic feature (The HTS full-context label) and sentence embedding mel spectrograms + channel embedding</td>
</tr>
<tr class="even">
<td><strong>灵伴</strong></td>
<td>text normalization, word segmentation, part-of-speech tagging, phonetic disambiguation word segmentation of the sentence, Part-of-Speeches (POS) of this word sequence and prosodic hierarchy</td>
<td>/</td>
<td>DNN-LSTM</td>
<td>Wavenet ground-truth mel-spectrograms plus F0</td>
<td>spectral envelope, fundamental frequency (F0), contextual labels (phone-related and word-related features)</td>
</tr>
<tr class="odd">
<td><strong>Horizon</strong> <strong>南京团队</strong></td>
<td>The corresponding texts were manually embedded into 476-dimensional vectors using our own text an- alyzing system. The embedded vectors consisted of one-hot encoded phonemes, tones, part-of-speech, prosodic boundaries and the position information. Prosody boundary: phoneme boundaries, syllable boundaries, phrase boundaries, secondary phrase boundaries</td>
<td></td>
<td>DCTTS[14] and Deep Voice 3[13]</td>
<td>WaveRNN</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>End-to-end TTS &amp; VC 文章总结</title>
    <url>/2021/07/20/end2end/</url>
    <content><![CDATA[<h1 id="end-to-end-tts-vc">End-to-end TTS &amp; VC</h1>
<h2 id="tts">TTS</h2>
<p><a href="https://arxiv.org/pdf/2006.03575.pdf">END-TO-END ADVERSARIAL TEXT-TO-SPEECH</a> -- EATS ~ ICLR 2021 ~ <strong>DeepMind</strong> ~ <a href="https://github.com/yanggeng1995/EATS">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.06103.pdf">Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</a> -- VITS ~ ICML 2021 ~ <strong>Kakao</strong> ~ <a href="https://github.com/jaywalnut310/vits">repo</a></p>
<p><a href="https://arxiv.org/pdf/2011.03568v1.pdf">WAVE-TACOTRON: SPECTROGRAM-FREE END-TO-END TEXT-TO-SPEECH SYNTHESIS</a> -- Wave-Tacotron ~ ICASSP 2021 ~ <strong>Google</strong></p>
<p><a href="https://arxiv.org/pdf/2106.09660.pdf">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</a> -- WaveGrad 2 ~ Interspeech 2021 ~ <strong>Google (<em>Heiga Zen</em>)</strong> ~ <a href="https://github.com/mindslab-ai/wavegrad2">repo</a></p>
<p><a href="https://arxiv.org/pdf/2012.03500.pdf">EfficientTTS: An Efficient and High-Quality Text-to-Speech Architecture</a> -- EFTS-Wav ~ ICML 2021 ~ <strong>PingAn (Chenfeng Miao)</strong> ~ <a href="https://github.com/liusongxiang/efficient_tts">repo</a></p>
<p><a href="https://arxiv.org/pdf/2006.04558.pdf">FASTSPEECH 2: FAST AND HIGH-QUALITY END-TO- END TEXT TO SPEECH</a> -- Fastspeech2 ~ ICLR 2021 ~ <strong>Zhejiang U &amp; Microsoft</strong> ~ <a href="https://github.com/ming024/FastSpeech2">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.02830.pdf">Reinforce-Aligner: Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</a> -- Reinforce-Aligner ~ Interspeech 2021 ~ Korea University</p>
<table>
<colgroup>
<col width="15%" />
<col width="16%" />
<col width="17%" />
<col width="27%" />
<col width="12%" />
<col width="10%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Model</strong></th>
<th><strong>Institution</strong></th>
<th><strong>Published conference</strong></th>
<th><strong>Method</strong></th>
<th><strong>Principal</strong></th>
<th><strong>Field</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>EATS</strong></td>
<td>DeepMind</td>
<td>ICLR 2021</td>
<td>Aligner + GAN-TTS decoder</td>
<td>Adversial network</td>
<td>GAN</td>
</tr>
<tr class="even">
<td><strong>VITS</strong></td>
<td>Kakao</td>
<td>ICML 2021</td>
<td>Glow-TTS + Hifi-GAN</td>
<td>GAN + Flow + VAE</td>
<td>GAN, Flow</td>
</tr>
<tr class="odd">
<td><strong>Wave-Tacotron</strong></td>
<td>Google</td>
<td>ICASSP 2021</td>
<td>Tacotron + Flow</td>
<td>Seq2seq + Flow</td>
<td>Seq2seq, Flow</td>
</tr>
<tr class="even">
<td><strong>WaveGrad2</strong></td>
<td>Google (Heiga Zen)</td>
<td>Interspeech 2021</td>
<td>Tacotron encoder + WaveGrad decoder</td>
<td>Diffusion</td>
<td>New</td>
</tr>
<tr class="odd">
<td><strong>EFTS-Wav</strong></td>
<td>PingAn (Chenfeng Miao)</td>
<td>ICML 2021</td>
<td>IMV aligner + Melgan</td>
<td>IMV</td>
<td>Non-AR</td>
</tr>
<tr class="even">
<td><strong>Fastspeech 2</strong></td>
<td>Zhejiang U &amp; Microsoft</td>
<td>ICLR 2021</td>
<td>Duration predictor + Waveform decoder</td>
<td>CNN wav decoder</td>
<td>Non-AR</td>
</tr>
<tr class="odd">
<td><strong>Reinforce-Aligner</strong></td>
<td>Korea U</td>
<td>Interspeech 2021</td>
<td>RL aligner + MRF</td>
<td>RL</td>
<td>RL</td>
</tr>
</tbody>
</table>
<h2 id="vc">VC</h2>
<p><a href="https://arxiv.org/pdf/1904.04169.pdf">Parrotron: An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation</a> -- Parrotron ~ Interspeech 2019 ~ <strong>Google</strong> ~ <a href="https://github.com/fd873630/Parrotron">repo</a></p>
<p><a href="https://export.arxiv.org/pdf/2104.07283">Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</a> -- F0-VC ~ WIP ~ <strong>Sorbonne</strong></p>
<p><a href="https://arxiv.org/pdf/2106.00992.pdf">NVC-Net: End-to-End Adversarial Voice Conversion</a> -- NVC-Net ~ UR ~ <strong>Sony</strong></p>
<p><a href="https://arxiv.org/pdf/2010.14150.pdf">FRAGMENTVC: ANY-TO-ANY VOICE CONVERSION BY END-TO-END EXTRACTING AND FUSING FINE-GRAINED VOICE FRAGMENTS WITH ATTENTION</a> -- FRAGMENTVC ~ ICASSP 2021 ~ Audio2Mel ~ <strong>NTU (<em>Hung-yi Lee</em>)</strong> ~ <a href="https://github.com/yistLin/FragmentVC">repo</a></p>
<p><a href="https://arxiv.org/pdf/2002.03808.pdf">Vocoder-free End-to-End Voice Conversion with Transformer Network</a> -- Transformer-VC ~ WIP ~ Raw_spectrum - to - raw_spectrum ~ <strong>KNU</strong></p>
<p><a href="https://arxiv.org/pdf/2104.02901v2.pdf">S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</a> S2VC ~ Interspeech 2021 ~ self-supervised ~ <strong>NTU (<em>Hung-yi Lee</em>)</strong> ~ <a href="https://github.com/howard1337/S2VC">repo</a></p>
<p><a href="https://arxiv.org/pdf/1906.00794.pdf">Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion</a> ~ flow-based end2end VC ~ NeurIPS2019 ~ Telefónica Research ~ <a href="https://github.com/joansj/blow">repo</a></p>
<h2 id="metric">METRIC</h2>
<p><a href="https://arxiv.org/pdf/2107.09392.pdf">SVSNet: An End-to-end Speaker Voice Similarity Assessment Model</a> -- SVSNet ~ Similarity</p>
<p><a href="https://arxiv.org/pdf/2104.03017v2.pdf">Utilizing Self-supervised Representations for MOS Prediction</a> -- MOS ~ NTU (<em>Hung-yi Lee</em>) ~ <a href="https://github.com/s3prl/s3prl">repo</a></p>
<p><a href="https://arxiv.org/pdf/1904.08352v3.pdf">MOSNet: Deep Learning-based Objective Assessment for Voice Conversion</a> -- MOSNet ~ IISAS ~ <a href="https://github.com/lochenchou/MOSNet">repo</a></p>
<p><a href="https://arxiv.org/pdf/2104.11673v1.pdf">Deep Learning Based Assessment of Synthetic Speech Naturalness</a> -- MOS ~ QUTTUB ~ <a href="https://github.com/gabrielmittag/NISQA">repo</a></p>
<p><a href="https://arxiv.org/pdf/2103.00110.pdf">MBNet: MOS Prediction for Synthesized Speech with Mean-Bias Network</a> -- MOS ~ ICASSP 2021 ~ USTC &amp; Microsoft (Xu Tan) ~ <a href="https://github.com/sky1456723/Pytorch-MBNet">repo</a></p>
<h2 id="others">Others</h2>
<p><a href="https://arxiv.org/pdf/2102.04040.pdf">LightSpeech: Lightweight and Fast Text to Speech with Neural Architecture Search</a> -- LightSpeech ~ ICASSP 2021 ~ USTC &amp; Microsoft (Xu Tan) ~ <a href="https://github.com/rishikksh20/LightSpeech">repo</a></p>
<h2 id="inspirations-from-asr">Inspirations from ASR</h2>
<p><a href="https://arxiv.org/pdf/2107.09428.pdf">Streaming End-to-End ASR based on Blockwise Non-Autoregressive Models</a> -- End2end ASR ~ Interspeech 2021 ~ Hopkins &amp; Yahoo Japan &amp; CMU</p>
<p><a href="https://arxiv.org/pdf/2105.10042.pdf">A Streaming End-to-End Framework For Spoken Language Understanding</a> -- StreamSLU ~ IJCAI 2021 ~ University of Waterloo &amp; Huawei Noah’s Ark Lab &amp; Tsinghua University</p>
<p><strong>Tips:</strong></p>
<ul>
<li><p>UR: Under Review</p></li>
<li><p>WIP: Work in Progress</p></li>
<li><p>KAIST：Korea Advanced Institute of Science and Technology，韩国科学技术院</p></li>
<li><p>KNU：Kyungpook National University，韩国庆北大学</p></li>
<li><p>IISAS：Institute of Information Science, Academia Sinica, Taipei, Taiwan，台湾中央研究院信息科学研究所</p></li>
<li><p>QUTTUB：Quality and Usability Lab, Technische Universita ̈t Berlin, Berlin, Germany，德国柏林工业大学质量和可用性实验室</p></li>
</ul>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Graphspeech syntax aware graph attention network for neural speech synthesis</title>
    <url>/2021/08/03/graphspeech/</url>
    <content><![CDATA[<h1 id="graphspeech用于神经语音合成的语法感知图注意力网络">GRAPHSPEECH：用于神经语音合成的语法感知图注意力网络</h1>
<h2 id="感想">感想</h2>
<p>读下来感觉与ICASSP 2020 发表的GraphTTS的方法差不多少，均是采用GGNN的方法进行TTS的图结构化建模。只不过GraphTTS基于Tacotron2建模，而Graphspeech 基于Transformer 建模，但是Graphspeech的实验部分相对充实简单清晰一些。</p>
<p><a href="https://arxiv.org/pdf/2010.12423.pdf">Paper</a> | <a href="https://ttslr.github.io/GraphSpeech/">Demo</a> | ICASSP 2021 | Rate: 🌟🌟🌟</p>
<h2 id="摘要">摘要</h2>
<p>基于注意力的端到端文本到语音合成（TTS）在很多方面都优于传统的统计方法。 基于Transformer的 TTS 是此类成功的实现之一。 虽然 Transformer TTS 使用自注意力机制很好地模拟了语音帧序列，但它没有从句子级别的句法角度将输入文本与输出话语关联起来。 我们提出了一种新的神经 TTS 模型，表示为 GraphSpeech，它是在图神经网络框架下制定的。 GraphSpeech 对句子中输入词法标记的句法关系进行显式编码，并结合这些信息为 TTS 注意力机制推导出句法驱动的字符嵌入。 实验表明，GraphSpeech 在话语的频谱和韵律渲染方面始终优于 Transformer TTS 基线。</p>
<p><strong>关键词：</strong>TTS, Graph Neural Network, Syntax</p>
<h2 id="introduction">Introduction</h2>
<p>文本转语音 (TTS) 旨在为输入文本合成类似人类的自然声音。 最近的进步使许多应用成为可能，例如智能语音助手、电影和游戏的配音、在线教育和智能家居。 最近，拼接和统计参数语音合成系统是主流技术。 我们注意到这两种技术都有复杂的pipelines，包括前端模型、持续时间模型和声学模型。</p>
<p>随着深度学习的出现，端到端TTS生成模型使用单个神经网络简化了合成pipelines。 基于 Tacotron 的神经 TTS 及其变体就是这样的例子。 在这些技术中，关键思想是将传统的 TTS pipelines集成到一个统一的编码器-解码器网络中，并直接从 <text, wav> 对中学习映射。 Tacotron 是基于循环神经网络 (RNN) 的成功编码器-解码器实现，例如 LSTM和GRU。 然而，循环性本质上限制了在训练和推理中并行计算的可能性。</p>
<p>自注意力网络（SAN）代表了另一种类型的编码器 - 解码器实现，它实现了高效的并行训练，并在机器翻译中具有良好的性能。 在 Transformer TTS 中实现，该模型允许并行计算。 SAN 的另一个好处是与内部注意力一起工作，它具有更短的路径来模拟长距离上下文。 尽管取得了进展，但 Transformer TTS 并没有在句子级别从句法的角度明确将输入文本与输出话语相关联，这在说话风格和韵律建模中被证明是有用的。 结果，特别是对于长句子，话语的呈现受到不利影响。</p>
<p>图神经网络 (GNN) 是连接主义模型，它通过图节点之间的消息传递来捕获图的依赖性。 在图到序列学习中，我们可以在图神经网络的框架中构建自注意力网络，其中将令牌序列视为未标记的全连接图（每个令牌作为一个节点 )，而self-attention机制被认为是一种特定的消息传递方案。 受此启发，我们提出了一种新颖的神经 TTS 模型，表示为 GraphSpeech，它采用两个新颖的编码模块：关系编码器（relation encoder）和图编码器（graph encoder）。 关系编码器从输入文本中提取语法树； 然后将语法树转化为图结构； 并对图中任意两个标记之间的关系进行编码。 图编码器提出了句法感知图注意机制，它不仅关注标记而且关注它们的关系，从而受益于有关句子结构的知识。</p>
<p>本文的主要贡献如下： 1）我们提出了一种新颖的神经 TTS 架构，表示为 GraphSpeech； 2）我们制定了从句法树到句法图的扩展，以及字符级的关系编码； 3）我们提出了一种语法感知图注意力机制，将语言知识纳入注意力计算； 4) GraphSpeech 在频谱和韵律建模方面都优于最先进的 Transformer TTS。 据我们所知，这是第一个在 Transformer TTS 中使用图神经网络视角的句法-感知的注意机制的实现。</p>
<p>本文的组织如下：在第 2 节中，我们重新审视了作为基准参考的 Transformer TTS 框架。 在第 3 节中，我们研究了提议的 GraphSpeech。 在第 4 节中，我们报告了评估结果。 我们在第 5 节总结了这篇论文。</p>
<h2 id="transformer-tts">Transformer TTS</h2>
<p>Transformer TTS 是一种基于自注意力网络 (SAN) 的神经 TTS 模型，具有编码器-解码器架构，可以并行训练和学习长距离依赖。 基于 SAN 的编码器具有多头自注意力机制，可以直接在任意两个令牌之间建立长时间的依赖关系。 给定输入文本的标记序列 <span class="math inline">\(x_{1:n}\)</span>​​，对于每个注意力头，标记 <span class="math inline">\(x_i\)</span>​​ 和标记 <span class="math inline">\(x_j\)</span>​​ 之间的注意力分数只是它们的查询向量和关键向量的点积，如下所示： <span class="math display">\[
\begin{equation}\label{eq1}
s_{ij} =f(x_i,x_j)=x_iW_q^TW_kx_j
\end{equation}
\]</span> 最终的注意力输出 <em>attn</em> 通过 softmax 函数进行缩放和归一化，如下所示： <span class="math display">\[
\begin{equation}\label{eq2}
attn = 􏰂\sum^n_{i=1} a_iW_vx_i
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}\label{eq3}
a_i = \frac{\text{exp}(s_{ij}/{\sqrt d}) } { \sum^n_{􏰁j=1} \text{exp}(s_{ij}/ \sqrt d)}
\end{equation}
\]</span></p>
<p>其中<span class="math inline">\(\sqrt d\)</span>是缩放因子，<span class="math inline">\(d\)</span> 是层状态的维度，<span class="math inline">\(W_q\)</span>、<span class="math inline">\(W_k\)</span> 和 <span class="math inline">\(W_v\)</span> 是可训练的矩阵。 最后，将所有注意力头的输出拼接并投影以获得最终的注意力值。</p>
<p>虽然 SAN-encoder 在任何两个标记之间建立远程依赖来建模全局上下文，但它没有对句子中单词的复杂句法依赖进行建模，这通常由树结构或 图形。 我们认为需要一种表示深层句法关系的机制来对输入文本和输出话语之间的关联进行建模。 接下来，我们在第 3 节中研究了一种新颖的 GraphSpeech 模型。</p>
<h2 id="graphspeech">Graphspeech</h2>
<div class="figure">
<img src="/images/graphspeech-fig1.png" alt="graphspeech-fig1" />
<p class="caption">graphspeech-fig1</p>
</div>
<p>图结构在自然语言处理 (NLP) 中扮演着重要角色，它经常作为表示句法、语义和知识的中心形式。 我们建议将图形建模合并到神经 TTS 架构中，称为 GraphSpeech，如图 1 所示，以对输入文本、其句法结构和输出语音之间的关联进行建模。</p>
<p>在 <em>GraphSpeech</em> 中，编码器的输入包括文本和句法知识。 众所周知，话语的语言韵律与句子的句法结构密切相关。 GraphSpeech 以语法树的形式使用此类语法信息来增强语音合成输入，这有望改善输入文本的语言表示。 在实践中，语法树用作更准确的自注意力的辅助信号。 GraphSpeech 有 3 步工作流程：1）关系编码器，2）图编码器和 3）解码器，这将在接下来讨论。</p>
<h3 id="relation-encoder">3.1 Relation Encoder</h3>
<p>Relation Encoder 将输入文本的语法树转换为语法图，描述所涉及的输入标记之间的全局关系。 句法依存分析树是描述词之间语言依存关系的传统方法之一。 在树状结构中，只有在句子中直接相关的词才会被连接起来。 其他人没有直接联系。 为了挖掘句子中两个词之间的句法关系，我们希望扩展句法树的拓扑结构以建立全连接通信。</p>
<div class="figure">
<img src="/images/graphspeech-fig2.png" alt="graphspeech-fig2" />
<p class="caption">graphspeech-fig2</p>
</div>
<p>为此，我们提出了语法图 <em>syntax graph</em>，它是图 2 所示的语法树的扩展。我们的想法是通过添加反向连接将单向连接变成双向连接。 此外，我们为每个单词引入了带有特定标签的自循环边。 这样，句子中的单词用节点表示，它们的连接用边表示。 通过图2（b）中的双向连接，一个词能够直接接收和发送信息给任何其他词，无论它们是否直接连接。</p>
<p>为了对两个节点之间的关系建模，将节点对之间的关系描述为它们之间的最短关系路径。 我们使用带有门控循环单元 (GRU) 的循环神经网络将关系序列转换为分布式表示。 节点 <span class="math inline">\(i\)</span> 和节点<span class="math inline">\(j\)</span> 之间的最短关系路径 <span class="math inline">\(sp_{i→j}\)</span> 表示为 <span class="math inline">\([sp_1, ..., sp_t, ..., sp_{n+1}] = [e(i, k_1), e(k_1, k_2), ..., e(k_n, j)]\)</span>，其中 <span class="math inline">\(e(·, ·)\)</span> 表示边标签，<span class="math inline">\(k_{1:n}\)</span> 是中继节点。 我们使用双向 GRU 进行路径序列编码： <span class="math display">\[
\begin{equation}\label{eq4}
\vec s_t =GRU_f(\vec s_{t−1},sp_t)
\end{equation}
\]</span></p>
<p><span class="math display">\[
\begin{equation}\label{eq5}
\stackrel{\leftarrow}s_t =GRU_b(\stackrel{\leftarrow} s_{t+1},sp_t)
\end{equation}
\]</span></p>
<p>前向 GRU 网络和后向 GRU 网络的最后隐藏状态连接起来形成最终的关系编码 <span class="math inline">\(r_{ij} = [\vec s_{n+1} ; \stackrel{\leftarrow}s_0 ].\)</span> 最终的关系编码表示两个词之间的语言关系。 在神经 TTS 中，句子的基本单位是字符标记。 我们在这里将 NLP 中的词级关系编码扩展到字符级编码。 如果两个字符属于同一个词，我们使用自循环边编码<span class="math inline">\(r_{ii}\)</span>来定义它们的关系编码； 如果两个字符属于不同的词，我们直接使用<span class="math inline">\(r_{ij}\)</span>，即它们所属词的关系编码来分配它们的关系编码。</p>
<p>关系编码为模型提供了一个关于如何收集和分发信息的全局视图，即在哪里参加。 接下来我们将讨论所提出的图编码器，该编码器旨在将句法关系编码合并到自注意力机制中以指示字符关系。</p>
<h3 id="graph-encoder">3.2 Graph Encoder</h3>
<p>图编码器旨在将输入字符嵌入序列和关系编码转换为相应的由句法驱动的字符嵌入序列。 我们将两个节点之间的显式关系表示合并到注意力计算中，表示为语法感知图注意力。</p>
<p>然后堆叠多个语法感知图注意和前馈层块以计算最终的字符表示。 在每个块中，基于所有其他字符嵌入和相应的关系编码更新字符嵌入。 最后一个块的结果字符嵌入被馈送到解码器以生成声学特征。</p>
<p>关系编码只对两个字符之间的最短路径进行编码。 为了在计算注意力时对连接的方向进行编码，我们首先将关系编码 <span class="math inline">\(r_{ij}\)</span> 分为前向关系编码 <span class="math inline">\(r_{i→j}\)</span> 和后向关系编码 <span class="math inline">\(r_{j →i} ： [r_{i→j} ; r_{j →i} ] = W_r r_{ij}\)</span> 。 然后，使用语法感知图注意力来计算注意力分数，该分数基于字符表示及其双向关系表示： <span class="math display">\[
\begin{equation}\label{eq6}
\begin{aligned}
s_{ij} &amp;=g(x_i,x_j,r_{ij}) \\
&amp;= ( x_i + r_{i → j} ) W_q^T W_k ( x_j + r_{j → i} ) \\
&amp;= \underbrace{x_i W_q^T W_k x_j}_{(a)} + \underbrace{x_i W_q^T W_k r_{j → i}}_{(b)} \\
&amp;+ \underbrace{r_{i→j}W_q^TW_kx_j}_{(c)} + \underbrace{r_{i→j}W_q^TW_kr_{j→i}}_{(d)}
\end{aligned}
\end{equation}
\]</span> 我们注意到以上方程中的项与公式的直观解释有关，如下所示：1) (a) 捕获纯粹基于内容的寻址，这与等式 (1) 一致; 2) (b) 项表示正向关系偏差； 3) (c) 项控制后向关系偏差； 和 4) (d) 项编码通用关系偏差。</p>
<p>通过语法感知图注意力，通过将显式句法关系约束纳入注意力机制，语法知识用于指导字符编码。 最后，解码器采用句法驱动的字符嵌入进行声学特征预测。</p>
<h3 id="解码器">3.3 解码器</h3>
<p>解码器遵循 Transformer TTS 中报告的相同结构。 我们使用 Mel Linear、Post-Net 和 Stop Linear 分别预测 mel 谱和停止标记。 如上所述，关系编码器对两个远距离字符之间的显式句法关系进行编码。 图编码器采用句法感知图注意力来导出句法驱动的字符表示。 解码器将字符表示作为输入并产生自然的声学特征。 通过这种方式，解码器学习将输入文本及其语法与输出话语相关联。</p>
<h2 id="实验">实验</h2>
<p>我们进行客观和主观评估来评估我们提出的 GraphSpeech 框架的性能。 我们使用 LJSpeech 数据库，该数据库由 13,100 个短片组成，其中一位演讲者阅读了大约 7 本书，总共将近 24 小时的演讲。 我们使用最先进的 Transformer TTS 作为基线。</p>
<h3 id="实验配置">4.1 实验配置</h3>
<p>在这项研究中，我们使用 Stanza 来提取句法依赖解析器树。 对于关系编码器，边缘嵌入的大小设置为 200 维随机初始化。 我们在两个方向上将 GRU 层的大小设置为 200，并生成 200 维的关系编码。 图编码器将 256 维字符嵌入或节点嵌入作为输入。 我们在图编码器中使用 N = 6 个块，在 GraphSpeech 的解码器中使用 N = 6 个块。 我们在图编码器和解码器中设计了 4 个用于多头注意力的头。 解码器生成一系列 80 通道梅尔谱声学特征作为输出。</p>
<p>在训练期间，我们提取帧大小为 50 毫秒和帧偏移为 12.5 毫秒的梅尔谱声学特征，进一步归一化为零均值和单位方差，作为参考目标。 我们使用 Adam 优化器训练模型，<span class="math inline">\(β_1 = 0.9，β_2 = 0.98\)</span>​​。 在我们的实验中采用了与Transformer中相同的学习率计划。</p>
<p>为了计算效率，我们在训练和测试批次中组合了所有不同的最短路径。 然后我们通过关系编码器将它们编码为向量表示，如第 3.1 节所述。 为了快速周转，我们使用 Griffin-Lim 算法来生成波形。</p>
<h3 id="客观评估">4.2 客观评估</h3>
<p>我们采用梅尔谱失真 (MCD) 来测量合成和参考梅尔谱特征之间的光谱距离。 我们使用均方根误差 (RMSE) 作为韵律评估指标。</p>
<div class="figure">
<img src="/images/graphspeech-tab1.png" alt="graphspeech-tab1" />
<p class="caption">graphspeech-tab1</p>
</div>
<p>在表 1 中，我们报告了比较研究中的 MCD 和 RMSE 结果。 我们观察到 <em>GraphSpeech</em> 提供了验证其有效性的较低 MCD 和 RMSE 值。 通过语法感知图注意力机制，GraphSpeech 不仅学习将输入文本关联起来，还学习将其语法结构与目标话语关联，从而改进 TTS 语音渲染。</p>
<h3 id="主观评估">4.3 主观评估</h3>
<p>我们进行主观评价的听力实验。 我们首先根据平均意见得分 (MOS) 评估 Transformer TTS、GraphSpeech 和 ground truth。 听者以5分制对话语进行评分：“5”为优秀，“4”为好，“3”为一般，“2”为差，“1”为差。 15 名受试者参与了这些实验，每人听了 100 个合成语音样本。</p>
<div class="figure">
<img src="/images/graphspeech-fig3.png" alt="graphspeech-fig3" />
<p class="caption">graphspeech-fig3</p>
</div>
<p>如图 3 所示，GraphSpeech 大大优于 Transformer TTS，并且取得了与Ground-truth自然语音相当的结果，我们认为这是非常了不起的。 这些结果清楚地表明，在 GraphSpeech 中提出和实施的语法感知图注意力策略有效地对语言知识建模并实现了高质量的合成语音。</p>
<div class="figure">
<img src="/images/graphspeech-fig4.png" alt="graphspeech-fig4" />
<p class="caption">graphspeech-fig4</p>
</div>
<p>我们进一步进行 AB 偏好测试，其中要求听众比较一对系统之间合成语音样本的质量和自然度，并选择更好的一个。 从图 4 中可以看出，听众始终喜欢所提出的 GraphSpeech 模型，这进一步验证了所提出的图注意力机制。 据悉，GraphSpeech合成的样本接近自然语音，令人鼓舞。</p>
<h2 id="结论">结论</h2>
<p>在这项工作中，我们提出了一种新颖的神经 TTS 架构，表示为 GraphSpeech。 所提出的语法感知图注意机制有效地对输入文本的任意两个字符之间的语言关系进行建模。 实验结果表明，GraphSpeech 在客观和主观评价中均优于原有的 self-attention 系统，并且在语音质量和韵律自然度方面取得了显着的表现。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>韵律建模思维导图</title>
    <url>/2020/09/20/prosody-modelling/</url>
    <content><![CDATA[<div class="figure">
<img src="/images/情感建模.png" alt="韵律建模论文思维导图" />
<p class="caption">韵律建模论文思维导图</p>
</div>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages</title>
    <url>/2021/06/21/mlms/</url>
    <content><![CDATA[<h1 id="paper-title-uniform-multilingual-multi-speaker-acoustic-model-for-statistical-parametric-speech-synthesis-of-low-resourced-languages------google-uk-interspeech-2017">Paper title: Uniform Multilingual Multi-Speaker Acoustic Model for Statistical Parametric Speech Synthesis of Low-Resourced Languages ---- Google UK (Interspeech 2017)</h1>
<h2 id="感想">感想</h2>
<p>这篇Google UK的论文，采用了一个MLMS的想法来解决低资源语种合成的问题，思想感觉上与采用IPA来统一音素特征是非常相似的，论文结构清晰，实验部分翔实充分，打分：🌟🌟🌟</p>
<h2 id="论文题目用于低资源语言统计参数语音合成的统一多语言多说话者声学模型------谷歌uk">论文题目：用于低资源语言统计参数语音合成的统一多语言多说话者声学模型 ---- 谷歌UK</h2>
<h3 id="abstract">Abstract</h3>
<p>痛点：对于低资源语种来说，获取大量的训练数据是昂贵且困难的，通常是仅仅能获取到一小部分 / 或者没有数据集。</p>
<p>解决方案：本文提出了一种利用长短期循环神经网络的声学模型，目的是解决小语种语言数据缺失的问题。“说话人自适应”系统目的在于在多种语言间保持说话人的相似度，而本方法的突出特征是，模型构建成功后，系统不需要再重新训练以解决集外的语种，这是由于<strong>语言和说话人-不可知的建模方法和通用的语言特征集。</strong></p>
<p>实验结果：1）在12种语言上的实验结果显示，对于集外语种，系统仍能生成智能、自然的声音。2）当提供了少量训练数据的情况下，pooling the data有时能够提高整体的智能性和自然度。3）有时，构建一个zero-shot的多语种系统好于few-shot 单说话人单语种系统。</p>
<h3 id="introduction-1-page">Introduction (1 page)</h3>
<p>近期发展：近些年，统计参数语音合成的方法，从HMM转向了神经网络系统，2013年 Heiga Zen 发布了第一个采用前向DNN网络的语音合成系统（Google，ICASSP 2013），且合成效果优于HMM系统。之后的LSTM-RNN模型提升了语音合成的效果，并且最近的PCM生成模型（WaveNet）近一步提升了模型效果。</p>
<p>挑战：1）语音数据的获取。当从先验收集到少量数据时，说话人-自适应方法可以被采用，这时候，需要将模型在新说话人的少量数据集上进行fine-tune。但是这种方法不能用于zero-shot的情境。2）获取多说话人的广泛数据集，并且构建一个平均音色模型。但是这个方法不能应用于缺少足够语言信息的语种上。</p>
<p>本文课题：对于指定的低资源语种数据，有最小的语言表示信息。</p>
<p>解决方法：一个多语种声学模型被训练，其中目标语种的数据集未包含在训练数据集集内。</p>
<p>本文贡献：一个通用的MLMS（multi-lingual multi-speaker）模型被训练，并且是采用语言和说话人-不可知的方法。</p>
<h3 id="multilingual-architecture-1-page">Multilingual Architecture (1 page)</h3>
<p>本文的优势：1）一个具象的输入特征空间，不需要在新语种上fine-tune；2）一个类似于单说话人的简单模型架构。</p>
<h4 id="文本特征">2.1 文本特征</h4>
<h5 id="典型语言表示">2.1.1 典型语言表示</h5>
<p>训练数据集是包含多种语言和口音的。首先将多语种全部转换至IPA。尽管这个转换过程有一些困难，如1）需要专家知识来做相应的转换，2）不能直接的转换。但是这个IPA还是能够为语言空间提供具象的特征。</p>
<h5 id="系统发育语言特征">2.1.2 系统发育语言特征</h5>
<p>基于BCP-47标注，我们采用语言和边界识别特征来建模同语种的不同口音。+ 一个系统语言分类树</p>
<h4 id="lstm-rnn-声学模型">2.2 LSTM-RNN 声学模型</h4>
<p>给定语言特征后，LSTM-RNN时长模型的作用是预测每个音素的发音时长。然后再将这个时长和语言特征一同输入到声学模型。以预测音频波形。音频波形的平滑性，是采用RNN的循环单元来建模的。</p>
<p>由于本文需要处理更大数量的数据集和更加多样的语言特征，所以本文的模型与baseline的区别在于ReLU的单元数量和LSTM的层数，以及声学模型输出层的循环单元的个数。</p>
<h3 id="experiments-2-page">Experiments (2 page)</h3>
<p>用于训练声学模型的数据集语料有超过800小时的语音，包含了37种不同的语言种类。这些语言属于原始的59组语言/地区对，一些语种，如英语，有不同的说话人数据集，对应不同的地域口音。对于一些口音（如EN-US）有多个说话人。一些音频是在消声室（anechoic chambers）录制的，而一些就是常规的录音室录制的（a regular recording studio）</p>
<h4 id="方法论系统细节">3.1 方法论：系统细节</h4>
<p>语音数据采用22.05KHz的数据集，LSTM-RNN模型输出的特征是音素的发音时长</p>
<h4 id="模型参数和评估">3.2 模型参数和评估</h4>
<p>实验被设计为两种情景：</p>
<ul>
<li>模型被在除去12种语言的语料上训练（其中有6种，是毫无语料的情况）。但每一种被排除的语种（除了其中2种）都有“亲戚”语种在训练数据集中。在对这些被排除在外的目标语种进行语音合成。其中的模型称为H</li>
<li>用所有语种的数据集来训练模型。其中的模型称为I</li>
</ul>
<p>因为声学模型可以被speaker和gender identifying特征控制，所以以下实验被设计来观察如何影响合成质量。</p>
<ul>
<li>speaker和gender特征 unset （default，D），set to the highest quality female speaker (EN-US, F), highest quality male speaker (EN-GB, M), speaker of the closet language (C).</li>
<li>Setting the speaker and gender features for this speaker (S)</li>
</ul>
<p>实验评估：100句集外话术，每个人最多听100句话。每句话有1min的评估时间。每一种语言有8个评分者。</p>
<h4 id="实验结果和讨论">3.3 实验结果和讨论</h4>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>语音顶会论文集总结</title>
    <url>/2020/12/03/speech-papers/</url>
    <content><![CDATA[<table>
<colgroup>
<col width="14%" />
<col width="16%" />
<col width="9%" />
<col width="39%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th><strong>会议地点</strong></th>
<th><strong>离线论文包</strong></th>
<th><strong>论文集网址</strong></th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Icassp 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>Interspeech 2020</strong></td>
<td>Virtual</td>
<td>Y</td>
<td>https://www.isca-speech.org/archive/Interspeech_2020/</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Neurips</strong></td>
<td>Virtual</td>
<td>N</td>
<td>https://proceedings.neurips.cc</td>
<td></td>
</tr>
<tr class="even">
<td><strong>BC 2020 &amp; VC 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td>https://www.isca-speech.org/archive/VCC_BC_2020/</td>
<td></td>
</tr>
<tr class="odd">
<td><strong>Iclr 2020</strong></td>
<td>Virtual</td>
<td>N</td>
<td>https://iclr.cc/virtual_2020/papers.html?filter=titles&amp;search=speech</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td><strong>论文合集网址</strong></td>
<td>-</td>
<td>-</td>
<td>https://www.isca-speech.org/iscaweb/index.php/archive/online-archive</td>
<td>含interspeech、ssw等论文合集</td>
</tr>
<tr class="even">
<td>https://openreview.net</td>
<td>含iclr、icml、acm等论文集</td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>简单的生活法则</title>
    <url>/2021/07/08/simple-rules-for-life-decisions/</url>
    <content><![CDATA[<h1 id="简单的生活法则">简单的生活法则</h1>
<ol style="list-style-type: decimal">
<li>Mental Models I Find Repeatedly Useful</li>
</ol>
<p>https://medium.com/<span class="citation">@yegg/mental-models-i-find-repeatedly-useful-936f1cc405d</span></p>
<ol start="2" style="list-style-type: decimal">
<li>THREAD: 15 of the most useful razors and rules I've found. Rules of thumb that simplify decisions.</li>
</ol>
<p>https://twitter.com/george__mack/status/1350513143387189248</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>Vocoders 模型总结</title>
    <url>/2020/12/03/vocoders/</url>
    <content><![CDATA[<p>语音合成声码器脉络总结如下，持续更新ing</p>
<table>
<colgroup>
<col width="4%" />
<col width="9%" />
<col width="3%" />
<col width="17%" />
<col width="7%" />
<col width="14%" />
<col width="16%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Order</strong></th>
<th><strong>Model</strong></th>
<th><strong>Year</strong></th>
<th><strong>Institution</strong></th>
<th><strong>Conference</strong></th>
<th><strong>Inherited Model (Base model)</strong></th>
<th><strong>Corresponding Author (Team leader)</strong></th>
<th><strong>URL</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>1</strong></td>
<td>WaveNet</td>
<td>2016.9</td>
<td>Google DeepMind</td>
<td>SSW 2016</td>
<td>CNN</td>
<td>Nal Kalchbrenner</td>
<td>https://arxiv.org/pdf/1609.03499.pdf</td>
</tr>
<tr class="even">
<td><strong>2</strong></td>
<td>WaveRNN</td>
<td>2018.6</td>
<td>DeepMind &amp; Google Brain</td>
<td>ICML 2018</td>
<td>RNN</td>
<td>Nal Kalchbrenner</td>
<td>https://arxiv.org/pdf/1802.08435.pdf</td>
</tr>
<tr class="odd">
<td><strong>3</strong></td>
<td>WaveGlow</td>
<td>2018.10</td>
<td>Nvidia</td>
<td>ICASSP 2019</td>
<td>WaveNet</td>
<td>Rafael Valle</td>
<td>https://arxiv.org/pdf/1811.00002.pdf</td>
</tr>
<tr class="even">
<td><strong>4</strong></td>
<td>LPCNet</td>
<td>2019.2</td>
<td>Mozilla, Google</td>
<td>ICASSP 2019</td>
<td>WaveRNN</td>
<td>Jean-Marc Valin</td>
<td>https://arxiv.org/pdf/1810.11846.pdf</td>
</tr>
<tr class="odd">
<td><strong>5</strong></td>
<td>WaveGAN</td>
<td>2019.2</td>
<td>UC San Diego</td>
<td>ICLR 2019</td>
<td>GAN</td>
<td>Miller Puckette</td>
<td>https://arxiv.org/pdf/1802.04208.pdf</td>
</tr>
<tr class="even">
<td><strong>6</strong></td>
<td>Multi-band WaveRNN</td>
<td>2019.4</td>
<td>Tecent AI Lab</td>
<td>Interspeech 2020</td>
<td>DurIAN, WaveRNN</td>
<td>Dong Yu</td>
<td>https://arxiv.org/pdf/1909.01700.pdf</td>
</tr>
<tr class="odd">
<td><strong>7</strong></td>
<td>MelGAN</td>
<td>2019.12</td>
<td>University of Montreal, Mila, Lyrebird AI</td>
<td>NeurIPS 2019</td>
<td>GAN</td>
<td>Yoshua Bengio</td>
<td>https://arxiv.org/pdf/1910.06711.pdf</td>
</tr>
<tr class="even">
<td><strong>8</strong></td>
<td>SqueezeWave</td>
<td>2020.1</td>
<td>UC Berkeley</td>
<td></td>
<td>WaveGlow</td>
<td>Bichen Wu</td>
<td>https://arxiv.org/pdf/2001.05685.pdf</td>
</tr>
<tr class="odd">
<td><strong>9</strong></td>
<td>Parallel WaveGAN (PWG)</td>
<td>2020.2</td>
<td>LINE Corp., NAVER Corp.</td>
<td></td>
<td>GAN</td>
<td>Ryuichi Yamamoto</td>
<td>https://arxiv.org/pdf/1910.11480.pdf</td>
</tr>
<tr class="even">
<td><strong>10</strong></td>
<td>Multi-band MelGAN</td>
<td>2020.5</td>
<td>西北工业大学，sogou</td>
<td></td>
<td>melgan, multi-band</td>
<td>Xielei</td>
<td>https://arxiv.org/pdf/2005.05106.pdf</td>
</tr>
<tr class="odd">
<td><strong>11</strong></td>
<td>FeatherWave</td>
<td>2020.10</td>
<td>Tecent</td>
<td>Interspeech 2020</td>
<td>MB LP, WaveRNN</td>
<td>Shan Liu</td>
<td>https://isca-speech.org/archive/Interspeech_2020/pdfs/1156.pdf</td>
</tr>
<tr class="even">
<td><strong>12</strong></td>
<td>WaveGrad</td>
<td>2020.10</td>
<td>Johns Hopkins University, Google Brain</td>
<td></td>
<td>CNN</td>
<td>Heiga Zen</td>
<td>https://arxiv.org/pdf/2009.00713.pdf</td>
</tr>
</tbody>
</table>
<p><a href="https://arxiv.org/pdf/2103.05236.pdf">GAN Vocoder: Multi-Resolution Discriminator Is All You Need</a></p>
<p>此篇论文尝试解释为什么近期涌现的GAN-based vocoders要好于过往的Flow-based或者Autoregressive的vocoders。文章通过消融实验分析认为原因主要在于Multi-Resolution Discriminator的设计使得GAN-based vocoders达到了一个新的水平。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised Learning For Sequence-to-sequence Text-to-speech For Low-resource Languages</title>
    <url>/2021/06/27/vqvae/</url>
    <content><![CDATA[<h1 id="低资源语种序列-序列语音合成无监督学习方法">低资源语种序列-序列语音合成无监督学习方法</h1>
<h2 id="abstract">Abstract</h2>
<p>待解决问题：近些年，包含attention机制的序列-到-序列在TTS领域获得了广泛的成功。这些模型能够以一个大的标注的语料库来生成近似人的语音。</p>
<p>解决方案：然而，准备这样一个大的数据集是昂贵且耗费人力的，为了解决数据依赖的问题，我们提出了一种创新性的无监督预训练机制。具体来讲，首先，我们采用VQVAE模型来从大规模公开发表的，未标注的数据中抽取无监督语言单元。然后，我们采用<无监督语言单元，语音>对来预训练序列-序列TTS模型。最终，我们采用目标说话人的小数据量的<text, audio>数据，来fine-tune模型。</p>
<p>实验结果：主观和客观实验结果均显示，我们提出的方案可以采用相同数量的成对训练数据，合成更加智能化和自然的语音。除此之外，我们将我们提出的方案延伸到假设的低资源语言中，采用客观评估方法，验证模型的有效性。</p>
<h2 id="introduction">Introduction</h2>
<p>序列到序列的TTS模型由一个编码器-解码器-attention的框架组成，能够生成自然语音。然而，训练这些S2S TTS模型需要成百上千的标注语音来生成近似人声的语音。尽管少量的数据需要被用来生成类人的语音，它限制了整体的自然度，并且模型容易造成不希望的错误。</p>
<p>尽管收集一个这样一个大型的标注语音语料库是昂贵的，耗费成本的，研究者开始调研TTS中数据的有效性。一些学者集中于采用少量数据集迁移TTS模型到新说话人。一些调研采用Speaker embeddings来在TTS中建模speaker identities。一些也探索使用一个speaker embeddings的组合并且fine-tune。一些人甚至致力于zero-shot 说话人自适应研究。</p>
<p>其他的一些学者探索通过通用数据来建模TTS模型。一些人尝试采用传统TTS的技术，将分布式的文本或语言信息引入TTS。一些人采用ASR数据或者通过数据筛选或分析找到的已有数据数据来训练TTS模型。最近，有学者提出了一个简单但是有效的半监督学习方法来仅仅采用语音预训练端到端TTS中的decoder。</p>
<p>目前有一些工作在研究低资源语种TTS的数据有效性，并且显示，训练一个多语种的统计参数语音合成方式，能够将adaption迁移到有小数据量的新语言。最近的工作调查显示从高资源的语种语言，迁移到低资源的语言也是有效的。</p>
<p>本文目的在于通过利用大量的，公开的，未标注的语音数据，来减轻S2S TTS训练中的数据需求量。我们提出了一个训练Tacotron2的无监督学习框架。具体来讲，我们首先通过VQ-VAE模型来从未标注语音中抽取无监督语言单元。然后通过使用<无监督语言单元，语音>对来预训练Tacotron。最终，我们采用目标说话人少量的<text, audio>文本语音对来fine-tune 模型。</p>
<p>我们的工作与[20]Y.-A Chung (ICASSP 2019) “Semi-supervised training for improving data efficiency in end-to-end speech synthesis”相关。然而，区别如下：</p>
<ul>
<li>我们的方法采用无监督学习方法抽取类似于音素的语言单元，使得有可能预训练整个TTS模型，然而[20]分开预训练模型的几个部分；</li>
<li>我们也在假设的低资源语言上，证实了方法的可行性；</li>
<li>最后，我们在实验中主要采用公开数据集，因此能够很容易被复现。</li>
</ul>
<h2 id="提出的方案">提出的方案</h2>
<p>我们采用一个baseline Tacotron的模型架构，其中采用location-sensitive attention 和 从文本中生成的音素序列。为了将预测的频谱转换为语音，我们采用Griffin-Lim算法for fast experiment cycles，因为我们是关注于数据有效性的问题，而不是生成高保真的语音。在baseline模型中，模型是从0开始训练的，意外着模型的所有parameters全都是被paired data来训练的。</p>
<div class="figure">
<img src="/images/vqvae-algorithm.png" alt="vqvae-algorithm" />
<p class="caption">vqvae-algorithm</p>
</div>
<h3 id="半监督预训练">2.1 半监督预训练</h3>
<p>在baseline Tacotron model中，模型应该同时学习到文本声学表示和他们之间的对齐。[20] 提出了两种模型预训练的方法来利用外部的文本和声学信息。对于文本表示来说，他们通过外部的word-vectors预训练了Tacotron的encoder，对于声学表示，他们通过未标注的语音，预训练了decoder。</p>
<p>[20]然后采用paired data来fine模型，在这一步，模型集中于学习textual representations和acoustic ones之间的对齐。</p>
<h3 id="无监督学习---预训练">2.2 无监督学习---预训练</h3>
<p>尽管[13] 展示出提出的半监督预训练的方法能够合成更加智能的语音，但是它也发现同时分开训练编码器和解码器不会相较于仅仅预训练解码器带来更多提升。然而，仅预训练解码器和fine-tune整个模型有一个不匹配。为了避免这种不匹配带来的潜在损失，并且进一步通过仅仅使用语音来提高数据有效性，我们提出从未标注语音中抽取无监督语言单元来预训练整个模型。</p>
<p>我们提出的方法提供在了算法1中。整体的框架包含两个模型：一个无监督模型，用来抽取类似于音素的语言特征，和Tacotron模型。</p>
<h4 id="无监督语言单元">2.2.1 无监督语言单元</h4>
<p>无监督语言表示在表示学习和特征解耦两个方面都带来了很大的提升。在它们之间，离散表示在语言和语音社区是较为流行的，因为直观上来看，语言和语音都是由有限的离散单元来组成的，例如文本中的字母和语音中的音素。本文利用VQ-VAE模型作为离散语言单元的抽取器。</p>
<p>在这种情况下，VQ-VAE模型作为一个类似于ASR的识别模型。然而，VQVAE模型和ASR模型的主要区别是VQVAE模型以一种无监督的方式来训练，然而ASR是采用一种有监督的方式。这种区别只要考虑到低资源语种的问题就会有所影响。当我们没有一个用于低资源语种的ASR模型时，这种提出的无监督方法对于提取低资源语种的语言表示单元是有意义的。</p>
<p>VQ-VAE模型有采用了一个encoder-decoder的框架，和一个码书（codebook dictionary）$e = C D <span class="math inline">\(，其中\)</span>C<span class="math inline">\(是字典中隐状态嵌入的数量，\)</span>D<span class="math inline">\(是每一个嵌入的维度。编码器\)</span>E$输入原始语音波形 <span class="math inline">\(x_{1:T}=x_1, x_2, ..., x_T\)</span>作为输入，并且生成编码状态<span class="math inline">\(z_{1:N}=E(x_{1:T})\)</span>，其中<span class="math inline">\(N\)</span>依赖于文本时间长度<span class="math inline">\(T\)</span>和编码器中下采样层的数量。然后，连续的隐状态表示<span class="math inline">\(z_{1:N}\)</span>能够被映射到<span class="math inline">\(\hat z_{1:N}\)</span>通过在字典中找到最近的预定义的离散嵌入<span class="math inline">\(\hat z = e_k\)</span>，其中<span class="math inline">\(k=argmin_j||z-e_j||\)</span>，<span class="math inline">\(e_j\)</span>是在码书中的第<span class="math inline">\(j\)</span>个嵌入，并且<span class="math inline">\(j\in 1,2,...,C\)</span>。最终，隐嵌入<span class="math inline">\(\hat z_{1:N}\)</span>和说话人嵌入<span class="math inline">\(s\)</span>被一同输入到解码器<span class="math inline">\(D\)</span>来重构语音波形<span class="math inline">\(\hat x = D(\hat z, s)\)</span>。</p>
<p>因为模型输入和输出是相同的，模型能够以一种auto-encoder的方式来训练。然而，梯度不能够通过<span class="math inline">\(argmin\)</span>计算来获取，因此直接采用梯度估计来近似。因此模型的整体loss为： <span class="math display">\[
L = -log(x|\hat z(x), s) + ||sg(z(x))-e_j||^2_2 + \beta \ast ||z(x) - sg(e_j)||^2_2
\]</span> 其中第一项是negative log-likelihood用来更新整体模型的；第二项更新码书字典，其中 <span class="math inline">\(sg\)</span>指代stop-gradient计算；第三项，指代承诺损失，鼓励编码器输出<span class="math inline">\(z\)</span>来接近于码书嵌入，超参数<span class="math inline">\(\beta\)</span>是用于给第三项增加一个权重。</p>
<div class="figure">
<img src="/images/vqvae-t2.png" alt="vqvae-t2" />
<p class="caption">vqvae-t2</p>
</div>
<h4 id="tacotron预训练和fine-tune">2.2.2 Tacotron预训练和fine-tune</h4>
<p>在VQVAE被训练后，我们抽取每句话的无监督语言单元。然后给无监督语言单元随机处理化一个嵌入表，通过查表得到的语言嵌入序列被用作Tacotron的输入。因此，我们能够通过<linguistic embedding, audio> 对来预训练Tacotron。</p>
<p>在模型被预训练后，我们采用一些paired语言数据来fine-tune模型。在这一步中，模型的输入是从文本中得到的音素序列。</p>
<h2 id="experiment">Experiment</h2>
<h3 id="实验设置">3.1 实验设置</h3>
<p>数据集：LJSpeech</p>
<p>VQ-VAE：与“Unsupervised speech representation learning using WaveNet autoencoders”相似的网络结构</p>
<p>当训练VQ-VAE的时候，我们采用39维MFCC作为模型输入。在我们调研学习后，码书的大小是256，并且每一个码书embedding的嵌入是64维。jitter rate 和<span class="math inline">\(\beta\)</span>是0.12和0.25。</p>
<p>[20]发现<strong>24-分钟语音是刚好不能构建一个Tacotron系统的语音的最小最大值</strong>。因此，我们集中于对比不同的仅仅采用24分钟paired数据训练的模型。</p>
<h3 id="在24-min数据上的结果">3.2 在24-min数据上的结果</h3>
<p>模型如下：</p>
<ul>
<li>Tac：仅仅采用LJSpeech训练的Tacotron</li>
<li>T-Dec: 以半监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-VQ: 以本文提出的学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune</li>
<li>T-phone: 以监督学习方式，采用外部数据集训练的Tacotron，然后在LJSpeech上fine-tune，意指模型的上限。</li>
</ul>
<p>外部数据集：VCTK</p>
<p>其中在预训练T-Dec和T-VQ的时候，仅仅采用语音数据，对于T-Phone，采用VCTK的文本语音对数据。</p>
<p>本文采用了客观和主观两种方式，来评估实验结果。对于客观方式，我们采用了Dynamic-time-warping Mel-cepstral Distortion(DTW MCD)，度量了合成语音和真实语音之间的距离，越小越好。我们采用了<strong>大约20分钟的unseen数据作为评估数据</strong>。对于主观评估方式，我们采用20条unseen utterances执行了一系列的AB tests。20 raters(10男10女)是地道的普通话中文者，英文熟练。</p>
<h4 id="mcd客观评估">3.2.1 MCD客观评估</h4>
<div class="figure">
<img src="/images/vqvae-results.png" alt="vqvae-results" />
<p class="caption">vqvae-results</p>
</div>
<p>MCD结果如上表1所示，如[20]中所描述的，仅仅预训练decoder能够降低MCD。然而，提出的方法实现了最好的效果，MCD相较于baselineTacotron低了14.3%。我们也发现了T-VQ的结果十分接近Upper bound（T-Phone）</p>
<h4 id="ab偏好主观测试">3.2.2 AB偏好主观测试</h4>
<p>AB tests的结果如表2所示，可以清晰看出预训练的技巧能够帮助提升模型性能。在Tacotron和预训练模型（T-Dec / T-VQ）中有一个较大的表现差距。我们发现采用LJSpeech数据从0开始训练模型能够很难得到智能数据，部分原因是LJSpeech的数据集质量也不是足够高。</p>
<p>在T-Dec和T-VQ的AB Test中，T-VQ获取了更好的表现，从不正式的听测中，我们注意到T-Dec合成的语音在智能性上更加中庸，T-VQ的智能性会更好。这显示通过无监督语言单元和语音来进行预训练能够进一步提升模型性能。原因是在提出的预训练的配置中，模型不仅仅能够学习到声学表示，也能够学习到对齐信息。尽管无监督的语言单元没有在fine-tune中使用，提出的预训练的方法对于textual representation learning也是有效的，因为这些无监督的语言单元被证明很像音素。</p>
<p>在Tac-VQ和T-phone的对比中，大多数的raters没有选择，但其中还是有20%的人在二者之中选择T-Phone。</p>
<h3 id="在其他数量数据上的实验结果">3.3 在其他数量数据上的实验结果</h3>
<p>实验结果表示：</p>
<ul>
<li>在使用24min数据时，Tacotron与其他三个模型有很大差异</li>
<li>随着数据量增大，差异缩小，证明了pre-training的作用在降低</li>
<li>T-VQ和T-Phone始终比Tac和T-Dec的方法效果要好</li>
</ul>
<h3 id="低资源语种的实验结果">3.4 低资源语种的实验结果</h3>
<p>本节验证提出的方法在2种低资源语种的实验效果。假设English和Mandarin Chinese是两种低资源语种。主要为了解决以下两个问题：</p>
<ul>
<li>此种方法能否在这种情境下提升数据有效性？</li>
<li>那些预训练的语种对于提出的方法更有效？与目标语种音素相近的还是不相关的？</li>
</ul>
<p>目标语种，英语的语料是LJSpeech，中文是内部数据集Xiaomin，新闻风格，女性。</p>
<p>训练VQ-VAE和预训练Tacotron的语言包含以下几种：韩语，日语，西班牙语，法语，德语。我们仅仅利用语音数据来训练VQ-VAE和预训练Tacotron。在训练VQ-VAE的时候，仅做了一个改动：码书的大小从256改变到512，因为这里采用了多语种的数据集。三种模型衍生如下：</p>
<ul>
<li>Tac：通过LJSpeech或者Xiaomin训练的Tacotron；</li>
<li>T-VQ-A：以本文提出的学习方式，采用亚洲数据集（韩语、日语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
<li>T-VQ-E：以本文提出的学习方式，采用欧洲数据集（西班牙语，韩语，德语）训练的Tacotron，然后在LJSpeech / Xiaomin上fine-tune</li>
</ul>
<div class="figure">
<img src="/images/low-resource-languages.png" alt="low-resource-languages" />
<p class="caption">low-resource-languages</p>
</div>
<p>To alleviate the burden of raters，我们仅仅评估MCD值。如表3和表4所示。我们提出的预训练的方式，能够有效提升合成语音的质量，对于低资源语种来说很有价值，因为语音的收集成本十分高。</p>
<p>除此之外，在English TTS中T-VQ-E结果好于T-VQ-A，在普通话实验中，T-VQ-A slightly out-performs T-VQ-E。这个结果展示出了，采用音素相近的语言来进行模型预训练，是更加有效的。此外，我们发现了随着fine-tune数据的增多，MCD的下降，这个结果与上一节结果是相似的。最后，通过对比English TTS中最好的模型T-VQ-E模型，和上一节中的T-VQ模型，我们发现这里尽管使用音素相似的语言，仍然会存在一个不可忽视的gap。</p>
<h2 id="结论">结论</h2>
<p>本文提出了一种在序列-序列TTS中，提升数据有效性的无监督学习方案。本方案首先从大规模未转译外部数据中，抽取文本和声学特征表示。然后在采用S2S模型来训练。具体来讲，采用了此种预训练的方式，Tacotron能够采用较少的数据，生成较好的语音。尽管我们是采用Tacotron来进行试验的，我们坚信我们的方法在其他的sequence-to-sequence模型中，也应该有效。我们也证实了此种方法在低资源语种上的有效性，这样的话，不需要目标语种的语音，我们的方法能够提供一个显著的效果提升。尽管，我们假设了两种低资源语种，但我们相信此种方法能够泛化到真实的低资源语种。这个结果给单语种和多语种TTS系统增加了曙光。</p>
<p>未来前进方向：可以尝试其他的无监督学习方式；采用小数据量来adaptation neural vocoders也同样需要被调研。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>考古两篇TTS with NN/DNN的开山之作</title>
    <url>/2021/06/21/1st-dnn-tts/</url>
    <content><![CDATA[<h1 id="paper-1-statistical-parametric-speech-synthesis-using-deep-neural-networks------google-heiga-zen-icassp-2013">Paper 1: STATISTICAL PARAMETRIC SPEECH SYNTHESIS USING DEEP NEURAL NETWORKS ---- Google, Heiga Zen (ICASSP 2013)</h1>
<h2 id="httpsstorage.googleapis.compub-tools-public-publication-datapdf40837.pdf">https://storage.googleapis.com/pub-tools-public-publication-data/pdf/40837.pdf</h2>
<h2 id="总结以及感想">总结以及感想</h2>
<p>看了考古文对科研又产生了新的理解，目前的论文大多修修补补，灌水严重，实验部分不是很充分，无法印证论文的可复现性和实验结论的可靠性。这篇论文虽然采用的DNN技术还是最早期的神经网络系统架构，但是对于系统设计的每一个细小的结构都进行了充分的实验对比验证，得到了可靠的实验结论。在结论处也给同行留下了更多想象和探索的空间。强烈建议TTS从业者逐字逐行阅读本文，学习论文构思和写作的思想，并且了解深度学习在TTS领域应用的起源，至少从实验部分的objective /subjective evaluation可以学习到客观评估TTS合成效果的方法，使得自己的TTS研究更加扎实可靠。打分：🌟🌟🌟🌟🌟</p>
<h2 id="abstract">Abstract</h2>
<p>痛点：传统的统计参数语音合成方式通常采用决策树，上下文依赖的HMM模型来表示给定文本的语音的概率密度。其中，语音参数是从概率密度中生成的来最大化它们的输出概率，然后再用生成的语音参数，重构语音波形。这种方法的缺点是，决策树对于建模复杂的上下文依赖关系是比较无效的。</p>
<p>解决方案：本文基于深度神经网络（DNN）。输入文本和声学表示的关系通过一个DNN来建模。DNN的使用能够解决许多传统方法的局限性。</p>
<p>实验结果：DNN效果优于HMM</p>
<h2 id="introduction">Introduction</h2>
<p>基于HMM的参数方法在过去十年间盛行，相对于波形拼接算法来讲，它的优势在于能够灵活地替换音色，小的踪迹和鲁棒性。然而，它的主要局限性是合成语音的质量。Zen等提出了合成质量的三个主要特征：声码器，声学模型的准确度，和过拟合。本文的方法主要在于解决声学模型的准确度。</p>
<p>影响语音的一定数量的上下文特征包含音素、语言和语法特征，会在统计参数合成过程中，参与建模。典型系统包含50种上下文。因此，这些复杂的上下文依赖的有效建模是统计参数语音合成的关键点。为了解决这种上下文问题的标准方法是基于HMM的统计参数语音合成算法。对于每一种独立的上下文组合，都采用一个独立的HMM模型，作为一个上下文相关的HMM模型。通常来讲，对于这种全部上下文依赖的HMM模型来说，训练数据是不充分的，难以学习到一个稳定的模型，能够覆盖到所有所需的上下文组合。</p>
<p>为了解决这种问题，基于自上而下的决策树算法的上下文聚类被广泛使用。在这种方法中，上下文-依赖的HMM的状态被区分为多个“簇”，并且每个“簇”的分布参数是共享的。HMM模型的任务是通过二分类决策树，检验每一个HMM的上下文组合，其中一个上下文相关的二分类问题是与每一个 非叶子结点 相关的。“簇”的数量，也是 叶子结点 的数量，决定了模型的复杂程度。决策树通过序列化挑选能够在训练数据集上产生最高mle的分数来挑选问题。树的大小是通过一个预定义的mle阈值，一个模型复杂度惩罚，以及交叉验证来决定的。采用了上下文相关的问题和状态参数共享后，未知的上下文和数据稀疏性问题得到了有效解决。就像在语音识别中所成功解决的，基于HMM的方法自然地对于有丰富数据的上下文有较好的效果。</p>
<p>尽管基于上下文决策树的HMM模型在统计参数语音合成方法中是有效的。但是，有以下局限性：</p>
<ul>
<li>对于复杂上下文依赖如“XOR”，奇偶校验和复用问题，这种方法是无效的；</li>
<li>这种方法将输入的空间区分开，并且对于每个区域都采用了独立的参数，每个区域对应着一个决策树的叶子结点。这导致了分裂训练数据集，并且在聚类和估计分布的时候，使得每个簇的数据不充分。</li>
</ul>
<p>有一个相对大的决策树，并且分裂训练数据集都会导致过拟合，损害合成语音的质量。</p>
<p>为了解决上述局限性，本文采用基于DNN的结构。上述基于决策树的方案，建模了从 文本中抽取的语言上下文到语音参数的映射。在这里的决策树被一个DNN模型所替代。值得注意的是，自从90年代开始，NN就尝试被应用于TTS中。</p>
<h2 id="dnn-vs-decision-tree-dt">DNN VS Decision tree (DT)</h2>
<ul>
<li>DT 在表达输入特征的复杂关系时无效，如XOR、d 位奇偶校验函数、或者多路复用的问题。为了表达上述情境下的问题，决策树可能会十分巨大。然而，这些关系能够被DNN模型来具象表示</li>
<li>决策树致力于分割输入空间，对每个空间采用一组独立的参数和一个叶子结点。这样会导致在每个区域的数据数量少和较差的泛化性能。Yu et al 证明了在采用决策树建模时，一些较弱的输入特征如语音中词级别的重读会被丢失。由于DNN的权重是从整体的训练数据得到的，所以DNN会得到更好的泛化性能。DNN也提供了输入高维、多种输入特征的可能性。</li>
<li>相较于决策树，通过反向传播来训练一个DNN模型通常需要大量的计算过程。在预测过程中，DNN需要在每一层都有一个矩阵乘法，但是决策树仅仅需要通过一个输入特征的子集从根结点遍历树直至叶子结点。</li>
<li>决策树的推理是更加可解释的，DNN中的权重很难在直观上获得解释。</li>
</ul>
<h2 id="基于dnn的语音合成">基于DNN的语音合成</h2>
<p>由于人类的发声系统是多层级的，才能够将文本信息转换为语音波形，所以本文尝试采用深度神经网络来进行语音建模。</p>
<div class="figure">
<img src="/images/DNN-based-tts.png" alt="DNN-based-tts" />
<p class="caption">DNN-based-tts</p>
</div>
<p>上图展示了一个基于DNN的语音合成框架。输入文本首先被转换为输入特征序列<span class="math inline">\(\{x^t_n\}\)</span>，其中的<span class="math inline">\(x^t_n\)</span>表示在第<span class="math inline">\(t\)</span>帧的第<span class="math inline">\(n\)</span>维输入特征。输入的特征是对于文本上下文关系的二分类问题，包含如（e.g is-current-phoneme-aa?）和数值（e.g. 在短语中的单词数量，在当前音素序列的当前帧的相对位置，和当前音素的发音时长）</p>
<p>然后输入特征通过一个训练好的DNN，采用前向传播的方法被映射到输出特征<span class="math inline">\(\{y^t_m\}\)</span>，其中<span class="math inline">\(y^t_m\)</span>表示在第<span class="math inline">\(t\)</span>帧的第<span class="math inline">\(m\)</span>个输出特征。输出特征包含频谱和激励参数以及他们的时间导数（动态特征）。DNN的权重能够采用从训练数据集中抽取的成对的输入和输出特征来进行训练。类似于HMM的方法，这样是可以生成语音参数的。通过从DNN中设定预测的输出特征作为均值向量，再加上从所有训练数据预先计算的方差作为协方差矩阵，语音参数的生成算法能够生成平滑的语音参数特征轨迹，满足了静态和动态特征的统计情况。最终，一个语音合成模块通过得到的语音参数来生成语音波形。</p>
<p>注意到，文本分析，语音参数生成，和波形生成模块可以与HMM模型共享，<strong>即仅仅从上下文依赖关系的标签生成统计参数的过程需要被替换。</strong></p>
<h2 id="experiments">Experiments</h2>
<h3 id="实验条件">4.1 实验条件</h3>
<p>实验数据：US-EN 女性语音数据，约33000条。语音分析的条件和方法论类似于Nitech-HTS2005系统的方法。语音数据首先从48KHz降采样到16KHz，然后每5ms抽取一次40维的Mel倒谱系数，<span class="math inline">\(log F_0\)</span>，和5段非周期性系数。每一个观察向量包含40维的mel倒谱系数，<span class="math inline">\(log F_0\)</span>，和5段非周期性系数，以及他们的delta和delta-delta特征。从左至右，包含5个状态的无跳过隐藏半马尔可夫模型 (HSMM)被采用。为了建模 <span class="math inline">\(log F_0\)</span>序列包含了声学和非声学观察序列。一个多空间的密度分布被使用（multi-space probability distribution (MSD)）。基于决策树的上下文聚类的问题数量是2554个。在HMM系统中的决策树大小是通过改变模型复杂度惩罚因子<span class="math inline">\(\alpha\)</span>来控制的（最小描述长度标准（MDL）是（<span class="math inline">\(\alpha=16,8,4,2,1,0.5,0.375,or 0.25\)</span>）。当<span class="math inline">\(\alpha=1\)</span>时，Mel频谱，<span class="math inline">\(log F_0\)</span>和频带非周期性的叶子结点的数量分别是12342, 26209, 和401（总共有3209991个参数）。</p>
<p>基于DNN系统的输入特征包含表征类别语言上下文（例如音素身份、重音标记）的342个二分类特征和表征数字语言上下文（例如，单词中的音节数、短语中当前音节的位置）的25个数值特征。除去文本上下文相关的输入特征，还包含了3个用于粗略编码当前音素序列中当前帧位置的数值特征，以及一个用于估计当前音节时长的数值特征。输出特征与HMM系统基本一致。为了通过DNN模型建模<span class="math inline">\(log F_0\)</span>序列，我们采用了显式发声建模（explicit voicing modeling）的方法来获取连续<span class="math inline">\(F_0\)</span> ，发声/不发声的二分类特征值被用于添加到输出特征，并且在不发声值中的 <span class="math inline">\(log F_0\)</span> 通过插值得到。为了降低计算成本，80%的静音段从训练数据中移除。DNN的权重被随机初始化，然后在最小化MSE的目标函数下得到最优化。优化策略为基于小批次的随机梯度下降（SGD）的后向传播算法。DNN的输入和输出特征均被正则化，其中输入特征被正则化至(0,1)分布，然后输出特征根据训练数据中的最大最小值被正则化至0.01-0.99隐藏层采用sigmoid激活函数。建模频谱和激励特征参数的DNN神经网络被训练。</p>
<p>在评估的语句中，语音参数通过语音参数生成算法被生成。在倒谱域采用了基于后过滤的频谱增强算法。语音波形通过source-filter模型来重构语音波形。</p>
<p><strong>为了客观评估HMM和DNN模型系统，MCD（mel-cepstral distortion）(dB)，Linear aperiodicity distortion (dB), 发声/不发声错误率（%），和<span class="math inline">\(log F_0\)</span>的RMSE被使用。</strong>音素发音时长在后面被使用，我们挑选了173句训练集外的语句用于模型评估。</p>
<h3 id="客观评估">4.2 客观评估</h3>
<div class="figure">
<img src="/images/5th-mcep-comparison.png" alt="5th-mcep-comparison" />
<p class="caption">5th-mcep-comparison</p>
</div>
<p>上图绘制了Ground-Truth、HMM预测值和DNN预测值的第五个mel倒谱系数，从图中可以观察到，三个模型都可以产生合理的语音参数轨迹。</p>
<p>在客观评估中，我们调查了预测结果和DNN结构（1，2，3，4，5层）的关系，以及与每层神经元个数（256，512，1024，2048）的关系，下图展示了实验结果。</p>
<div class="figure">
<img src="/images/dnn-tts-exp-res.png" alt="dnn-tts-exp-res" />
<p class="caption">dnn-tts-exp-res</p>
</div>
<p>基于DNN的系统一直都比HMM系统要好在 &quot;voiced/unvoiced error rate&quot;和&quot;aperiodicity prediction&quot;。在MCD中，有多层的DNN模型要相似于或者好于HMM模型。然而，在<span class="math inline">\(log F_0\)</span>的预测中，HMM在大多数情况下要好于DNN模型。其中，所有的不发音帧都被插值作为发音帧来建模。我们认为这种方法会降低<span class="math inline">\(log F_0\)</span>的预测效果，因为这些插值的<span class="math inline">\(F_0\)</span>对于DNN模型来说是一个bias。对于MCD和aperiodicity预测中，模型深度的提升比在每一层上增加神经元的个数更加有效。</p>
<p>以上的客观指标并不能评估合成语音的自然度，但是可以作为评估声学模型准确率的指标。</p>
<h3 id="主观评估">4.3 主观评估</h3>
<p>173句话被评估，每个评估人最多评估30句话，这些话术是随机打乱的。每一对语音被5个人评估。评估人有带耳机。在听完一对语音后，评估人需要选择一个更喜欢的语音，如果觉得两个语音很相似的话，可以选择“中立”，在这个评估过程中，HMM系统和DNN系统采用相似的模型数量来被评估。DNN模型采用了4个隐藏层，神经元的个数也进行了多组实验（256，512，1024个神经元）</p>
<div class="figure">
<img src="/images/dnn-tts-mos.png" alt="dnn-tts-mos" />
<p class="caption">dnn-tts-mos</p>
</div>
<p>上表展示了实验结果，可以从以上结果看出，在相似的参数数量配置的情况下，DNN模型要远优于HMM模型。我们认为较好的MCD代表了更佳的效果。</p>
<h2 id="结论">结论</h2>
<p>这篇文章examined the use of the DNNs to perform speech synthesis. DNN模型有潜力解决传统DT-HMM模型的局限性。主观评估和客观评估均显示DNN能够实现较好的效果。HMM的一个优势在于模型参数较少，计算开销较小。在合成时，HMM模型便利决策树来找到每一个状态的参数。然而，本文提出的DNN算法是在每一帧进行输入到输出的预测，接下来的工作可以在如何降低DNN模型的计算开销，添加更多的输入特征包括一些弱特征如重读，并且可以探索如果获得一个更好的 <span class="math inline">\(log F_0\)</span>建模方案。</p>
<h1 id="paper-2-speech-synthesis-with-neural-networks------motorola-orhan-karaali-1996-sep-world-congress-on-neural-networks-invited-paper">Paper 2: Speech Synthesis with Neural Networks ---- Motorola, Orhan Karaali (1996, Sep, World Congress on Neural Networks Invited Paper)</h1>
<h2 id="httpsarxiv.orgpdfcs9811031.pdf">https://arxiv.org/pdf/cs/9811031.pdf</h2>
<h2 id="感想与总结">感想与总结</h2>
<p>这篇1996年的文章算是nn-tts的创世之作，文章采用了一个duration mode来预测音素的发音时长，以及一个phonetic network来预测每一个音素的声学特征，这个idea让我不由得想到当前的如Fastspeech等与这个想法如出一辙，同样的也是需要预测duration和音素的声学特征，但本文的行文思路尤其是实验部分，感觉没有上一篇论文更加翔实充分，所以打分的话我会给：🌟🌟🌟🌟</p>
<h2 id="abstact">Abstact</h2>
<p>传统的文本-语音转换通过拼接短的语音单元或者采用基于规则的系统来将语音的音素表示转换为声学表示形式，然后被转换为语音。本文描述了一种采用时延神经网络（time-delay neural network TDNN）的方案来进行音素-声学特征的建模，不需要额外的神经网络来控制生成语音的timing。这个神经网络系统相较于拼接算法可以降低对于系统存储资源的需求，对比于其他的商业系统表现良好。</p>
<h2 id="introduction-1">Introduction</h2>
<h3 id="description-of-problem">1.1 Description of Problem</h3>
<p>文语转换通常包含了先将文本转换为语音参数，再将语音参数转换为语音波形。计算机交互可以采用对话沟通交互方式，也可以配置到移动端。拼接系统首先制作拼接数据库，然后再拼接时，调整音素的发音时长，平滑转接点来生成语音参数。拼接系统的主要问题是存储成本高昂。基于规则的合成方法将每一种可能的音素表示存储好目标声学参数。然后根据衔接点的情况来根据规则选择语音参数，主要问题是：拼接点不自然，因为转接规则倾向于只生成少量的转接风格。另外，大量的转接规则需要进行存储，会造成合成的机械音。</p>
<h3 id="discussion-of-the-use-of-networks">1.2 Discussion of the use of networks</h3>
<ul>
<li><p>先前提到的两种方法都是语言-依赖的，而NN方法是语言-不相关的；</p></li>
<li><p>拼接系统的高昂存储成本导致了难以配置到移动端，而NN通过生成具象的表示方式，能够降低拼接系统的冗余性；</p></li>
</ul>
<p>下图展示了TTS的流程图（终于找到了现有的TTS流程设计的出处）</p>
<div class="figure">
<img src="/images/tts-diagram.png" alt="tts-diagram" />
<p class="caption">tts-diagram</p>
</div>
<h2 id="系统描述">系统描述</h2>
<h3 id="数据库">2.1 数据库</h3>
<p>38岁男性，居住在Florida和Chicago。录制时采用了类似于近距离麦克风的DAT录制起。文本包含480个音素均衡的语句，是从Harvard sentence list中筛选出来的。除此之外，160句在其他情境下的话术也被录制了下来。录音音频通过数字方式转移到计算机，每句话对应一个音频。每句话被归一化，以使得每句话都在非静音段有相同的平均信号能量。文本信息被标记为音素、节奏和音调信息。</p>
<p>标记方式采用了类似于TIMIT数据库的标记方式（是ARPABET的变种），停止标记为关闭和释放作为单独的音素。这使得模型能够有效预测到停顿，以及开始。精准的对齐对于帧级的损失函数是有效的。</p>
<p>音素不是唯一的输入特征，音素时长，F0曲线（通常被音节重读和语法边界影响）。语法边界（syntactic labelling）标记了音节，词，短语，从句和句子的开始和结束时间。语法重读（lexical stress：primary, secondary, or none）被应用到词汇的每一个字母中。词性(function word (article, pronoun, conjunction or preposition) or content word)；每个词语都有一个层级(level)，基于生成F0的rule-based系统。</p>
<p>尽管语法（syntactic）和重读（lexical stress）对于语音的韵律变化很重要，但是这些信息没有完全决定了这些韵律变化。<strong>说话者对于语句的重读可能取决于句子中的对比度，比如在遇到陌生词汇的时候，可能会不自觉的重读。</strong>因此标记如此音调重读的实际位置到字幕上，或者词语间的强对比性是有效的。在英文中的标准是ToBI（Tone and Break Index）系统。</p>
<h3 id="从音素表示上生成片段时长">2.2 从音素表示上生成片段时长</h3>
<p>神经网络的两个任务之一是去决策，从音素顺序和语法和韵律信息上，每一个音素的发音时长。</p>
<p>网络的输入大多数采用二分类数值，分类数值代表了采用1-out-of-n codes和一些通过bar codes表示的小的整数值。表示音素片段的输入数据包含音素片段，它的发音特点，字幕凸起的描述和包含片段的词语，以及片段接近的任何语法边界。网络结构被训练来生成时长的log。</p>
<p>时长预测网络结构如图2所示，网络有两个输入值（2和3），通过I/O block 1 和 2 输入。（Stream 2 包含了通过shift register来给一个音素提供上下文描述），stream 3 包含了仅仅用于一个特定音素时长的生成过程。当神经网络被用于生成时长的过程中，I/O block 6写入输出的数据流。在训练过程中，Block 6 读取目标值并且生成error value。Block 3、4和5是单层的神经网络模块，模块7、8和recurrent buffer控制了循环生成的机制。</p>
<div class="figure">
<img src="/images/duration-prediction.png" alt="duration-prediction" />
<p class="caption">duration-prediction</p>
</div>
<h3 id="从音素和时长信息生成声学信息">2.3 从音素和时长信息生成声学信息</h3>
<p>系统中使用的第二个神经网络从音素、语法和时长信息来生成语音参数信息。更精准地来说，网络从一个帧级的音素上下文信息生成语音10-ms帧的声学表示。</p>
<h4 id="网络输出----coder">2.3.1 网络输出 -- Coder</h4>
<p>神经网络不会直接生成语音，这个的计算资源十分昂贵，并且不太可能生成好的结果。该网络为声码器的分析-合成风格的合成部分生成数据帧。许多语音编码的研究致力于数据压缩的问题；然而，神经网络对于coder的需求没有被大多数的数据压缩技术所满足。具体来讲，将语音编码成每帧的数值向量是有价值的，这样的话，向量的每个元素对于每一帧都会有一个定义好的数值，因此用于训练的神经网络的错误度量是合适的。（例如，如果神经网络生成向量，并且错误度量相对于训练向量是较小的话，生成语音的质量，即通过running these vectors通过coder的合成部分的话，将得到较好的语音质量。）加权Euclidean距离被用作error criterion使得coder没有使用二分类输出值是明智的，并且根据其他的向量元素，向量元素的含义没有改变。</p>
<p>coder是LPC声码器的形式，采用线性频谱（line spectral frequencies）来表示filter coefficients和一个2-band的激励模型（不同的filter coefficients的表示形式被测试，模型对于线性频谱表现良好）。2-band激励模型是一个multi-band激励模型的变种，包含一个低频带的voiced band，和一个高频带的unvoiced band。两个bands之间的边界是coder之一的参数。F0 和 power of the voice signam是剩下的参数。F0在不发音的帧级，被插值为一个高频的数值。</p>
<h4 id="网络输入">2.3.2 网络输入</h4>
<p>音素网络的输入包含了时长网络的所有输入，和时长网络输出的timing information。网络采用了一定数量的不同的输入coding技术。blocks 5，6，20和21采用了300毫秒的TDNN的风格输入窗口。窗口的采样不是均匀的，最优的采样区间是通过分析神经网络从TDNN窗口的不同部分来决策神经网络对信息的使用。Blocks 6 和20 处理了一组与输入音素相关的特征，Blocks 7和8为音素和语法边界编码时长和距离信息。网络的输入数据是二分类数据的混合，1-out-of-codes和bar codes</p>
<h4 id="网络结构">2.3.3 网络结构</h4>
<p>决定好的网络结构需要大量的实验，也就需要大量的计算资源，然而本课题的复杂程度和数据集的大小使得训练时间成为了主要瓶颈。因此，一个 in house neural network simulator被开发来降低训练时间（多个月-&gt;几天），并且可以同时验证多个方法。一些神经网络的技术和理论通过这种方式被pass掉了。</p>
<p>最终的网络结构整合了TDNN、recurrent、和modular网络，和一些实验过程中演化的技巧。下图是当前方法的图示，其中六边形模块是I/O或者用户写入的子程序，方块是神经网络的模块。神经网络的模块采用后向传播来训练。网络的模块化是通过专家知识来手动调整的。</p>
<p>网络通过逐渐降低的学习率和momentum方法来训练，以一种新型的顺序和随机混合的训练模式来训练。训练的网络需要&lt;100 Kilobyters of 8-bit quantized weights ，对比于拼接算法，得到了显著降低。</p>
<div class="figure">
<img src="/images/phonetic-network.png" alt="phonetic-network" />
<p class="caption">phonetic-network</p>
</div>
<h2 id="系统效果">系统效果</h2>
<h3 id="语音质量和自然度">3.1 语音质量和自然度</h3>
<div class="figure">
<img src="/images/tts-nn-exp-results.png" alt="tts-nn-exp-results" />
<p class="caption">tts-nn-exp-results</p>
</div>
<p>上图展示了GT语音频谱和系统生成的语音频谱（生成的频谱没有采用ToBI标注系统）。为了对比更加清晰，有两种合成语音频谱被展示出来。第一种，phonetic 特征是网络预测的，而duration是真实的，为了仅仅展示phonetic network的效果。第二种，duration和phonetic都是预测的。对比实验发现，在语音接受度（Acceptability）上，本方法生成的质量远好于其他的系统。在片段的拟人度方面（Segmental Intelligibility），本方法仍旧有提升空间，而本次试验中的较差的数据可能是由于缺少单字语音样本所导致的。</p>
<div class="figure">
<img src="/images/tts-nn-table1.png" alt="tts-nn-table1" />
<p class="caption">tts-nn-table1</p>
</div>
<h3 id="实时合成">实时合成</h3>
<p>最开始模型是在Sun SPARCstation平台来通过ANSI C语言实现的。最近这个被插入到Power Macintosh 8500/120，PowerPC快速的乘法和加法使得合成器能够实时合成。</p>
<h2 id="结论-1">结论</h2>
<p>本方法从Acceptability角度来看，是优于传统算法的，但是仍旧可以有一些提升，如数据库可以扩充，来获得更多的音调变化，包含更多的音素上下文特征，数据库可以包含更多的短语，单字，和长段的语句。在coder、network architecture、和训练方法上也可以做出一些提升。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>金载勋跆拳道- 2020年总结</title>
    <url>/2020/12/31/2020-jhk/</url>
    <content><![CDATA[<p>被两个极喜欢拖着我一起立flag折腾自己的狐朋狗友又一次立了一个flag，本想做一个视频，想到社会蓝的故事编辑技能树需要发光发亮，那我也来记录下这一年多的金载勋生涯吧。</p>
<p>2010年的暑假是我第一次接触tkd并深爱上跆拳道，似乎是2012年初二的暑假结束了跆拳道课程，止步于蓝带。印象中结束的最后一节课只有我一个学员上课，在我十分敬佩的葛教练指导下进行十分难得的1v1教学，上的课是当年我最爱的反应踢（教练随意出腿法脚靶，然后迅速反应）。当年留有贝克汉姆发型的我在葛教练的逼迫和鼓励下，顺利学会了俯卧撑、握拳俯卧撑和三指俯卧撑，也在一次次地习练中打开了横竖叉任督二脉。那几年的我，似乎达到了身体素质的局部巅峰。也是这种一次次突破的感觉，让我深爱习练tkd，深信自己也会成为一名优秀的跆拳道选手。然而，同伴的退出和教练的不停更换和面临初三中考的压力，我最后也选择了退出。尽管早已偷偷学会太极七章，尽管再没有机会弥补上痛失金牌的愿望，最终仍止步于蓝带。</p>
<p>中考后的我，还曾经和朋友一起回到道馆，尝试找回曾经的回忆，可笨重的自己和课堂上灵活的小朋友们一起上课后的感受只有“没有对比，就没有伤害”。即便还是曾经的那个道馆，年龄的增长还是劝退了我。</p>
<p>本想大学过后找到tkd社团就继续完成我的黑带梦想，可itf的发声方式，教练要改口为师范，以及从头再来的打击又一次将我劝退，充满棱角的我选择不愿意变更流派。虽然大学四年间每次路过道馆都会观察是不是wtf，有没有成人训练。可奈何都没有合适之选。我也逐渐转移了注意力到其他的运动上，从连续一个月每日中午晚上的舞狮训练上又找回了那种走火入魔的快乐。</p>
<p>直到2019年，工作上稍微稳定些的我继续开始寻觅自己的黑带梦想能够在哪里得以实现。在对比了解了不下十家道馆的我，被国际化的金载勋道馆所吸引了，于是预约了第一次的体验课。</p>
<p>尽管初次体验迟到了大概十五分钟，但耐心又漂亮温柔的前台小姐姐，1对1的体验，小阁楼式的装修，和隔板间的换衣房让我对这国际化道馆的好感印象不断上升。柴师范是我的试课师范，尽管距离当年的意气风发已久隔近十年，但当年对跆拳道的热爱使我还没有忘记腿法的基本动作，已学过的腿法都能够顺利完成。至于拳法的话，对于wtf的我就是一团糟了，只能在师范的指导下尝试模仿。</p>
<p>课上师范的鼓励和表扬和更加专业体系化的指导让我似乎找回了当年的快乐，也对这里的课程逐渐产生了信任。课后与丁师范近一个小时的聊天也让我在残酷的社会和异地漂泊的寒冷中找到了一丝温暖。唯一的顾虑还是itf or wtf。</p>
<p>我记得我有问是哪一种，dsf的回答十分巧妙而又深刻，两种都不是，但两种都会学，并且作为崔洪熙将军直系弟子的金载勋师范传授下来的课程，与市面上更流行普适的跆拳道课程不同，是更加地道和专业的。</p>
<p>另外印象深刻的一点是，我说为什么以前的课程都是一个半小时，现在一个小时感觉时间太短了，无法得到充分的练习。丁师范的回答十分坦诚，说“我们也不希望课程里面穿插很多无用的热身活动，比如跑步就跑了半个小时，如果排成一个半小时，那需要排的课更少了，对我们来说反而是轻松了。大家时间都很宝贵，我们还是希望能在课程里教给学员们更多的专业技能”。这一番言论在dsf绵绵细语下格外真诚，让我逐渐放下了心中对常规销售套路的防备心。</p>
<p>介意从头再来的我仍然问了那个问题，是否可以从以前的级别继续习练。dsf对此也是包容和支持，希望腰间的带色能够勉励我们自己不停努力，加紧训练，尽快追赶上自己腰间的色带。最终，csf对我天赋的鼓励和dsf对道馆体系化教学的讲解双面夹击，我放下了对教练称呼和训练体系改变的排斥，当场报名了半年的课程，希望能一展宏图。</p>
<p>然而，后面一个月内的课程大多是不习惯，不习惯课前没有充分的热身就开始进行腿法练习，不习惯每一次课前热身动作的重复无趣，不习惯不练习俯卧撑，不习惯打拳的时候抬手准备，不习惯做动作的时候要垫脚，不习惯要自己摸索动作结束的标准位置，不习惯很多腿法的动作细节，不习惯分散练习时无人指导的茫然，不习惯韧带没有拉开仍需要勉强完成一些大幅度的腿法动作。这一个月的习练甚至让我开始怀疑自己报名的冲动，最难受的是怀疑自己是不是并不适合这项运动，乃至开始咨询深圳内的其他成人道馆。</p>
<p>这一切直至遇到成长背景都十分相似，同龄又同样有执念的黑带胡哲才算得到一些开解。胡哲是我在跆拳道生涯中遇到过最让我敬佩的学员，尽管已经身为黑带还是在课后刻苦习练，尽管大学时候查有膝盖积水，仍旧在朗朗诗声中咬牙完成自己的横叉梦想，即便动作都已经非常熟练还是一遍又一遍的熟悉，同为wtf转方向，仍旧能够耐下心来完成新的体系的学习。这一切令我感到有些找回当年热血的感觉。课后我开始向她讨教腿法，品势的细节动作，曾经也是wtf的她帮助我开始逐渐熟悉了金载勋的训练体系。</p>
<p>说来很神奇，陌生人面前比较孤冷的我见到她之后话匣子似乎被打开了，她给我讲述大学中的一件件训练故事，分享给我大学的训练日记，文武双全的她让我开始感到崇拜。有她在的每一节课变得不再那么的孤单和艰辛，她不在的日子我都似乎又关闭了自己的说话功能，选择不去接触更多的人，把这一份热情和温暖唯独留给我欣赏的人。</p>
<p>除了同伴的收获，还有一次是杨师范的训练课也令我印象深刻。丁师范的课程还是比较偏向金载勋的训练体系，我还没有从幼时游戏式的热身，更多体能、全方面身体素质方面训练的热身中走出来，尤其在分散练习独自一人无人指导，也无熟悉同伴共同训练的时候倍感孤单，甚至觉得这样好似报名了一个健身房，得不到一些及时的指导和团队协作和竞争上的训练。可是ysf的课程是让排成两队，做一些交叉步，青蛙跳等素质练习的动作，逐渐让我找回了舒适圈，一些动作也得到了杨师范的认可让我逐渐找回了自信。课上的腿法练习中，杨师范总是可以观察一会，很准确的找出动作的问题所在，并给出改进方案，在尝试了师范的方式后，自我感觉也得到了显著的提升，那种成就感令我找回了一些快乐。那节课是我一个月以来最快乐的一次，从那以后开始甚至有点期待杨师范的课程。然而半年来都没有期盼到。</p>
<p>所幸的是，胡哲也与杨师范交情甚好，课后总是会在杨师范的指导下进行1对1柔韧度练习。我深知柔韧度对跆拳道训练的重要性，柔韧度很大程度上决定了腿法动作的幅度、标准度和灵活度，对挑战横叉的胡哲我眼中尽是钦佩和羡慕，所以选择留下来帮助胡哲按住一条腿。可长久以来都无法鼓起勇气说自己也想要完成这个柔韧度的训练。最终，选择报名了一个舞蹈软开度课程。课程结束也没有达到一些显著的提升。</p>
<p>在胡哲的横叉飞速进步中，很快就迎接来了过年，随着本命年的到来，居然是动荡全球的疫情风云。很早回到深圳的我还是不能够回到道馆继续训练。在家里的一个月时间中，我时不时会跟着keep进行一些简单的软开度训练，但都是2周没有见到效果就放弃了。</p>
<p>疫情期间一次公司聚餐中，意外接到了熟悉的dsf打来的电话，听到熟悉而亲切的声音的我激动但却不得不继续公司里的工作，因此和dsf解释说是否可以晚点联系。聚餐后匆忙给dsf打回去的我，打过去却是金载勋的总机电话，那一天时间里似乎都在期盼着dsf再次拨回来。又一次接到电话的我赶忙问何时可以开课，已经迫不及待了，dsf说会随政府安排，尽快，但是叮嘱说在家也要勤加练习哦。真是一个有爱有人情味的道馆。</p>
<p>终于熬到了四月底，又可以回到道馆了。之后的一个月就认识了肖宝杰小妹妹。对bj的印象是，总会有一个男孩子在楼上等着她😂好像不是很多话。</p>
<p>后面就迎来了开新的分馆，一直打探分馆主教练的我其实内心早就在想这样是不是就可以上更多的杨师范的课了，说不定还可以找杨师范帮忙拉开韧带。心里想着，似乎黑带梦想又重燃了希望。</p>
<p>分馆如期开张，杨师范也如期升职为分馆主教练，更开心的是，本舍不得在杨师范和丁师范中做个选择，海岸城的课程居然是两位主教练轮换教课，这也解除了我的一部分顾虑，不会因为总是上一个师范的课而感到略显重复枯燥。</p>
<p>宝杰也转到了分馆，并且在我第一次去的时候，发现宝杰也开始了自己的横叉磨练。杨师范似乎从我眼中看出了我的心思，便问我要不要也一起来，终于稍微鼓起勇气的我开始尝试。也在杨师范的“狠绝”下第二天就达到了一个小目标。</p>
<p>于是就这样开始了两个人的横叉之旅。印象很深的是一次我已经满头大汗了，杨师范还是心里一坚持，又加了一些强度，那一刻感觉两边的腿内侧的筋似乎开始燃烧了，然而神奇的是，从那以后两侧的筋就再也不痛了。但是，让我略生退却心思的是，有时候拉完韧带回到家，会导致前半夜腿痛，影响睡眠，甚至一次需要敷上止痛膏药才可以稍微缓解一些这种疼痛，并且有时候的腰痛也开始让我怀疑自己这把老骨头是不是回笼胯，是不是天生就是这么硬，因为小的时候也是竖叉很快就拉开了，横叉花费了多一倍的时间才拉开。</p>
<p>尽管心生疑虑，但是还是忍住不敢讲出口，怕师范会过于担心我的身体，我就更加放弃自己了。</p>
<p>这其中，印象很深的一次是，那次是隔了四五天没有压韧带，我心里很害怕退步了，太过紧张所以导致肌肉紧绷，大师兄也在一旁说“世上无难事，只要肯放弃”，丁师范也在一旁劝我不要勉强，最后就选择放弃了。回家路上我很严肃，责怪自己，又不知道怎样开解自己。只是心里充斥了恨铁不成钢。第二天都没有颜面去上杨师范的课了，感觉似乎辜负了师范。然而还是厚着脸皮去上了晚课，那一天，又到了一天一度的“地狱”时刻了，我心里仍旧是害怕，身体上还是诚实地去多热了热身，希望这样能缓解一些。宝杰已经完成了自己的每日任务，昨天就放弃的我，开始犹豫。杨师范给我喊了过去，一只很有力气的脚推在我的小腿上，说“奥兰你今天就别想跑了”。听了这话的我心里也想，不能再辜负师范的努力和期望了，眼镜一扔，眼睛一闭，就想豁出去了，一定要完成。哎，很神奇，很快就到达了目标了啊，没有想象中的那么痛啊？甚至比一周前还要好很多？？那天我真的十分感激杨师范帮我克服心中的魔鬼，终究是没有中断和放弃。</p>
<p>后面坚持和宝杰一起开横叉的时光艰辛而又互相勉励。柔韧度进步后在一些腿法上感觉更加收放自如，自我都感觉来到海岸城的这段时间实则进步了一大截。终于迎来了红蓝带考试，我也很骄傲的到后海馆给许久未见的其他同学和老师们看在杨师范指导半年下的飞速进步。</p>
<p>可惜后面宝杰韧带拉伤，我自己虽然心里不想放弃，但是也找不到主动去找杨师范的理由，所以也就随着宝杰的韧带拉伤中止了每天的撕心裂肺。</p>
<p>下半年的击破表演和红带考试是2020后半年的重点，两次都有各自的遗憾，但我似乎也开始慢慢学会开解自己的遗憾，不再懊恼和自责，而且在训练中多加补足，争取下一次能弥补遗憾。下半年的周六也增加了更多的体能训练，自我感觉在体能上也得到了长足进步，不再那么的担心考场中体力不支而无法呈现出最好的表现了。</p>
<p>宝杰应该是我跆拳道生涯里第二敬佩的学员。半年来的习练无论是在带色还是技术上的飞速进步是她每日坚持的成果，多次的韧带拉伤也没有打消她的韧带梦想，再加上实战时候的勇敢和坚定，生活中的不服输和挑战自我都让我这个废柴似乎又找回了一点年轻时的热血。</p>
<p>说到实战，胡姐姐头脑清晰，技术全面，实战经验丰富却仍谦虚勤练，宝杰呢，勇猛坚定，时机紧握，不畏级别高低，实战场上的自信令哪怕是较她级别高许多的学员都心生几分畏惧。这两个人不仅能打，还好打，每逢周六晚上都已经不想去练习枯燥的基本动作了，直接是一通暴打。红带时没有实战压力的我丝毫不想卷入这两个强者之间的战争。然而考完红带后，考试的压力让我不得不去面对自己畏惧的对抗互捶。</p>
<p>第一次参与其中的我还是一团混乱，找不到节奏，除了打不着别人就是被别人狂打头，僵硬的颈椎都放松了好几分。但是鼓起勇气的我在两个强者的陪伴下，再加上有事没事就开始在视频上学习对打技巧，几次习练后似乎找到了一些门道，克服了心中的恐惧，找到了适合自己的一些技巧，在和丁师范对打时丁师范也夸奖到进步了。</p>
<p>另一件让我感到十分温暖的事情是，在考黑红带前大约半个月的时间，偶然问到师范多久可以考黑带，当时大概是10月底，师范说，大概明年2月考黑红带，明年10月考黑带，当时已经学完考试内容的我略感疑惑，感觉自己每日的坚持没有了动力，习惯了临时抱佛脚的我觉得那为何不等快考试的几个月再加紧习练呢。于是是一个月的停练和心中放弃的魔鬼又跑了出来。偶然间和宝杰胡哲提到心中想法后，两位同伴一个苦口婆心的劝导，另一个还特地打来电话慰问。甚至二位还帮我将内心说不出口的想法讲给了师范，看看有没有解决办法。于是乎，师范通知，可以尽快准备考试了。说来也为自己的功利感到羞愧，本身想要踏实完成黑带考试，成为一名优秀黑带选手的我在面对选择的时候，还是选择了更加功利的早些得到黑带。但我很感激同伴的热情帮我争取这次机会，我也很感激师范的信任让我能够拥有这一次机会，因此也是倍加珍惜，尽量争取能够多加习练，争取能够多多出勤，不辜负同伴和师范的期望。也在持证教练肖教练的帮助下逐步跟上了品势内容。说到这里，不得不cue，肖教练真的是有教练的天赋，恩威并施，耐心又严厉，我猜她如果有此志向的话，一定会成为优秀的女教练。</p>
<p>世上无难事，只怕有心人。一句俗语，可也一直在陪伴我的成人跆拳道生涯。好汉不提当年勇，年轻的灵活敏捷早已该放到过去，对现实的困难发起挑战是要从几位师范、同伴身上学习的，“礼义廉耻，忍耐克己，百折不屈”曾经陪伴过我俯卧撑从0-n的突破，陪伴过我冬季3000m长跑的咬牙坚持，和许多运动生涯的艰辛瞬间。年长的我虽然心智在逐渐成熟，但对跆拳道的意志力却与幼时的自己不增反减。很感谢遇到的两位鸡血同伴挽救了一个废柴跆拳道选手，很感谢这一个又一个被迫立下的flag目前还没有轰然倒塌，很感谢我们即便面对质疑与世俗的不解，仍然坚持在自己热爱的道路上。</p>
<p>对成年后仍在这条道路的我，不期待三天打鱼，两天晒网的短暂热情，期待能够习得自律的习惯和稳步进步，期待能够像几位师范和同伴一样，热情不减，勇敢更多。许愿自己2021flag不倒，心愿达成。祝愿师范事业继续蒸蒸日上，生活温馨顺利。祝愿两位伙伴不忘初心，永远热忱。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>EfficientTTS An Efficient and High-Quality Text-to-Speech Architecture</title>
    <url>/2021/07/30/EFTS-wav/</url>
    <content><![CDATA[<h1 id="efficienttts一种高效高质量的文本转语音架构">EfficientTTS：一种高效、高质量的文本转语音架构</h1>
<h2 id="感想">感想</h2>
<p>基于IMV的并行TTS框架，文章创新的重点在于IMV的提出，使得TTS任务可并行化，但听了几个EFTS-CNN复现的Demo之后，无法达到论文提到的效果。另外类似的模型框架被应用于歌声合成系统(EfficientSing)，通过改变模型的输入形式，也得到了合理的结果，EfficientSing 发表至Interspeech 2021。</p>
<p><a href="https://arxiv.org/pdf/2012.03500.pdf">Paper</a> | <a href="https://github.com/mcf330/EfficientTTSAudioSamples">Demo</a> | <a href="https://github.com/liusongxiang/efficient_tts">Code</a> | ICML 2021 | Rate: 🌟🌟🌟🌟</p>
<h2 id="摘要">摘要</h2>
<p>在这项工作中，我们通过提出一种称为 EfficientTTS 的非自回归架构来解决文本到语音（TTS）任务。与主要的非自回归 TTS 模型需要外部对齐器进行训练不同，EfficientTTS 使用稳定的端到端训练程序来优化其所有参数，同时允许合成高质量的语音，是一种快速有效的方式。 EfficientTTS 是由一种新的单调对齐建模（monotonic alignment modeling）方法（也在本工作中引入）推动的，该方法在几乎不增加计算量的情况下指定对序列对齐的单调约束。通过将 EfficientTTS 与不同的前馈网络结构相结合，我们开发了一系列 TTS 模型，包括 text-to-melspectrogram 和 text-to-waveform 网络。我们的实验表明，所提出的模型在语音质量、训练效率和合成速度方面明显优于 Tacotron 2 (Shen et al., 2018) 和 Glow-TTS (Kim et al., 2020) 等对应模型，同时仍然产生强大的稳健性和多样性的演讲。此外，我们证明了所提出的方法可以轻松扩展到自回归模型，例如 Tacotron 2。</p>
<h2 id="introduction">Introduction</h2>
<p>文本到语音（TTS）是语音处理中的一项重要任务。随着深度学习的飞速发展，TTS 技术近年来受到了广泛关注。最流行的神经 TTS 模型是基于编码器-解码器框架的自回归模型。在这个框架中，编码器将文本序列作为输入并学习其隐藏表示，而解码器逐帧生成输出，即以自回归方式。随着自回归模型性能的大幅提升，综合效率正成为新的研究热点。</p>
<p>最近，大量努力致力于非自回归 TTS 模型的开发。然而，大多数现有的非自回归 TTS 模型都存在训练过程复杂、计算成本或训练时间成本高的问题，使其不适合实际应用。在这项工作中，我们提出了 EfficientTTS，一种高效且高质量的文本转语音架构。我们的贡献总结如下，</p>
<ul>
<li>除了几乎不增加计算量的一般注意力机制之外，我们提出了一种新方法来为序列到序列模型产生软或硬单调对齐。 最重要的是，所提出的方法可以纳入任何注意力机制，而不受网络结构的限制。</li>
<li>我们提出了 EfficientTTS，这是一种非自回归架构，可在没有额外对齐器的情况下从文本序列执行高质量语音生成。 EfficientTTS 是完全并行、完全卷积的，并且经过端到端训练，因此对于训练和推理都非常有效。</li>
<li>我们开发了一系列基于 EfficientTTS 的 TTS 模型，包括： (1) EFTS-CNN，一个卷积模型以高训练效率学习 melspectrogram； (2) EFTS-Flow，一种基于流的模型，可实现具有可控语音变化的并行 melspectrogram 生成； (3) EFTS-Wav，一个完全端到端的模型，直接从文本序列中学习波形生成。 我们通过实验表明，与对应模型 Tacotron 2 和 Glow-TTS 相比，所提出的模型在语音质量、合成速度和训练效率方面取得了显着改善。</li>
<li>我们还表明，所提出的方法可以很容易地扩展到自回归模型，如本文末尾的 Tacotron 2。</li>
</ul>
<p>本文的其余部分的结构如下。 第 2 节讨论相关工作。 我们在第 3 节中介绍了使用索引映射向量（index mapping vector）的单调对齐建模（monotonic alignment modeling）。第 4 节介绍了 EfficientTTS 架构。在第 5 节中，介绍了 EfficientTTS 模型。 第 6 节展示了实验结果和实现细节。 最后，第 7 节总结了本文。</p>
<h2 id="related-work">Related Work</h2>
<h3 id="non-autoregressive-tts-models">2.1 Non-Autoregressive TTS models</h3>
<p>在 TTS 任务中，输入文本序列 <span class="math inline">\(x = \{x_0,x_1,...,x_{T_1−1}\}\)</span>​​​​ 通过编码器-解码器帧转换为输出序列 <span class="math inline">\(y = \{y_0 , y_1 , ..., y_{T_2 −1} \}\)</span>​​​​​。 通常情况下，<span class="math inline">\(x\)</span>​​首先通过编码器 <span class="math inline">\(f: h = f(x)\)</span>​ 转换为一系列隐藏状态 <span class="math inline">\(h = \{h_0,h_1,...,h_{T_1−1}\}\)</span>​，然后通过解码器产生输出<span class="math inline">\(y\)</span>​。 对于每个输出时间步，注意力机制允许搜索 <span class="math inline">\(h\)</span> 的整个元素以生成上下文向量 <span class="math inline">\(c\)</span>： <span class="math display">\[
c_j = \sum^{T_1-1}_{i=0}􏰂 α_{i,j} ∗h_i,
\]</span></p>
<p>其中 <span class="math inline">\(α = \{α_{i,j} \} ∈ \mathcal{R}^{(T_1 ,T_2 )}\)</span>​ 是对齐矩阵。然后将 <span class="math inline">\(c\)</span> 馈送到另一个网络 <span class="math inline">\(g\)</span> 以生成输出 <span class="math inline">\(y：y = g(c)\)</span>。 <span class="math inline">\(f\)</span> 和 <span class="math inline">\(g\)</span>​ 的网络可以很容易地用并行结构替换，因为它们都获得一致的输入和输出长度。<strong>因此，构建非自回归 TTS 模型的关键在于并行对齐预测。</strong>在之前的工作中，大多数非自回归 TTS 模型从外部模型或工具中学习对齐，这使得训练变得复杂。最近，提出了 Flow-TTS（Miao 等，2020）、Glow-TTS（Kim 等，2020）和 EATS（Donahue 等，2020）。 Flow-TTS 和 EATS 在训练过程中直接从文本序列的隐藏表示中学习比对，没有考虑从输出序列中提取比对，使得训练效率低下。 Glow-TTS 使用独立算法提取每个输入token的持续时间，该算法排除了使用标准反向传播。另一方面，EfficientTTS 通过单个网络以完全端到端的方式联合学习序列对齐和语音生成，同时保持稳定高效的训练。</p>
<h3 id="monotonic-alignment-modeling">2.2 Monotonic alignment modeling</h3>
<p>如第 2.1 节所述，通用注意力机制会在每个输出时间步检查每个输入步。 这种机制经常会遇到错位并且训练成本很高，尤其是对于长序列。 因此，如果结合了一些先验知识，它一定会有所帮助。 <strong>一般来说，单调对齐应该遵循严格的标准</strong>，如图1所示，包括：（1）<strong>单调性</strong>，在每个输出时间步，对齐位置永不倒退； (2) <strong>连续性</strong>，在每个输出时间步，对齐的位置最多向前移动一步； (3) <strong>完整性</strong>，对齐的位置必须覆盖输入标记的所有位置。 已经提出了许多先前的研究来确保正确的对齐（Li 等人，2020 年），但其中大多数需要连续的步骤，并且经常无法满足上述所有标准。 在这项工作中，我们提出了一种有效且高效地产生单调注意力的新方法。</p>
<h2 id="采用imv进行单调对齐建模">采用IMV进行单调对齐建模</h2>
<p>我们通过提出<strong>索引映射向量 (IMV: index mapping vector)</strong> 开始本节，然后我们在单调对齐建模中利用 IMV。 我们进一步展示了如何将 IMV 合并到一般的序列到序列模型中。</p>
<h3 id="imv定义">3.1 IMV定义</h3>
<p>设 <span class="math inline">\(α∈\mathcal{R}^{(T_1,T_2)}\)</span>​​​ 为输入序列 <span class="math inline">\(x∈\mathcal{R}^{(D_1,T_1)}\)</span>​​ ​和输出序列 <span class="math inline">\(y∈ \mathcal{R}^{(D_2,T_2)}\)</span>​​ 之间的比对矩阵。 我们将索引映射向量 (IMV) <span class="math inline">\(π\)</span>​​ 定义为索引向量 <span class="math inline">\(p = \{0, 1, · · · , T_1 − 1\}\)</span>​ 的总和，由 <span class="math inline">\(α\)</span> 加权： <span class="math display">\[
π_j = \sum^{T_1 -1}_{i=0} 􏰂 α_{i,j} ∗p_i,
\]</span> 其中，<span class="math inline">\(0 ≤ j ≤ T_2 −1,π ∈ \mathcal{R}^{T_2},􏰂\sum^{T_1-1}_{i=0}α_{i,j} = 1\)</span>​​.</p>
<p>我们可以将 IMV 理解为每个的预期位置输出时间步长，其中期望值是从 <span class="math inline">\(0\)</span> 到 <span class="math inline">\(T_1-1\)</span>​ 的所有可能输入位置。</p>
<h3 id="采用imv进行单调对齐建模-1">3.2 采用IMV进行单调对齐建模</h3>
<p><strong>连续性和单调性。</strong> 我们首先证明对齐矩阵 <span class="math inline">\(α\)</span>​ 的连续性和单调性标准等价于以下约束： <span class="math display">\[
0 ≤ ∆π_i ≤ 1
\]</span> 其中，<span class="math inline">\(∆π_i = π_i −π_{i−1},1 ≤ i ≤ T_2 −1\)</span>​。 详细的验证见附录 A。</p>
<p><strong>完整性。</strong> 给定 <span class="math inline">\(π\)</span>​ 是连续单调的，完备性等价于边界条件： <span class="math display">\[
π_0=0,
\]</span></p>
<p><span class="math display">\[
π_{T_2−1} = T_1 − 1.
\]</span></p>
<p>这可以从 <span class="math inline">\(α_0 = \{1, 0, ..., 0\}\)</span>​​​​​​ 和 <span class="math inline">\(α_{T_2 −1} = \{0,0,...,1\}\)</span>​​​​​​ 推导出，其中<span class="math inline">\(α_0 =\{α_{i,j} |0≤i≤T_1−1， j=0\}\)</span>​​​​​​ 和 <span class="math inline">\(α_{T_2 −1} = \{α_{i,j} | 0 ≤ i ≤ T_1 − 1，j = T_2 − 1\}\)</span>​​​​​​​​。</p>
<h3 id="将imv融合到网络中">3.3 将IMV融合到网络中</h3>
<p>我们提出了两种将 IMV 纳入序列到序列网络的策略：软单调比对 (SMA) 和硬单调比对 (HMA)。</p>
<p><strong>软单调对齐 (SMA)</strong>。 让序列到序列模型使用3.2节给出的3个约束条件进行训练。 一个自然的想法是将这些约束转化为训练目标。 我们将这些约束表述为 SMA 损失，计算如下： <span class="math display">\[
L_{SMA} = λ_0∥|∆π| − ∆π∥_1 + λ_1∥|∆π − 1| + (∆π − 1)∥_1 + λ_2∥\frac {π_0} {T_1−1} ∥_2 + λ_3∥ \frac {π_{T_2−1}}{T_1 -1} − 1∥_2,
\]</span> 其中<span class="math inline">\(∥·∥_1\)</span>和<span class="math inline">\(∥·∥_2\)</span>​分别为L1范数和L2范数，<span class="math inline">\(λ_0、λ_1、λ_2、λ_3\)</span>​​为正系数。 可以看出，<span class="math inline">\(L_{SMA}\)</span> 是非负的，只有当 <span class="math inline">\(π\)</span>​ 满足所有约束时才为零。 <span class="math inline">\(L_{SMA}\)</span> 的计算只需要对齐矩阵 <span class="math inline">\(α\)</span>（索引向量 <span class="math inline">\(p\)</span>​ 总是已知的），因此，将 SMA 损失合并到序列到序列网络中而不改变它们的网络结构是很容易的。 一般来说，SMA 扮演着与引导注意（Guided Attention）相似的角色。 然而，SMA 优于引导注意，因为 SMA 理论上对对齐提供了更准确的约束。</p>
<p><strong>硬单调对齐 (HMA)</strong>。 虽然 SMA 允许序列到序列网络通过结合 SMA 损失来产生单调对齐，但这些网络的训练可能仍然很昂贵，因为网络无法在训练的开始阶段产生单调对齐。 相反，他们一步一步地学习这种能力。 为了解决这个限制，我们提出了另一种单调策略，我们称之为 HMA，用于硬单调对齐。 HMA 的核心思想是建立一个具有战略设计结构的新网络，允许在没有监督的情况下产生单调对齐。</p>
<p>首先，我们根据IMV的定义从对齐矩阵 <span class="math inline">\(α\)</span> 计算 IMV <span class="math inline">\(π&#39;\)</span>​。尽管 <span class="math inline">\(π&#39;\)</span> 不是单调的，但它随后通过使用 ReLU 激活强制 <span class="math inline">\(Δπ &gt; 0\)</span> 被转换为 <span class="math inline">\(π\)</span>，这是一个严格单调的 IMV。 <span class="math display">\[
∆π_j^′ =π_j^′ −π^′_{j−1}, 0&lt;j≤T_2−1,
\]</span></p>
<p><span class="math display">\[
∆π_j =\text{ReLU}(∆π_j^′), 0&lt;j≤T_2−1,
\]</span></p>
<p><span class="math display">\[
π_j= 
\begin{cases}
0, &amp; j=0\\
\sum_{m=0}^j  ∆π_m &amp; 0&lt;j≤T_2−1
\end{cases}􏰁
\]</span></p>
<p>此外，要将 <span class="math inline">\(π\)</span> 的域限制在完整性等式中给出的区间 <span class="math inline">\([0, T_1 − 1]\)</span>。我们将 <span class="math inline">\(π\)</span> 乘以一个正标量： <span class="math display">\[
π_j^∗ = π_j ∗ \frac {T_1 − 1} {max(π)} =  π_j ∗ \frac {T_1 − 1} {π_{T_2} −1},0 ≤ j ≤ T_2 − 1.
\]</span> 回想一下，我们的目标是构建单调对齐。 为了实现这一点，我们引入了以下变换，通过利用以 <span class="math inline">\(π^∗\)</span>为中心的高斯核来重建对齐： <span class="math display">\[
α^′_{i,j} = \frac {exp (−σ^{−2}(p_i − π_j^∗)^2)} {􏰁\sum^{T_1−1}_{m=0} exp (−σ^{−2}(p_m − π_j^∗)^2)}
\]</span> 其中，<span class="math inline">\(σ^2\)</span>​ 表示表示对齐变化的超参数。 <span class="math inline">\(α&#39;\)</span>​ 用作原始对齐 <span class="math inline">\(α\)</span> 的替代。 <span class="math inline">\(α\)</span> 和 <span class="math inline">\(α&#39;\)</span> 之间的区别在于 <span class="math inline">\(α&#39;\)</span> 保证是单调的，而 <span class="math inline">\(α\)</span> 对单调性没有约束。 HMA 降低了学习单调对齐的难度，从而提高了训练效率。 与 SMA 类似，HMA 可用于任何序列到序列网络。</p>
<h2 id="efficienttts-模型架构">EfficientTTS 模型架构</h2>
<p>EfficientTTS 的整体架构设计如图 2 所示。在训练阶段，我们通过 IMV 生成器从文本序列和 melspectrogram 的隐藏表示中学习 IMV。 文本序列和梅尔谱图的隐藏表示分别从文本编码器和梅尔编码器中学习。 然后将 IMV 转换为二维对齐矩阵，该矩阵进一步用于通过对齐重建层生成时间对齐表示。 时间对齐的表示通过解码器生成输出梅尔光谱图或波形。 我们同时训练一个对齐位置预测器，它学习在每个输入文本标记的输出时间步长中预测对齐位置。 在推理阶段，我们从预测的对齐位置重建对齐矩阵。 我们在以下小节和附录 D 中每个组件的伪代码中展示了详细的实现。</p>
<div class="figure">
<img src="/images/fig2.png" alt="fig2" />
<p class="caption">fig2</p>
</div>
<h3 id="文本编码器和mel谱编码器">4.1 文本编码器和Mel谱编码器</h3>
<p>我们使用文本编码器和梅尔编码器将文本符号和梅尔谱图分别转换为强大的隐藏表示。</p>
<p>在文本编码器的实现中，我们使用学习嵌入(learned embedding)将文本序列转换为高维向量序列。 然后，高维向量通过一堆卷积，散布着权重归一化和 Leaky ReLU 激活。 我们还为每个卷积添加了一个残差连接以允许深度网络。</p>
<p>在梅尔编码器的实现中，我们首先通过线性投影将梅尔谱图转换为高维向量。 与文本编码器相同，mel-encoder 由一堆卷积组成，其中散布着权重归一化、Leaky ReLU 激活和残差连接。 请注意，mel-encoder 仅用于训练阶段。</p>
<h3 id="imv-生成器">4.2 IMV 生成器</h3>
<p>为了在训练阶段生成单调 IMV，我们首先通过等式中给出的缩放点积注意力学习输入和输出之间的对齐 <span class="math inline">\(α\)</span>。 然后从<span class="math inline">\(α\)</span>​计算IMV。 <span class="math display">\[
α_{i,j}= \frac {\text{exp}(−D^{−0.5}(q_j ·k_i))} {\sum^{T_1−1}_{m=0}exp(−D^{−0.5}(q_j·k_m))}
\]</span> 其中，<span class="math inline">\(q\)</span> 和 <span class="math inline">\(k\)</span> 是 mel-encoder 和 text-encoder 的输出，<span class="math inline">\(D\)</span> 是 <span class="math inline">\(q\)</span> 和 <span class="math inline">\(k\)</span> 的维度。</p>
<p>计算 IMV 的一种简单方法是遵循3.1中定义的等式 (2)。然而，由于缩放点积注意力对单调性没有限制，在我们的初步实验中，将 SMA 损失纳入训练。 但我们进一步发现 HMA 更有效。 我们遵循3.3中定义的方程 (7,8,9,10) 实施 HMA。 在实验中，我们比较了不同单调策略的效果。</p>
<h3 id="对齐位置预测器">4.3 对齐位置预测器</h3>
<p>在推理阶段，模型需要从文本序列 <span class="math inline">\(h\)</span>​​ 的隐藏表示中预测 IMV <span class="math inline">\(π\)</span>​​，这在实践中具有挑战性。 有两个限制：（1）<span class="math inline">\(π\)</span>​​​是时间对齐的，分辨率高，但是<span class="math inline">\(h\)</span>​​分辨率低； (2) 由于3.3中的等式（9）中引入的累积求和运算，<span class="math inline">\(π_i\)</span> 的每个预测都会影响 <span class="math inline">\(π_j (j &gt; i)\)</span>​ 的后续预测，使得并行预测 <span class="math inline">\(π\)</span>​变得困难。 幸运的是，可以通过预测每个输入标记的对齐位置 <span class="math inline">\(e\)</span> 来缓解这些限制。 我们定义3.1中的方程 (2) 作为变换 <span class="math inline">\(m(·): π = m(q)\)</span>​。 由于 <span class="math inline">\(π\)</span> 和 <span class="math inline">\(q\)</span> 在时间步长上都是单调连续的，这意味着变换 <span class="math inline">\(m(·)\)</span>​ 是单调连续的，因此 <span class="math inline">\(m(·)\)</span> 是可逆的： <span class="math display">\[
q = m^{−1}(π),
\]</span> 每个输入标记的输出时间步长中对齐的位置 <span class="math inline">\(e\)</span> 可以计算为： <span class="math display">\[
e=m^{−1}(p), p=\{0,1,...,T_1 −1\}.
\]</span></p>
<p><img src="/images/fig3.png" alt="fig3" />我们在图 3 中说明了 <span class="math inline">\(m(·)\)</span>、<span class="math inline">\(e\)</span>、<span class="math inline">\(π\)</span> 的关系。为了计算 <span class="math inline">\(e\)</span>，我们首先利用与3.3中的方程（11) 类似的变换来计算概率密度矩阵 <span class="math inline">\(γ\)</span>。 唯一的区别是概率密度是在不同维度上计算的。 对齐位置 <span class="math inline">\(e\)</span> 是由 <span class="math inline">\(γ\)</span> 加权的输出索引向量 <span class="math inline">\(q\)</span>​ 的加权和。 <span class="math display">\[
γ_{i,j} = \frac {\text{exp}(−σ^{−2}(p_i −π_j)^2)} {􏰁\sum ^{T_2−1}_{n=0} \text{exp} (−σ^{−2}(p_i − π_n)^2)}
\]</span></p>
<p><span class="math display">\[
e_i = \sum^{T_2-1}_{n=0}􏰂 γ_{i,n} ∗q_n
\]</span></p>
<p>可以看出，<span class="math inline">\(e\)</span> 的计算是可微的，允许通过梯度方法进行训练，因此可以用于训练和推理。 此外，<span class="math inline">\(e\)</span>​ 是可预测的，因为：(1) 分辨率 <span class="math inline">\(e\)</span>​ 与 <span class="math inline">\(h\)</span> 相同； (2) 我们可以学习相对位置<span class="math inline">\(∆e,(∆e_i =e_i−e_{i−1},1≤i≤T_1−1)\)</span> 而不是直接学习<span class="math inline">\(e\)</span> 来克服第二个限制。</p>
<p>对齐位置预测器由 2 个卷积组成，每个卷积之后是层归一化和 ReLU 激活。 我们将根据 <span class="math inline">\(π\)</span> 计算出的 <span class="math inline">\(Δe\)</span> 作为训练目标。 估计位置 <span class="math inline">\(∆ \hat e\)</span> 和目标位置 <span class="math inline">\(∆e\)</span>​ 之间的损失函数计算如下： <span class="math display">\[
L_{ap} =∥log(∆ \hat e+ε)−log(∆e+ε)∥_1,
\]</span> 其中，<span class="math inline">\(ε\)</span>​ 是一个小数，以避免数值不稳定性。 对数尺度损失的目标是准确拟合小值，这对于训练的后期阶段往往更为重要。 对齐位置预测器与模型的其余部分共同学习。 因为我们通过利用对齐的位置来生成对齐，作为附带好处，EfficientTTS 继承了基于持续时间的非自回归 TTS 模型的语速控制的能力。</p>
<h3 id="对齐重构">4.4 对齐重构</h3>
<p>为了将输入隐藏表示 <span class="math inline">\(h\)</span> 映射到时间对齐表示，需要一个对齐矩阵，用于训练和推理。 我们也可以从 IMV 或对齐位置构建对齐。 对于大多数情况，3.3 的方程 (11) 是从 IMV 重建对齐矩阵的有效方法。 但是因为我们在推理过程中预测对齐位置而不是 IMV，为了保持一致，我们从对齐位置 <span class="math inline">\(e\)</span> 重建对齐矩阵，用于训练和推理。具体来说，我们采用从4.3中的方程 (15) 计算的对齐位置 <span class="math inline">\(e\)</span>​​​​​ 用于训练，以及来自对齐位置预测器的预测用于推理。</p>
<p>我们遵循 EATS 的类似思想，通过引入以对齐位置 <span class="math inline">\(e\)</span> 为中心的高斯核来重建对齐矩阵 <span class="math inline">\(α&#39;\)</span>​。 <span class="math display">\[
α^{&#39;}_{i,j} = \frac {\text{exp}(−σ^{−2}(e_i −q_j)^2)} {􏰁\sum^{T_1−1}_{m=0} \text{exp}(−σ^{−2}(e_m −q_j)^2)}
\]</span> 其中 <span class="math inline">\(q = \{0, 1, ..., T_2 − 1\}\)</span> 是输出序列的索引向量。 输出序列 <span class="math inline">\(T_2\)</span> 的长度在训练中已知，在推理中从 <span class="math inline">\(e\)</span>​ 计算： <span class="math display">\[
T_2 = e_{T_1−1} + ∆e_{T_1−1}
\]</span> 尽管重建的对齐矩阵可能不如由3.3中的等式(11)计算的矩阵准确（由于<span class="math inline">\(e\)</span>的分辨率低），对输出的影响很小，因为网络能够补偿。 因此，由于训练和推理的一致性不断提高，我们享受到语音质量的提高。</p>
<p>我们在图 4 中说明了 <span class="math inline">\(π\)</span> 和相同话术的重构 <span class="math inline">\(α\)</span>​​​。可以看出，<span class="math inline">\(α\)</span> 在第一个训练步骤是对角线的，并且在第 <span class="math inline">\(10k^{th}\)</span> 个训练步骤快速收敛，这是非常快的。 我们通过使用遵循2.1中的等式(1)的 <span class="math inline">\(α&#39;\)</span> 将文本编码器 <span class="math inline">\(h\)</span> 的输出映射到时间对齐的表示。然后将时间对齐的表示作为输入馈送到解码器。</p>
<h3 id="解码器">4.5 解码器</h3>
<p>由于解码器的输入和输出都是时间对齐的，因此很容易实现具有并行结构的解码器。 在下一节中，我们开发了三种基于具有不同解码器实现的 EfficientTTS 的模型。</p>
<h2 id="efficienttts-models">EfficientTTS Models</h2>
<h3 id="efts-cnn">5.1 EFTS-CNN</h3>
<p>我们首先通过一堆卷积来参数化解码器。 每个卷积都穿插了权重归一化、Leaky ReLU 激活和残差连接。 我们在最后添加一个线性投影来生成melspectrogram。 均方误差 (MSE) 用作重建误差。 EFTS-CNN 的总体训练目标是 melspectrogram 的对齐位置损失和 MSE 损失的组合。</p>
<h3 id="efts-flow">5.2 EFTS-Flow</h3>
<p>为了让 TTS 模型能够控制生成语音的变化，我们实现了一个基于流的解码器。 在训练阶段，我们通过直接最大化似然，以时间对齐表示为条件，学习从melspectrogram到高维高斯分布<span class="math inline">\(\mathcal{N}(0, 1)\)</span>​的变换<span class="math inline">\(f\)</span>​。 <span class="math inline">\(f\)</span>​ 是可逆的，具有战略设计的结构。 具体来说，它由几个流程步骤组成，每个流程步骤由两个基本的可逆变换组成：一个可逆线性层和一个仿射耦合层。 为了提高生成语音的多样性，我们在推理过程中从高斯分布 <span class="math inline">\(\mathcal{N} (0, 1)\)</span> 中采样潜在变量 <span class="math inline">\(z\)</span>，并使用温度因子 <span class="math inline">\(t\)</span> 用零向量 <span class="math inline">\(o\)</span> 解释 <span class="math inline">\(z\)</span>，并逆变换 <span class="math inline">\(f\)</span> 以生成梅尔频谱图 . <span class="math display">\[
z^{&#39;} =t∗z+o∗(1−t),0≤t≤1
\]</span></p>
<p><span class="math display">\[
x = f^{−1}(z^{&#39;})
\]</span></p>
<p>为简单起见，我们在实现基于流的解码器时遵循 Flow-TTS的解码器结构。 EFTS-Flow 的总体训练目标是对齐位置损失和最大似然估计 (MLE) 损失的组合。</p>
<h3 id="efts-wav">5.3 EFTS-Wav</h3>
<p>为了简化 2 阶段训练管道并以完全端到端的方式训练 TTS 模型，我们通过将 EfficientTTS 与扩张卷积对抗解码器相结合来开发文本到 wav 模型。解码器结构类似于 MelGAN，除了： (1) 生成器的输入是高维隐藏表示，而不是 80 通道的梅尔频谱图； (2) 多分辨率 STFT 损耗包含在生成器的末端。我们采用与 MelGAN 鉴别器相同的结构进行对抗训练。与 ClariNet 和 EATS 类似，我们通过调节与 1s 音频剪辑相对应的切片输入来训练 MelGAN 部分，而其他部分则在全长话语上进行训练。我们在全长解码器输入上添加线性投影以同时生成melspectrogram，这允许EFTS-Wav学习每个训练步骤的全长对齐。 <strong>EFTS-Wav 生成器的总体训练目标是 melspectrogram 的重建损失、MelGAN 生成器损失、多分辨率 STFT 损失和对齐位置损失的线性组合。</strong></p>
<h2 id="experiments">Experiments</h2>
<p>在本节中，我们首先在语音保真度、训练和推理效率方面将提出的模型与其对应模型进行比较。 然后，我们分析了所提出的单调方法在 EFTS-CNN 和 Tacotron 2 上的有效性。我们还证明了所提出的模型可以在本节末尾生成非常多样化的语音。</p>
<h3 id="实验设置">6.1 实验设置</h3>
<p><strong>数据集。</strong> 我们在来自 DataBaker 的开源标准普通话数据集上进行了大部分实验，该数据集包含来自单个女性演讲者的 10,000 个中文片段，采样率为 22.05kHZ。 剪辑的长度从 1 秒到 10 秒不等，剪辑的总长度约为 12 小时。 我们遵循Tacotron将波形转换为 80 通道 melspectrogram，FFT 大小为 1024，跳长为 256，窗口大小为 1024。我们还使用 LJ-Speech 数据集进行了一些实验，这是一个由 13100条音频片段组成的单个女性演讲者的 24 小时波形音频集，采样率22.05kHZ。</p>
<p><strong>实施细则。</strong> 我们的 EfficientTTS 实现由文本编码器（ 5 个卷积）和梅尔编码器（3 个卷积）组成，所有卷积的内核大小和维度大小分别设置为 5 和 512。 我们在具有相同卷积配置的 EFTS-CNN 解码器中使用 6 层卷积堆栈。 EFTS-Flow 的解码器由 8 个流程步骤组成，我们在实现多尺度架构时每 3 个流程步骤就提前输出 20 个通道。 我们遵循 MelGAN 的配置来实现 EFTS-wav。 我们使用 HiFi-GAN 声码器从 EFTS-CNN 和 EFTS-Flow 生成的梅尔谱图生成波形。 我们使用具有 HiFi-GAN-V1 配置的 <a href="https://github.com/jik876/hifi-gan">HiFi-GAN</a>的开放实现。</p>
<p><strong>对照模型。</strong> 我们在以下实验中将提出的模型与自回归 Tacotron 2 和非自回归 Glow-TTS 进行比较。 我们直接使用带有默认配置的 <a href="https://github.com/NVIDIA/tacotron2">Tacotron 2</a> 和 <a href="https://github.com/jaywalnut310/glow-tts">Glow-TTS</a> 的开源实现。</p>
<p><strong>训练。</strong> 我们在单个 Tesla V100 GPU 上训练所有模型。对于 EFTS-CNN 和 EFTS-Flow，我们使用 Adam 优化器，批量大小为 96，学习率为 <span class="math inline">\(1 × 10^{−4}\)</span>。 对于 EFTS-Wav，我们使用批次大小为 48 的 Adam 优化器。我们对 EFTS-Wav 生成器使用 <span class="math inline">\(1 × 10^{−4}\)</span> 的学习率，对EFTS-Wav 鉴别器使用 <span class="math inline">\(5 × 10^{−5}\)</span>​的学习率。 在 DataBaker 上训练 EFTS-CNN 需要 270k 步直到收敛，EFTS-Flow 需要 400k 训练步骤，EFTS-Wav 需要 560k 训练步骤。</p>
<h3 id="与对照模型对比">6.2. 与对照模型对比</h3>
<div class="figure">
<img src="/images/efts-table2.png" alt="efts-table2" />
<p class="caption">efts-table2</p>
</div>
<div class="figure">
<img src="/images/efts-table3.png" alt="efts-table3" />
<p class="caption">efts-table3</p>
</div>
<p><strong>语音质量。</strong>我们对 DataBaker 数据集进行了 5 级平均意见得分 (MOS) 评估，以衡量合成音频的质量。每个音频至少由 15 名测试人员收听，他们都是母语人士。我们将 EfficientTTS 系列生成的音频样本的 MOS 与真实音频以及对应模型生成的音频样本进行比较。具有 95% 置信区间的 MOS 结果显示在上表2中。 我们观察到 EfficientTTS 系列优于对应模型。 Tacotron 2 由于教师强制训练和自回归推理之间的不一致导致语音质量下降，而 Glow-TTS 复制了文本序列的隐藏表示，这破坏了隐藏表示的连续性。 EfficientTTS 使用 IMV 重建对齐，这比令牌持续时间更具表现力，因此实现了更好的语音质量。此外，EfficientTTS 的对齐部分与模型的其余部分一起训练，进一步提高了语音质量。由于我们的训练设置可能与对应模型的原始设置不同，我们进一步将我们的模型 EFTS-CNN 与 LJ-Speech 数据集上的 Tacotron 2 和 Glow-TTS 的相关模型进行比较。如表3所示。 EFTS-CNN 在 LJ-Speech 上的表现也明显优于对应模型。</p>
<div class="figure">
<img src="/images/efts-table1.png" alt="efts-table1" />
<p class="caption">efts-table1</p>
</div>
<p><strong>训练和推理速度。</strong>由于非自回归和完全卷积，所提出的模型对于训练和推理都非常有效。 训练时间和推理延迟的定量结果显示在表1中。可以看出，<strong>EFTS-CNN 需要最少的训练时间</strong>。 尽管 EFTS-Flow 需要与 Tacotron 2 相当的训练时间，但它比 Glow-TTS 快得多。 至于推理延迟，EfficientTTS 模型比 Tacotron 2 和 Glow-TTS 更快。 特别是，EFTS-CNN 的推理延迟为 6ms，比 Tacotron 2 快 130 倍，并且比 Glow-TTS 快得多。 由于去除了 melspectrogram 生成，<strong>EFTS-Wav 明显快于 2-staged 模型，从文本序列合成测试音频仅需 16 毫秒，比 Tacotron 2 快 54 倍。</strong></p>
<h3 id="单调对齐方法的评估">6.3 单调对齐方法的评估</h3>
<p>为了评估所提出的单调方法的行为，我们在 EFTS-CNN 和 Tacotron 2 上进行了多次实验。我们首先比较了 EFTS-CNN 上的训练效率，然后对 Tacotron 2 和 EFTS-CNN 进行了稳健性测试。</p>
<div class="figure">
<img src="/images/efts-fig5.png" alt="efts-fig5" />
<p class="caption">efts-fig5</p>
</div>
<div class="figure">
<img src="/images/efts-table4.png" alt="efts-table4" />
<p class="caption">efts-table4</p>
</div>
<p><strong>EFTS-CNN 上的实验。</strong>我们用不同的设置训练 EFTS-CNN，包括：（1）EFTS-HMA，EFTS-CNN 的默认实现，带有硬单调 IMV 生成器。 (2) EFTS-SMA，一个带有软单调 IMV 生成器的 EFTS-CNN 模型。 (3) EFTS-NM，一个没有单调性常数的EFTS-CNN模型。 EFTS-NM 的网络结构与 EFTS-SMA 相同，不同之处在于 EFTS-SMA 是在 SMA 损失的情况下训练的，而 EFTS-NM 是在没有 SMA 损失的情况下训练的。我们首先发现EFTS-NM根本不收敛，它的对齐矩阵不是对角的，而EFTS-SMA和EFTS-HMA都能够产生合理的对齐。我们在图 5 中绘制了 EFTS-SMA 和 EFTS-HMA 的melspectrogram 损失曲线。<strong>可以看出，EFTS-HMA 比 EFTS-SMA 实现了显着的加速</strong>。因此，我们可以得出结论，单调对齐对于所提出的模型非常重要。我们的方法，对于 SMA 和 HMA，成功地学习了单调对齐，而原始注意力机制失败了。由于严格的单调对齐，EFTS-HMA 显着提高了模型性能。更多训练细节显示在表4中。</p>
<div class="figure">
<img src="/images/efts-table5.png" alt="efts-table5" />
<p class="caption">efts-table5</p>
</div>
<p><strong>稳健性。</strong>许多 TTS 模型在合成时遇到错位，特别是对于自回归模型。我们在本小节中分析了 EfficientTTS 的注意力错误，错误包括：重复单词、跳过单词和错误发音。我们对 50 个句子的测试集进行了稳健性评估，其中包括对 TTS 系统特别具有挑战性的案例，例如特别长的句子、重复的字母等。我们将 EFTS-CNN 与 Tacotron 2 进行比较。我们还将 SMA 和 HMA 合并到Tacotron 2 进行更详细的比较（Tacotron2-SMA 和 Tacotron2-HMA 的详细实现以及更多实验结果显示在附录 C 中）。稳健性测试的实验结果见表 5，可以看出EFTS-CNN在Tacotron 2遇到很多错误的情况下，有效地消除了重复错误和跳过错误。然而，通过利用 SMA 或 HMA，Tacotron 2 的合成错误显着减少，这表明所提出的单调方法可以提高 TTS 模型的鲁棒性。</p>
<h3 id="多样性">6.4 多样性</h3>
<p>为了合成多样化的语音样本，大多数 TTS 模型利用外部条件，例如样式嵌入或说话者嵌入，或者在推理过程中仅依靠 drop-out。 然而，EfficientTTS 能够以多种方式合成各种语音样本，包括：（1）用不同的对齐方案合成语音。 对齐方案可以是由mel编码器和IMV生成器从现有音频中提取的IMV，也可以是持续时间序列或对齐位置序列； (2) 通过在预测的对齐位置上乘以一个标量来合成不同语速的语音，这类似于其他基于时长的非自回归模型； (3) 通过在推理过程中改变潜在变量 <span class="math inline">\(z\)</span> 的温度 <span class="math inline">\(t\)</span>​，为 EFTS-Flow 合成具有不同语音变体的语音。 我们在附录 B 中绘制了从相同文本序列生成的各种melspectrograms。</p>
<h2 id="结论和未来工作">结论和未来工作</h2>
<p>通常假设模型效率与语音质量之间存在不可避免的权衡。自回归模型，例如 Tacotron 2 和 TransformerTTS，可实现类似人类的语音质量，但由于其自回归结构，通常合成速度较慢。非自回归模型可以快速合成，但无法有效训练。在这项工作中，我们提出了一种非自回归架构，可以实现高质量的语音生成以及高效的训练和合成。我们开发了一系列基于 EfficientTTS 的模型，涵盖文本到光谱图和文本到波形的生成。通过大量实验，我们观察到改进的定量结果，包括训练效率、合成速度、鲁棒性以及语音质量。我们表明，与现有的 TTS 模型相比，所提出的模型非常具有竞争力。</p>
<p>未来的工作有很多可能的方向。 EfficientTTS 不仅可以在给定的对齐方式下生成语音，还可以从给定的语音中提取对齐方式，使其成为语音转换和歌唱合成的绝佳选择。将所提出的单调方法应用于其他单调对齐很重要的序列到序列任务也是一个不错的选择，例如自动语音识别 (ASR)、神经机器翻译 (NMT) 和光学字符识别 (OCR) 。此外，我们也对 IMV 的进一步研究非常感兴趣，包括与对齐矩阵相比的优缺点。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>新心愿达成，新身份新快乐收获</title>
    <url>/2021/03/13/jhk-teaching-assistant-1st-time/</url>
    <content><![CDATA[<p>纪念一下收获了爱与能量的一天！</p>
<p>早上8点多起来，就是为了能不放同学鸽子，毕竟已经放了道友们好多次鸽子了，再放鸽子感觉我人品有问题，必须得去！所以就这样开启了神奇的一天。</p>
<p>早上8-9就起床了去看表演，自己做的很烂，朋友们都比我现场看的好很多哈哈哈哈，dsf剪辑也有点厉害啊，（就像胡哲说的，了解了一个人的人物传记之后，就会对这个人很有好感），我今天又成为dsf的迷妹了，感觉又在发着光。</p>
<p>但印象很深刻的是马玲的冲木前面几个动作巨好看，完美，养眼，赏心悦目以及自愧不如（可能这就是宝杰说的那种感受吧，她觉得很完美，但或许当事人不觉得）。然后就看到我的后踢又驼背了，是三个人里最差的，腰背部力量不够，身体没撑起来（难道这也是ysf晚上突然又cue我们做后踢辅助动作的原因之一吗？难道他也发现问题了？哈哈哈哈）</p>
<p>其他道馆，尽量发现的话还是有几个精品，但是普遍水平感觉没差很多，高级别很帅，看来tkd也是吃经验的。</p>
<p>10:15，换了便装的上身卫衣，匆匆赶到南山馆，还可以继续看一会，海岸城馆气势很大，哈哈哈哈，感觉果真什么师范带出什么样的学生。</p>
<p>10:50，收拾道具，迷茫，还傻乎乎的问ksf需不需要帮忙，没眼力见，在dsf问“你俩谁可以拍照”的时候，终于又勇敢了一把，舒坦爽快，也就开启了勇敢的一天。</p>
<p>11:00，开始30来个学员的儿童班课程，级别从白带到黑带，年纪从三四岁，到十来岁，看得头都大了，无法想象怎么上课，但是作为大头兵的我就哪里需要哪里搬就好了。</p>
<p>11:05，ksf整队敬礼，带队热身活动，迷茫，不知道该站哪，不知道该不该一起热身，不知道该不该帮助小朋友，这时的马师范已经开始自觉热身了，还得到了dsf的表扬，🍋，后面才知道原来msf之前有过这种经验，一回生二回熟哈哈哈</p>
<p>11:10，30个小朋友按级别分成了五队，五个助教，一人带一队，当场感叹师范简直是人才，这么简单的除法就解决了30个熵混乱的问题哈哈哈哈。尴尬的是，ksf喊我们三个过去的时候，作为打头的我愣住了，不知道该从哪个路径走过去，恍惚了几秒，一向超级超级super-nice温柔的ksf突然很严肃的说，助教要快一点啊！妈呀被嫌弃了有点尴尬，还是要硬着头皮迎接接下来的任务啊。</p>
<p>11:15，ksf做师范，助教帮忙拿脚把，妈呀第一脚就被踢，第二脚又被踢了，都怪我脚把拿多高我都不知道，而且小朋友身高一会高一会低，脚把位置一直在变动，我的天，为啥有的小孩踢左脚，有的右脚，有的正面，有的背面，有的格斗式，有的并脚，刚才ksf怎么做的？？？脑子一面空白，dsf过来旁边告诉我，你放松一点，不要脚把抓的那么紧，dsf也帮助小朋友指正动作，这时候我终于只需要做个不需要思考的拿把机器就好了，舒坦，但是dsf走了之后又陷入了慌张和迷茫。终于ksf喊停了，第一组动作，带小朋友一起划水结束哈哈哈哈</p>
<p>11:20，这一次的示范动作我很认真的观察ksf怎样站位，怎样准备的，怎样起腿，怎样结束动作，脚把位置多高，甚至关键点是什么。到我这边实践后，小朋友们还是乱七八糟，我开始学会说，换脚，面向我，背对我，左腿，右腿等简单的方位动作，终于捋顺了大家的完整动作，从一个方向出腿了。开始也去观察主要的问题点，并且提出合理的建议，或者做出简单的解决方案，才到我大腿个子的小朋友（看似也就三四岁），居然可以听懂我的简单指令？？？成功建立沟通连接了之后，感觉开始上手了。居然也可以听懂我的解决方案？？小朋友真厉害！！真的进步了一点点点！好棒！！！（虽然内心疯狂喜悦，肾上腺激素疯狂上飙，但是嘴巴上的我还是一个敷衍的表扬机器），“很好”，“还不错”，我自己都感觉自己的表扬十分敷衍，但是又学不会dsf的那种“对iiiiii了！”，“对iiiii，就是这样做嘛！”太厉害了，佩服的五体投地了。</p>
<p>渐入佳境的我完成了几组动作指导后，发现队伍有点乱，不知道该怎么管理，发现后面小朋友在自己玩，不知道该咋办，不敢凶他们，发现小朋友东张西望了，不知道该怎么勾回他们的注意力，只能先注意到在做动作的这个人身上了。</p>
<p>小朋友们的性格，脾气，也各式各样，各有千秋，蓝带高个子朋友打头，最高级别，年龄也相对较大一些，所以能够沟通，动作上也是最好的。所以多是表扬，和给他更多的一些挑战。比如旁边两组高级别都是跳后旋，他在我们这组里级别最高，我看他也可以尝试，就叫他尝试了一下，发现果真还不错、他的表情令我感觉他自己做了高级别的动作，也有点小满足，小骄傲，小自豪，哈哈哈，他的满足也是我的快乐，我也接收到快乐了。</p>
<p>绿带也是大腿高度的小男孩，从一开始就一直笑笑的，笑的人都要融化掉了，僵硬冰冷了这么多年的我（3年）感觉被暖到了，所以我也卸掉了社会中的面具，变得笑笑的，微笑也给我带来了满足和快乐。这个小男孩更逗的一点是，后面练拳法，我不知道为啥他总要连续快速做，本来只要做一个就好，我说你这咋还变成连环拳了，他笑笑继续他的连环拳。柴师范过来说，哎，你不可以这样对待姐姐哦，要好好表现，他果真听话了？？？开始一拳一拳做。原来他刚才不是没听懂或者不会做？柴师范一说他咋就懂了？？所以刚才是故意的？引起我的注意？？被小孩的聪明才智折服也骗到了，怪不得老师都喜欢偏爱坏学生，原来坏学生总是能很聪明的挑战到他的底线，让她觉得教育这件事都变有趣了，更想要管教好她，所以才会选择偏爱更有趣和更有挑战性的学生。哈哈哈哈哈神奇。</p>
<p>绿黄带小孩给我印象也很深刻，几次勾踢和后旋踢有点惊艳到了，感觉不是他这个级别应该领悟到的东西啊。哈哈哈，或许这就是低级别的快乐。总是会被夸奖。哈哈哈哈，反正黑红带以前我也总是被夸奖，自从要考黑带了就每日被diss。说回绿黄带，本来期望绿黄带应该不太能做这两个动作吧，居然甩了几下有模有样的？？？ksf也予以了多次的表扬和认可，原来不止我觉得这个有天赋，是个苗子。更逗乐的是，他的每次出拳之前，别人都是ha之后就出拳了，我不知道为啥，这个小孩看起来只有三四岁，他好像思考了有一两秒，攥紧了拳头，好像在蓄力的样子，打出的拳头，虽然那么小只，感觉也很有力气啊我的天，或许这就是传说中的天赋？？这个蓄力的动作太神奇了，感觉一个三四岁小孩都充满了神秘感，不知道他的小脑袋瓜在想什么。（哈哈哈这里有个插曲，就是，dsf跑过来，说，让姐姐看到你的力气哦！我心想，dsf真尊重我们，真社会，真客气，真乖，哈哈哈哈我都不好意思了，也不知道小宝贝心里怎么想的。）在思考出拳方向？还是在蓄力？不知道，但是打出的拳头还是不错的</p>
<p>还有多次dsf跑过来说，一开始说，你放松一点，你不要拿脚把那么紧张，你不要扎着马步，这一天腰都断了，放松自然一点。你要适时有感染性的鼓励一下小朋友，你要带有情绪的，“哇！真棒！”，“好棒啊”，做得好的甚至可以“give me five”。这个时候我感觉自己才是道场里的学生，dsf和小朋友都是我的老师，在教化着我怎么去应对这个不太擅长的局面，dsf是我的指路人，用他的经验告诉我怎样是对的，小朋友们是我的助教，陪我实践，给我反馈，一次又一次表情上的反馈告诉我这样做是不是对的。大多数都会是肯定我，我这样做确实是对的，他们很开心，他们有快乐，有成就感。天呐，没想到，我才是这堂课收获最多的人。</p>
<p>好了说回我队的小宝贝，绿带高个子男孩，看起来有四五年级了，很成熟稳重，容易沟通，还能在我搞不了小朋友的时候，帮我指导一下，只不过就是有点好像循规蹈矩，很认真做，但感觉还是不太灵动，所以会感觉有点死板，再加上最明显的问题就是速度慢了，所以对他的要求主要是提高速度，也是希望能提高灵动性，他一开始没有讲过话，后面中间一次他好像问了一句，是不是这样，那个时候我感觉好像互相产生信任了，我给他的回复也给了他安全感。他也更加卖力去做了。也很明显感觉速度快了动作舒服很多。</p>
<p>蓝绿带软萌小女孩，妈呀太可爱了，头发好多，软萌软萌的，不忍心碰她，哎，没想到协调性贼好，转身动作极其完美酷炫，又超乎想象了。小拳头不是打过来，好像是砸过来的，太可爱了，都不忍心纠正她可爱的动作了，哈哈哈哈。萌化。</p>
<p>最后好像还有一个绿带还是蓝绿带的小宝贝，他应该是做的都还不错，所以没有花心思给他改正动作的话，印象似乎没有太深刻，这可能就是那种平凡的孩子？自己很乖，很优秀，很听话，动作也很标准了，但是就是不容易给人留下印象了，突然有点代入一向“普普通通”“平平凡凡”的自己。</p>
<p>总而言之，这次助教课收获颇多！这种能量的突变确实力量巨大，从去之前的恐惧，担心是熊孩子，从小觉得自己没有孩子缘，担心自己管不住小孩，等等，居然全都得到了救赎，师范们把小朋友们带的很好，该听话的时候很听话不闹，小朋友们像小天使一般，用他们的笑容，单纯，善良，进取，努力，甚至调皮等都带给了我巨大能量，让我这一天都充满了能量，（也导致我现在话匣子打开都停不下来了），有点上头，有点快乐。</p>
<p>课后dsf暗示我说，我在楼下坐，你有什么技术上的交流可以来找我，我温习了今天的课程之后，留了十分钟，本想找dsf看一步对打，没想到坐下就开始聊教课体验，哈哈哈。我真诚的表达了自己的一些小想法，他也很快乐，似乎觉得给我的能量似乎也反馈给他了一部分快乐，我能从表情里看到他对于这件事安排的满意程度，主要是因为他觉得我乐在其中，很有收获。我说这个事情，有点有成就感啊，dsf说，我们做师范最大的就是成就感，blablabla说了一堆，总而言之就是表达我这种小想法的认可，有成绩感是正常的。应该要有成就感。然后我说我没有经验，很紧张，dsf说以后还可以找你来，我很开心，也爽快的答应了。甚至还说，如果你以后，敲代码敲烦了，不想敲了，就给我打电话，（略带哭腔）地说，师范，我代码敲的不快乐了，我马上给你安排助教课，给你找找成就感，然后你再回去快乐的敲代码。天啊啊啊啊啊啊啊啊，听到这句话的我真的彻底融化，在这个诺大的城市里，受了委屈，有师范给我兜底，我真的心怀感激，备感荣幸，也感受到了深深的安全感，就算捅了篓子，也有人有事情能救回我的感觉，这也太适合我这个时不时低沉但是内心始终温暖热血的表面丧人了。听完这话的我，虽然内心知道这一把年纪了，肯定是不会好意思这样做的，最多也就是师范需要我，我会很乐意去帮忙，但是内心也是着实被暖到了，感觉似乎在深圳这个城市里伪装的冰冷，今天被小朋友和师范们全都融化了。表达完我有点快乐有点上头的情绪后，师范似乎也放宽了心，说以后还会找我来，让我慢慢接触尝试，说不定就转师范了哈哈哈哈哈，这种职业道路的变更，虽然想过，想过无数次、很多次，数万次，但是太需要勇气了，风险极大，落差极大，自己的专业性和天赋能力也远远不足，只能慢慢来，先享受小天使们带给我的快乐和满足吧。我现在算是体会到dsf所说的大师兄是自己享受指导和帮助人的过程，我现在算是理解了，之前可能是我们理解错了吧。到现在也算又圆了一个小时候的跆拳道助教梦想哈哈哈哈，人生真的太神奇了，儿时的梦想居然一个一个接一个的实现了，这种肾上腺素飙升的感觉也太快乐了。</p>
<p>至此，先鞠躬敬礼感谢丁师范的信任给我这一次机会，感谢dsf在看我迷茫的时候一直耐心的指导我该怎样做，哪里不对，（比指导我技术的时候耐心多了哈哈哈哈），感谢小朋友们包容我这个新手助教，给我信任、微笑、力量和快乐，感谢杨师范包容我中途跑掉，还不回来上课了，晚上还跑过来蹭场地的同时，晚上还不计前嫌地帮我个人抠动作了有三四十分钟，感谢柴师范还是用他的一如既往的温暖温暖着我，感谢自己珍惜这次机会才得偿所愿，感谢心中有梦的自己这么多年还没有忘记最初的梦想，人间又值得了！！！！！</p>
<p>最后上升一些生活哲学， 1. 六个小朋友不同的性格，似乎都让我看到了社会和人生。 2. 你带的学生，最后就会变成你自己的一面镜子。dsf这句话真的绝了。联想到现实更绝。ksf和dsf，dsf和cj，ysf和bj，hjj和我算是dsf和ysf的混交吧，hjj偏dsf，我算是混的比较平均的。哈哈哈哈也算是这种结合方式的首次尝试，也挺特别和快乐的。 3. tkd快乐今日double、乃至triple，hundredable，突然感觉被赋予了使命，感受到了教育的快乐 4. 又一次感受到了几个师范的温柔，简直温柔了深漂的岁月，今天繁师范问我为什么成人还要练tkd，之前一个小朋友的爸爸也问过我，我第一次回答对黑带的执念（或许是我最初的梦想），今天回答工作之余的发泄和对现实生活的逃避，感觉有点官方现实，但也有点映射哲学了。现在总结一下，于我而言，最开始是对黑带的执念，小时候不断突破自己的回忆让我埋下种子，在更有时间精力的时候完成小时候未竟的梦想，在参加了之后完全是被金载勋吸引了，道馆师范对自己要求太高，包括技术上，心理上，各种综合素质都相对于其他成人道馆要成熟太多太多了。对深漂的自己是一种解脱，感觉像是在深圳的一个家，朋友之间没有利益冲突，可以很坦率的交朋友，师范燃烧自己，照亮他人，最关键的是，师范自身的经历，相近的年龄，能给自己带来很多人生的启迪，有时候工作不顺训练上身体解压，朋友上互相开导，师范也会给出建议以及甚至给你兜底，懂得教育心理学的师范们，即便你受欺负了，也能帮你解脱出来。女孩子独自一人在外的护身防卫，不要以为是花架子，金载勋不一样，不仅仅玩运动竞技场上的规则，尤其是成人班很强调实用性的，很多动作你是真实可以应用到实际情景的，上海两例女性自我保护事件，这个东西会给自己在外漂泊更多安全感。价格不贵的， 挠痒痒一样，对道馆来说，成人班只是他们积累更多经验，基本不赚钱的，你说这种性价比这么高的活动，不值得吗？时间也不会耽误很多，但是需要坚持，但是培养任何一项业务爱好和特长，都需要坚持。最后才是，减肥，运动，保持身材，柔韧性，协调性，力量等等很多问题。如果想要自己的生活，不仅仅是工作，家庭，没有别的味道了，那还是非常值得推荐的。总而言之，工作之外的生活，变得更加丰富多彩，快乐，充实和解压。</p>
<p>对自己黑带的要求是：小于3次失误，击破、一步对打和品势因为只有一次机会，所以要0失误。横叉flag不要倒啊加油啊不要倒啊。</p>
<p>好了，放过自己，今天的输入太多，终于输出完毕，今天可以圆满结束了。晚安蓝蓝。</p>
]]></content>
      <categories>
        <category>生活</category>
      </categories>
  </entry>
  <entry>
    <title>VITS</title>
    <url>/2021/07/15/vits/</url>
    <content><![CDATA[<h1 id="Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech"><a href="#Conditional-Variational-Autoencoder-with-Adversarial-Learning-for-End-to-End-Text-to-Speech" class="headerlink" title="Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech"></a>Conditional Variational Autoencoder with Adversarial Learning for End-to-End Text-to-Speech</h1><h2 id="感想"><a href="#感想" class="headerlink" title="感想"></a>感想</h2><p>基于VAE，GAN，Normaling flows的端到端TTS神经网络结构。文章简单综述了end2end TTS的发展情况，且指出当前的single-stage模型大多相较于multi-stage模型较差。模型的创新点在首次采用VAE的方式并行生成语音。模型实验效果从MOS和推理效率上都远好于目前的baseline模型。</p>
<p><a href="https://arxiv.org/pdf/2106.06103.pdf">Paper</a> | <a href="https://jaywalnut310.github.io/vits-demo/index.html">Demo</a> | <a href="https://github.com/jaywalnut310/vits">Code</a> | ICML 2021 | Rate: 🌟🌟🌟🌟🌟</p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>最近提出了几种支持单阶段训练和并行采样的端到端文本到语音（TTS）模型，但它们的样本质量与两阶段 TTS 系统的样本质量不匹配。在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。我们的方法采用变分推理，并通过归一化流程和对抗性训练过程进行增强，从而提高了生成建模的表达能力。我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量和随机持续时间预测器的不确定性建模，我们的方法表达了自然的一对多关系，其中可以以多种方式以不同的音高和节奏说出文本输入。对 LJ Speech（单个说话者数据集）的主观人类评估（平均意见分数或 MOS）表明，我们的方法优于最好的公开可用的 TTS 系统，并实现了与Ground-Truth实况相当的 MOS。 </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文本到语音 (TTS) 系统从给定文本通过多个组件合成原始语音波形。 随着深度神经网络的快速发展，除了文本规范化和音素化等文本预处理之外，TTS 系统管道已被简化为两阶段生成建模。 第一阶段是从预处理的文本中生成中间语音表示，如 Melspectrograms 或语言特征，第二阶段是生成以中间表示为条件的原始波形。 每个两级管道的模型都是独立开发的。</p>
<p>基于神经网络的自回归 TTS 系统已经显示出合成真实语音的能力，但它们的顺序生成过程使得现代并行处理器难以充分利用。为了克服这个限制并提高合成速度，已经提出了几种非自回归方法。在文本到谱图生成步骤中，尝试从预训练的自回归教师网络中提取注意力图，以降低学习文本和谱图之间对齐的难度.最近，基于似然的方法通过估计或学习最大化目标梅尔谱图的可能性的比对来进一步消除对外部比对器的依赖。同时，在第二阶段模型中探索了生成对抗网络（GAN）。基于 GAN 的前馈网络具有多个鉴别器，每个鉴别器区分不同尺度或周期的样本，实现高质量的原始波形合成。</p>
<p>尽管并行 TTS 系统取得了进展，但两级管道仍然存在问题，因为它们需要顺序训练或微调才能进行高质量的生产，其中后期模块使用早期模型生成的样本进行训练。此外，它们对预定义中间特征的依赖妨碍了应用学习到的隐藏表示来获得性能的进一步改进。最近，几项工作，即 FastSpeech 2s 和 EATS ，提出了有效的端到端训练方法，例如对短音频剪辑而不是整个波形进行训练，利用梅尔谱图解码器来帮助文本表示学习，并设计专门的谱图损失来减轻目标和生成语音之间的长度不匹配问题。然而，尽管通过利用学习的表示可能提高性能，但它们的合成质量落后于两阶段系统。 </p>
<p>在这项工作中，我们提出了一种并行的端到端 TTS 方法，它比当前的两阶段模型生成更自然的声音。使用变分自编码器 (VAE)，我们通过潜在变量连接 TTS 系统的两个模块，以实现高效的端到端学习。为了提高我们方法的表达能力，以便可以合成高质量的语音波形，我们将标准化流应用于我们的条件先验分布和波形域的对抗训练。除了生成细粒度的音频之外，TTS 系统表达一对多关系也很重要，在这种关系中，文本输入可以以多种方式以不同的变化（例如，音调和持续时间）说出。为了解决一对多的问题，我们还提出了一个随机持续时间预测器来从输入文本合成具有不同节奏的语音。通过对潜在变量的不确定性建模和随机持续时间预测器，我们的方法可以捕获无法用文本表示的语音变化。</p>
<p>与最好的公开可用的 TTS 系统 Glow-TTS和 HiFi-GAN相比，我们的方法获得了更自然的语音和更高的采样效率。 我们公开演示页面和源代码。</p>
<h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><p><img src="/images/vits_fig1.png" alt="vits_fig1"></p>
<p>在本节中，我们将解释我们提出的方法及其架构。 所提出的方法主要在前三个小节中描述：条件 VAE 公式； 来自变分推理的对齐估计； 用于提高合成质量的对抗性训练。 本节末尾描述了整体架构。 图 1a 和 1b 分别显示了我们方法的训练和推理过程。 从现在开始，我们将把我们的方法称为端到端文本到语音（VITS）对抗学习的变分推理。</p>
<h3 id="变分推断"><a href="#变分推断" class="headerlink" title="变分推断"></a>变分推断</h3><h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>VITS 可以表示为条件 VAE，其目标是最大化数据 $log p_θ (x|c)$ 的难以处理的边际对数似然的变分下界，也称为证据下界 (ELBO: the evidence lower bound)：</p>
<script type="math/tex; mode=display">
log p_θ (x|c) ≥ E_{q_{φ(z|x)}}  [log p_θ (x|z)−log \frac{q_φ(z|x)} {p_θ (z|c)}]</script><p>其中 $p_θ(z|c)$​ 表示给定条件 $c$​ 的潜在变量 $z$​ 的先验分布，$p_θ(x|z)$​ 是数据点 $x$​ 的似然函数，$q_φ(z|x)$​ 是近似后验分布。 训练损失是负的 ELBO，可以看作是重建损失的总和 $- log p_θ(x|z)$​ 和 KL 散度 $log q_φ(z|x) - log p_θ(z|c)$​，其中 $z ∼ q_φ(z|x)$​​。</p>
<h4 id="重构损失"><a href="#重构损失" class="headerlink" title="重构损失"></a>重构损失</h4><p>作为重建损失中的目标数据点，我们使用梅尔谱图而不是原始波形，用 $x_{mel}$ 表示。 我们通过解码器将潜在变量 $z$ 上采样到波形域 $\hat y$，并将 $\hat y$ 变换到梅尔谱图域 $\hat x_{mel}$。 然后将预测和目标梅尔谱图之间的 $L_1$ 损失用作重建损失：</p>
<script type="math/tex; mode=display">
L_{recon} = ∥x_{mel} − \hat x_{mel}∥_1</script><p>这可以看作是最大似然估计，假设数据分布为拉普拉斯分布并忽略常数项。 我们定义了梅尔谱域中的重建损失，以通过使用近似人类听觉系统响应的梅尔尺度来提高感知质量。 请注意，来自原始波形的梅尔谱图估计不需要可训练的参数，因为它只使用 STFT 和线性投影到梅尔尺度上。 此外，估计仅在训练期间使用，而不是在推理期间使用。 在实践中，我们不会对整个潜在变量 $z$ 进行上采样，而是使用部分序列作为解码器的输入，这是用于高效端到端训练的窗口生成器训练。</p>
<h4 id="KL-散度"><a href="#KL-散度" class="headerlink" title="KL-散度"></a>KL-散度</h4><p>先验编码器 $c$ 的输入条件由从文本中提取的音素 $c_{text}$ 和音素和潜在变量之间的对齐 $A$ 组成。 对齐是一个硬单调注意矩阵，带有 $|c_{text}| × |z|$ 维度表示每个输入音素扩展到与目标语音时间对齐的时间。 因为对齐没有真实标签，我们必须在每次训练迭代时估计对齐，我们将在第 2.2.1 节中讨论。 在我们的问题设置中，我们旨在为后验编码器提供更多高分辨率信息。 因此，我们使用目标语音 $x_<br>{lin}$ 的线性频谱图而不是梅尔频谱图作为输入。 请注意，修改后的输入不会违反变分推理的属性。 KL散度则为：</p>
<script type="math/tex; mode=display">
L_{kl} =logq_φ(z|x_{lin})−logp_θ(z|c_{text},A),</script><script type="math/tex; mode=display">
z ∼ q_φ(z|x_{lin}) = N(z; μ_φ(x_{lin}), σ_φ(x_{lin}))</script><p>分解正态分布用于参数化我们的先验和后验编码器。 我们发现增加先验分布的表现力对于生成真实样本很重要。 因此，我们应用了归一化流 $f_θ$ ，它允许在分解的正态先验基础之上，按照变量变化规则将简单分布可逆变换为更复杂的分布分配：</p>
<script type="math/tex; mode=display">
p_{\theta} (z|c) = N(f_{\theta} (z);μ_{\theta} (c),σ_{\theta} (c))􏰀􏰀􏰀 | det \frac {∂f_θ(z)} {∂ z}􏰀􏰀􏰀|,</script><script type="math/tex; mode=display">
c = [c_{text}, A]</script><h3 id="对齐估计"><a href="#对齐估计" class="headerlink" title="对齐估计"></a>对齐估计</h3><h4 id="MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索"><a href="#MONOTONIC-ALIGNMENT-SEARCH-单调对齐搜索" class="headerlink" title="MONOTONIC ALIGNMENT SEARCH 单调对齐搜索"></a>MONOTONIC ALIGNMENT SEARCH 单调对齐搜索</h4><p>为了估计输入文本和目标语音之间的对齐 A，我们采用了单调对齐搜索 (MAS)（Kim 等人，2020 年），这是一种搜索对齐的方法，该方法可以最大化由归一化流 f 参数化的数据的可能性：</p>
<script type="math/tex; mode=display">
A = arg max_{\hat A} log p(x|c_{text} , \hat A) = arg max_{\hat A} log N(f(x); μ(c_{text}, \hat A), σ(c_{text}, \hat A))</script><p>在人类按顺序阅读文本而不跳过任何单词的事实之后，候选对齐被限制为单调和非跳过。 为了找到最佳对齐方式，Kim 等人 (2020) 使用动态规划。 在我们的设置中直接应用 MAS 很困难，因为我们的目标是 ELBO，而不是确切的对数似然。 因此，我们重新定义 MAS 以找到最大化 ELBO 的对齐，这简化为找到最大化潜在变量 $z$ 的对数似然的对齐：</p>
<script type="math/tex; mode=display">
arg max_{\hat A} log p_θ(x_{mel}|z) − log \frac {q_φ(z|x_{lin})} {p_θ(z|c_{text}, \hat A)}
= arg max_{\hat A} log p_θ(z|c_{text}, \hat A)</script><script type="math/tex; mode=display">
= log N(f_θ(z); μ_θ(c_{text}, \hat A), σ_θ(c_{text}, \hat A))</script><p>由于等式 5 与等式 6 相似，我们可以不加修改地使用原始 MAS 实现。 附录 A 包括 MAS 的伪代码。</p>
<h4 id="来自文本的持续时间预测"><a href="#来自文本的持续时间预测" class="headerlink" title="来自文本的持续时间预测"></a>来自文本的持续时间预测</h4><p>我们可以通过对估计对齐$\sum_j A_{i,j}$的每一行中的所有列求和来计算每个输入标记 $d_i$ 的持续时间。 正如之前的工作（Kim 等人，2020 年）所提出的那样，持续时间可用于训练确定性持续时间预测器，但它无法表达一个人每次以不同语速说话的方式。 为了生成类似人类的语音节奏，我们设计了一个随机持续时间预测器，使其样本遵循给定音素的持续时间分布。随机持续时间预测器是基于流的生成模型，通常通过最大似然估计进行训练。 然而，最大似然估计的直接应用是困难的，因为每个输入音素的持续时间是 1) 一个离散整数，需要对其进行反量化以使用连续归一化流，以及 2) 一个标量，它阻止了高维 可逆性引起的变换。我们应用变分反量化（Ho et al., 2019）和变分数据增强（Chen et al., 2020）来解决这些问题。 具体来说，我们引入两个随机变量 $u$ 和 $ν$，它们具有与持续时间序列 $d$ 相同的时间分辨率和维数，分别用于变分去组化和变分数据增强。 我们将 $u$ 的支持度限制为 $[0, 1)$，以便差 $d - u$ 成为正实数序列，并且我们将 $ν$ 和 $d$ 通道级联以形成更高维的潜在表示。 我们通过近似后验分布 $q_φ(u, ν|d, c_{text})$ 对两个变量进行采样。 由此产生的目标是音素持续时间的对数似然的变分下界：</p>
<script type="math/tex; mode=display">
logp_θ(d|c_{text}) ≥ E_{q_φ (u,ν |d,c_{text} )} log \frac {p_θ(d-u,v|c_{text})} {q_φ (u, ν |d, c_{text} )}</script><p>训练损失 $L_{dur}$ 是负变分下界。 我们将停止梯度算子 (van den Oord et al., 2017) 应用于输入条件，以防止反向传播输入的梯度，以便持续时间预测器的训练不会影响其他模块的训练。</p>
<p>取样程序比较简单； 通过随机持续时间预测器的逆变换从随机噪声中采样音素持续时间，然后将其转换为整数。</p>
<h3 id="对抗训练"><a href="#对抗训练" class="headerlink" title="对抗训练"></a>对抗训练</h3><p>为了在我们的学习系统中采用对抗性训练，我们添加了一个鉴别器 $D$，用于区分解码器 $G$ 生成的输出和真实波形 $y$。 在这项工作中，我们使用了两种成功应用于语音合成的损失类型； 用于对抗训练的最小二乘损失函数 (Mao et al., 2017)，以及用于训练生成器的附加特征匹配损失 (Larsen et al., 2016)：</p>
<script type="math/tex; mode=display">
L_{adv}(D) = E_{(y,z)}􏰄 [(D(y) − 1)^2 + (D(G(z)))^2],</script><script type="math/tex; mode=display">
L_{adv}(G) = E_z [(D(G(z)) − 1)^2] ,</script><script type="math/tex; mode=display">
L_{fm}(G) = E_{(y,z)} [\sum_{l=1}^T \frac {1} {N_l} ∥D^l (y) − D^l (G(z))∥_1 ]</script><p>其中 $T$​ 表示鉴别器中的总层数，$D^l$​ 输出具有 $N_l$​ 个特征的鉴别器第 $l$​ 层的特征图。 值得注意的是，特征匹配损失可以看作是在鉴别器的隐藏层中测量的重建损失，建议作为 VAE 逐元素重建损失的替代方案。</p>
<h3 id="最终的损失函数"><a href="#最终的损失函数" class="headerlink" title="最终的损失函数"></a>最终的损失函数</h3><p>通过 VAE 和 GAN 训练的结合，训练我们的条件 VAE 的总损失可以表示如下：</p>
<script type="math/tex; mode=display">
L_{vae} = L_{recon} +L_{kl} +L_{dur} +L_{adv}(G)+L_{fm}(G)</script><h3 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h3><p>所提出模型的整体架构由后验编码器、先验编码器、解码器、鉴别器和随机持续时间预测器组成。 后验编码器和鉴别器仅用于训练，不用于推理。 附录 B 中提供了架构详细信息。</p>
<h4 id="2-5-1-后验编码器"><a href="#2-5-1-后验编码器" class="headerlink" title="2.5.1 后验编码器"></a>2.5.1 后验编码器</h4><p>对于后验编码器，我们在 WaveGlow 和 Glow-TTS 中使用非因果 WaveNet 残差块。 WaveNet 残差块由带有门控激活单元 (gated activation unit) 和跳过连接 (skip connection) 的扩张卷积层 (dilated convolutions) 组成。 块上方的线性投影层产生正态后验分布的均值和方差。 对于多说话人的情况，我们在残差块中使用全局调节来添加说话人嵌入。 </p>
<h4 id="2-5-2-先验编码器"><a href="#2-5-2-先验编码器" class="headerlink" title="2.5.2 先验编码器"></a>2.5.2 先验编码器</h4><p>先验编码器由处理输入音素 $c_{text}$ 的文本编码器和提高先验分布灵活性的归一化流 $f_θ$ 组成。 文本编码器是一个转换器 (Transformer) 编码器，它使用相对位置表示而不是绝对位置编码。 我们可以通过文本编码器和文本编码器上方的线性投影层从 $c_{text}$ 中获得隐藏表示 $h_{text}$，该线性投影层产生用于构建先验分布的均值和方差。 归一化流程是一堆仿射耦合层，由一堆 WaveNet 残差块组成。 为简单起见，我们将标准化流设计为一个体积保持变换 (volume-perserving transformation) ，雅可比行列式为 1。 对于多说话人的情况，我们通过全局条件将说话人嵌入添加到归一化流程中的残差块中。</p>
<h4 id="2-5-3-解码器"><a href="#2-5-3-解码器" class="headerlink" title="2.5.3 解码器"></a>2.5.3 解码器</h4><p>解码器本质上是 HiFi-GAN V1 生成器。 它由一堆转置卷积组成，每个卷积后面跟着一个多感受野融合模块（MRF）。 MRF 的输出是具有不同感受野大小的残差块的输出之和。 对于多说话人设置，我们添加一个线性层来转换说话人嵌入并将其添加到输入潜在变量 $z$。</p>
<h4 id="2-5-4-鉴别器"><a href="#2-5-4-鉴别器" class="headerlink" title="2.5.4 鉴别器"></a>2.5.4 鉴别器</h4><p>我们遵循 HiFi-GAN中提出的多周期鉴别器的鉴别器架构。 多周期鉴别器是基于马尔可夫窗口的子鉴别器的混合，每个子鉴别器都对输入波形的不同周期模式进行操作。</p>
<h4 id="2-5-5-随机持续时间预测器"><a href="#2-5-5-随机持续时间预测器" class="headerlink" title="2.5.5 随机持续时间预测器"></a>2.5.5 随机持续时间预测器</h4><p>随机持续时间预测器根据条件输入 $h_{text}$ 估计音素持续时间的分布。 为了对随机持续时间预测器进行有效的参数化，我们用扩张的和深度可分离的卷积层堆叠残差块。 我们还将神经样条流应用于耦合层，该流通过使用单调有理二次样条采用可逆非线性变换的形式。 与常用的仿射耦合层相比，神经样条流以相似数量的参数提高了变换表达能力。 对于多说话人设置，我们添加一个线性层来转换扬声器嵌入并将其添加到输入 $h_{text}$。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在两个不同的数据集上进行了实验。我们使用 LJ Speech 数据集与其他公开可用的模型和 VCTK 数据集进行比较，以验证我们的模型是否可以学习和表达不同的语音特征。 LJ Speech 数据集由单个说话人的 13,100 个短音频剪辑组成，总长度约为 24 小时。音频格式为 16 位 PCM，采样率为 22 kHz，我们使用它时没有进行任何操作。我们将数据集随机分成训练集（12,500 个样本）、验证集（100 个样本）和测试集（500 个样本）。 VCTK 数据集包含大约 44,000 个短音频片段，由 109 位英语为母语的人发出，带有各种口音。音频剪辑的总长度约为 44 小时。音频格式为 16 位 PCM，采样率为 44 kHz。我们将采样率降低到 22 kHz。我们将数据集随机分为训练集（43,470 个样本）、验证集（100 个样本）和测试集（500 个样本）。</p>
<h3 id="预处理"><a href="#预处理" class="headerlink" title="预处理"></a>预处理</h3><p>我们使用可以通过短时傅立叶变换 (STFT) 从原始波形中获得的线性频谱图作为后验编码器的输入。 变换的FFT大小、窗口大小和跳跃大小分别设置为1024、1024和256。 我们使用 80 波段梅尔尺度频谱图进行重建损失，这是通过将梅尔滤波器组应用于线性频谱图而获得的。 </p>
<p>我们使用国际音标 (IPA) 序列作为先验编码器的输入。 我们使用开源软件将文本序列转换为 IPA 音素序列，并且在实现 Glow-TTS 之后，转换后的序列中穿插着空白标记。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>网络使用 AdamW 优化器进行训练，其中 $β_1 = 0.8$、$β_2 = 0.99$ 和权重衰减 $λ = 0.01$。 学习率衰减按每个 epoch 的 $0.999^{1/8}$ 因子进行调度，初始学习率为 $2 × 10^{−4}$。 继之前的工作之后，我们采用了窗口生成器训练，这是一种仅生成一部分原始波形的方法，以减少训练期间的训练时间和内存使用量。 我们随机提取窗口大小为 32 的潜在表示片段以馈送至解码器，而不是馈送整个潜在表示，并且还从地面实况原始波形中提取相应的音频片段作为训练目标。 我们在 4 个 NVIDIA V100 GPU 上使用混合精度训练。 每个 GPU 的批量大小设置为 64，模型最多训练 800k 步。</p>
<h3 id="用于比较的实验设置"><a href="#用于比较的实验设置" class="headerlink" title="用于比较的实验设置"></a>用于比较的实验设置</h3><p>我们将我们的模型与最好的公开可用模型进行了比较。 我们使用自回归模型 Tacotron 2 和基于流的非自回归模型 Glow-TTS 作为第一阶段模型，将 HiFi-GAN 作为第二阶段模型。 我们使用了他们的公开实现和预训练的权重。 由于两阶段 TTS 系统理论上可以通过顺序训练实现更高的合成质量，我们将微调的 HiFi-GAN 包括了高达 100k 步的预测输出 第一阶段模型。 我们根据经验发现，在教师强制模式下，使用 Tacotron 2 生成的梅尔谱图对 HiFi-GAN 进行微调，与使用生成的梅尔谱图微调相比，Tacotron 2 和 Glow-TTS 的质量都会更好。 Glow-TTS，因此我们在 Tacotron 2 和 Glow-TTS 中附加了更好的微调 HiFi-GAN。</p>
<p>由于每个模型在采样过程中都有一定程度的随机性，我们在整个实验过程中固定了控制每个模型随机性的超参数。 Tactron 2 的 pre-net 中的 dropout 概率设置为 0.5。 对于 Glow-TTS，先验分布的标准偏差设置为 0.333。 对于 VITS，随机持续时间预测器的输入噪声标准差设置为 0.8，我们将比例因子 0.667 乘以先验分布的标准差。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="语音合成质量"><a href="#语音合成质量" class="headerlink" title="语音合成质量"></a>语音合成质量</h3><p>我们进行了众包 MOS 测试来评估质量。 评分者聆听随机选择的音频样本，并以 1 到 5 的 5 分制对它们的自然度进行评分。评分者被允许对每个音频样本进行一次评估，我们将所有音频片段标准化以避免幅度差异对分数的影响。 这项工作中的所有质量评估都是以这种方式进行的。</p>
<p><img src="/images/vits_result1.png" alt="vits_result1"></p>
<p>评估结果如表 1 所示。 VITS 优于其他 TTS 系统，并实现了与Ground-Truth实况相似的 MOS。 VITS (DDP) 采用 Glow-TTS 中使用的相同确定性持续时间预测器架构而不是随机持续时间预测器，在 MOS 评估中在 TTS 系统中得分第二高。 这些结果意味着 1）随机持续时间预测器比确定性持续时间预测器生成更真实的音素持续时间；2）我们的端到端训练方法是制作比其他 TTS 模型更好的样本的有效方法，即使保持相似 持续时间预测器架构。</p>
<p><img src="/images/vits_result2.png" alt="vits_result2"></p>
<p>我们进行了一项消融研究以证明我们方法的有效性，包括先验编码器中的归一化流和线性尺度频谱图后验输入。 消融研究中的所有模型都经过最多 30 万步的训练。 结果如表 2 所示。去除先前编码器中的归一化流导致 1.52 MOS 比基线降低，这表明先验分布的灵活性显着影响合成质量。 用梅尔谱图替换后验输入的线性尺度谱图导致质量下降（-0.19 MOS），表明高分辨率信息对于 VITS 提高合成质量是有效的。</p>
<h3 id="泛化至多说话人TTS"><a href="#泛化至多说话人TTS" class="headerlink" title="泛化至多说话人TTS"></a>泛化至多说话人TTS</h3><p><img src="/images/vits_tab3.png" alt="vits_tab3"></p>
<p>为了验证我们的模型可以学习和表达不同的语音特征，我们将我们的模型与 Tacotron 2、Glow-TTS 和 HiFi-GAN 进行了比较，以显示扩展到多说话人语音合成的能力。 我们在 VCTK 数据集上训练模型。 我们将说话人嵌入添加到我们的模型中，如第 2.5 节所述。 对于 Tacotron 2，我们扩展说话人嵌入并将其与编码器输出concat起来，对于 Glow-TTS，我们参照2.5，应用了全局调节global conditioning。 评估方法与第 4.1 节中描述的相同。 如表 3 所示，我们的模型实现了比其他模型更高的 MOS。 这表明我们的模型以端到端的方式学习和表达各种语音特征。</p>
<h3 id="语音多样性"><a href="#语音多样性" class="headerlink" title="语音多样性"></a>语音多样性</h3><p>我们验证了随机持续时间预测器产生了多少不同长度的语音，以及合成样本具有多少不同的语音特征。</p>
<p><img src="/images/vits_fig2.png" alt="vits_fig2"></p>
<p><img src="/images/vits_fig3.png" alt="vits_fig3"></p>
<p>类似于 Valle 等人。 （2021），这里的所有样本都是从一句话<em>How much variation is there?</em>生成的。图 2a 显示了每个模型生成的 100 个话语的长度的直方图。由于确定性的持续时间预测器，Glow-TTS 仅生成固定长度的话语，我们模型中的样本遵循与 Tacotron 2 相似的长度分布。图 2b 显示了在多说话人设置中，使用五个说话者身份，生成的 5*100 个话语的长度，以展示模型能够学习到说话人相关的音素持续时间。图 3 中使用 YIN 算法 (De Cheveigne ́ &amp; Kawahara, 2002) 提取的 10 个话语的 F0 轮廓显示我们的模型生成具有不同音高和节奏的语音，图 3d 中的每个不同说话者身份生成的五个样本证明我们的模型为每个说话者身份表达了非常不同的语音长度和音调。请注意，Glow-TTS 可以通过增加先验分布的标准偏差来增加音高的多样性，但相反，它会降低合成质量。</p>
<h3 id="合成速度"><a href="#合成速度" class="headerlink" title="合成速度"></a>合成速度</h3><p><img src="/images/vits_tab4.png" alt="vits_tab4"></p>
<p>我们将模型的合成速度与并行的两阶段 TTS 系统 Glow-TTS 和 HiFi-GAN 进行了比较。 我们测量了整个过程的同步经过时间，以从 LJ Speech 数据集的测试集中随机选择 100 个句子的音素序列生成原始波形。 我们使用了一个批处理大小为 1 的 NVIDIA V100 GPU。结果如表 4 所示。由于我们的模型不需要用于生成预定义中间表示的模块，因此其采样效率和速度得到了极大的提高。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><h3 id="端到端文本转语音"><a href="#端到端文本转语音" class="headerlink" title="端到端文本转语音"></a>端到端文本转语音</h3><p>目前，具有两级管道的神经 TTS 模型可以合成类人语音。 但是，它们通常需要使用第一阶段模型输出来进行训练或微调的声码器，这会导致训练和部署效率低下。 他们也无法获得端到端方法的潜在好处，该方法可以使用学习的隐藏表示而不是预定义的中间特征。</p>
<p>最近，单阶段端到端 TTS 模型被提出来解决更具挑战性的任务，即直接从文本生成原始波形，其中包含比梅尔频谱图更丰富的信息（例如，高频响应和相位）。FastSpeech 2s 是 FastSpeech 2 的扩展，它通过采用对抗性训练和帮助学习文本表示的辅助梅尔谱解码器来实现端到端并行生成。然而，为了解决一对多问题，FastSpeech 2s 必须从作为训练输入条件的语音中提取音素持续时间、音调和能量。 EATS（Donahue 等人，2021 年）也采用对抗性训练和可微分对齐方案。为了解决生成语音和目标语音之间的长度不匹配问题，EATS采用通过动态规划计算的软动态时间规整损失(soft DTW loss)。 Wave Tacotron（Weiss 等人，2020 年）将标准化流(Normalizing flows)与 Tacotron 2 相结合，形成端到端结构，但仍保持自回归。上述所有端到端 TTS 模型的音频质量都低于两级模型。</p>
<p>与前面提到的端到端模型不同，通过利用条件 VAE，我们的模型 1) 学习直接从文本中合成原始波形，而无需额外的输入条件，2) 使用动态规划方法 MAS 进行搜索 最佳对齐而不是计算损失，3）并行生成样本，4）优于最好的公开可用的两阶段模型。</p>
<h3 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h3><p>VAE (Kingma &amp; Welling, 2014) 是使用最广泛的基于似然的深度生成模型之一。 我们对 TTS 系统采用有条件的 VAE。 条件 VAE 是一种条件生成模型，其中观察到的条件调节用于生成输出的潜在变量的先验分布。 在语音合成中，Hsu 等人。 (2019) 和 Zhang 等人 (2019) 结合 Tacotron 2 和 VAE 来学习说话风格和韵律。 BVAE-TTS（Lee 等人，2021 年）基于双向 VAE 并行生成梅尔谱图（Kingma 等人，2016 年）。 与之前将 VAE 应用于第一阶段模型的工作不同，我们将 VAE 应用于并行的端到端 TTS 系统。</p>
<p>Rezende &amp; Mohamed (2015), Chen 等(2017) 和 Ziegler &amp; Rush (2019) 通过使用标准化流增强先验和后验分布的表达能力来提高 VAE 性能。 为了提高先验分布的表示能力，我们将归一化流添加到我们的条件先验网络中，从而生成更真实的样本。</p>
<p>与我们的工作类似，Ma 等人 (2019) 提出了一种条件 VAE，在非自回归神经机器翻译 FlowSeq 的条件先验网络中对流进行归一化。 然而，我们的模型可以明确地将潜在序列与源序列对齐这一事实与 FlowSeq 不同，后者需要通过注意力机制学习隐式对齐。 我们的模型通过 MAS 将潜在序列与时间对齐的源序列进行匹配，从而消除了将潜在序列转换为标准正常随机变量的负担，这允许标准化流的更简单架构。</p>
<h3 id="非自回归TTS中的持续时间预测"><a href="#非自回归TTS中的持续时间预测" class="headerlink" title="非自回归TTS中的持续时间预测"></a>非自回归TTS中的持续时间预测</h3><p>自回归 TTS 模型通过其自回归结构和一些技巧（包括在推理和启动期间保持丢失概率）生成具有不同节奏的多样化语音。 另一方面，并行 TTS 模型依赖于确定性持续时间预测。 这是因为并行模型必须在一个前馈路径中预测目标音素持续时间或目标语音的总长度，这使得很难捕获语音节奏的相关联合分布。 在这项工作中，我们提出了一种基于流的随机持续时间预测器，它可以学习估计音素持续时间的联合分布，从而并行生成不同的语音节奏。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项工作中，我们提出了一个并行的 TTS 系统 VITS，它可以以端到端的方式学习和生成。我们进一步引入了随机持续时间预测器来表达不同的语音节奏。由此产生的系统直接从文本合成自然发声的语音波形，而无需经过预定义的中间语音表示。我们的实验结果表明，我们的方法优于两阶段 TTS 系统并达到接近人类的质量。我们希望所提出的方法将用于许多语音合成任务，其中使用了两阶段 TTS 系统，以实现性能提升并享受简化的训练过程。我们还想指出，即使我们的方法在 TTS 系统中集成了两个分离的生成管道，仍然存在文本预处理的问题。研究语言表征的自监督学习可能是去除文本预处理步骤的一个可能方向。我们将发布我们的源代码和预训练模型，以促进未来许多方向的研究。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Unsupervised speech representation learning using WaveNet autoencoders</title>
    <url>/2021/06/28/unsupervised-representation-learning/</url>
    <content><![CDATA[<h1 id="unsupervised-speech-representation-learning-using-wavenet-autoencoders-httpsexport.arxiv.orgpdf1901.08810">Unsupervised speech representation learning using WaveNet autoencoders https://export.arxiv.org/pdf/1901.08810</h1>
<h2 id="感想">感想</h2>
<p>第一次完整的精读完一篇期刊文章，一开始还是自己逐字句翻译的，但后面发现这样太耗费时间了（主要是突然解锁了typora的copy-paste功能），所以这篇文章从模型部分开始主要是Google翻译的。通篇来说论文Idea很容易理解，模型结构主要展示在了图1的模型结构里面。但是后面采用了6页的科学实验来验证这个想法，并且分析这个想法中的每一步细节。总体来讲，与MelGAN这种A类的长篇论文来对比的话，我更青睐于MelGAN的这种论文，是由于读到本文的一些实验细节，并且我尝试从图中去分析实验现象的时候，个人感觉实验现象都相对来说有一些牵强，尽管实验内容很详细，但是没有很好的控制变量，以得到有效的实验结果，并且很多实验结果有点晦涩难懂。（并且这篇文章采用了4个TPU训练了一周，没有资源还是不要轻易去re-train了，或许这个方向也像BERT一样，需要投入大量的资金和精力才能够实现宇宙第一的效果，所以还是建议轻易不要挑战）。所以本次打分：🌟🌟🌟</p>
<h2 id="abastract">Abastract</h2>
<p>现状：无监督的语音表示学习可以通过对语音进行自编码来实现。目标是从语音中抽取出高层级语义特征，例如，音素id，与语音信号低层级的混淆信息如音高轮廓或者背景噪声无关。</p>
<p>解决方案：因为learned representation 被tuned来仅仅包含音素内容，我们借助于一个高容量的WaveNet decoder来从之前的样本点中推理encoder丢失的信息。除此之外，自编码器模型的行为依赖于被应用到隐状态表示的约束条件。我们将三种variants进行对比：一个简单的dimensionality reduction瓶颈特征，一个高斯VAE，和一个离散的VQ-VAE。</p>
<p>实验分析：我们通过speaker independence，预测音素内容的能力，准确重构独立频谱帧的能力三个方面来评估learned representation的质量。除此之外，对于采用VQ-VAE模型提取的离散编码，我们度量从它们映射到phonemes的难度。我们引入了一个“正则化”的机制，来强迫representations集中于语句的音素内容的建模，并且报告出效果是与Zerospeech 2017 无监督声学单元发现任务的前几名能够匹敌的。</p>
<p>关键字：autoencoder，speech representation learning，unsupervised learning, acoustic unit discovery</p>
<h2 id="introduction-1-page">Introduction (1 page)</h2>
<p>Deep learning被高层级表示学习算法所触发，如stacked Restricted Boltzman Machines和Denoising Autoencoders. 然而，近期在计算机视觉，机器翻译，语音识别和语言理解的突破，依赖于大规模的标记数据集，而几乎没有利用无监督表示学习。这样有两点缺陷：1）大规模人工标注的数据集的需求经常使得deep learning models的发展十分昂贵。2）尽管一个深度模型能够擅长于解决一个given task，但是这种方法对于问题域产生有限的启发，主要的启发是从显著输入特征的可视化中组成的，一种仅仅能够应用于问题域的策略，很容易被人类所解释。</p>
<p>本文集中于评估和提高无监督语音表示学习。具体来讲，我们集中于从音素内容中学习能够区分说话人特征（特别是说话人性别和id）的表示，这种特性类似于从语音识别器学习到的内部表示特征。这样的特征在多个任务中都有价值，例如低资源语音识别（当仅仅小数据量的标签训练数据被提供）。在这种情景下，有限的数据可能足够基于无监督发现的表征训练一个声学模型，但是对于训练一个声学模型或者以一种有监督的方式来学习数据表征是不充足的。</p>
<p>我们集中于将自编码器学习到的表征应用到原始语音和频谱特征，并且采用LibriSpeech来调研learned representations的质量。我们tune learned 隐状态表征来仅仅编码phonetic content并且移除其他的混淆特征。然而，为了做信号重构，我们采用了一个自回归的WaveNet模型作为解码器， 来推理被编码器拒绝掉的信息。这个解码器的作用是一个inductive bias，使得编码器无需使用它的capacity来表示低层级细节，而是允许编码器集中于高层级的语义特征。我们发现当使用ASR特征，如MFCC作为输入，原始语音作为decoder targets的时候，能够得到最好的representations. 这强迫系统学习生成在特征抽取的过程中，被移除的sample level的细节。此外，我们注意到VQ-VAE模型可以生成在声学内容和说话人信息之间最好的分割。我们调研VQ-VAE的可视化直观性，通过将它们映射到phonemes，展示了模型超参数在可视化上的影响，并且提出了一个新的正则化机制，能够提高latent representation -&gt; phonetic content 的质量。最终，我们展示了模型在ZeroSpeech 2017声学单元发现任务上的有效性，度量了当一句话的音素发生了最小改变的时候，learned representation的区分度。</p>
<h2 id="representation-learning-with-neural-networks-1.5-page">Representation learning with neural networks (1.5 page)</h2>
<p>神经网络是高层级信息处理的模型，常用多层计算单元来进行实现。每一层可以被理解为是一个特征抽取器，它的输出被传递到上游单元。特别是在图像领域，神经网络学习到的特征可以被展示出一个可视化原子的层级结构，能够与可视化的脑皮层组织在一定程度上匹配。相似地，当应用到语音领域时，神经网络倾向于在音乐，语音的低层级上学习到听觉的频谱特征。</p>
<p>A. 有监督特征学习</p>
<p>神经网络能够采用有监督或者无监督的方式学习到数据表征。在有监督的情况下，从大规模数据集中学习的特征是能直接被使用的，除了data-poor tasks。例如，在视觉领域，ImageNet中发现的特征也会被用作其他计算机视觉任务的输入表示。相似的，语音社区采用从在音素预测任务的网络中抽取的瓶颈特征也可以作为ASR的特征表示。相似的，在NLP，可以从机器翻译或者语言推理任务训练的网络抽取通用文本特征。</p>
<p>B. 无监督特征学习</p>
<p>在这篇文章中，我们关注于无监督特征学习。因为没有训练标签，所以我们采用自编码器，i.e. 网络任务是重构她们的输入。自编码器采用一个encoding network来抽取隐状态表示，然后这个隐状态表示被输入到一个解码器网络来恢复原始数据。理想情况下，隐状态表示保留了原始数据的显著特征，是容易分析和解决的，e.g. 通过解耦数据中不同因子的变化，并且摒弃虚假特征（噪音）。这些渴望的性质典型是通过一个包含正则化技巧和contraints or bottlenecks的明智的应用。因此，自动编码器学习到的表示会受到两种相互竞争的力量的影响。一方面，它会提供给解码器以必要信息为了完美的重构，因此会在隐状态提取尽可能多的输入数据特征模式。另一方面，contraints限制了一些信息需要被摒弃，避免了latent representation颠倒到微不足道。因此，瓶颈特征对于强迫网络学习到一个非-微不足道的数据转换是必要的。</p>
<p>降低隐表示的维度是应用到隐向量的最基本的约束，autoencoder扮演作为一个非线性的variant of 线性低维数据映射，如PCA和SVD。然而，这样的表示是很难直观解释的，因为输入的重构完全依赖于所有的隐状态特征。对比来讲，字典学习的技巧(dictionary learning techniques)，例如sparse和non-negative decompositions，从一个大池子里使用了小数据量的挑选特征来表示每一种输入特征，能够帮助他们的可解释性。基于VQ的离散特征学习能够被视为sparseness的一种极端形式，其中，重构仅需使用字典中唯一的一个元素。</p>
<p>VAE提出了一种特征学习的不同的表示方式，follows 概率框架。自编码网络结构是从一个latent variable generative model衍生的。首先，一个latent vector <span class="math inline">\(z\)</span> 被从一个先验概率<span class="math inline">\(p(z)\)</span> 采样（典型情况下，是一个多维的正态分布）。然后，数据采样点<span class="math inline">\(x\)</span>是采用一个深度解码器神经网络来生成的，神经网络的参数是<span class="math inline">\(\theta\)</span>, <span class="math inline">\(x \sim p(x|z;\theta)\)</span>。然而，在最大似然估计训练中，计算精准的后验概率分布<span class="math inline">\(p(z|x)\)</span>是很困难的。取而代之的是，VAE给后验概率引入了一个变分估计，<span class="math inline">\(q(z|x;\phi)\)</span>，这个变分估计是通过一个参数为<span class="math inline">\(\phi\)</span>的编码器神经网络建模的。因此VAE模拟了一个传统的自编码器的过程，其中，其中，编码器生成隐状态的分布，而不是deterministic encodings，然而解码器是从这个分布生成的样本上进行训练。编码和解码网络是联合训练的，以最大化一个lower bound on the log-likelihood of data point x: <span class="math display">\[
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - \beta D_{KL}(q(z|x;\phi)||p(z))
\]</span> 以上公式中的两个terms分别是autoencoder的重构损失，以及一个对隐状态应用了penalty term。特别地，KL散度表示了网络中的信息数量，即隐表示携带的有关数据样本的信息数量。因此它作为一个隐表示的一个信息瓶颈，其中<span class="math inline">\(\beta\)</span>控制了重构质量和表示相似度之间的trade-off。</p>
<p>VAE目标函数的另一种表示方式明确的限制了隐表示的信息： <span class="math display">\[
J_{VAE}(\theta, \phi; x) = E_{q(z|x;\phi)}[log p(x|z;\theta)] - max(B, D_{KL}(q(z|x;\phi)||p(z)))
\]</span> 其中常数<span class="math inline">\(B\)</span>是指<span class="math inline">\(q\)</span>中自由信息的数量，因为模型仅仅会被惩罚，如果它比隐状态的先验损失传递了超过B的信息。</p>
<p>近期提出的一种VQ-VAE的方式，用确定的量化特征，取代了连续和随机的隐向量。VQ-VAE维护了一些原型向量<span class="math inline">\(e_i, i=1,...,K\)</span>。在前向计算的过程中，编码器生成的representations被原型向量中最近的一个替代。Formally, 让<span class="math inline">\(z_e(x)\)</span>是编码器在量化之前的输出。VQ-VAE通过 <span class="math inline">\(q(x)=argmin_i||z_e(x)-e_i||^2_2\)</span>找到最近的原型，并且将其使用作隐状态表征<span class="math inline">\(z_q(x)=e_{q(x)}\)</span>输入到解码器中。当模型应用到downstream tasks时，learned representation 能够被当作一个distributed representation处理（其中的每一个采样点都被一个连续vector表示），或者一个离散的representation（每个sample都被一个原型ID / token ID表示）。</p>
<p>在反向传播时，loss对于预先量化向量的梯度通过straight-through估计器被估计。 <span class="math display">\[
\frac{\partial \mathcal{L}}{\partial z_e(x)} \approx \frac {\partial \mathcal{L}}{\partial z_q(x)}
\]</span> In TensorFlow, 可以通过如下公式来实现： <span class="math display">\[
z_q(x)=z_e(x)+stop\_gradient(e_{q(x)} - z_e(x))
\]</span> 这些原型，是通过延伸learning objective with terms 来最优化quantization来训练的。原型被强迫与它们所替换的vectors近一些，被称为承诺损失(commitment loss)，用于鼓励编码器生成靠近原型的vectors。如果没有承诺损失，VQ-VAE 训练可以通过生成unbounded magnitude来发散。因此，<strong>VQ-VAE模型的loss包含如下三项，NLL(negative log-likelihood) of the reconstruction, (use the straight-through estimator来反向传播), 2个VQ-相关的loss, 2) prototype 与 assigned vectors 之间的距离，3）commitment cost</strong> <span class="math display">\[
\mathcal{L} = log p(x|z_q(x)) + ||sg(z_e(x)) -e_{q(x)} ||^2_2 + \gamma|| z_e(x) - sg(e_{q(x)})||^2_2
\]</span> 其中，<span class="math inline">\(sg(\cdot)\)</span>是<em>stop gradient</em>,来在反向传播的时候，清空gradient。</p>
<p>VQ-VAE中的量化是一个information bottleneck。编码器可以被理解为一个概率模型，puts all probability mass on the selected discrete token (prototype ID). 假设K个tokens服从均匀先验分布(uniform prior distribution)，KL 散度是常数，并且等于<span class="math inline">\(log K\)</span>。因此KL项不需要被包含在VQ-VAE训练标准中，取而代之的，成为了一个与prototype inventory大小相关的超参数。</p>
<p>VQ-VAE被定性地展示出能够学习到一个区分开文本内容和说话人id的representation。除此之外，discovered tokens能够在有限的设置下被映射到phonemes。</p>
<p>C. 序列数据的自编码</p>
<p>序列数据，如语音或文本，经常包含了能够被生成模型探索的局部依赖信息。事实上，序列数据的纯自回归模型，基于历史数据预测下一个观测点，是非常成功的。对于文本来说，很容易想到与n-gram模型相关的卷积神经语言模型。相似地，WaveNet是一个时域采样点的自回归模型。</p>
<p>这样的自回归模型的缺点是，它们不能够生成数据的隐空间表示。然而，可以将如此的生成模型与一个隐表示抽取器来结合使用。目前有两种解决方案，1是编码器能够处理整句话术，生成一个隐状态向量，然后输入到自回归解码器中，2是编码器能够周期性的生成隐状态特征来输入到解码器中。我们采用方案2。</p>
<p>训练这样的latent variable和自回归的混合模型容易造成隐空间坍塌，其中，解码器学习时，忽略受限隐表示，仅仅使用自回归的无约束信号。对于VAE来说，可以给KL项增加一个权重来阻止这种隐空间坍塌，并且使用free-information formulation。VQ-VAE 自然地对潜在崩溃具有弹性，因为 KL 项是一个超参数，它没有使用给定模型的梯度训练进行优化。</p>
<h2 id="model-description-2-pages">Model description (2 pages)</h2>
<div class="figure">
<img src="/images/VQVAE-model.png" alt="VQVAE-model" />
<p class="caption">VQVAE-model</p>
</div>
<p>模型结构如图1所示。编码器（为了keep the autoencoder viewpoint， encoder可以理解为一层固定的信号处理层）读取原始语音采样点序列，或者是语音特征, 抽取出一个隐向量序列(a sequence of hidden vectors)，被输入到一个bottleneck来成为一个隐表示序列(a sequence of latent representations)。latent vectors的哪一个frequency 被抽取是通过number of strided convolutions applied by the encoder来决定的。</p>
<p>解码器通过采用WaveNet来条件于encoder抽取的latent representation和一个speaker embedding来重构语音。解码器显式条件于speaker identity使得encoder不需要在latent representation中捕捉说话人信息。具体来说，<strong>解码器1) 输入encoder outputs，2）选择性地应用随机正则化到latent vectors. 3）使用卷积函数来将neighboring time steps抽取的latent vectors 整合， 4）上采样至目标采样率。</strong> 语音采样点是采用WaveNet来进行重构的，整合了所有的条件信息，包括：<strong>autoregressive information</strong> about past samples, <strong>global information about</strong> <strong>the speaker</strong>, <strong>latent information</strong> about past and future samples extracted by the encoder. 我们发现encoder's bottleneck和提出的regularization在提取数据表示时是关键的。如果没有bottleneck, 该模型倾向于学习一种简单的重建策略，该策略可以逐字复制未来的样本。我们还注意到编码器与说话人无关，只需要语音数据，而解码器也需要说话人信息。</p>
<p>我们考虑了三种bottleneck：1）简单的降维，2）具有不同潜在表示维数和不同容量的高斯 VAE，3）具有不同数量量化原型的 VQ-VAE。所有bottleneck都可选地跟随下面描述的 dropout 启发的时间抖动正则化(time-jitter regularization)。此外，我们使用原始波形、对数梅尔滤波器组（log-mel filterbank）和梅尔频率倒谱系数 (MFCC) 特征对不同的输入和输出表示进行试验，这些特征丢弃了频谱图中存在的音高信息。</p>
<p>A. Time-jitter regularization</p>
<p>是一个类似于RNN中的Zoneout dropout的本文自主创新的正则化的方法，用于防止过拟合，详情不作描述。</p>
<h2 id="实验-6页">实验 (6页)</h2>
<p>我们在两个数据集上评估模型：LibriSpeech(clean subset)和ZeroSpeech 2017 Contest Track 1 data. 两个数据集的共性是：多说话人，清晰，的read speech(sourced from audio books)以16 kHz录制。除此之外，ZeroSpeech Challenge控制了每个说话人的数量，主要的数据是被其中的几个说话人来讲述的。</p>
<p>第 IV-B 节中介绍的初始实验比较了不同的bottleneck变体，并确定模型在图 1 所示的四个不同探测点处生成的连续潜在表示中保留了来自输入音频的哪些类型的信息。使用在每个探测点计算的表示，我们测量几个预测任务的性能：音素预测（每帧准确度）、说话人身份和性别预测准确度，以及频谱图帧的 L2 重建误差。我们确定 VQ-VAE 学习了在语音内容和说话者身份之间具有最强解缠结的潜在表示，并在以下实验中关注该架构。</p>
<p>在第 IV-C 节中，我们通过将每个离散标记映射到小标记数据集 (LibriSpeech dev) 强制对齐中最常见的对应音素来分析 VQ-VAE 标记的可解释性，并报告单独集上映射的准确性 （LibriSpeech 测试）。 直观地说，这捕获了单个令牌的可解释性。</p>
<p>然后，我们将 VQ-VAE 应用于第 IV-D 节中的 ZeroSpeech 2017 声学单元发现任务 [20]。 此任务评估表示相对于语音类的判别力。 最后，在第 IV-E 节中，我们测量了不同超参数对性能的影响。</p>
<p>A. 默认的模型超参数</p>
<p>我们最好的模型使用 <strong>MFCC 作为编码器输入</strong>，但在<strong>解码器输出处重建原始波形</strong>。 我们使用每 10 毫秒（即以 100 Hz 的速率）提取的标准 13 个 MFCC 特征，并用它们的时间一阶和二阶导数进行扩充。 这些特征最初是为语音识别而设计的，并且大多对音频信号中的音调和类似的混淆细节是不变的。 编码器有 9 个层，每层使用 768 个单元和 ReLU 激活，组织成以下组：2 个预处理卷积层，过滤器长度为 3 和残差连接，1 个步幅卷积长度减少层，过滤器长度为 4，步幅为 2（对信号进行下采样 因子二），然后是 2 个长度为 3 的卷积层和残差连接，最后是 4 个具有残差连接的前馈 ReLU 层。 产生的潜在向量以 50 Hz（即每隔一帧）提取，每个潜在向量取决于 16 个输入帧的感受野。我们还使用了一个具有两个长度缩减层的替代编码器，它以 25 Hz 的频率提取潜在表示，接受场为 30 帧。</p>
<p>当未指定时，潜在表示为 64 维，适用时限制为 14 位。 此外，对于 VQ-VAE，我们使用推荐的 γ = 0.25 [19]。</p>
<p>解码器应用了随机时间抖动正则化（参见第 III-A 部分）。 在训练期间，每个潜在向量被替换为它的任何一个邻居，概率为 0.12。 抖动的潜在序列通过具有滤波器长度 3 和 128 个隐藏单元的单个卷积层，以混合相邻时间步长的信息。 然后对该表示进行 320 次上采样（以匹配 16kHz 音频采样率），并与表示当前说话者的单热向量连接以形成自回归 WaveNet 的调节输入。 WaveNet 由 20 个因果扩张卷积层组成，每个层使用 368 个带有残差连接的门控单元，组织成两个“循环”，每层 10 层，扩张率为 1,2,4,...,29。 调节信号分别传递到每一层。 使用跳过连接将来自 WaveNet 每一层的信号传递到输出。 最后，信号通过 2 个 ReLU 层，256 个单元。 应用 Softmax 来计算下一个样本概率。 我们在 mu-law 压扩后使用了 256 个量化级别。</p>
<p>所有模型都在从训练数据集中均匀采样的 64 个序列的小批量上训练，长度为 5120 个时域样本（320 毫秒）。 <strong>在 4 个 Google Cloud TPU（16 个芯片）上训练一个模型需要一周时间。</strong> 我们使用 Adam 优化器，初始学习率为 4 × 10−4，在 400k、600k 和 800k 步后减半。 Polyak 平均应用于所有用于模型评估的检查点。</p>
<p>B. Bottleneck comparison</p>
<p>我们在 LibriSpeech 上训练模型，并分析在自动编码器瓶颈周围的隐藏表示中捕获的信息，如图 1 所示：</p>
<ul>
<li><span class="math inline">\(p_{enc}\)</span> (768 dim) encoder output prior to the bottleneck,</li>
<li><span class="math inline">\(p_{proj}\)</span> (64 dim) within the bottleneck after projecting to lower dimension,</li>
<li><span class="math inline">\(p_{bn}\)</span> (64 dim) bottleneck output, corresponding to the quantized representation in VQ-VAE, or a random sample from the variational posterior in VAE, and</li>
<li><span class="math inline">\(p_{cond}\)</span> (128 dim) after passing <span class="math inline">\(p_{bn}\)</span> through a convolution layer which captures a larger receptive field over the latent encoding.</li>
</ul>
<p>在每个探测点，我们在四个任务中的每一个上训练具有 2048 个隐藏单元的单独 MLP 网络：对整个片段的说话人性别和身份进行分类（在整个信号中平均汇集潜在向量之后），预测每一帧的音素类（每个潜在向量都制作几个预测），并在每个帧中重建对数梅尔滤波器组特征（再次从每个潜在向量预测几个连续帧）。从信号中捕获高层级语义内容的表示，同时对令人讨厌的低级信号细节保持不变，将具有高音素预测精度和高频谱重建误差。解开的表示还应该具有较低的说话人预测精度，因为该信息明确地提供给解码器调节网络，因此不需要在latent encoding中保留。由于我们主要对发现构建的表示中存在哪些信息感兴趣，因此我们报告了训练性能并且不调整探测网络以进行泛化。</p>
<p>图 2 展示了使用具有不同超参数（latent dimensionality和bottleneck bitrate）的三个瓶颈中的每一个的模型比较，说明了信息通过网络传播的程度。此外，图 3 突出显示了使用不同配置获得的语音内容和说话者身份的分离。</p>
<p>图 2 显示，每种瓶颈类型始终会丢弃 <span class="math inline">\(p_{enc}\)</span> 和 <span class="math inline">\(p_{bn}\)</span> 探针位置之间的信息，这可以从每个任务的性能降低中得到证明。 瓶颈还会影响前面层的信息内容。 特别是对于简单地降低维数的 vanilla 自动编码器 (AE)，<span class="math inline">\(p_{enc}\)</span> 的说话人预测精度和滤波器组重建损失取决于瓶颈的宽度，更窄的宽度会导致更多的信息在编码器的较低层被丢弃。 同样，与维数和比特率匹配的 VQ-VAE 相比，VQ-VAE 和 AE 在 <span class="math inline">\(p_{enc}\)</span> 上产生了更好的滤波器组重建和说话人身份预测，这对应于 VQ-VAE 的令牌数量的对数，以及与先验的 KL 散度 VAE，我们通过设置允许的空闲位数来控制。</p>
<p><strong>正如预期的那样，AE 丢弃的信息最少。 在 <span class="math inline">\(p_{cond}\)</span> 上，表示仍然对说话者和音素具有高度预测性，并且其滤波器组重建是所有配置中最好的。 然而，从无监督学习的角度来看，AE 潜在表示不太有用，因为它混合了源信号的所有属性。</strong></p>
<p>相比之下，VQ-VAE 模型产生的表示可以高度预测信号的语音内容，同时有效地丢弃说话者身份和性别信息。 在更高的比特率下，音素预测与 AE 一样准确。 滤波器组重建也不太准确。 我们观察到说话者信息主要在 <span class="math inline">\(p_{proj}\)</span> 和 <span class="math inline">\(p_{bn}\)</span> 之间的量化步骤中被丢弃。 在 <span class="math inline">\(p_{cond}\)</span> 表示中组合几个潜在向量会产生更准确的音素预测，但额外的上下文无助于恢复说话者信息。 这种现象在图 3 中突出显示。请注意，VQ-VAE 模型对瓶颈维度的依赖性很小，因此我们以默认设置 64 呈现结果。</p>
<p>最后，VAE 模型比简单的降维更好地分离说话人和语音信息，但不如 VQ-VAE。 VAE 比 VQ-VAE 更一致地丢弃语音和说话者信息：在 <span class="math inline">\(p_{bn}\)</span> 处，VAE 的音素预测不太准确，而其性别预测更准确。 此外，在 <span class="math inline">\(p_{cond}\)</span> 上结合更广泛的感受野的信息并不能像 VQ-VAE 模型那样提高音素识别。 对瓶颈维度的敏感性（如图 2 所示）也令人惊讶，与较宽的 VAE 瓶颈相比，较窄的 VAE 瓶颈丢弃的信息更少。 这可能是由于 VAE 的随机操作：为了提供与低瓶颈维度相同的 KL 散度，需要在高维度添加更多噪声。 这种噪声可能会掩盖表示中存在的信息。</p>
<p><strong>基于这些结果，我们得出结论，VQ-VAE 瓶颈最适合于学习潜在表示，这些表示捕获语音内容同时对底层说话者身份保持不变。</strong></p>
<p>C. VQ-VAE token interpretability</p>
<p>到目前为止，我们已经使用 VQ-VAE 作为量化潜在向量的瓶颈。在本节中，我们寻求对离散原型 ID 的解释，评估 VQ-VAE 令牌是否可以映射到音素，即语音的潜在离散成分。示例令牌 ID 显示在图 4 的中间窗格中，我们可以看到令牌 11 始终与瞬态“T”音素相关联。为了评估其他标记是否有类似的解释，我们测量了逐帧音素识别准确度，其中每个标记被映射到 41 个音素中的一个。我们使用 460 小时干净的 LibriSpeech 训练集进行无监督训练，并使用来自干净的开发子集的标签将每个标记与最可能的音素相关联。我们通过在干净的测试集上以 100 Hz 的帧速率计算逐帧电话识别准确度来评估映射。使用来自 s5 LibriSpeech 配方 [55] 的 Kaldi tri6b 模型从强制对齐中获得真实音素边界。</p>
<p>表 I 显示了在 LibriSpeech 上获得 VQ-VAE 令牌到音素的最佳准确度的配置的性能。 在两个时间点给出识别精度：在 200k 梯度下降步骤之后，当可以评估模型的相对性能时，以及在模型收敛后 900k 步之后。 我们没有观察到训练时间较长的过度拟合。 为所有帧预测最常见的静音音素将准确度下限设置为 16%。 在完整的 460 小时训练集上有区别地训练以预测具有与 25 Hz 编码器相同架构的音素的模型实现了 80% 的逐帧音素识别准确率，而没有时间减少层的模型将上限设置为 88%。</p>
<p>表 I 表明映射精度随着标记数量的增加而提高，使用 32768 个标记的最佳模型达到 64.5% 的精度。 然而，最大的准确度增益出现在 4096 个令牌时，随着令牌数量的进一步增加，收益递减。 该结果与 Kaldi tri6b 模型中使用的 5760 个绑定三音素状态大致对应。</p>
<p>我们还注意到，增加令牌的数量并不会轻易提高准确性，因为我们衡量的是泛化，而不是集群纯度。 在为每个帧分配不同标记的限制下，由于对我们建立映射的小型开发集过度拟合，准确性会很差。 然而，在我们的实验中，我们始终观察到提高的准确性。</p>
<p>D. 无监督ZeroSpeech 2017声学单元发现任务</p>
<p>ZeroSpeech 2017 语音单元发现任务评估representation区分不同声音的能力，而不是将representation映射到预定义语音单元的难易程度。因此，它是对上一节中使用的音素分类准确度度量的补充。 ZeroSpeech 评估方案使用最小对 ABX 测试，该测试评估模型区分三个音素长语音段对的能力，这些语音仅在中间音素（例如“get”和“得到了”）。我们在提供的训练数据（英语 45 小时，法语 24 小时，普通话 2.5 小时）上训练模型，并使用官方评估脚本对测试数据进行评估。为了确保我们不会过拟合 ZeroSpeech 任务，我们只考虑了在LibriSpeech 上找到的最佳超参数设置（参见第 IV-E 部分）。此外，为了最大限度地遵守 ZeroSpeech 约定，我们对所有语言使用了相同的超参数，在表 II 中表示为 VQ-VAE（每语言、MFCC、<span class="math inline">\(p_{cond}\)</span>）。</p>
<p>在带有足够大训练数据集的英语和法语上，尽管使用了独立于说话人的编码器，但我们取得的结果比顶级参赛者更好。</p>
<p>结果与我们对 VQ-VAE 瓶颈执行的信息分离的分析一致：在更具挑战性的跨说话者评估中，最佳性能使用 <span class="math inline">\(p_{cond}\)</span> 表示，它结合了瓶颈表示（VQ-VAE）的几个相邻帧 ,（表 II 中的每个 lang、MFCC、<span class="math inline">\(p_{cond}\)</span>））。 比较说话人内部和说话人之间的结果同样与第 IV-B 部分中的观察结果一致。 在说话人内部的情况下，没有必要从语音内容中分离说话人身份，因此 <span class="math inline">\(p_{proj}\)</span> 和 <span class="math inline">\(p_{bn}\)</span> 探测点之间的量化会损害性能（尽管在英语中，通过考虑 <span class="math inline">\(p_{cond}\)</span> 的更广泛的上下文来纠正这一点）。 在跨说话人的情况下，量化提高了英语和法语的分数，因为丢弃混杂说话人信息的收益抵消了一些语音细节的损失。 此外，丢弃的语音信息可以通过在 <span class="math inline">\(p_{cond}\)</span> 处混合相邻的时间步长来恢复。</p>
<p><strong>VQ-VAE 在普通话上的表现更差</strong>，我们可以将其归因于三个主要原因。 首先，训练数据集仅包含 2.4 小时或语音，导致过度拟合（参见第 IV-E7 节）。 这可以通过多语言训练得到部分改善，如 VQ-VAE，（所有语言，MFCC，<span class="math inline">\(p_{cond}\)</span>）。 <strong>其次，普通话是一种声调语言，而默认输入特征 (MFCC) 会丢弃音高信息。</strong> 我们注意到在 mel filterbank 特征（VQ-VAE，（所有 lang，<span class="math inline">\(f_{bank}\)</span>，<span class="math inline">\(p_{proj}\)</span>））上训练的多语言模型略有改进。 第三，VQ-VAE 被证明不会在潜在表示中编码韵律。 比较各个探测点的结果，我们发现普通话是唯一一种 VQ 瓶颈会丢弃信息并降低跨说话者测试制度中性能的语言。 尽管如此，多语言预量化特征产生的精度与相当。</p>
<p>我们不认为需要更多无监督训练数据是一个问题。 未标记的数据非常丰富。 我们认为，需要并且可以更好地利用大量未标记训练数据的更强大的模型比性能在小数据集上饱和的更简单的模型更可取。 然而，增加训练数据量是否有助于普通话 VQ-VAE 学会丢弃更少的音调信息还有待验证（多语言模型可能已经学会这样做以适应法语和英语）。</p>
<p>E. Hyperparameter impact</p>
<p>所有 VQ-VAE 自动编码器超参数都使用多组网格搜索 (grid-search) 在 LibriSpeech 任务上进行了调整，优化了最高的音素识别精度。 我们还在 ZeroSpeech 挑战任务的英语部分验证了这些设计选择。 事实上，我们发现所提出的时间抖动正则化提高了所有输入表示的 ZeroSpeech ABX 分数。 使用 MFCC 或滤波器组特征会产生比使用波形更好的分数，并且当使用更多令牌时，模型始终会获得更好的分数。</p>
<ol style="list-style-type: decimal">
<li>时间抖动正则化：在表 III 中，我们分析了时间抖动正则化对 VQ-VAE 编码的有效性，并将其与两种 dropout 变体进行比较：常规 dropout 应用于编码的各个维度和 dropout 随机应用于整个 在各个时间步进行编码。 常规 dropout 不会强制模型在相邻的时间步长中分离信息。 Step-wise dropout 促进了跨时间步独立的编码，并且性能比时间抖动略差6。</li>
</ol>
<p>所提出的时间抖动正则化大大提高了令牌映射的准确性，并扩展了性能良好的令牌帧速率范围，包括 50 Hz。 虽然 LibriSpeech 令牌精度在 25 Hz 和 50 Hz 下相当，但更高的令牌发射频率对于 ZeroSpeech AUD 任务很重要，50 Hz 模型在该任务上明显更好。 这种行为是由于 25 Hz 模型容易忽略短音（第 IV-E6 节），这会影响 ABX 在 ZeroSpeech 任务上的结果。</p>
<p>我们还分析了 VQ-VAE、VAE 和简单降维 AE 瓶颈的四个探测点的信息内容，如图 5 所示。对于所有瓶颈机制，正则化限制了滤波器组重建的质量并提高了音素识别精度 在约束表示中。 然而，在 <span class="math inline">\(p_{cond}\)</span> 探测点中组合了相邻的时间步之后，这种好处就变小了。 此外，对于 VQ-VAE 和 VAE，正则化会降低性别预测的准确性，并使表示对说话者的敏感度略低。</p>
<ol start="2" style="list-style-type: decimal">
<li>输入表示：在这组实验中，我们使用不同的输入表示来比较性能：原始波形、log-mel 频谱图或 MFCC。 原始波形编码器使用 9 个跨步卷积层，这导致令牌提取频率为 30 Hz。 然后，我们用常规的 ASR 数据管道替换了波形：每 10 毫秒从 25 毫秒长的窗口中提取 80 个对数梅尔滤波器组特征，从梅尔滤波器组输出中提取 13 个 MFCC 特征，两者都增加了它们的一阶和二阶时间导数。 在编码器中使用两个跨步卷积层导致这些模型的令牌速率为 25 Hz。</li>
</ol>
<p>结果报告在表III的底部。 高级特征，尤其是 MFCC，比波形表现更好，因为按照设计，它们会丢弃有关音高的信息并提供一定程度的说话人不变性。 使用这种简化的表示迫使编码器向解码器传输更少的信息，作为对更多说话者不变潜在编码的归纳偏置。</p>
<p>3）输出表示：我们构建了一个自回归解码器网络，重建滤波器组特征而不是原始波形样本。 受文本到语音系统最近进展的启发，我们实现了一个类似 Tacotron 2 的解码器，在自回归信息流上有一个内置的信息瓶颈，这被发现在 TTS 应用程序中至关重要。 与 Tacotron 2 类似，滤波器组特征首先由一个小的“预网络”处理，我们应用了大量的 dropout 并将解码器配置为并行预测多达 4 帧。 然而，这些修改最多产生 42% 的音素识别准确率，明显低于本文中描述的其他架构。 然而，该模型的训练速度要快一个数量级。</p>
<p>最后，我们分析了解码 WaveNet 的大小对 VQ-VAE 提取的表示的影响。 我们发现整体感受野 (RF) 的影响大于 WaveNet 的深度或宽度。 特别是，当解码器的感受野跨越大约 10 毫秒时，潜在表示的属性会发生很大的变化。 如图 6 所示，对于较小的 RF，调节信号包含更多说话人信息：性别预测接近 80%，而逐帧音素预测准确度仅为 55%。 对于较大的 RF，性别预测准确率约为 60%，而音素预测的峰值接近 65%。 最后，虽然重建对数似然随着 WaveNet 深度提高到 30 层，但音素识别准确度稳定在 20 层。 由于 WaveNet 的计算成本最大，我们决定保留 20 层配置。</p>
<ol start="4" style="list-style-type: decimal">
<li><p>Decoder speaker conditioning：WaveNet 解码器基于三个信息源生成样本：先前发出的样本（通过自回归连接）、对说话者或其他时间固定的信息的全局调节以及提取的时变表示 从编码器。 我们发现禁用全局speaker调节会使音素分类准确度降低 3 个百分点。 这进一步证实了我们关于 VQ-VAE 瓶颈引起的解缠结的发现，这使模型偏向于丢弃以更明确形式提供的信息。 在我们的实验中，我们使用了独立于speaker的编码器。 但是，使编码器适应speaker可能会进一步改善结果。 事实上，展示了使用说话人自适应方法对 ZeroSpeech 任务的改进。</p></li>
<li><p>编码器超参数：我们尝试调整编码器卷积层的数量、滤波器的数量和滤波器长度。 一般来说，使用更大的编码器会提高性能，但是我们确定必须仔细控制编码器的感受野，性能最好的编码器对于每个生成的令牌可以看到大约 0.3 秒的输入信号。</p></li>
</ol>
<p>可以使用两种机制控制有效的感受野：通过仔细调整编码器架构，或通过设计具有宽感受野的编码器，但将训练期间看到的信号段的持续时间限制为所需的感受野。 通过这种方式，模型永远不会学会使用其全部容量。 当模型在 2.5s 长段上训练时，感受野为 0.3s 的编码器的帧音素识别准确率为 56.5%，而感受野为 0.8s 的编码器的得分仅为 54.3%。 当在 0.3 秒的片段上训练时，两个模型的表现相似。</p>
<ol start="6" style="list-style-type: decimal">
<li>瓶颈比特率：语音 VQ-VAE 编码器可以看作是使用非常低的比特率对信号进行编码。 为了达到预定的目标比特率，可以控制令牌率（即通过控制编码器跨步卷积中的下采样程度）和每一步提取的令牌数（或等效的比特数）。 我们发现标记率是一个必须谨慎选择的关键参数，在 50 Hz（56.0% 音素识别准确率）和 25 Hz（56.3%）下获得 200k 训练步骤后获得最佳结果。 准确率在较高的令牌率（100 Hz 时为 49.3%）时突然下降，而较低的令牌率会错过非常短的电话（12.5 Hz 时为 53% 的准确率）。</li>
</ol>
<p>与令牌的数量相比，VQ-VAE 嵌入的维度对表示质量具有次要影响。 我们发现 64 是一个很好的设置，小得多的维度会降低具有少量标记的模型的性能，而更高的维度会对具有大量标记的模型的性能产生负面影响。</p>
<p>为完整起见，我们观察到即使对于具有最大令牌库存的模型，整体编码器比特率也很低：50 Hz 时为 14 位 = 700 bps，这与经典语音编解码器的最低比特率相当。</p>
<ol start="7" style="list-style-type: decimal">
<li>训练语料库大小：我们在 LibriSpeech 训练集的子集上试验了训练模型，大小从 4.6 小时 (1%) 到 460 小时 (100%) 不等。 对 4.6 小时的数据进行训练，音素识别准确率在 100k 步时达到 50.5% 的峰值，然后下降。 9 小时的训练在 180k 集上达到了 52.5% 的峰值准确率。 当训练集的大小增加超过 23 小时时，音素识别率在大约 90 万步后达到 54%。 通过对完整 460 小时的数据进行训练，没有发现进一步的改进。 我们没有观察到任何过度拟合，并且为了获得最佳结果，训练模型直到达到 900k 步而没有提前停止。未来一个有趣的研究领域将是研究增加模型容量以更好地利用大量未标记数据的方法。</li>
</ol>
<p>数据集大小的影响在 ZeroSpeech Challenge 结果（表二）中也可见：VQ-VAE 模型在英语（45 小时的训练数据）和法语（24 小时）上表现良好，但在普通话上表现不佳（ 2.5 小时）。 此外，在英语和法语上，我们使用在单语数据上训练的模型获得了最好的结果。 使用对所有语言的数据联合训练的模型在普通话上获得了稍好的结果。</p>
<h2 id="related-work-1页">Related Work (1页)</h2>
<p>序列数据的 VAE 在 [49] 中被引入。 该模型使用 LSTM 编码器和解码器，而潜在表示由编码器的最后一个隐藏状态形成。 该模型被证明对自然语言处理任务很有用。 然而，它也证明了潜在表示崩溃的问题：当一个强大的自回归解码器与潜在编码的惩罚同时使用时，比如 KL 先验，VAE 倾向于忽略先验并表现得好像它是一个 纯自回归序列模型。 这个问题可以通过改变 KL 项的权重来缓解，并通过使用 word dropout 限制自回归路径上的信息量 [49]。 在确定性自动编码器中也可以避免潜在崩溃，例如 [64]，它将卷积编码器耦合到强大的自回归 WaveNet 解码器 [18]，以学习由来自各种乐器的孤立音符组成的音乐音频的潜在表示。</p>
<p>我们凭经验验证，根据说话人信息调节解码器会导致编码更具有说话人不变性。[54] 给出了一个严格的证明，这种方法产生的表示对于明确提供的信息是不变的，并将其与域对抗训练相关联，这是另一种旨在对已知干扰因素实施不变性的技术 [65]。</p>
<p>当应用于音频时，VQ-VAE 使用 WaveNet 解码器从建模信息中释放潜在表示，这些信息很容易从最近的过去 [19] 中恢复。 它通过使用具有统一先验的离散潜在代码来避免后折叠问题，从而导致恒定的 KL 惩罚。 我们采用相同的策略来设计潜在表示正则化器：我们没有使用可能导致潜在空间崩溃的惩罚项来扩展成本函数，而是依靠潜在变量的随机副本来防止它们的共同适应并促进稳定性 时间。</p>
<p>本文中引入的随机时间抖动正则化受到数据的缓慢表示 [48] 和 dropout 的启发，dropout 在训练神经元期间随机删除以防止它们的协同适应 [50]。 它也与 Zoneout [51] 非常相似，后者依赖于所选神经元的随机时间副本来规范循环神经网络。</p>
<p>几位作者最近提议使用使用变量层次结构的 VAE 对序列进行建模。 [66] 探索了一个分层潜在空间，它将序列相关变量与序列无关变量分开。 他们的模型被证明可以执行说话人转换并在存在域不匹配的情况下提高自动语音识别 (ASR) 性能。 [67] 为序列数据引入了一个随机潜在变量模型，该模型还可以产生解开的表示，并允许在生成的序列之间进行内容交换。 这些其他方法可能会受益于规范潜在表示以实现进一步的信息解开。</p>
<p>声学单位发现系统旨在将声学信号转换成一系列类似于音素的可解释单位。 它们通常涉及声学帧、MFCC 或神经网络瓶颈特征的聚类，使用概率先验进行正则化。 DP-GMM [68] 在高斯混合模型上强加了狄利克雷过程先验。 使用 HMM 时间结构为子语音单元扩展它会导致 DP-HMM 和 HDP-HMM [69]、[70]、[71]。 HMM-VAE 建议使用深度神经网络代替 GMM [72]、[73]。 这些方法通过 HMM 时间平滑和时间建模来强制执行自上而下的约束。 语言单元发现模型在类似单词的级别检测重复出现的语音模式，找到具有约束动态时间扭曲的常见重复段 [74]。</p>
<p>在分段无监督语音识别框架中，神经自动编码器用于将可变长度的语音段嵌入到一个公共向量空间中，在那里它们可以被聚类为单词类型 [75]。 [76] 用一个模型代替分段自动编码器，该模型可以预测附近的语音片段，并证明该表示与词嵌入共享许多属性。 结合无监督的分词算法和在单独的语料库 [77] 上发现的词嵌入的无监督映射，该方法产生了一个在不成对的语音和文本数据上训练的 ASR 系统 [78]。</p>
<p>ZeroSpeech 2017 挑战赛的几个条目依赖于神经网络来发现语音单元。 [61] 在使用无监督术语发现系统 [79] 找到的语音段对上训练自动编码器。 [59]首先对语音帧进行聚类，然后训练神经网络来预测聚类 ID，并将其隐藏表示用作特征。 [60] 使用在 MFCC 上训练的自动编码器发现的特征扩展了该方案。</p>
<h2 id="conclusion-半页">Conclusion (半页)</h2>
<p>我们将序列自动编码器应用于语音建模并比较了不同的<strong>信息瓶颈，包括 VAE 和 VQ-VAE</strong>。 我们使用可解释性标准以及区分相似语音的能力仔细评估了诱导的潜在表示。 <strong>瓶颈的比较表明，使用 VQ-VAE 获得的离散表示保留了最多的语音信息，同时也是最大的说话人不变性。</strong> 提取的表示允许将提取的符号准确映射到音素，并在 ZeroSpeech 2017 声学单元发现任务中获得有竞争力的表现。 Cho 等人的 VQ-VAE 编码器和 WaveNet 解码器的类似组合。 在 ZeroSpeech 2019 [80] 中具有最佳的声学单元发现性能。</p>
<p>我们确定模型需要一个信息瓶颈来学习将内容与说话者特征分开的表示。 此外，我们观察到，<strong>通过使瓶颈强度成为模型超参数，或者完全去除它（如在 VQ-VAE 中）</strong>，或者通过使用自由信息 VAE 目标，可以<strong>避免由太强的瓶颈引起的潜在崩溃问题 .</strong></p>
<p>为了进一步提高表示质量，我们引入了一种<strong>时间抖动正则化方案，该方案限制了潜在代码的容量，但不会导致潜在空间的崩溃。</strong> 我们希望这可以类似地提高与其他问题域中的自回归解码器一起使用的潜在变量模型的性能。</p>
<p>VAE 和 VQ-VAE 都限制了潜在表示(latent representation)的信息带宽(information bandwidth)。 然而，VQ-VAE 使用量化机制，它确定性地强制编码等于原型，而 VAE 通过注入噪声来限制信息量。 <strong>在我们的研究中，VQ-VAE 比 VAE 产生了更好的信息分离。</strong> 然而，需要进一步的实验来充分理解这种影响。 <strong>特别是，这是量化的结果还是确定性操作的结果？</strong></p>
<p>我们还观察到，虽然 VQ-VAE 产生离散表示，但为了获得最佳结果，它使用了一个如此大的标记集，以至于为每个标记分配一个单独的含义是不切实际的。 特别是，在我们的 ZeroSpeech 实验中，<strong>我们使用了每个令牌的密集嵌入表示，这提供了比简单地使用令牌标识更细微的令牌相似性度量。 也许需要更结构化的潜在表示，其中可以以连续方式调制一小组单元。</strong></p>
<p>广泛的超参数评估表明，优化编码器和解码器网络的感受野大小对于良好的模型性能很重要。 多尺度建模方法可以进一步分离韵律信息。 我们的自动编码方法还可以与更专门用于语音处理的惩罚相结合。 在 [73] 中引入 HMM 先验可以促进潜在表示，从而更好地模仿语音的时间语音结构。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Auto-Encoding Variational Bayes and Tutorials</title>
    <url>/2021/06/30/vae/</url>
    <content><![CDATA[<h1 id="auto-encoding-variational-bayes-自编码变分贝叶斯-httpsarxiv.orgpdf1312.6114v5.pdf-----diederik-p.-kingma-max-welling-machine-learning-group-universiteit-van-amsterdam-may-2014">Auto-Encoding Variational Bayes 自编码变分贝叶斯 https://arxiv.org/pdf/1312.6114v5.pdf --- Diederik P. Kingma, Max Welling (Machine Learning Group, Universiteit van Amsterdam) May 2014</h1>
<h2 id="感想">感想</h2>
<p>看了个Abstract就隐约感觉自己看不懂文章正文了。。。果断跳到tutorials（tutorials在下面）。看完tutorials后又回来想看看原文，看了四个章节，发现着实看不懂。。。没办法给分了。。</p>
<h2 id="abstract">Abstract</h2>
<p>在存在具有难以处理的后验分布的连续潜在变量和大型数据集的情况下，我们如何在有向概率模型中执行有效的推理和学习？我们引入了一种可扩展到大型数据集的随机变分推理和学习算法，并且在一些温和的可微性条件下，甚至可以在棘手的情况下工作。我们的贡献是双重的。 首先，我们展示了变分下界的重新参数化产生了下界估计量，可以使用标准随机梯度方法直接优化该估计量。 其次，我们证明对于 i.i.d. 对于每个数据点具有连续潜在变量的数据集，通过使用建议的下限估计器将近似推理模型（也称为识别模型）拟合到棘手的后验模型，可以使后验推理特别有效。 理论优势体现在实验结果上。</p>
<h2 id="introduction">Introduction</h2>
<p>我们如何使用定向概率模型执行有效的近似推理和学习，这些模型的连续潜在变量和/或参数具有难以处理的后验分布？ 变分贝叶斯 (VB) 方法涉及对难以处理的后验的近似的优化。 不幸的是，常见的平均场方法需要期望 w.r.t. 的解析解。 近似后验，这在一般情况下也是难以处理的。 我们展示了变分下界的重新参数化如何产生下界的简单可微无偏估计量； 这种 SGVB（随机梯度变分贝叶斯）估计器可用于几乎任何具有连续潜在变量和/或参数的模型中的有效近似后验推断，并且可以直接使用标准随机梯度上升技术进行优化。</p>
<p>对于 i.i.d. 数据集和每个数据点的连续潜在变量，我们提出了自动编码 VB (AEVB) 算法。 在 AEVB 算法中，我们通过使用 SGVB 估计器优化识别模型使推理和学习特别高效，该模型允许我们使用简单的祖先采样执行非常有效的近似后验推理，这反过来又使我们能够有效地学习模型参数，而无需 每个数据点需要昂贵的迭代推理方案（例如 MCMC）。 学习到的近似后验推理模型还可用于许多任务，例如识别、去噪、表示和可视化目的。 当神经网络用于识别模型时，我们就得到了变分自动编码器。</p>
<h2 id="方法">方法</h2>
<p>本节中的策略可用于为具有连续潜在变量的各种有向图模型推导出下界估计量（随机目标函数）。 我们将把自己限制在我们有一个 i.i.d 的常见情况下。 每个数据点具有潜在变量的数据集，以及我们喜欢对（全局）参数执行最大似然（ML）或最大后验（MAP）推断，以及对潜在变量进行变分推断。例如，可以直接将此场景扩展到我们还对全局参数执行变分推理的情况； 该算法放在附录中，但这种情况下的实验留待未来工作。 请注意，我们的方法可以应用于在线的非平稳设置，例如 流数据，但为了简单起见，我们在这里假设一个固定的数据集。</p>
<h3 id="问题情景">2.1 问题情景</h3>
<p>让我们考虑一些由 <span class="math inline">\(N\)</span>个 i.i.d. 组成的数据集 <span class="math inline">\(X = \{x(i)\}^N_{i=1}\)</span>。 一些连续或离散变量 <span class="math inline">\(x\)</span> 的样本。 我们假设数据是由一些随机过程生成的，涉及一个未观察到的连续随机变量 <span class="math inline">\(z\)</span>。 该过程包括两个步骤：（1）从一些先验分布 <span class="math inline">\(p_{θ∗}(z)\)</span> 生成一个值 <span class="math inline">\(z(i)\)</span>； (2) 值 <span class="math inline">\(x^{(i)}\)</span> 是从一些条件分布 <span class="math inline">\(p_{θ∗} (x|z)\)</span> 生成的。 我们假设先验 <span class="math inline">\(p_{θ∗} (z)\)</span> 和似然 <span class="math inline">\(p_{θ∗} (x|z)\)</span> 来自分布 <span class="math inline">\(p_{θ} (z)\)</span> 和 <span class="math inline">\(p_θ (x|z)\)</span> 的参数族，并且它们的 PDF 几乎在任何地方都是可微的 w.r.t. <span class="math inline">\(θ\)</span> 和 <span class="math inline">\(z\)</span>。 不幸的是，这个过程中有很多是隐藏在我们看来的：真实参数 <span class="math inline">\(θ∗\)</span> 以及潜在变量 <span class="math inline">\(z^{(i)}\)</span> 的值对我们来说是未知的。</p>
<p>非常重要的是，我们没有对边际或后验概率做出常见的简化假设。 相反，我们在这里对一种通用算法感兴趣，该算法甚至在以下情况下也能有效工作：</p>
<ol style="list-style-type: decimal">
<li>难处理性：边际似然 <span class="math inline">\(p_θ(x) = 􏰇\int p_θ(z)p_θ(x|z)dz\)</span> 的积分是难处理的（因此我们无法评估或区分边际似然）的情况，其中真正的后验 密度 <span class="math inline">\(p_θ(z|x) = p_θ(x|z)p_θ(z)/p_θ(x)\)</span> 是难以处理的（因此不能使用 EM 算法），以及任何合理的平均场 VB 算法所需的积分 也难治。 这些难以处理的问题很常见，出现在中等复杂的似然函数 <span class="math inline">\(p_θ(x|z)\)</span> 的情况下，例如 具有非线性隐藏层的神经网络。</li>
<li>大数据集：我们有太多的数据，批量优化成本太高； 我们希望使用小批量甚至单个数据点进行参数更新。 基于采样的解决方案，例如 Monte Carlo EM 通常会太慢，因为它涉及每个数据点的典型昂贵的采样循环。</li>
</ol>
<p>我们对上述场景中的三个相关问题感兴趣并提出了解决方案：</p>
<ol style="list-style-type: decimal">
<li>参数 <span class="math inline">\(θ\)</span> 的有效近似 ML 或 MAP 估计。 参数本身可能很有趣，例如 如果我们正在分析一些自然过程。 它们还允许我们模拟隐藏的随机过程并生成类似于真实数据的人工数据。</li>
<li>给定参数 <span class="math inline">\(θ\)</span> 的观测值 <span class="math inline">\(x\)</span> 的潜在变量 <span class="math inline">\(z\)</span> 的有效近似后验推断。 这对于编码或数据表示任务很有用。</li>
<li>变量<span class="math inline">\(x\)</span>的有效近似边际推断。 这使我们能够执行需要先验于 <span class="math inline">\(x\)</span> 的各种推理任务。 计算机视觉中的常见应用包括图像去噪、修复和超分辨率。</li>
</ol>
<p>为了解决上述问题，我们引入一个识别模型<span class="math inline">\(q_{\phi}(z|x)\)</span>：对难处理的真实后验<span class="math inline">\(p_θ(z|x)\)</span>的近似。 请注意，与平均场变分推理中的近似后验相反，它不一定是阶乘的，并且它的参数 <span class="math inline">\(\phi\)</span> 不是从某些封闭形式的期望中计算出来的。 相反，我们将介绍一种学习识别模型参数 <span class="math inline">\(\phi\)</span> 和生成模型参数 <span class="math inline">\(θ\)</span> 的方法。</p>
<p>从编码理论的角度来看，未观察到的变量 <span class="math inline">\(z\)</span> 具有作为潜在表示或代码的解释。 因此，在本文中，我们还将识别模型 <span class="math inline">\(q_{\phi}(z|x)\)</span> 称为概率编码器，因为给定一个数据点 <span class="math inline">\(x\)</span>，它会在代码 <span class="math inline">\(z\)</span> 的可能值上产生分布（例如高斯分布），数据点 <span class="math inline">\(x\)</span> 来自该值 本来可以生成的。 类似地，我们将 <span class="math inline">\(p_θ(x|z)\)</span> 称为概率解码器，因为给定一个代码 <span class="math inline">\(z\)</span>，它会在 <span class="math inline">\(x\)</span> 的可能对应值上产生分布。</p>
<h3 id="变分边界">2.2 变分边界</h3>
<p>边际似然由单个数据点的边际似然总和 <span class="math inline">\(log p_θ(x^{(1)}, · · · , x^{(N)}) = 􏰍\sum^N_{i=1} log p_θ(x^{(i)})\)</span> 组成，每个都可以是 改写为： <span class="math display">\[
log p_θ(x^{(i)}) = D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z|x^{(i)})) + \mathcal{L}(θ, \phi; x^{(i)})
\]</span> 第一个 RHS 项是近似值与真实后验值的 KL 散度。 由于这个 KL 散度是非负的，第二个 RHS 项 <span class="math inline">\(\mathcal{L}(θ, \phi; x^{(i)})\)</span> 被称为数据点 i 的边际似然的（变分）下界，可以写成： <span class="math display">\[
log p_θ(x^{(i)}) ≥ \mathcal{L}(θ, \phi; x^{(i)}) = E_{q_{\phi}(z|x)} [− log q_{\phi}(z|x) + log p_θ(x, z)]
\]</span> 也可以写成： <span class="math display">\[
\mathcal{L}(θ,\phi;x^{(i)} )=−D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z))+E_{q_{\phi}(z|x^{(i)})} [logp_θ(x^{(i)} |z)]
\]</span> 我们想要区分和优化下界 <span class="math inline">\(L(θ,\phi;x^{(i)})\)</span> w.r.t. 变分参数 <span class="math inline">\(\phi\)</span> 和生成参数 <span class="math inline">\(θ\)</span>。 然而，下界的梯度 w.r.t. <span class="math inline">\(\phi\)</span> 有点问题。 这类问题通常的 (na ̈ıve) Monte Carlo 梯度估计量是： <span class="math inline">\(∇_{\phi}E_{q_{\phi}(z)} [f(z)] = E_{q_{\phi} (z)} 􏰋f(z)∇_{q_{\phi} (z)} log q_{\phi}(z) ≃ \frac{1} {􏰍L} \sum^L_{l=1} f(z)∇_{q_{\phi}(z^{(l)})} log q_{\phi}(z^{(l)})\)</span> 其中 $ z^{(l)} ∼ q_{}(z|x^{(i)})$。 这个梯度估计器表现出非常高的方差（参见例如 [BJP12]），对于我们的目的来说是不切实际的。</p>
<h3 id="sgvb-估计器和-aevb-算法">2.3 SGVB 估计器和 AEVB 算法</h3>
<p>在本节中，我们将介绍一个实际估计器的下界及其w.r.t参数的导数。 我们假设近似后验形式为 <span class="math inline">\(q_{\phi}(z|x)\)</span>，但请注意，该技术也适用于 <span class="math inline">\(q_{\phi}(z)\)</span> 的情况，即我们不以 <span class="math inline">\(x\)</span> 为条件的情况。 用于推断参数后验的全变分贝叶斯方法在附录中给出。</p>
<p>在 2.4 节概述的某些温和条件下，对于选定的近似后验 <span class="math inline">\(q_{\phi}(z|x)\)</span>，我们可以使用（辅助）噪声变量<span class="math inline">\(ε\)</span>的可微变换 <span class="math inline">\(g_{\phi} (ε, x)\)</span> 重新参数化随机变量 <span class="math inline">\(􏰐\tilde z ∼ q_{\phi} (z|x)\)</span> ： <span class="math display">\[
􏰐z=g_{\phi}(ε,x), ε∼p(ε)
\]</span></p>
<p>有关选择此类适当分布 <span class="math inline">\(p(ε)\)</span> 和函数 <span class="math inline">\(g_{\phi}(ε,x)\)</span> 的一般策略，请参见第 2.4 节。 我们现在可以对某函数 <span class="math inline">\(f(z)\)</span> w.r.t. $q_{}(z|x) $的期望形成蒙特卡罗估计。 如下： <span class="math display">\[
E_{q_{\phi}(z|x^{(i)})} [f(z)] = E_{p(ε)} [f(g_{\phi}(ε, x^{(i)}))] ≃ \frac{1}{L} \sum^L_{l=1} f(g_{\phi}(\epsilon^{(l)}, x^{(i)})), ε^{(l)} ∼ p(ε)
\]</span></p>
<p>我们将这个技巧应用到变分下界，公式（2），可以生成通用“随机梯度变分贝叶斯 (SGVB) 估计器” $ L^A(θ, ; x^{(i)}) ≃ (θ, ; x^{(i)}):$ <span class="math display">\[
\tilde L^A(θ, \phi; x^{(i)}) =\frac{1}{L} \sum^L_{l=1}log p_θ(x^{(i)}, z^{(i,l)}) − log q_{\phi}(z^{(i,l)}|x^{(i)})
\]</span> 其中 <span class="math inline">\(z^{(i,l)} = g_{\phi}(ε^{(i,l)}, x^{(i)}), ε^{(l)} ∼ p(ε)\)</span></p>
<p>通常，方程(3)的 KL 散度 <span class="math inline">\(D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z))\)</span>可以通过解析积分（见附录 B），使得只有预期的重构误差 <span class="math inline">\(E_{q_{\phi}(z|x^{(i)}) }􏰋logp_θ(x^{(i)}|z)\)</span> 需要抽样估计。 KL 散度项可以被解释为正则化 <span class="math inline">\(\phi\)</span>，鼓励近似后验接近先验 <span class="math inline">\(p_θ(z)\)</span>。 这产生了第二个版本SGVB 估计量 <span class="math inline">\(\tilde L^B(θ,\phi;x^{(i)}) ≃ L(θ,\phi;x^{(i)})\)</span>，对应于方程 (3)，它通常比通用估计量的方差更小： <span class="math display">\[
\tilde L^B(θ,\phi;x^{(i)}) = - D_{KL}(q_{\phi}(z|x^{(i)})||p_θ(z)) + \frac{1}{L} \sum^L_{l=1}log p_θ(x^{(i)}, z^{(i,l)})
\]</span> 其中，<span class="math inline">\(z^{(i,l)} = g_{\phi}(ε^{(i,l)}, x^{(i)}), ε^{(l)} ∼ p(ε)\)</span></p>
<p>给定来自具有 <span class="math inline">\(N\)</span> 个数据点的数据集 <span class="math inline">\(X\)</span> 的多个数据点，我们可以构建完整数据集的边际似然下界，基于小批量： <span class="math display">\[
L(θ,\phi;X) ≃\tilde L^M(θ,\phi;X^M) = \frac{N}{M}\sum^M_{i=1}\tilde L(θ,\phi;x^{(i)})
\]</span> 其中小批量 <span class="math inline">\(X^M = \{x^{(i)}\}^M_{i=1}\)</span> 是从具有 <span class="math inline">\(N\)</span> 个数据点的完整数据集 <span class="math inline">\(X\)</span> 中随机抽取的 <span class="math inline">\(M\)</span> 个数据点样本。 在我们的实验中，我们发现只要小批量大小 <span class="math inline">\(M\)</span> 足够大，每个数据点的样本数<span class="math inline">\(L\)</span> 可以设置为 1，例如 <span class="math inline">\(M = 100\)</span>。可以取导数 <span class="math inline">\(∇_{θ,\phi}L􏰐(θ;X^M)\)</span>，得到的梯度可以与随机优化方法（例如 SGD 或 Adagrad）结合使用。 有关计算随机梯度的基本方法，请参阅算法 1。</p>
<p>当查看 以上公式给出的目标函数时，与自动编码器的联系变得清晰。公式（7） 第一项是（近似后验与先验的 KL 散度）充当正则化器，而第二项是预期的负重建误差。 选择函数 <span class="math inline">\(g_{\phi}(.)\)</span> 使其将数据点 <span class="math inline">\(x^{(i)}\)</span> 和随机噪声向量 <span class="math inline">\(ε^{(l)}\)</span> 映射到来自该数据点的近似后验的样本： <span class="math inline">\(z^{(i,l)} = g_{\phi}(ε^{(l)}, x^{(i)})\)</span> 其中 <span class="math inline">\(z^{(i,l)} ∼ q_{\phi}(z|x^{(i)})\)</span>。 随后，样本 <span class="math inline">\(z^{(i,l)}\)</span> 被输入到函数 <span class="math inline">\(logp_θ(x^{(i)}|z^{(i,l)})\)</span> 中，它等于给定 <span class="math inline">\(z^{(i,l)}\)</span>数据点 <span class="math inline">\(x^{(i)}\)</span> 在生成式模型下的概率密度（或质量）。 该术语是自动编码器术语中的negative reconstruction error。</p>
<h3 id="重新参数化的技巧">2.4 重新参数化的技巧</h3>
<h2 id="例子vae">3 例子：VAE</h2>
<p>在本节中，我们将给出一个示例，其中我们将神经网络用于概率编码器 <span class="math inline">\(q_{\phi} (z|x)\)</span>（生成模型 <span class="math inline">\(p_θ (x, z)\)</span> 的后验的近似值）以及参数 <span class="math inline">\(\phi\)</span> 和 <span class="math inline">\(θ\)</span> 与 AEVB 算法联合优化。</p>
<p>让潜在变量的先验是中心各向同性多元高斯 <span class="math inline">\(p_θ(z) = \mathcal{N}(z;0,I)\)</span>。 请注意，在这种情况下，先验缺少参数。 我们让 <span class="math inline">\(p_θ(x|z)\)</span> 是一个多元高斯（在实值数据的情况下）或伯努利（在二进制数据的情况下），其分布参数是从 <span class="math inline">\(z\)</span> 用 MLP（一个全连接的神经网络，具有 单个隐藏层，见附录 C）。 请注意，在这种情况下，真正的后验 <span class="math inline">\(p_θ(z|x)\)</span> 是难以处理的。 虽然 <span class="math inline">\(q_{\phi}(z|x)\)</span> 的形式有很多自由度，但我们将假设真实的（但难以处理的）后验呈现为具有近似对角协方差的近似高斯形式。在这种情况下，我们可以让变分近似后验是一个具有对角协方差结构的多元高斯： <span class="math display">\[
log q_{\phi}(z|x^{(i)}) = log \mathcal{N} (z; μ^{(i)}, σ^{2(i)}I)
\]</span> 其中 近似后验的均值和标准差 <span class="math inline">\(μ^{(i)}\)</span> 和 <span class="math inline">\(σ^{(i)}\)</span> 是编码 MLP 的输出，即数据点 <span class="math inline">\(x^{(i)}\)</span> 和变分参数 <span class="math inline">\(\phi\)</span> 的非线性函数（参见附录 C）。</p>
<p>如第 2.4 节所述，我们使用 <span class="math inline">\(z^{(i,l)} = g_{\phi}(x^{(i)}, ε^{(l)}) = μ^{(i)} + σ^{(i)} ⊙ ε^{(l)}\)</span> 其中 <span class="math inline">\(ε^{(l)} ∼ \mathcal{N} (0, I)\)</span>。 <span class="math inline">\(⊙\)</span> 我们表示element-wise product。 在这个模型中，<span class="math inline">\(p_θ(z)\)</span>（先验）和 q_{}(z|x) 都是高斯分布的； 在这种情况下，我们可以使用 公式（7）的估计量，其中（7）无需估计即可计算和区分 KL 散度（参见附录 B）。 该模型的结果估计量和数据点 <span class="math inline">\(x^{(i)}\)</span> 为： <span class="math display">\[
L(θ,\phi;x^{(i)})≃ 􏰏 \frac{1}{2}\sum^J_{j=1}(1+log((σ^{(i)}_j)^2)−(μ^{(i)}_j)^2 −(σ^{(i)}_j)^2 + \frac{1}{L} \sum^L_{l=1}􏰏logp_θ(x^{(i)}|z^{(i,l)})
\]</span> 其中，<span class="math inline">\(z^{(i,l)} = μ^{(i)} + σ^{(i)} ⊙ ε^{(l)}, ε^{(l)} ∼ \mathcal{N} (0, I)\)</span></p>
<p>如上所述和附录 C 中，解码项 <span class="math inline">\(log p_θ (x^{(i)} |z^{(i,l)} )\)</span> 是伯努利或高斯 MLP，取决于我们建模的数据类型。</p>
<h1 id="vae-tutorials-httpsarxiv.orgpdf1606.05908v2.pdf-----carl-doersch-cmu-uc-berkeley-aug-2016">VAE Tutorials https://arxiv.org/pdf/1606.05908v2.pdf --- Carl Doersch CMU / UC Berkeley Aug 2016</h1>
<h2 id="感想-1">感想</h2>
<p>直到2.1章节，公式（5）以上其实还能够看得懂，再往下的数学推断就有点迷茫，摸不清头脑了，所以这一次先记住公式（5）的内容，下一次继续深入理解吧。文章打分：🌟🌟🌟🌟🌟，果然初创论文以及相关的tutorials才是带来灵感的更多碰撞。 <span class="math display">\[
log P(X)- \mathcal{D} [Q(z|X)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z|X)||P(z)]
\]</span></p>
<h2 id="abstract-1">Abstract</h2>
<p>在短短三年内，变分自动编码器 (VAE) 已成为最流行的复杂分布无监督学习方法之一。 VAE 很有吸引力，因为它们建立在标准函数逼近器（神经网络）之上，并且可以使用随机梯度下降进行训练。 VAEs 已经显示出在生成多种复杂数据方面的前景，包括手写数字 [1, 2]、人脸 [1, 3, 4]、门牌号码 [5, 6]、CIFAR 图像 [6]、 场景 [4]、分割 [7] 和从静态图像预测未来 [8]。 本教程介绍了 VAE 背后的直觉，解释了它们背后的数学原理，并描述了一些经验行为。 本文假设读者没有变分贝叶斯方法的先验知识。</p>
<h2 id="introduction-1">1 Introduction</h2>
<p>“生成建模” (Generative modelling) 是机器学习的一个广泛领域，它处理分布 P(X) 的模型，在一些潜在的高维空间 <span class="math inline">\(\mathcal{X}\)</span>中的数据点 <span class="math inline">\(X\)</span> 上定义。 例如，图像是一种流行的数据，我们可以为其创建生成模型。 每个“数据点”（图像）都有数千或数百万个维度（像素），生成模型的工作是以某种方式捕获像素之间的依赖关系，例如，附近像素具有相似的颜色，并被组织成对象。 “捕获”这些依赖关系的确切含义取决于我们想要对模型做什么。 一种简单的生成模型允许我们以数值方式计算 P(X)。在图像的情况下，看起来像真实图像的 <span class="math inline">\(X\)</span> 值应该获得高概率，而看起来像随机噪声的图像应该获得低概率。 然而，像这样的模型不一定有用：知道一个图像不太可能并不能帮助我们合成一个可能的图像。</p>
<p>相反，人们通常关心生成更多与数据库中已有的示例类似但又不完全相同的示例。 我们可以从原始图像数据库开始，然后合成新的、看不见的图像。 我们可能会采用植物等 3D 模型的数据库，并生成更多的模型来填充视频游戏中的森林。 我们可以采用手写文本并尝试生成更多手写文本。 像这样的工具实际上可能对图形设计师有用。 我们可以通过说我们根据某个未知分布 $P_{gt}(X) $得到分布的样本 X 来形式化这个设置，我们的目标是学习一个我们可以从中采样的模型 P，使得 P 尽可能类似于 <span class="math inline">\(P_{gt}\)</span>。</p>
<p>训练这种类型的模型一直是机器学习社区中的一个长期问题，而且经典地，大多数方法都具有三个严重缺陷之一。 首先，他们可能需要对数据结构进行强有力的假设。 其次，他们可能会进行严格的近似，从而导致次优模型。 或者第三，他们可能依赖于像马尔可夫链蒙特卡罗这样的计算昂贵的推理程序。 最近，一些工作在通过反向传播将神经网络训练为强大的函数逼近器方面取得了巨大进展。 这些进步催生了有前景的框架，这些框架可以使用基于反向传播的函数逼近器来构建生成模型。</p>
<p>最流行的此类框架之一是变分自动编码器 ，这是本教程的主题。 这个模型的假设很弱，而且通过反向传播训练很快。 VAE 确实进行了近似，但考虑到高容量模型，这种近似引入的误差可以说是很小的。 这些特点使其受欢迎程度迅速上升。</p>
<p>本教程旨在对 VAE 进行非正式介绍，而不是关于它们的正式科学论文。 它针对那些可能对生成模型有用，但在变分贝叶斯方法和 VAE 所基于的“最小描述长度”编码模型方面可能没有很强背景的人。 本教程最初是为加州大学伯克利分校和卡内基梅隆大学的计算机视觉阅读小组提供的演示文稿，因此偏向于视觉受众。</p>
<h3 id="预备知识潜在变量模型">1.1 预备知识：潜在变量模型</h3>
<p>在训练生成模型时，维度之间的依赖关系越复杂，模型训练就越困难。以生成手写字符图像的问题为例。为简单起见，我们只关心数字 0-9 的建模。如果字符的左半部分包含 5 的左半部分，则右半部分不能包含 0 的左半部分，否则该字符将非常明显地不像任何真正的数字。直观地说，<strong>如果模型在为任何特定像素分配值之前首先决定要生成哪个字符，这会有所帮助。这种决策正式称为潜在变量。</strong>也就是说，在我们的模型绘制任何东西之前，它首先从集合 [0, ..., 9] 中随机采样一个数字值 z，然后确保所有笔画都匹配该字符。<span class="math inline">\(z\)</span> 被称为“潜在”，因为仅给定模型生成的字符，我们不一定知道潜在变量的哪些设置生成了字符。我们需要使用诸如计算机视觉之类的东西来推断它。</p>
<p>在我们可以说我们的模型代表我们的数据集之前，我们需要确保对于数据集中的每个数据点 <span class="math inline">\(X\)</span>，都有一个（或多个）潜在变量设置，这会导致模型生成与 <span class="math inline">\(X\)</span> 非常相似的东西 . 正式地说，假设我们在高维空间 <span class="math inline">\(\mathcal{Z}\)</span> 中有一个潜在变量 <span class="math inline">\(z\)</span> 的向量，我们可以根据在 <span class="math inline">\(\mathcal{Z}\)</span> 上定义的一些概率密度函数 (PDF) <span class="math inline">\(P(z)\)</span> 轻松地对其进行采样。</p>
<p>然后，假设我们有一系列确定性函数 <span class="math inline">\(f (z; θ)\)</span>，由向量<span class="math inline">\(\theta\)</span>在空间<span class="math inline">\(\mathcal{\Theta}\)</span>参数化，其中 <span class="math inline">\(f :\mathcal{Z}×\mathcal{\Theta}→\mathcal{X}\)</span>。 <span class="math inline">\(f\)</span> 是确定性的，但如果 <span class="math inline">\(z\)</span> 是随机的且 <span class="math inline">\(θ\)</span> 是固定的，则 <span class="math inline">\(f (z; θ)\)</span> 是空间 X 中的随机变量。 我们希望优化 <span class="math inline">\(θ\)</span>，以便我们可以从 <span class="math inline">\(P(z)\)</span> 中采样 z，并且很有可能，<span class="math inline">\(f (z; θ)\)</span> 将类似于我们数据集中的 <span class="math inline">\(X\)</span>。</p>
<p>为了使这个概念在数学上精确，我们的目标是在整个生成过程中最大化训练集中每个 <span class="math inline">\(X\)</span> 的概率，根据： <span class="math display">\[
P(X) = \int P(X|z; θ)P(z)dz.
\]</span> 在这里，<span class="math inline">\(f (z; θ)\)</span> 已被分布 <span class="math inline">\(P(X|z; θ)\)</span> 取代，这使我们能够通过使用总概率定律（the law of total probability）明确 <span class="math inline">\(X\)</span> 对 <span class="math inline">\(z\)</span> 的依赖关系。 这个框架背后的直觉——称为“最大似然”——是如果模型可能产生训练集样本，那么它也可能产生相似的样本，而不太可能产生不同的样本。 在 VAE 中，此输出分布的选择通常是高斯分布，即 <span class="math inline">\(P(X|z;θ) = \mathcal{N}(X|f(z;θ),σ^2 ∗ I)\)</span>。也就是说，它的均值 <span class="math inline">\(f(z;θ)\)</span> 和协方差等于单位矩阵 <span class="math inline">\(I\)</span> 乘以某个标量 <span class="math inline">\(σ\)</span>（这是一个超参数）。这种替换对于形式化直觉是必要的，即某些 <span class="math inline">\(z\)</span> 需要产生仅像 <span class="math inline">\(X\)</span> 的样本。一般来说，特别是在训练初期，我们的模型不会产生与任何特定 <span class="math inline">\(X\)</span> 相同的输出。通过高斯分布，我们可以使用梯度下降（或任何其他优化技术）通过使某些<span class="math inline">\(z\)</span>的 <span class="math inline">\(f(z; θ)\)</span> 逼近$ X$来增加 P(X)，即在生成模型下逐渐使训练数据更有可能。 如果 <span class="math inline">\(P(X|z)\)</span> 是 Dirac delta 函数，这将是不可能的，就像我们确定性地使用 <span class="math inline">\(X = f (z; θ)\)</span> 一样！ 请注意，输出分布不需要是高斯分布：例如，如果 <span class="math inline">\(X\)</span> 是二进制的，那么 $P(X|z) $可能是由 <span class="math inline">\(f (z; θ)\)</span> 参数化的伯努利。 重要的性质很简单，<span class="math inline">\(P(X|z)\)</span> 可以计算，并且在 <span class="math inline">\(θ\)</span> 上是连续的。 从这里开始，我们将从 <span class="math inline">\(f (z; θ)\)</span> 中省略 <span class="math inline">\(θ\)</span> 以避免混乱。</p>
<h2 id="variational-autoencoders">2 Variational Autoencoders</h2>
<p>VAEs 的数学基础实际上与经典的自编码器关系不大，例如 稀疏自编码器或去噪自编码器。 根据图 1 所示的模型，VAE 近似最大化Equation 1。它们被称为“自动编码器”只是因为从该设置导出的最终训练目标确实具有编码器和解码器，并且类似于传统的自动编码器。 <strong>与稀疏自编码器不同，通常没有类似于稀疏惩罚的调整参数。</strong></p>
<div class="figure">
<img src="/images/vae_tutorials_fig1.png" alt="vae_tutorials_fig1" />
<p class="caption">vae_tutorials_fig1</p>
</div>
<p>与稀疏和去噪自编码器不同，我们可以直接从 <span class="math inline">\(P(X)\)</span> 中采样（无需执行马尔可夫链蒙特卡罗）。要解决方程 1，VAE 必须处理两个问题：如何定义潜在变量 <span class="math inline">\(z\)</span>（即决定它们代表什么信息），以及如何处理 <span class="math inline">\(z\)</span> 上的积分。 VAE 对两者都给出了明确的答案。</p>
<p>首先，我们如何选择潜在变量 <span class="math inline">\(z\)</span> 以捕获潜在信息？ 回到我们的数字示例，模型在开始绘制数字之前需要做出的“潜在”决定实际上相当复杂。 它不仅需要选择数字，还需要选择绘制数字的角度、笔画宽度以及抽象的风格属性。 更糟糕的是，这些属性可能是相关的：如果写得更快，可能会产生更多倾斜的数字，这也可能会导致笔画更细。理想情况下，我们希望避免手动决定 <span class="math inline">\(z\)</span> 的每个维度编码的信息（尽管我们可能希望为某些维度手动指定它）。 我们还希望避免明确描述 <span class="math inline">\(z\)</span> 维度之间的依赖关系——即潜在结构。 VAE 采取了一种不同寻常的方法来处理这个问题：他们假设 <span class="math inline">\(z\)</span> 的维度没有简单的解释，而是断言 <span class="math inline">\(z\)</span> 的样本可以从一个简单的分布中抽取，即 <span class="math inline">\(\mathcal{N} (0, I)\)</span>，其中 <span class="math inline">\(I\)</span> 是单位矩阵。</p>
<div class="figure">
<img src="/images/vae_tutorials_dist_mapping.png" alt="vae_tutorials_dist_mapping" />
<p class="caption">vae_tutorials_dist_mapping</p>
</div>
<p><strong>关键是要注意，任何 <span class="math inline">\(d\)</span> 维分布都可以通过取一组正态分布的 <span class="math inline">\(d\)</span> 变量并通过足够复杂的函数映射它们来生成样本。</strong>例如，假设我们想构造一个值位于环上的二维随机变量。如果 <span class="math inline">\(z\)</span> 是二维且正态分布的，则 <span class="math inline">\(g(z) = z/10 + z/||z||\)</span> 大致呈环形，如图 2 所示。因此，如果提供强大的函数逼近器，我们可以简单地学习一个函数，该函数将我们独立的、正态分布的 <span class="math inline">\(z\)</span> 值映射到模型可能需要的任何潜在变量，然后映射那些<span class="math inline">\(X\)</span> 的潜在变量。事实上，回想一下 <span class="math inline">\(P(X|z; θ) = \mathcal{N} (X| f (z; θ), σ^2 ∗ I)\)</span>。如果<span class="math inline">\(f (z; θ)\)</span> 是一个多层神经网络，那么我们可以想象网络使用它的前几层将正态分布的 <span class="math inline">\(z\)</span> 映射到潜在值（如数字标识、笔画粗细、角度等）完全正确的统计数据。然后它可以使用后面的层将这些潜在值映射到一个完全渲染的数字。一般来说，我们不需要担心确保潜在结构存在。如果这种潜在结构有助于模型准确地再现（即最大化）训练集的可能性，那么网络将在某个层学习该结构。</p>
<div class="figure">
<img src="/images/vae_tutorials_fig3.png" alt="vae_tutorials_fig3" />
<p class="caption">vae_tutorials_fig3</p>
</div>
<p>现在剩下的就是最大化 Equation 1，其中 <span class="math inline">\(P(z) = \mathcal{N} (z|0, I)\)</span>。 正如机器学习中常见的那样，如果我们可以找到 <span class="math inline">\(P(X)\)</span> 的可计算公式，并且我们可以采用该公式的梯度，那么我们就可以使用随机梯度上升来优化模型。 实际上，近似计算 <span class="math inline">\(P(X)\)</span> 在概念上很简单：我们首先对大量 <span class="math inline">\(z\)</span> 值 <span class="math inline">\(\{z_1, ..., z_n\}\)</span> 进行采样，然后计算 <span class="math inline">\(P(X) ≈ \frac{1}{n} ∑_i P(X|z_i)\)</span>。 这里的问题是在高维空间中，在我们准确估计 <span class="math inline">\(P(X)\)</span> 之前，<span class="math inline">\(n\)</span> 可能需要非常大。 要了解原因，请考虑我们的手写数字示例。假设我们的数字数据点存储在像素空间中，在 28x28 图像中，如图 3 所示。 由于 <span class="math inline">\(P(X|z)\)</span> 是各向同性高斯分布 (isotropic Gaussian)，因此 <span class="math inline">\(X\)</span> 的负对数概率是<span class="math inline">\(f (z)\)</span> 和 <span class="math inline">\(X\)</span> 之间的欧几里德距离的平方的比例 . 假设图 3(a) 是我们试图为其寻找 <span class="math inline">\(P(X)\)</span> 的目标 (<span class="math inline">\(X\)</span>)。 产生图 3(b) 所示图像的模型可能是一个糟糕的模型，因为这个数字不像 2。因此，我们应该设置我们的高斯分布的 <span class="math inline">\(σ\)</span> 超参数，使得这种错误的数字不会 对 <span class="math inline">\(P(X)\)</span> 做出贡献。 另一方面，产生图 3(c)（与 <span class="math inline">\(X\)</span> 相同但向下和向右移动半个像素）的模型可能是一个很好的模型。 我们希望这个样本对 <span class="math inline">\(P(X)\)</span> 有贡献。然而不幸的是，我们不能同时拥有它：<strong><span class="math inline">\(X\)</span> 和图 3(c) 之间的平方距离是 <span class="math inline">\(.2693\)</span>（假设像素范围在 0 和 1 之间），但在 <span class="math inline">\(X\)</span> 和图 3(b) 之间它只是 <span class="math inline">\(.0387\)</span>。 这里的教训是，为了拒绝像图 3(b) 这样的样本，我们需要将 <span class="math inline">\(σ\)</span> 设置得非常小，这样模型需要生成比图 3(c) 更像 X 的东西！</strong> 即使我们的模型是一个准确的数字生成器，我们也可能需要对数千个数字进行采样，然后才能生成与图 3(a) 中的 2 非常相似的 2。 <strong>我们可能会通过使用更好的相似性度量来解决这个问题，但在实践中，这些很难在视觉等复杂领域进行工程设计，而且如果没有指示哪些数据点彼此相似的标签，它们也很难训练。 相反，VAE 会改变采样程序以使其更快，而不会改变相似性度量，解决了这个棘手的问题。</strong></p>
<h3 id="设定目标函数">2.1 设定目标函数</h3>
<p>在使用采样来计算公式 1 时，我们可以采取什么捷径吗？ <strong>实际上，对于大多数 <span class="math inline">\(z\)</span>，<span class="math inline">\(P(X|z)\)</span> 几乎为零，因此对我们对 <span class="math inline">\(P(X)\)</span> 的估计几乎没有贡献。 变分自编码器背后的关键思想是尝试对可能产生 X 的 z 值进行采样，并仅从这些值中计算 P(X)。</strong> 这意味着我们需要一个新函数 <span class="math inline">\(Q(z|X)\)</span>，它可以取 <span class="math inline">\(X\)</span> 的值，并为我们提供可能产生 <span class="math inline">\(X\)</span> 的 <span class="math inline">\(z\)</span> 值的分布。<strong>希望可能在 <span class="math inline">\(Q\)</span> 下的 <span class="math inline">\(z\)</span> 值的空间会远小于所有可能在先验 <span class="math inline">\(P(z)\)</span> 下的 <span class="math inline">\(z\)</span> 的空间。</strong> 例如，这让我们可以相对容易地计算 <span class="math inline">\(E_{z∼Q}P(X|z)\)</span>。但是，如果 <span class="math inline">\(z\)</span> 是从PDF <span class="math inline">\(Q(z)\)</span>的任意分布中采样的，而不是 <span class="math inline">\(\mathcal{N}(0, I)\)</span>，那么这如何帮助我们优化 <span class="math inline">\(P(X)\)</span>？ 我们需要做的第一件事是关联 <span class="math inline">\(E_{z∼Q}P(X|z)\)</span> 和 <span class="math inline">\(P(X)\)</span>。 稍后我们将看到 <span class="math inline">\(Q\)</span> 的来源。</p>
<p><span class="math inline">\(E_{z∼Q}P(X|z)\)</span> 和<span class="math inline">\(P(X)\)</span>之间的关系是变分贝叶斯方法的基石之一。 我们首先定义 <span class="math inline">\(P(z|X)\)</span> 和 <span class="math inline">\(Q(z)\)</span> 之间的 Kullback-Leibler 散度（KL 散度或 <span class="math inline">\(\mathcal{D}\)</span>），对于一些任意 <span class="math inline">\(Q\)</span>（可能取决于也可能不取决于 <span class="math inline">\(X\)</span>）： <span class="math display">\[
D [Q(z)∥P(z|X)] = E_{z∼Q} [log Q(z) − log P(z|X)] .
\]</span> 通过对 <span class="math inline">\(P(z|X)\)</span> 应用贝叶斯规则<span class="math inline">\(P(z|X)=\frac{P(X|z)\cdot P(z)}{P(X)}\)</span>，我们可以将 <span class="math inline">\(P(X)\)</span> 和 <span class="math inline">\(P(X|z)\)</span> 都放入这个方程中： <span class="math display">\[
D [Q(z)∥P(z|X)] = E_{z∼Q} [log Q(z) − log P(X|z) − log P(z)] + log P(X).
\]</span> 这里，<span class="math inline">\(log P(X)\)</span> 跳出了期望<span class="math inline">\(E\)</span>，因为它不依赖于 <span class="math inline">\(z\)</span>。 将两边取反、重新排列，将 <span class="math inline">\(E_{z∼Q}\)</span> 的部分项总结为另一个 KL 散度项，产生： <span class="math display">\[
log P(X)- \mathcal{D} [Q(z)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z)||P(z)]
\]</span> 请注意，<span class="math inline">\(X\)</span> 是固定的，并且 <span class="math inline">\(Q\)</span> 可以是<strong>任何</strong>分布，而不仅仅是将 <span class="math inline">\(X\)</span> 很好地映射到可以产生 <span class="math inline">\(X\)</span> 的 <span class="math inline">\(z\)</span> 的分布。 由于我们对推断 <span class="math inline">\(P(X)\)</span> 感兴趣，因此构造一个 <span class="math inline">\(Q\)</span> 确实取决于（does depend on） <span class="math inline">\(X\)</span>，特别是使 <span class="math inline">\(\mathcal{D} [Q(z)∥P(z|X)]\)</span> 变小的一个是有意义的： <span class="math display">\[
log P(X)- \mathcal{D} [Q(z|X)∥P(z|X)] = E_{z∼Q} [log P(X|z)] − \mathcal{D}[Q(z|X)||P(z)]
\]</span> <strong>这个方程是变分自编码器的核心，值得花一些时间思考它所说的内容</strong>（从历史上看，这个数学公式（尤其是公式 5）早在 VAE 之前就已为人所知。 例如， Helmholtz Machines [16]（见公式 5）使用几乎相同的数学，但有一个关键区别。 期望中的积分被 Dayan [16] 的总和 (sum) 所取代。 因为Helmholtz Machines假设潜在变量为离散分布。 这种选择可以避免一些转换使 VAE 中的梯度下降易于处理。） 在以上两个公式中，左边有我们想要最大化的量：<span class="math inline">\(log P(X)\)</span>（加上一个误差项，这使得 <span class="math inline">\(Q\)</span> 产生可以重现给定 <span class="math inline">\(X\)</span> 的 <span class="math inline">\(z\)</span>；如果 <span class="math inline">\(Q\)</span> 是高容量的，这个项会变小 ）。 右侧是我们可以通过随机梯度下降优化 <span class="math inline">\(Q\)</span> 的正确选择（尽管它可能还不明显）。 <strong>请注意，该框架——尤其是等式 5 的右侧——突然采用了一种看起来像自动编码器的形式，因为 <span class="math inline">\(Q\)</span> 将 <span class="math inline">\(X\)</span>“编码”为 <span class="math inline">\(z\)</span>，而 <span class="math inline">\(P\)</span> 正在“解码”它以重建 <span class="math inline">\(X\)</span>。</strong>我们' 稍后将更详细地探讨这种联系。</p>
<p>现在详细了解方程 5。从左侧开始，我们最大化 <span class="math inline">\(log P(X)\)</span>，同时最小化 <span class="math inline">\(\mathcal{D}[Q(z|X)∥P(z|X)]\)</span>。 <span class="math inline">\(P(z|X)\)</span> 不是我们可以分析计算的：它描述了 <span class="math inline">\(z\)</span> 的值，这些值可能会在我们的模型下产生像图 1 中的 <span class="math inline">\(X\)</span> 这样的样本。 然而，左边的第二项拉动 <span class="math inline">\(Q( z|x)\)</span> 匹配 <span class="math inline">\(P(z|X)\)</span>。 假设我们对 <span class="math inline">\(Q(z|x)\)</span> 使用任意高容量模型，那么 <span class="math inline">\(Q(z|x)\)</span> 将有望实际匹配 <span class="math inline">\(P(z|X)\)</span>，在这种情况下，这个 KL-散度项将为零，我们将 直接优化 <span class="math inline">\(log P(X)\)</span>。 作为额外的奖励，我们使棘手的 <span class="math inline">\(P(z|X)\)</span> 变得易于处理：我们可以只使用 <span class="math inline">\(Q(z|x)\)</span> 来计算它。</p>
<h3 id="最优化目标函数">2.2 最优化目标函数</h3>
<p>那么我们如何在等式 5 的右侧执行随机梯度下降呢？ 首先，我们需要更具体地了解 <span class="math inline">\(Q(z|X)\)</span> 将采用的形式。 通常的选择是说 <span class="math inline">\(Q(z|X) = \mathcal{N} (z|μ(X; θ), Σ(X; θ))\)</span>，其中 <span class="math inline">\(μ\)</span> 和 <span class="math inline">\(Σ\)</span> 是任意确定性函数，其参数 θ 可以从数据中学习 （我们将在后面的方程中省略 θ）。 在实践中，μ 和 Σ 再次通过神经网络实现，并且 <span class="math inline">\(Σ\)</span> 被约束为对角矩阵。 这种选择的优点是计算性的，因为它们清楚地说明了如何计算右侧。 最后一项——<span class="math inline">\(D [Q(z|X)∥P(z)]\)</span>——现在是两个多元高斯分布之间的 KL 散度，可以用封闭形式计算为： <span class="math display">\[
D[\mathcal{N}(μ_0, Σ_0)∥ \mathcal{N}(μ_1, Σ_1)] = \frac{1}{2} 􏰃(tr(􏰃Σ^{-1}_1Σ_0) 􏰄+(μ_1 −μ_0 )^⊤ Σ^{-1}_1(μ_1−μ_0 )−k+log􏰃(\frac{detΣ_1}{detΣ_0}) 􏰄)
\]</span> 其中 <span class="math inline">\(k\)</span> 是分布的维数。 在我们的例子中，这简化为：</p>
<p><span class="math display">\[
D[\mathcal{N}(μ(X), Σ(X))∥ \mathcal{N} (0, I)] = \frac{1}{2}(tr (Σ(X)) + (μ(X))⊤ (μ(X)) − k − log det (Σ(X)) 􏰄 .
\]</span> 公式 5 右侧的第一项有点棘手。 我们可以使用采样来估计 <span class="math inline">\(E_{z∼Q} [log P(X|z)]\)</span>，但是要得到一个好的估计需要通过 <span class="math inline">\(f\)</span> 传递 <span class="math inline">\(z\)</span> 的许多样本，这将是昂贵的。 因此，作为随机梯度下降的标准，我们取 <span class="math inline">\(z\)</span> 的一个样本并将该 <span class="math inline">\(z\)</span> 的 <span class="math inline">\(log P(X|z)\)</span> 视为 <span class="math inline">\(E_{z∼Q} [log P(X|z)]\)</span> 的近似值。 毕竟，我们已经在对从数据集 <span class="math inline">\(D\)</span> 中采样的不同 <span class="math inline">\(X\)</span> 值进行随机梯度下降。因此我们想要优化的目标函数整理如下： <span class="math display">\[
E_{X∼D} [log P(X) − \mathcal{D} [Q(z|X)∥P(z|X)]] = E_{X∼D} [E_{z∼Q} [log P(X|z)] − \mathcal{D} [Q(z|X)∥P(z)]] .
\]</span></p>
<p>如果我们取这个方程的梯度，梯度符号可以移动到期望中。 因此，我们可以从分布 <span class="math inline">\(Q(z|X)\)</span> 中采样单个 <span class="math inline">\(X\)</span> 值和单个 <span class="math inline">\(z\)</span> 值，并计算梯度： <span class="math display">\[
log P(X|z) − \mathcal{D} [Q(z|X)∥P(z)] .
\]</span> 然后我们可以在 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(z\)</span> 的任意多个样本上平均这个函数的梯度，结果收敛到方程 8 的梯度。</p>
<p>然而，方程 9 存在一个重大问题。 <span class="math inline">\(E_{z∼Q} [log P(X|z)]\)</span> 不仅取决于 <span class="math inline">\(P\)</span> 的参数，还取决于 <span class="math inline">\(Q\)</span> 的参数。 然而，在方程 9 中，这种依赖性有 消失了！ 为了使 VAE 工作，必须驱动 <span class="math inline">\(Q\)</span> 为 <span class="math inline">\(X\)</span> 生成 <span class="math inline">\(P\)</span> 可以可靠解码的代码。 从不同的角度来看问题，等式 9 中描述的网络与图 4（左）所示的网络非常相似。 该网络的前向传递工作正常，如果输出对 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(z\)</span> 的许多样本进行平均，则会产生正确的预期值。</p>
<div class="figure">
<img src="/images/vae_fig4.png" alt="vae_fig4" />
<p class="caption">vae_fig4</p>
</div>
<p>然而，我们需要通过一个从 <span class="math inline">\(Q(z|X)\)</span> 中采样 <span class="math inline">\(z\)</span> 的层来反向传播误差，这是一个非连续的操作，没有梯度。 通过反向传播的随机梯度下降可以处理随机输入，但不能处理网络中的随机单元！ 这种方法，被称为“重新参数化技巧”，解决方案是将采样移动到输入层。 给定 <span class="math inline">\(μ(X)\)</span> 和 <span class="math inline">\(Σ(X)\)</span>——<span class="math inline">\(Q(z|X)\)</span> 的均值和协方差——我们可以通过第一次采样 <span class="math inline">\(ε ∼ N (0, I)\)</span> 从 <span class="math inline">\(\mathcal{N} (μ(X), Σ(X))\)</span> 中采样 ，然后计算 <span class="math inline">\(z = μ(X) + Σ^{1/2}(X) ∗ ε\)</span>。 因此，我们实际取梯度的方程是： <span class="math display">\[
E_{X\sim D}E_{\epsilon \sim \mathcal{N}(0, I)} [log P(X|z = μ(X) + Σ^{1/2}(X) ∗ ε)] − \mathcal{D} [Q(z|X)∥P(z)]􏰊
\]</span> 这在图 4（右）中示意性地显示。 请注意，没有任何期望与依赖于我们模型参数的分布有关，因此我们可以安全地将梯度符号移动到它们中，同时保持相等。 也就是说，给定一个固定的 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(ε\)</span>，这个函数在 <span class="math inline">\(P\)</span> 和 <span class="math inline">\(Q\)</span> 的参数中是确定性和连续的，这意味着反向传播可以计算出一个适用于随机梯度下降的梯度。 值得指出的是，“重新参数化技巧”只有在我们可以通过评估函数 <span class="math inline">\(h(η, X)\)</span> 从 <span class="math inline">\(Q(z|X)\)</span> 中采样时才有效，其中 <span class="math inline">\(η\)</span> 是来自未被学习的分布的噪声。 此外，<span class="math inline">\(h\)</span> 必须在 <span class="math inline">\(X\)</span> 中连续，以便我们可以反向传播它。 这意味着 <span class="math inline">\(Q(z|X)\)</span>（因此 <span class="math inline">\(P(z)\)</span>）不能是离散分布！ 如果 <span class="math inline">\(Q\)</span> 是离散的，那么对于固定的 <span class="math inline">\(η\)</span>，要么 <span class="math inline">\(h\)</span> 需要忽略 <span class="math inline">\(X\)</span>，要么需要某个点 <span class="math inline">\(h(η, X)\)</span> 从 <span class="math inline">\(Q\)</span> 的样本空间中的一个可能值“跳跃”到另一个，即 不连续性。</p>
<h3 id="测试学习到的模型">2.3 测试学习到的模型</h3>
<div class="figure">
<img src="/images/vae_fig5.png" alt="vae_fig5" />
<p class="caption">vae_fig5</p>
</div>
<p>在测试时，当我们想要生成新样本时，我们只需将 <span class="math inline">\(z ∼ N (0, I)\)</span> 的值输入到解码器中。 也就是说，我们移除了“编码器”，包括会改变 <span class="math inline">\(z\)</span> 分布的乘法和加法运算。 这个（非常简单的）测试时间网络如图 5 所示。</p>
<p>假设我们要评估模型下测试示例的概率。 这通常是难以处理的。 但是请注意，<span class="math inline">\(\mathcal{D} [Q(z|X)∥P(z|X)]\)</span> 是正数，这意味着等式 5 的右侧是 P(X) 的下限。 由于对 z 的期望，这个下界仍然不能完全以封闭形式计算，这需要采样。 然而，从 <span class="math inline">\(Q\)</span> 中采样 <span class="math inline">\(z\)</span> 给出了期望的估计量，其收敛速度通常比第 2 节中讨论的从 <span class="math inline">\(\mathcal{N} (0, I)\)</span> 中采样 <span class="math inline">\(z\)</span> 快得多。因此，这个下界可以成为获得粗略想法的有用工具，例如我们的模型捕获特定数据点 <span class="math inline">\(X\)</span> 的效果如何。</p>
<h2 id="直观理解目标函数">2.4 直观理解目标函数</h2>
<p>到现在为止，您希望相信 VAE 中的学习是易于处理的，并且它优化了我们整个数据集的 <span class="math inline">\(log P(X)\)</span> 之类的东西。 然而，我们并没有精确地优化 <span class="math inline">\(log P(X)\)</span>，因此本节旨在更深入地了解目标函数实际在做什么。 我们讨论三个主题。 首先，我们探求通过优化 <span class="math inline">\(\mathcal{D}[Q(z|X)∥P(z|X)]\)</span>，会给 <span class="math inline">\(log P(X)\)</span>引入多少误差。 其次，我们描述了 VAE 框架——尤其是 r.h.s. 等式 5——在信息论方面，将其与基于最小描述长度的其他方法联系起来。 最后，我们研究了 VAE 是否具有类似于稀疏自编码器中的稀疏惩罚的“正则化参数”。</p>
<h3 id="来自-qzxpzx的误差">2.4.1 来自 $ [Q(z|X)∥P(z|X)]$的误差</h3>
<p>该模型的易处理性依赖于我们的假设，即 <span class="math inline">\(Q(z|X)\)</span> 可以建模为具有某均值 <span class="math inline">\(μ(X)\)</span> 和方差 <span class="math inline">\(Σ(X)\)</span>的高斯分布。<strong>当且仅当 <span class="math inline">\(\mathcal{D}[Q(z|X)∥P(z|X)]\)</span> 趋近于0时，<span class="math inline">\(P(X)\)</span> 收敛（在分布中）到真实分布。</strong>不幸的是，要确保这种情况发生并不容易。即使我们假设 <span class="math inline">\(μ(X)\)</span> 和 <span class="math inline">\(Σ(X)\)</span> 是任意高的容量，对于我们用来定义 <span class="math inline">\(P\)</span> 的任意 <span class="math inline">\(f\)</span> 函数，后验 <span class="math inline">\(P(z|X)\)</span> 不一定是高斯的。对于固定的 <span class="math inline">\(P\)</span>，这可能意味着 <span class="math inline">\(D[Q(z|X)∥P(z|X)]\)</span> 永远不会变为零。然而，好消息是，给定足够高容量的神经网络，有许多 <span class="math inline">\(f\)</span> 函数会导致我们的模型生成任何给定的输出分布。这些函数中的任何一个都会同样好地最大化 <span class="math inline">\(log P(X)\)</span>。因此，我们所需要的只是一个函数 <span class="math inline">\(f\)</span>，它既最大化 <span class="math inline">\(log P(X)\)</span> 又导致 <span class="math inline">\(P(z|X)\)</span> 对于所有 <span class="math inline">\(X\)</span> 都是高斯分布的。如果是这样，$ [Q(z|X)∥P(z|X) ]$ 会将我们的模型拉向分布的参数化。那么，对于我们可能想要近似的所有分布，这样的函数是否存在？我还没有意识到有人总体上证明了这一点，但事实证明，如果 <span class="math inline">\(σ\)</span> 相对于真实分布的 CDF 的曲率很小（至少在 1D 中），则可以证明这样的函数确实存在。 （证据包含在附录 A）中。 在实践中，这么小的 <span class="math inline">\(σ\)</span> 可能会给现有的机器学习算法带来问题，因为梯度会变得很糟糕。 <strong>然而，令人欣慰的是，至少在这种情况下，VAE 的近似误差为零。 这一事实表明，未来的理论工作可能会向我们展示 VAE 在更实际的设置中具有多少近似误差。</strong> （在我看来，应该可以将附录 A 中的证明技术扩展到多个维度，但这留待以后的工作。）</p>
<h3 id="信息论解释">2.4.2 信息论解释</h3>
<p>查看等式 5 右侧的另一个重要方式是信息论，特别是“最小描述长度”原则，它激发了许多 VAE 的前辈，如亥姆霍兹机 [16]、Wake-睡眠算法 [17]、深度信念网 [18] 和玻尔兹曼机 [19]。 - $log P(X) $可以看作是在我们的模型下使用理想编码构造给定 <span class="math inline">\(X\)</span> 所需的总位数。等式 5 的右侧将此视为构造 <span class="math inline">\(X\)</span> 的两步过程。我们首先使用一些位来构造 <span class="math inline">\(z\)</span>。回想一下，KL 散度以位（或更准确地说，nats）为单位。具体来说，<span class="math inline">\(\mathcal{D}[Q(z|X)||P(z)]\)</span> 是将 <span class="math inline">\(P(z)\)</span> 中的无信息样本转换为 <span class="math inline">\(Q(z|X)\)</span> 中的样本所需的预期信息（所谓的KL 散度的“信息增益”解释）。也就是说，当 <span class="math inline">\(z\)</span> 来自 <span class="math inline">\(Q(z|X)\)</span> 而不是来自 <span class="math inline">\(P(z)\)</span> 时，它测量我们获得的关于 <span class="math inline">\(X\)</span> 的额外信息量（有关更多详细信息，请参阅 [20, 21] 的“bits back”参数） ）。在第二步中，<span class="math inline">\(P(X|z)\)</span> 测量在理想编码下从 <span class="math inline">\(z\)</span> 重建 <span class="math inline">\(X\)</span> 所需的信息量。因此，总位数 (− <span class="math inline">\(log P(X)\)</span>) 是这两个步骤的总和，减去我们为 <span class="math inline">\(Q\)</span> 作为次优编码 (<span class="math inline">\(\mathcal{D}[Q(z|X)||P(z) |X)]\)</span>)。</p>
<h3 id="vaes和正则化参数">2.4.3 VAEs和正则化参数</h3>
<p>查看等式 5，将 <span class="math inline">\(\mathcal{D}[Q(z|X)||P(z)]\)</span> 视为正则化项很有趣，很像稀疏自编码器中的稀疏正则化 [10]。 从这个角度来看，询问变分自编码器是否有任何“正则化参数”是很有趣的。 也就是说，在稀疏自编码器目标中，我们在目标函数中有一个 <span class="math inline">\(λ\)</span> 正则化参数，如下所示： <span class="math display">\[
∥\phi(ψ(X)) − X∥^2 + λ∥ψ(X)∥_0
\]</span> 其中 <span class="math inline">\(ψ\)</span> 和 <span class="math inline">\(\phi\)</span> 分别是编码器和解码器函数，<span class="math inline">\(∥·∥_0\)</span> 是鼓励编码稀疏的 <span class="math inline">\(L_0\)</span> 范数。 这个 <span class="math inline">\(λ\)</span> 必须手动设置。</p>
<p><strong>然而，变分自编码器通常没有这样的正则化参数，这很好，因为程序员需要调整的参数少了一个。</strong> 然而，对于某些模型，我们可以让它看起来像这样一个正则化参数存在。 很容易认为这个参数可以来自改变 <span class="math inline">\(z ∼N(0, I)\)</span> 到类似 <span class="math inline">\(z′ ∼ \mathcal{N}(0,λ∗I)\)</span> 的东西，但事实证明这不会改变模型。要看到这一点，请注意我们可以将这个常数吸收到 <span class="math inline">\(P\)</span> 中 和 <span class="math inline">\(Q\)</span> 将它们写成 <span class="math inline">\(f&#39;(z&#39;) = f(z&#39;/λ), μ&#39;(X) = μ(X) ∗ λ\)</span>, 并且 <span class="math inline">\(Σ&#39;(X) = Σ(X) ∗ λ^2\)</span>。 这将产生一个目标函数，其值（等式 5 的右侧）与我们在 <span class="math inline">\(z ∼ \mathcal{N} (0, I)\)</span> 中的损失相同。 此外，采样 <span class="math inline">\(X\)</span> 的模型将是相同的，因为 <span class="math inline">\(z&#39;/λ ∼ \mathcal{N} (0, I)\)</span>。</p>
<p>然而，正则化参数可以来自另一个地方。回想一下，对于我们提供的一些 <span class="math inline">\(σ\)</span>，连续数据的输出分布的一个很好的选择是 <span class="math inline">\(P(X|z) ∼ \mathcal{N} ( f (z), σ^2 ∗ I)\)</span>。因此，<span class="math inline">\(log P(X|z) = C − \frac{1}{2} ∥X − f (z)∥^2/σ^2\)</span>（其中 <span class="math inline">\(C\)</span> 是一个不依赖于 <span class="math inline">\(f\)</span> 的常数，在优化过程中可以忽略）。当我们编写完整的优化目标时，<span class="math inline">\(σ\)</span> 出现在等式5的 r.h.s. 的第二项，但不是第一个；从这个意义上说，我们选择的 <span class="math inline">\(σ\)</span> 表现得像控制两项权重的 <span class="math inline">\(λ\)</span>。但是请注意，此参数的存在取决于我们对给定 <span class="math inline">\(z\)</span> 的 <span class="math inline">\(X\)</span> 分布的选择。如果 <span class="math inline">\(X\)</span> 是二元的并且我们使用伯努利输出模型，那么这个正则化参数就会消失，唯一的方法就是使用复制 <span class="math inline">\(X\)</span> 的维度之类的技巧。从信息论的角度来看，这是有道理的：当 <span class="math inline">\(X\)</span> 是二进制，我们实际上可以计算编码 <span class="math inline">\(X\)</span> 所需的位数，公式 5 右侧的两项都使用相同的单位来计算这些位数。但是，当 <span class="math inline">\(X\)</span> 是连续的时，每个样本都包含无限的信息。我们对 <span class="math inline">\(σ\)</span> 的选择决定了我们期望模型重建 <span class="math inline">\(X\)</span> 的准确程度，这是必要的，以便信息内容可以变得有限。</p>
<h2 id="条件变分自编码器">3. 条件变分自编码器</h2>
<p>让我们回到生成手写数字的运行示例。 假设我们不只是想生成新数字，而是想将数字添加到由一个人编写的现有数字字符串中。 这类似于计算机图形学中一个真正实际的问题，称为填孔（hole filling）：给定一个现有图像，其中用户删除了不需要的对象，目标是用看似合理的像素填充孔。这两个问题的一个重要困难是似真输出的空间是多模态的：下一个数字或外推像素有很多可能性。 在这种情况下，标准回归模型将失败，因为训练目标通常会惩罚单个预测与真实情况之间的距离。 面对这样的问题，回归器可以产生的最佳解决方案是介于可能性之间的东西，因为它最小化了预期距离。在数字的情况下，这很可能看起来像一个毫无意义的模糊，它是所有可能的数字和所有可能出现的样式的“平均图像”。 我们需要的是一种算法，它接受一个字符串或一个图像，并产生一个复杂的多模态分布，我们可以从中进行采样。 进入条件变分自编码器 (CVAE)，它通过简单地调节输入的整个生成过程来修改上一节中的数学。 CVAE 允许我们解决输入到输出映射是一对多的问题，而无需我们明确指定输出分布的结构。</p>
<p>给定一个输入 <span class="math inline">\(X\)</span> 和一个输出 <span class="math inline">\(Y\)</span>，我们想创建一个模型 <span class="math inline">\(P(Y|X)\)</span>，这最大限度地提高了基本事实的概率（我很抱歉在这里重新定义 <span class="math inline">\(X\)</span>。但是，标准机器学习符号将 <span class="math inline">\(X\)</span> 映射到 <span class="math inline">\(Y\)</span>，所以我也会这样做）。 我们通过引入一个潜在变量 <span class="math inline">\(z ∼ N (0, I)\)</span> 来定义模型，使得： <span class="math display">\[
P(Y|X) = \mathcal{N} ( f (z, X), σ^2 ∗ I)
\]</span> 其中 <span class="math inline">\(f\)</span> 是我们可以从数据中学习的确定性函数。 我们可以将方程 2 到 5 对 <span class="math inline">\(X\)</span> 的条件重写如下： <span class="math display">\[
\mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] = E_{z∼Q(·|Y,X)} [log Q(z|Y, X) − log P(z|Y, X)
\]</span></p>
<p><span class="math display">\[
\mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] =
E_{z∼Q(·|Y,X)} [log Q(z|Y, X) − log P(Y|z, X) − log P(z|X)] + log P(Y|X)
\]</span></p>
<p><span class="math display">\[
log P(Y|X) − \mathcal{D} [Q(z|Y, X)∥P(z|Y, X)] =
E_{z∼Q(·|Y,X)} [log P(Y|z, X)] − \mathcal{D} [Q(z|Y, X)∥P(z|X)]
\]</span></p>
<p>请注意，<span class="math inline">\(P(z|X)\)</span> 仍然是 <span class="math inline">\(\mathcal{N} (0, I)\)</span>，因为我们的模型假设 <span class="math inline">\(z\)</span> 在测试时独立于 <span class="math inline">\(X\)</span> 进行采样。 该模型的结构如下图6。</p>
<div class="figure">
<img src="/images/vae-fig6.png" alt="vae-fig6" />
<p class="caption">vae-fig6</p>
</div>
<p>在测试时，我们可以简单地从分布 <span class="math inline">\(P(Y|X)\)</span> 中采样 <span class="math inline">\(z ∼ \mathcal{N} (0, I)\)</span>。</p>
<h2 id="案例">4. 案例</h2>
<h3 id="mnist-变分自编码器">4.1 MNIST 变分自编码器</h3>
<div class="figure">
<img src="/images/vae-fig7.png" alt="vae-fig7" />
<p class="caption">vae-fig7</p>
</div>
<p>为了演示 VAE 框架的分布学习能力，让我们在 MNIST 上训练一个变分自编码器。 为了表明该框架并不严重依赖于初始化或网络结构，我们不使用现有的已发布的 VAE 网络结构，而是采用 Caffe 中包含的基本 MNIST AutoEncoder 示例。 （然而，我们使用 ReLU 非线性 和 ADAM，因为两者都是加速收敛的标准技术。）虽然 MNIST 是实值的，但它被限制在 0 和 1 之间，所以我们对 <span class="math inline">\(P(X|z)\)</span> 使用 Sigmoid 交叉熵损失。 这有一个概率解释：假设我们通过将每个维度独立采样为 <span class="math inline">\(Xi&#39; ∼ Bernoulli(Xi)\)</span> 来创建一个新的数据点 <span class="math inline">\(X&#39;\)</span>。 交叉熵测量 <span class="math inline">\(X&#39;\)</span> 的预期概率。 因此，我们实际上是在建模 <span class="math inline">\(X&#39;\)</span>，即 MNIST 的随机二值化版本，但我们只给出了该数据 <span class="math inline">\(X\)</span> 的 <span class="math inline">\(q\)</span> 摘要。诚然，这与 VAE 框架规定的不太一样，但在实践中运行良好，并且用于其他 VAE 文献。 尽管我们的模型比 [1] 和 [3] 深得多，但训练模型并不困难。 训练只运行了一次（尽管训练重新开始了 5 次以找到使损失下降最快的学习率）。 噪声产生的数字如图 7 所示。值得注意的是，这些样本很难评估，因为没有简单的方法来衡量这些样本与训练集的不同 [24]。 然而，失败案例很有趣：虽然大多数数字看起来很现实，但重要的数字是“中间”不同的数字。 例如，最左边一列从上数的第七个数字显然介于 7 和 9 之间。发生这种情况是因为我们正在通过平滑函数映射连续分布。</p>
<p>在实践中，模型似乎对 <span class="math inline">\(z\)</span> 的维度非常不敏感，除非 <span class="math inline">\(z\)</span> 过大或过小。 <span class="math inline">\(z\)</span> 太少意味着模型无法再捕获所有变化：少于 4 个 <span class="math inline">\(z\)</span> 维度会产生明显更差的结果。 1,000 个 <span class="math inline">\(z\)</span> 的结果很好，但 10,000 个 <span class="math inline">\(z\)</span> 的结果也下降了。 理论上，如果有 <span class="math inline">\(n\)</span>个<span class="math inline">\(z\)</span> 的模型是好的，那么有 <span class="math inline">\(m&gt;&gt;n\)</span> 的模型应该不会更糟，因为模型可以简单地学会忽略额外的维度。 然而，在实践中，当 <span class="math inline">\(z\)</span> 非常大时，随机梯度下降似乎很难保持 <span class="math inline">\(D[Q(z|X)||P(z)]\)</span> 低。</p>
<div class="figure">
<img src="/images/vae-fig8.png" alt="vae-fig8" />
<p class="caption">vae-fig8</p>
</div>
<h2 id="mnist-条件变分自编码器">5. MNIST 条件变分自编码器</h2>
<p>我原本打算展示一个条件变分自动编码器，只给定每个数字的一半来完成 MNIST 数字。虽然 CVAE 为此目的工作得很好，但不幸的是，回归器实际上也工作得很好，产生了相对清晰的样本。明显的原因是 MNIST 的大小。与 4.1 节中的网络容量相似的网络可以轻松记住整个数据集，因此回归量过拟合严重。因此，在测试时，它产生的预测表现类似于最近邻匹配，实际上非常敏锐。当给定训练示例输出不明确时，CVAE 模型最有可能胜过简单回归。因此，让我们对问题进行两次修改以使其更加模糊，但代价是使其更加人为化。首先，输入是取自数字中间的一列像素。在 MNIST 中，每个像素都有一个介于 0 和 1 之间的值，这意味着即使在这一列像素中仍然有足够的信息供网络识别特定的训练示例。因此，第二个修改是用二进制值（0 或 1）替换我们列中的每个像素，选择 1 的概率等于像素强度。每次向网络传递一个数字时，都会对这些二进制值进行重新采样。图 8 显示了结果。请注意，回归器模型通过模糊其输出来处理歧义（尽管在某些情况下，回归器在做出错误猜测时非常自信，这表明过度拟合仍然是一个问题）。回归器输出中的模糊使与可能产生输入的许多数字集的距离最小化。另一方面，CVAE 通常会选择一个特定的数字来输出并且不会模糊，从而产生更可信的图像。</p>
<h2 id="附录a-一维情况下证明给定任意强大的学习器vae-的近似误差为零">附录A 一维情况下证明给定任意强大的学习器，VAE 的近似误差为零。</h2>
<p>基本上全部是数学公式证明，隐约感觉难以理解，暂时先不做过多赘述了。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Wave-tacotron spectrogra-free end-to-end text-to-speech synthesis</title>
    <url>/2021/08/06/wave-tacotron/</url>
    <content><![CDATA[<h1 id="Wave-tacotron-无需频谱图的端到端文本转语音合成"><a href="#Wave-tacotron-无需频谱图的端到端文本转语音合成" class="headerlink" title="Wave-tacotron: 无需频谱图的端到端文本转语音合成"></a>Wave-tacotron: 无需频谱图的端到端文本转语音合成</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们描述了一个序列到序列的神经网络，它直接从文本输入生成语音波形。该架构通过将归一化流 (normalizing flow) 合并到自回归解码器循环中来扩展 Tacotron 模型。输出波形被建模为一系列不重叠的固定长度块，每个块包含数百个采样点。每个模块内波形采样点的相互依赖性使用归一化流 (normalizing flow) 建模，从而实现并行训练和合成。通过在前面的块上条件依赖于每个流，自回归处理长期依赖关系。<strong>该模型可以直接以最大似然进行优化，无需使用中间的、手工设计的特征或额外的损失项。</strong>当代最先进的文本到语音 (TTS) 系统使用一系列单独学习的模型：一个（例如 Tacotron）从文本生成中间特征（例如频谱图），然后是声码器（例如WaveRNN）从中间特征生成波形样本。相比之下，所提出的系统不使用固定的中间表示，而是端到端地学习所有参数。实验表明，所提出的模型生成的语音质量接近最先进的神经 TTS 系统，生成速度显着提高。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>现代文本到语音 (TTS) 合成系统使用在大量数据上训练的深度神经网络，能够生成自然语音。 TTS 是一个多模态生成问题，因为文本输入可以通过多种方式实现，就总体结构而言，例如，具有不同的韵律或发音，以及低级信号细节，例如，具有不同的，可能在感知上相似的相位。最近的系统将任务分为两个步骤，每个步骤都针对一个粒度。首先，合成模型从文本中预测中间音频特征，通常是频谱图、声码器特征或语言特征，控制生成语音的长期结构。接着是神经声码器，它将特征转换为时域波形样本，填充低级信号细节。这些模型最常单独训练，但可以联合微调。神经声码器示例包括自回归模型，例如 WaveNet 和 WaveRNN 、使用归一化流normalizing flow的完全并行模型 、支持高效训练和采样的基于耦合的流或并行建模长期结构但自回归建模精细细节的混合流。其他方法避开了概率模型，使用 GAN 或精心构建的频谱损失。</p>
<p>这种两步走的方法使端到端 TTS 变得易于处理； 然而，它会使训练和部署复杂化，因为获得最高质量通常需要微调或由于后者的弱点而在合成模型的输出上训练声码器。</p>
<p>由于难以有效地对波形中的强时间依赖性（例如，相位必须随时间保持一致）进行有效建模，因此难以从文本中端到端生成波形样本。 sample-level的自回归声码器通过条件于所有先前生成的样本来生成每个波形样本，来处理这种依赖性。 由于它们的高度顺序性，它们在现代并行硬件上的采样效率很低。</p>
<p>在本文中，我们将基于流的声码器集成到类似于 Tacotron 的语音波形块级block-level自回归模型中，以字符或音素序列为输入条件。 序列到序列解码器处理输入文本，并为生成波形块的归一化流生成条件特征。</p>
<p>输出波形被建模为一系列固定长度的块，每个块包含数百个样本。 每个模块内样本之间的依赖关系使用normalizing flow建模，从而实现并行训练和合成。 通过条件于先前块，对长期依赖性进行自回归建模。 由此产生的模型解决了比文本到频谱图生成更困难的任务，因为它必须综合波形中的精细时间结构（确保连续输出块的边缘正确排列）以及产生连贯的长 术语结构（例如，韵律和语义）。 尽管如此，我们发现该模型能够对齐文本并生成高保真语音，而不会产生framing artifacts。</p>
<p>最近的工作将归一化流与序列到序列的 TTS 模型相结合，以启用非自回归解码器，或改进中间梅尔谱图 的建模。Fastspeech2 和 EATs 还提出了直接生成波形的端到端 TTS 模型，但依靠频谱损失和梅尔频谱图进行对齐。 相比之下，我们完全避免了频谱图的生成，而是使用normalizing flow直接对时域波形建模，采用完全端到端的方法，简单地maximizing likelihood。</p>
<h2 id="Wave-Tacotron-Model"><a href="#Wave-Tacotron-Model" class="headerlink" title="Wave-Tacotron Model"></a>Wave-Tacotron Model</h2><p><img src="/images/wave-tacotron-fig1.png" alt="wave-tacotron-fig1"></p>
<p>Wave-Tacotron 扩展了 Tacotron 基于注意力的序列到序列模型，生成非重叠波形样本块而不是频谱图帧。 有关概览，请参见图 1。 编码器是一个 CBHG，它对一系列 I 标记（字符或音素）嵌入 $x_{1:I}$ 进行编码。 使用注意力将文本编码传递给块自回归解码器，为每个输出步骤 $t$ 生成条件特征 $c_t$。 normalizing flow 使用这些特征来采样该步的输出波形， $y_t$ ：</p>
<script type="math/tex; mode=display">
\begin{equation} \label{eqa1}
e_{1:I} = encode(x_{1:I})
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation} \label{eqa2}
c_t = decode(e_{1:I}, y_{1:t−1})
\end{equation}</script><script type="math/tex; mode=display">
\begin{equation} \label{eqa3}
y_t = g(z_t;c_t), \text{ where } z_t ∼ \mathcal{N}(0,I)
\end{equation}</script><p>流 $g$​ 将从高斯采样的随机向量 $z_t$​ 转换为波形块。 $c_t$​ 上的线性分类器计算 $t$​​ 是最终输出步骤（在之后称为停止标记）的概率 $\hat s_t$​​：</p>
<script type="math/tex; mode=display">
\begin{equation} \label{eqa4}
\hat s_t = σ(W_sc_t + b_s)
\end{equation}</script><p>其中 $σ$​ 是 sigmoid 非线性。 默认情况下，每个输出 $y_t ∈ R^K$​ 包含 40 ms 的语音：$K = 960$，采样率为 $24 kHz$。 $K$​ 的设置通过normalizing flow控制可并行性与通过自回归的样本质量之间的权衡。</p>
<p>网络结构遵循Tacotron，对解码器稍作修改。 我们使用位置敏感注意力，它比非基于内容的 GMM 注意力更稳定。 我们在 pre-net 中用 tanh 替换 ReLU 激活。 由于我们从解码器循环中的normalizing flow中采样，因此我们不需要在采样时应用 pre-net dropout。 我们也不使用 post-net。 在每个解码器步骤生成的波形块被简单地连接起来形成最终信号。 <strong>最后，我们在解码器 pre-net 和注意力层上添加了一个跳过连接skip connection，让flow直接访问当前块之前的样本。 这对于避免块边界处的不连续性至关重要。</strong></p>
<p>与 Tacotron 类似，$y_t$ 的大小 $K$ 由缩减因子超参数 $R$ 控制，其中 $K = 320 · R$，默认情况下 $R = 3$​。 与Tacotron 一样，解码器的自回归输入仅包含来自上一步  $y_{t-1}$ 的输出的最终 $K/R$​​ 个采样点。</p>
<h3 id="2-1-flow"><a href="#2-1-flow" class="headerlink" title="2.1 flow"></a>2.1 flow</h3><p>归一化流 $g(z_t ; c_t )$​ 是可逆变换的组合，它将从球面高斯抽取的噪声样本映射到波形段。 它的表现力来自于组合许多简单的函数。 在训练过程中，逆函数 $g^{−1}$​ 将目标波形映射到球面高斯下，一个密度易于计算的点。</p>
<p><img src="/images/wave-tacotron-fig2.png" alt="wave-tacotron-fig2"></p>
<p>由于训练和生成分别需要有效的密度计算和采样，$g$​ 是使用仿射耦合层构建的。 我们的标准化流是 Glow 的一维变体，类似于FloWaveNet。 在训练期间，$g^{−1}$​ 的输入，$y_t ∈ R^K$​，首先被压缩到 $J$​ 帧的序列中，每个帧的维度 $L = 10$。流被分为 $M$​​ 个阶段，每个阶段在不同的时间分辨率下运行，遵循多尺度中描述的配置。 这是通过交错挤压interleaving squeeze 操作来实现的，该操作重塑每个阶段之间的序列，将时间步数减半并将维度加倍，如图 2 所示。在最后阶段之后，解压unsqueeze操作将输出 $z_t$​ 展平为 $R^K$ 中的向量 . 在生成过程中，为了通过 $g$ 将 $z_t$ 转换为 $y_t$​，每个操作都被颠倒并且它们的顺序颠倒。</p>
<p>每个阶段进一步由 $N$ 个flow steps组成，如图 1 顶部所示。我们不像Glow那样对我们的 ActNorm 层使用数据相关的初始化。 仿射耦合层使用内核大小为 3、1、3 和 256 个通道的 3 层 1D convnet 输出变换参数。 通过使用挤压层squeeze layers，不同阶段的耦合层使用相同的内核宽度，但相对于 $y_t$ 具有不同的有效感受野。 默认配置使用 $M = 5$ 和 $N = 12$​，总共 60 步。</p>
<p>条件信号 $c_t$ 由每个解码器步骤的单个向量组成，它必须跨数百个样本对 $y_t$ 的结构进行编码。 由于flow内部将 $z_t$ 和 $y_t$ 视为 $J$ 帧的序列，我们发现对 $c_t$ 进行上采样以匹配此帧很有帮助。 具体来说，我们复制 $c_t$$L$​​ 次并将正弦位置嵌入(sinusoidal position embeddings)连接到每个时间步，类似于Transformer，尽管频率之间具有线性间距。 上采样的条件特征附加到每个耦合层的输入。</p>
<p>为了避免与将实值分布拟合到离散波形样本相关的问题，我们通过将样本量化为 $[0, 2^{16} − 1]$​ 级别来调整 RNADE 中的方法，并通过在 $[0, 1]$​ 中添加均匀噪声进行 去量化 ，并重新缩放回 $[−1, 1]$​​​​。 我们还发现预加重波形很有帮助，使用 FIR 高通滤波器将信号频谱白化with a zero at 0.9。 生成的信号在模型的输出端被 去加重。 这种预加重可以被认为是一个没有任何可学习参数的流程，作为一种归纳偏置inductive bias，将更高的权重放在高频上。 虽然不是关键，但我们发现这改善了主观听力测试结果。</p>
<p>作为generative flows的常见做法，我们发现在采样时降低先验分布的temperature是有帮助的。 这相当于简单地缩放方程 3 中 $z_t$ 的先验协方差，通过一个因子 $T^2 &lt;1$。我们发现$T =0.7$​是一个好的设置条件。</p>
<p>在训练期间，flow的逆，$g^{−1}$​，将目标波形 $y_t$​ 映射到相应的 $z_t$​。 条件特征 $c_t$​ 是使用序列到序列网络的教师强制计算的。 步骤 $t$​​​ 中流flow的负对数似然目标是 NICE：</p>
<script type="math/tex; mode=display">
\begin{equation} \label{eqa5}
\begin{aligned}
L_{flow}(y_t) &= − \text{ log } p(y_t|x_{1:I} , y_{1:t−1}) = − \text{ log } p(y_t|c_t) \\
&= − \text{ log } \mathcal{N} (z_t; 0, I) − \text{ log } |det(∂z_t/∂y_t)| 
\end{aligned}
\end{equation}</script><p>其中$z_t = g^{−1} (y_t ; c_t)$​. 停止标记分类器使用二元交叉熵损失：</p>
<script type="math/tex; mode=display">
\begin{equation} \label{eqa7}
\begin{aligned}
L_{eos}(s_t) &= − \text{ log } p(s_t|x_{1:I} , y_{1:t−1}) = − \text{ log } p(s_t|c_t) \\
&= −s_t \text{ log } \hat s_t − (1 − s_t) \text{ log }(1 − \hat s_t)
\end{aligned}
\end{equation}</script><p>其中$s_t$是ground truth stop token标签，表示$t$是否是话语的最后一步。 在实践中，我们用几个标记为 $s_t = 1$ 的块对信号进行零填充，以便为分类器提供更好的平衡训练信号。 我们使用公式5和6在所有解码器步骤 $t$​ 的的平均值作为整体损失。</p>
<h3 id="2-2-Flow-vocoder"><a href="#2-2-Flow-vocoder" class="headerlink" title="2.2 Flow vocoder"></a>2.2 Flow vocoder</h3><p>作为基线，我们在完全前馈声码器上下文中使用类似的流网络进行实验，该网络从 mel 谱图生成波形。 我们遵循图 2 中的架构，流帧长度 L = 15，M = 6 个阶段，N = 每阶段 10 个步骤。 输入 mel 频谱图特征使用 5 层扩张卷积网络调节堆栈进行编码，每层使用 512 个通道。 特征被上采样以匹配 1600 Hz 的流帧速率，并与正弦嵌入连接以指示每个特征帧内的位置，如上。 这个模型是高度并行化的，因为它不使用自回归。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>模型训练时间：500k steps on 32 Google TPUv3 cores, batch size: 128 for LJSpeech</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们提出了一种端到端（标准化）文本到语音波形合成模型，将normalizing flow合并到自回归 Tacotron 解码器循环中。 Wave-Tacotron 使用单一模型直接生成高质量的语音波形，以文本为条件，无需单独的声码器。 训练不需要手工设计的频谱图或其他中级特征的复杂损失，而只是最大化训练数据的可能性。 混合模型结构将基于注意力的 TTS 模型的简单性与归一化流的并行生成能力相结合，直接生成波形样本。</p>
<p>尽管网络结构将输出块大小 K 作为超参数，但解码器基本上仍然是自回归的，需要顺序生成输出块。与并行 TTS的最新进展相比，这使该方法处于劣势，除非输出步长可以变得非常大。探索使所提出的模型适应完全并行 TTS 生成的可行性仍然是未来工作的一个有趣方向。 TTS 任务的两步分解中固有的关注点分离，即将高保真波形生成与文本对齐和长期韵律建模的任务分离，可能导致实践中更有效的模型。两个模型的单独训练还允许在不同的数据上训练它们的可能性，例如，在更大的未转录语音语料库上训练声码器。最后，在类似的文本到波形设置中探索更有效的流替代方案会很有趣，例如，扩散概率模型或 GAN，它们可以更容易地在一个寻求模式的方式可能比对完整数据分布建模更有效。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>FASTSPEECH 2 FAST AND HIGH-QUALITY END-TO-END TEXT TO SPEECH</title>
    <url>/2021/08/06/fastspeech2/</url>
    <content><![CDATA[<h1 id="Fastspeech2-快速、高质量的端到端语音合成"><a href="#Fastspeech2-快速、高质量的端到端语音合成" class="headerlink" title="Fastspeech2 快速、高质量的端到端语音合成"></a>Fastspeech2 快速、高质量的端到端语音合成</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>非自回归文本到语音 (TTS) 模型，例如 FastSpeech 可以比以前具有可比质量的自回归模型更快地合成语音。 FastSpeech 模型的训练依赖于自回归教师模型进行持续时间预测（提供更多信息作为输入）和知识蒸馏（简化输出中的数据分布），可以缓解一对多映射问题（即多个语音变体对应于同一文本）在 TTS 中。然而，FastSpeech 有几个缺点：1）师生蒸馏管道复杂且耗时，2）从教师模型中提取的持续时间不够准确，从教师模型中提取的目标梅尔谱图由于数据简化而导致信息丢失，这两者都限制了语音质量。在本文中，我们提出了 FastSpeech 2，它解决了 FastSpeech 中的问题，并通过以下方式更好地解决了 TTS 中的一对多映射问题：1）直接用真实目标训练模型，而不是从教师学习的简单输出，以及 2) 引入更多的语音变化信息（例如，音调、能量和更准确的持续时间）作为条件输入。具体来说，我们从语音波形中提取时长、音调和能量，并直接将它们作为训练的条件输入，并在推理中使用预测值。我们进一步设计了 FastSpeech 2s，这是第一次尝试直接从文本并行生成语音波形，享受完全端到端推理的好处。实验结果表明：1）FastSpeech 2 的训练速度比 FastSpeech 提高了 3 倍，FastSpeech 2s 的推理速度更快； 2）FastSpeech 2和2s在语音质量上优于FastSpeech，FastSpeech 2甚至可以超越自回归模型。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>近年来，基于神经网络的文本转语音 (TTS) 取得了快速进展，并在机器学习和语音界引起了广泛关注。以前的神经 TTS 模型首先从文本中自回归生成梅尔谱图，然后使用单独训练的声码器从生成的梅尔谱合成语音。它们通常存在推理速度缓慢和鲁棒性（单词跳过和重复）问题。近年来，非自回归 TTS 模型旨在解决这些问题，以极快的速度生成梅尔谱图并避免鲁棒性问题，同时实现与以前的自回归模型相当的语音质量。</p>
<p>在这些非自回归 TTS 方法中，FastSpeech 是最成功的模型之一。 FastSpeech 设计了两种方法来缓解一对多映射问题：1）通过使用从自回归教师模型生成的梅尔谱图作为训练目标（即知识蒸馏）来减少目标端的数据方差。 2）引入持续时间信息（从教师模型的注意力图中提取）以扩展文本序列以匹配mel-spectrogram序列的长度。虽然 FastSpeech 中的这些设计简化了 TTS 中一对多映射问题的学习，但它们也带来了几个缺点：1）两阶段的师生培训管道使培训过程变得复杂。 2）从教师模型生成的目标梅尔频谱图与真实情况相比有一些信息损失，因为从生成的梅尔频谱图合成的音频质量通常比Ground-truth的音频质量差. 3）从教师模型的注意力图中提取的时长不够准确。</p>
<p>在这项工作中，我们提出 FastSpeech 2 来解决 FastSpeech 中的问题，并更好地处理非自回归 TTS 中的一对多映射问题。为了简化训练管道并避免师生蒸馏中由于数据简化而导致的信息丢失，我们直接使用真实目标而不是教师的简化输出来训练 FastSpeech 2 模型。减少输入（文本序列）和目标输出（梅尔谱图）之间的信息差距（输入不包含预测目标的所有信息）并缓解非自回归 TTS 的一对多映射问题模型训练，我们在FastSpeech中引入了一些语音的变化信息，包括音调、能量和更准确的时长：在训练中，我们从目标语音波形中提取时长、音调和能量，直接作为条件输入；在推理中，我们使用由与 FastSpeech 2 模型联合训练的预测器预测的值。考虑到音高对于语音韵律很重要，并且由于随时间的大波动而难以预测，我们使用连续小波变换将音高轮廓转换为音高谱图并预测频域中的基音，可以提高预测基音的准确性。为了进一步简化语音合成管道，我们引入了 FastSpeech 2s，它不使用梅尔谱图作为中间输出，直接从推理中的文本生成语音波形，推理时延迟低。在 LJSpeech (Ito, 2017) 数据集上的实验表明 1) FastSpeech 2 比 FastSpeech 拥有更简单的训练管道（训练时间减少 3 倍），同时继承了其快速、稳健和可控（甚至在音高和能量上更可控）的优势语音合成，FastSpeech 2s 推理速度更快； 2）FastSpeech 2和2s在语音质量上优于FastSpeech，FastSpeech 2甚至可以超越自回归模型。</p>
<p>这项工作的主要贡献总结如下：<br>• FastSpeech2 通过简化训练流程实现了 3 倍的训练速度提升 FastSpeech。<br>• FastSpeech 2 缓解了 TTS 中的一对多映射问题并实现了更好的语音质量。<br>• FastSpeech 2s 通过直接从文本生成语音波形，进一步简化了语音合成的推理管道，同时保持了高语音质量。</p>
<h2 id="Fastspeech2-和-Fastspeech-2s"><a href="#Fastspeech2-和-Fastspeech-2s" class="headerlink" title="Fastspeech2 和 Fastspeech 2s"></a>Fastspeech2 和 Fastspeech 2s</h2><p>在本节中，我们首先描述 FastSpeech 2 中的设计动机，然后介绍 FastSpeech 2 的架构，旨在改进 FastSpeech 以更好地处理一对多映射问题，具有更简单的训练管道和更高的语音质量 . 最后，我们将 FastSpeech 2 扩展到 FastSpeech 2s，以实现完全端到端的文本到波形合成 。</p>
<h3 id="2-1-Motivation"><a href="#2-1-Motivation" class="headerlink" title="2.1 Motivation"></a>2.1 Motivation</h3><p>TTS 是一个典型的一对多映射问题，因为多个由于语音的变化，例如音高、持续时间、音量和韵律，可能会有多个语音序列可以对应于文本序列。在非自回归 TTS 中，唯一的输入信息是文本，而文本不足以完全预测语音的多样性。在这种情况下，模型容易过拟合训练集中目标语音的变化，导致泛化能力差。如第 1 节所述，虽然 FastSpeech 设计了两种方法来缓解一对多映射问题，但它们也带来了几个问题，包括：1）复杂的训练管道； 2) 表1中分析的目标mel谱图的信息丢失；和 3) 如表 5a 所示，Ground-truth持续时间不够准确。在下面的小节中，我们将介绍旨在解决这些问题的 FastSpeech 2 的详细设计。</p>
<h3 id="2-2-Model-overview"><a href="#2-2-Model-overview" class="headerlink" title="2.2 Model overview"></a>2.2 Model overview</h3><p><img src="/images/fastspeech2-fig1.png" alt="fastspeech2-fig1"></p>
<p>FastSpeech 2 的整体模型架构如图 1a 所示。编码器将音素嵌入序列 phoneme embedding 转换为音素隐藏序列phoneme hidden sequence，然后方差适配器Variance adaptor 将持续时间Duration、音调Pitch和能量Energy等不同的方差信息添加到隐藏序列中，最后mel-spectrogram解码器将adapted hidden sequence 并行化转换为mel-spectrogram序列。我们使用前馈 Transformer 块作为编码器的基本结构，它是一个自注意力层和 FastSpeech中的一维卷积的堆栈，作为encoder和mel-spectrogram decoder的基本结构。与依赖于师生蒸馏管道和来自教师模型的音素持续时间的 FastSpeech 不同，FastSpeech 2 进行了多项改进。首先，我们去除了师生蒸馏管道，直接使用ground-truth mel-spectrograms作为模型训练的目标，这样可以避免蒸馏mel-spectrograms中的信息丢失并提升语音质量的上限。其次，我们的方差适配器不仅包含持续时间预测器，还包含音调和能量预测器，其中 1) 持续时间预测器使用通过强制对齐forced alignment获得的音素持续时间作为训练目标，从第 3.2.2 节中通过实验验证的，比自回归教师模型的注意力图提取的结果更准确； 2）额外的音调和能量预测器可以提供更多的方差信息，这对于缓解 TTS 中的一对多映射问题很重要。第三，为了进一步简化训练管道并将其推向完全端到端的系统，我们提出了 FastSpeech 2s，它直接从文本生成波形，没有级联的梅尔频谱图生成（声学模型）和波形生成（声码器）。在以下小节中，我们描述了我们方法中方差适配器和直接波形生成的详细设计。</p>
<h3 id="2-3-Variance-adaptor"><a href="#2-3-Variance-adaptor" class="headerlink" title="2.3 Variance adaptor"></a>2.3 Variance adaptor</h3><p>方差适配器旨在将方差信息（例如，持续时间、音调、能量等）添加到音素隐藏序列中，这可以为 TTS 中的一对多映射问题提供足够的信息来预测变体语音。我们简单介绍一下方差信息： 1）音素持续时间，表示语音发声的时间长短； 2）音高，这是传达情感的一个关键特征，对语音韵律有很大影响； 3）能量，它表示梅尔频谱图的帧级幅度，直接影响语音的音量和韵律。可以在方差适配器中添加更多方差信息，例如情感、风格和说话者，我们将其留待以后的工作。相应地，方差适配器由 1) 持续时间预测器（即，FastSpeech 中使用的长度调节器）、2) 音调预测器和 3) 能量预测器组成，如图 1b 所示。在训练中，我们将从录音中提取的持续时间、音调和能量的真实值作为隐藏序列的输入来预测目标语音。同时，我们使用真实的时长、音调和能量作为目标来训练时长、音调和能量预测器，这些预测器用于推理以合成目标语音。如图 1c 所示，持续时间、音调和能量预测器共享相似的模型结构（但模型参数不同），它由具有 ReLU 激活的 2 层 1D 卷积网络组成，每个网络后跟层归一化和 dropout 层，和一个额外的线性层将隐藏状态投影到输出序列中。在下面的段落中，我们分别描述了三个预测器的细节。</p>
<p><strong>Duration Predictor</strong> 时长预测器以音素隐藏序列为输入，预测每个音素的时长，表示这个音素对应多少个mel帧，并转化为对数域，便于预测。 持续时间预测器使用均方误差 (MSE) 损失进行优化，将提取的持续时间作为训练目标。 我们没有在 FastSpeech 中使用预训练的自回归 TTS 模型提取音素持续时间，而是使用蒙特利尔强制对齐 (MFA)  工具来提取音素持续时间，以提高对齐精度，从而减少模型输入和输出之间的信息差距。</p>
<p><strong>Pitch Predictor</strong> 以前基于神经网络的具有音高预测的 TTS 系统通常直接预测音高轮廓。然而，由于真实音高变化很大，预测音高值的分布与真实分布非常不同，如第 3.2.2 节所述。为了更好地预测音高轮廓的变化，我们使用连续小波变换 (CWT) 将连续的音高序列分解为音高谱图，并将音高谱图作为训练使用 MSE 损失优化的音高预测器的目标。在推理中，基音预测器预测基音谱图，然后使用逆连续小波变换 (iCWT) 将其进一步转换回基音轮廓。我们在附录 D 中描述了基音提取、CWT、iCWT 和基音预测器架构的细节。为了将基音轮廓作为训练和推理的输入，我们量化了每一帧的基音 F0（训练/推理的真实值/预测值）到对数尺度的 256 个可能值，并进一步将其转换为音调嵌入向量 p 并将其添加到扩展的隐藏序列中。</p>
<p><strong>Energy Predictor</strong> </p>
<p>我们将每个短时傅立叶变换 (STFT) 帧的幅度amplitude的 L2 范数计算为能量。 然后我们将每帧的能量统一量化为 256 个可能的值，将其编码为能量嵌入 e 并将其添加到扩展的隐藏序列中，类似于基音。 我们使用能量预测器来预测能量的原始值而不是量化值，并使用 MSE 损失来优化能量预测器。</p>
<h3 id="2-4-Fastspeech-2s"><a href="#2-4-Fastspeech-2s" class="headerlink" title="2.4 Fastspeech 2s"></a>2.4 Fastspeech 2s</h3><p>为了实现完全端到端的文本到波形生成，在本小节中，我们将 FastSpeech 2 扩展到 FastSpeech 2s，它直接从文本生成波形，没有级联梅尔频谱图生成（声学模型）和波形生成（声码器） . 如图 1a 所示，FastSpeech 2s 在中间隐藏层上生成波形调节，通过丢弃梅尔频谱图解码器使其推理更加紧凑，并实现与级联系统相当的性能。 我们首先讨论非自回归文本到波形生成的挑战，然后描述 FastSpeech 2s 的细节，包括模型结构以及训练和推理过程。</p>
<p><strong>文本到波形生成中的挑战</strong> <strong>当将 TTS 管道推向完全端到端框架时，有几个挑战：1）由于波形包含比梅尔谱图更多的方差信息（例如相位）， 输入和输出的信息差大于文本到频谱图生成中的输入和输出。 2）由于极长的波形样本和有限的GPU内存，很难在对应于全文序列的音频剪辑上进行训练。 因此，我们只能在对应于部分文本序列的短音频片段上进行训练，这使得模型难以捕捉不同部分文本序列中音素之间的关系，从而损害文本特征提取。</strong></p>
<p><strong>我们的方法</strong> 为了应对上述挑战，我们在波形解码器中进行了多项设计：1) <strong>考虑到相位信息难以使用方差预测器进行预测，我们在波形解码器中引入对抗训练以强制它自己隐式恢复相位信息。</strong> 2) 我们利用 FastSpeech 2 的 mel-spectrogram 解码器，对全文序列进行训练，以帮助进行文本特征提取。如图 1d 所示，波形解码器基于 WaveNet的结构，包括非因果卷积和门控激活。<strong>波形解码器将对应于短音频剪辑的切片隐藏序列作为输入</strong>，并使用转置的 1D 卷积对其进行上采样以匹配音频剪辑的长度。对抗训练中的鉴别器采用与 Parallel WaveGAN 中相同的结构，它由十层非因果扩张的一维卷积和泄漏 ReLU 激活函数组成。波形解码器通过多分辨率 STFT 损失和 Parallel WaveGAN 之后的 LSGAN 鉴别器损失进行了优化。<strong>在推理中，我们放弃了梅尔谱解码器，只使用波形解码器来合成语音音频。</strong></p>
<h3 id="2-5-Discussions"><a href="#2-5-Discussions" class="headerlink" title="2.5 Discussions"></a>2.5 Discussions</h3><p>在本小节中，我们将讨论 FastSpeech 2 和 2s 与之前和当下工作的区别。</p>
<p>与 Deep Voice、Deep Voice 2 和其他自回归生成波形并预测方差信息，如持续时间和音调的方法相比，Fastspeech 2 和 2s 采用基于自注意力的前馈网络并行生成梅尔谱图或波形。虽然一些现有的非自回归声学模型主要侧重于提高持续时间的准确性，但 FastSpeech 2 和 2s 提供了更多的变化信息（持续时间、音高和能量）作为输入，以减少输入和输出之间的信息差距。一项当下的工作Fastpitch在音素级别采用音调预测，而 FastSpeech 2 和 2s 在帧级别预测更细粒度的音调轮廓。此外，为了改善合成语音中的韵律，FastSpeech 2 和 2s 进一步引入了连续小波变换来模拟音调的变化。</p>
<p>虽然 ClariNet 等一些文本到波形模型联合训练自回归声学模型和非自回归声码器，但 FastSpeech 2s 包含完全非自回归架构以进行快速推理。 一项名为 EATS (Donahue et al., 2020) 的并发工作也采用非自回归架构和对抗训练将文本直接转换为波形，主要侧重于使用可微单调插值预测每个音素端到端的持续时间方案。 与 EATS 相比，FastSpeech 2s 额外提供了更多的变化信息，以缓解 TTS 中的一对多映射问题。</p>
<p>以前的非自回归声码器不是完整的文本到语音系统，因为它们转换时间对齐的语言特征转换为波形，并且需要一个单独的语言模型将输入文本转换为语言特征，或者需要一个声学模型将输入文本转换为声学特征（例如，melspectrograms）。 FastSpeech 2s 是第一次尝试从完全并行的音素序列中直接生成波形，而不是语言特征或梅尔谱图。</p>
<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><h3 id="3-1-实验设置"><a href="#3-1-实验设置" class="headerlink" title="3.1 实验设置"></a>3.1 实验设置</h3><p><strong>数据集</strong> 我们在 LJSpeech 数据集上评估 FastSpeech 2 和 2s。 LJSpeech 包含 13,100 个英文音频剪辑（约 24 小时）和相应的文本转录。 我们将数据集分为三组：用于训练的 12,228 个样本、用于验证的 349 个样本（文档标题为 LJ003）和用于测试的 523 个样本（文档标题为 LJ001 和 LJ002）。 对于主观评价，我们在测试集中随机选择 100 个样本。 为了缓解发音错误的问题，我们使用开源字素将文本序列转换为音素序列到音素工具。 我们按照 Shen 等人的方法将原始波形转换为梅尔谱图，并将帧大小和跳跃大小设置为 1024 和 256，相对于采样率 22050。</p>
<p><strong>模型配置 </strong>我们的 FastSpeech 2 由编码器和梅尔频谱图解码器中的 4 个前馈变换器 (FFT) 块组成。 解码器中的输出线性层将隐藏状态转换为 80 维梅尔谱图，我们的模型使用平均绝对误差 (MAE) 进行优化。 我们在附录 A 中添加了我们实验中使用的 FastSpeech 2 和 2s 的更详细配置。 附录 B 中添加了训练和推理的详细信息。</p>
<h3 id="3-2-实验结果"><a href="#3-2-实验结果" class="headerlink" title="3.2 实验结果"></a>3.2 实验结果</h3><p>在本节中，我们首先评估 FastSpeech 2 和 2s 的音频质量、训练和推理加速。 然后我们对我们的方法进行分析和消融研究。</p>
<h4 id="3-2-1-模型表现"><a href="#3-2-1-模型表现" class="headerlink" title="3.2.1 模型表现"></a>3.2.1 模型表现</h4><p><img src="/images/fastspeech2-tab1.png" alt="fastspeech2-tab1"></p>
<p><strong>音频质量</strong> 为了评估感知质量，我们对测试集进行平均意见得分 (MOS) 评估。要求 20 名以英语为母语的人对合成语音样本进行质量判断。文本内容在不同系统之间保持一致，所有测试人员只检查音频质量，没有其他干扰因素。我们将 FastSpeech 2 和 FastSpeech 2s 生成的音频样本的 MOS 与其他系统进行了比较，包括 1) GT，真实记录； 2) GT (Mel + PWG)，我们首先将真实音频转换为梅尔频谱图，然后使用 Parallel WaveGAN (PWG) 将梅尔频谱图转换回音频； 3) Tacotron 2 (Mel + PWG)； 4) Transformer TTS (Mel + PWG)； 5) FastSpeech (Mel + PWG)。 3)、4) 和 5) 中的所有系统都使用 Parallel WaveGAN 作为声码器以进行公平比较。结果如表1所示，可以看出FastSpeech 2可以超越，FastSpeech 2s可以匹配自回归模型Transformer TTS和Tacotron 2的语音质量。重要的是，FastSpeech 2优于FastSpeech，这证明了提供如音高、能量和更准确的持续时间等方差信息，和直接以真实语音作为训练目标，不使用师生蒸馏管道的有效性。</p>
<p><img src="/images/fastspeech2-tab2.png" alt="fastspeech2-tab2"></p>
<p><strong>训练和推理加速</strong> FastSpeech 2 通过去除师生蒸馏过程简化了 FastSpeech 的训练流程，从而减少了训练时间。我们在表2中列出了Transformer TTS（自回归教师模型）、FastSpeech（包括Transformer TTS教师模型和FastSpeech学生模型的训练）和FastSpeech 2的总训练时间。可以看出FastSpeech 2减少了总训练时间与 FastSpeech 相比提高了 3.12 倍。注意这里的训练时间只包括声学模型训练，没有考虑声码器训练。因此，我们这里不比较 FastSpeech 2s 的训练时间。然后，我们评估 FastSpeech 2 和 2s 与自回归 Transformer TTS 模型相比的推理延迟，该模型具有与 FastSpeech 2 和 2s 相似的模型参数数量。我们在表 2 中展示了波形生成的推理加速。可以看出，与 Transformer TTS 模型相比，FastSpeech 2 和 2s 在波形合成中分别将音频生成加速了 47.8 倍和 51.8 倍。我们还可以看到，由于完全端到端的生成，FastSpeech 2s 比 FastSpeech 2 快。</p>
<h4 id="3-2-2-偏差信息分析"><a href="#3-2-2-偏差信息分析" class="headerlink" title="3.2.2 偏差信息分析"></a>3.2.2 偏差信息分析</h4><p><strong>合成语音中更准确的方差信息</strong>  在该段中，我们衡量在 FastSpeech 2 和 2s 中提供更多方差信息（例如，音调和能量）作为输入是否确实可以合成具有更准确音调和能量的语音。</p>
<p><img src="/images/fastspeech2-tab3.png" alt="fastspeech2-tab3"></p>
<p>对于音高，我们计算矩（标准偏差 (σ)、偏度 (γ) 和峰态 (K)）和平均动态时间扭曲 (DTW) 真实语音和合成语音的音高分布的距离。 结果如表3所示。可以看出，与FastSpeech相比，FastSpeech 2/2s生成音频的矩（σ、γ和K）更接近于真实音频，并且平均DTW距离与其他方法相比更近，这可以展示出 FastSpeech 2/2s 可以生成比 FastSpeech 更自然的音高轮廓（可以产生更好的韵律）的语音。 我们还在附录 D 中对生成的音高轮廓进行了案例研究。</p>
<p><img src="/images/fastspeech2-tab4.png" alt="fastspeech2-tab4"></p>
<p>对于能量，我们计算从生成的波形中提取的逐帧能量与真实语音之间的平均绝对误差 (MAE)。 为了保证合成语音和ground-truth语音的帧数一致，我们在FastSpeech和FastSpeech 2中都使用了MFA提取的ground-truth时长，结果如表4所示。我们可以看到FastSpeech 2/2s 的能量MAE小于 FastSpeech 的能量MAE，表明它们合成的语音音频与真实音频具有更相似的能量。</p>
<p><img src="/images/fastspeech2-tab5.png" alt="fastspeech2-tab5"></p>
<p><strong>模型训练更准确的持续时间</strong>  我们然后分析提供的持续时间信息的准确性以训练持续时间预测器，以及基于 FastSpeech 的更准确的持续时间对于获得更好的语音质量的有效性。我们手动将教师模型生成的 50 个音频与音素级别的相应文本对齐，并获得真实音素级别的持续时间。我们分别使用来自 FastSpeech 的教师模型和本文中使用的 MFA 的持续时间来计算绝对音素边界差异的平均值the average of absolute phoneme boundary differences。结果如表5a所示。我们可以看到，MFA 可以生成比 FastSpeech 的教师模型更准确的持续时间。接下来，我们将 FastSpeech 中使用的持续时间（来自教师模型）替换为 MFA 提取的持续时间，并进行 CMOS测试以比较使用不同持续时间训练的两个 FastSpeech 模型之间的语音质量。结果列于表 5b 中，可以看出更准确的持续时间信息提高了 FastSpeech 的语音质量，这验证了我们从 MFA 改进的持续时间的有效性。</p>
<h4 id="3-2-3-消融实验"><a href="#3-2-3-消融实验" class="headerlink" title="3.2.3 消融实验"></a>3.2.3 消融实验</h4><p><img src="/images/fastspeech2-tab6.png" alt="fastspeech2-tab6"></p>
<p><strong>音高和能量输入</strong>  我们进行消融研究以证明 FastSpeech 2 和 2s 的几个方差信息的有效性，包括音高和能量。 我们对这些消融研究进行 CMOS 评估。 结果如表6所示。 我们发现在FastSpeech 2和2s中去除能量（两个子表中的第3行）导致语音质量方面的性能下降（分别为-0.040和-0.160 CMOS），表明能量在 FastSpeech 2 中对于提高语音质量方面是有效的，对 FastSpeech 2s 更有效。 我们还发现在 FastSpeech 2 和 2s 中去除pitch（两个子表中的第 4 行）分别导致 -0.245 和 -1.130 CMOS，这证明了pitch的有效性。 当我们同时去除音高和能量（两个子表中的最后一行）时，语音质量进一步下降，表明音高和能量都有助于提高 FastSpeech 2 和 2s 的性能。</p>
<p><strong>在频域中预测音高</strong> 为了研究使用连续小波变换 (CWT) 在频域中预测音高的有效性，如第 2.3 节所述，我们直接使用 FastSpeech 2 和 2s 中的能量等均方误差拟合音高轮廓。 我们进行 CMOS 评估，FastSpeech 2 和 2s 的 CMOS 下降分别为 0.185 和 0.201。 我们还计算了表 3 中第 6 行（表示为 FastSpeech 2 - CWT）所示的音调矩和平均 DTW 距离。 结果表明，CWT 可以帮助更好地模拟音高，改善合成语音的韵律，从而获得更好的 CMOS 分数。</p>
<p><strong>FastSpeech 2s 中的梅尔谱解码器</strong> 为了验证 FastSpeech 2s 中的梅尔谱解码器对文本特征提取的有效性，如第 2.4 节所述，我们移除梅尔谱解码器并进行 CMOS 评估。 它会导致 0.285 CMOS 压降，这表明梅尔频谱图解码器对于高质量波形生成至关重要。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在这项工作中，我们提出了 FastSpeech 2，一种快速且高质量的端到端 TTS 系统，以解决 FastSpeech 中的问题并缓解一对多映射问题：1）我们直接用Ground-truth mel谱训练模型， 与 FastSpeech 相比，可以简化训练流程并避免信息丢失； 2）我们提高了持续时间精度并引入了更多的方差信息，包括基音和能量以缓解一对多映射问题，并通过引入连续小波变换来改进基音预测。此外，基于FastSpeech 2，我们进一步开发了FastSpeech 2s，一种非自回归的文本到波形生成模型，它享有完全端到端推理的好处，并实现更快的推理速度。我们的实验结果表明，FastSpeech 2 和 2s 的性能优于 FastSpeech，并且 FastSpeech 2 在语音质量方面甚至可以超越自回归模型，训练流程更加简单，同时继承了 FastSpeech 快速、稳健和可控的语音合成优势。</p>
<p>无需任何外部库的高质量、快速和完全端到端的训练绝对是神经 TTS 的最终目标，也是一个非常具有挑战性的问题。为了保证 FastSpeech 2 的高质量，我们使用了外部高性能对齐工具和音调提取工具，这些工具看起来有点复杂，但对于高质量和快速的语音合成非常有帮助。我们相信未来会有更简单的解决方案来实现这一目标，我们肯定会在没有外部对齐模型和工具的情况下致力于完全端到端的 TTS。我们还将考虑更多的方差信息，以进一步提高语音质量并通过更轻量级的模型加速推理。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>END-TO-END ADVERSARIAL TEXT-TO-SPEECH</title>
    <url>/2021/08/09/EATs/</url>
    <content><![CDATA[<h1 id="端到端对抗式文本转语音模型">端到端对抗式文本转语音模型</h1>
<h2 id="摘要">摘要</h2>
<p>现代文本到语音合成管道通常涉及多个处理阶段，每个阶段都是独立于其他阶段设计或学习的。在这项工作中，我们承担了学习以端到端的方式从标准化文本或音素合成语音的挑战性任务，从而产生<strong>直接对字符或音素输入序列进行操作并产生原始语音音频输出</strong>的模型。<strong>我们提出的生成器是前馈的</strong>，因此使用<strong>基于令牌长度预测的可微对齐方案</strong>对训练和推理都很有效。它通过对抗性反馈和预测损失的组合来学习产生高保真音频，从而限制生成的音频在其总持续时间和梅尔频谱图方面与基本事实大致匹配。为了允许模型捕捉生成音频的时间变化，我们在基于频谱图的预测损失中采用软动态时间扭曲。由此产生的模型在 5 分制上的平均意见得分超过 4，这与依赖多阶段训练和额外监督的最先进模型相当。</p>
<h2 id="introduction">Introduction</h2>
<p>文本到语音 (TTS) 系统处理自然语言文本输入以生成合成的类人语音输出。典型的 TTS 管道由多个独立训练或设计的阶段组成——例如文本规范化、对齐语言特征化、梅尔频谱图合成和原始音频波形合成。尽管这些管道已被证明能够进行逼真的高保真语音合成并在当今广泛使用，但这些模块化方法有许多缺点。它们通常需要在每个阶段进行监督，在某些情况下需要昂贵的“ground truth”标注来指导每个阶段的输出，以及阶段的顺序训练。此外，他们无法从机器学习的许多预测和合成任务领域中广泛观察到的数据驱动的“端到端”学习的全部潜在回报。</p>
<p>在这项工作中，我们旨在简化 TTS 管道，并以端到端的方式承担从文本或音素合成语音的挑战性任务。我们提出了 EATS – End-to-end Adversarial Text-to-Speech – TTS 的生成模型，用于对抗性训练的 TTS，它对纯文本或原始（时间未对齐）音素输入序列进行操作，并产生原始语音波形作为输出。这些模型通过在整个网络中维护学习的中间特征表示，消除了大多数最先进的 TTS 引擎中存在的典型中间瓶颈。</p>
<p>我们的语音合成模型由两个高层级子模块组成，详见第 2 节。<strong>对齐器处理原始输入序列并在其自己学习的抽象特征空间中生成相对低频 (200 Hz) 对齐的特征。</strong>对齐器输出的特征可以被认为取代了典型 TTS 管道的早期阶段——例如，时间对齐的质谱图或语言特征。然后将这些特征输入到解码器，解码器通过一维卷积对来自对齐器的特征进行上采样，以产生 24 kHz 的音频波形。</p>
<p>通过精心设计对齐器并结合对抗性反馈和特定领域的损失函数来指导训练，我们证明了 TTS 系统可以几乎端到端地学习，从而产生接近状态的高保真自然语音： 最先进的 TTS 系统。 我们的主要贡献包括：</p>
<ul>
<li>一种完全可微且高效的前馈对齐器架构，可预测每个输入令牌的持续时间并生成音频对齐表示。</li>
<li>使用灵活的基于动态时间扭曲DTW的预测损失来强制与输入条件对齐，同时允许模型捕获人类语音中时间的可变性。</li>
<li>整体系统的平均意见得分为 4.083，接近使用更丰富的监督信号训练的模型的最新技术水平。</li>
</ul>
<h2 id="method">Method</h2>
<div class="figure">
<img src="/images/eats-fig1.png" alt="eats-fig1" />
<p class="caption">eats-fig1</p>
</div>
<p>我们的目标是学习一个神经网络（生成器），它将字符或音素的输入序列映射到 24 kHz 的原始音频。除了输入和输出信号的长度差异很大之外，这项任务也具有挑战性，因为输入和输出没有对齐，即事先不知道每个输入标记将对应哪个输出标记。为了应对这些挑战，我们将生成器分为两个部分：（i）对齐器，它将未对齐的输入序列映射到与输出对齐的表示，但采样率较低，为 200 Hz； (ii) 解码器，它将对准器的输出上采样到完整的音频频率。整个生成器架构是可微的，并且经过端到端的训练。重要的是，它也是一个前馈卷积网络，这使得它非常适合快速批量推理很重要的应用程序：我们的 EATS 实现在单个 NVIDIA V100 GPU 上以 200 倍的实时速度生成语音。如图 1 所示。</p>
<p>生成器的灵感来自 GAN-TTS，这是一种基于对齐语言特征运行的文本到语音生成对抗网络。我们在我们的模型中使用 GAN-TTS 生成器作为解码器，但它的输入来自对齐器块，而不是对预先计算的语言特征进行上采样。我们通过在潜在向量 z 旁边输入说话人嵌入 s 使其成为说话人条件，从而能够在具有来自多个说话人的录音的更大数据集上进行训练。我们还采用了来自 GAN-TTS 的多个随机窗口鉴别器 (RWD)，这些鉴别器已被证明对对抗性原始波形建模有效，<strong>并且我们通过应用简单的 μ-law 变换对真实音频输入进行了预处理。因此，生成器被训练在 μ-law 域中产生音频，我们在采样时对其输出应用逆变换。</strong></p>
<p>我们用来训练生成器的损失函数如下： <span class="math display">\[
\begin{equation} \label{eqa1}
L_G =L_{G,adv} +λ_{pred} ·L^{′′}_{pred} +λ_{length}·L_{length} ,
\end{equation}
\]</span> 其中 <span class="math inline">\(L_{G,adv}\)</span> 是对抗性损失，在鉴别器的输出中呈线性，与用作鉴别器目标的铰链损失配对，如 GAN-TTS。 使用对抗性 (Goodfellow et al., 2014) 损失是我们方法的一个优势，因为这种设置允许有效的前馈训练和推理，并且这种损失在实践中往往是mode-seeking，这是一种有用的行为，即在一个强条件设置下，其中在文本到语音的情况下，现实主义realism是一个重要的设计目标。 在本节的其余部分，我们详细描述了对齐器网络和辅助预测 (<span class="math inline">\(L^{&#39;&#39;}_{pred}\)</span> ) 和长度 (<span class="math inline">\(L_{length}\)</span>​) 损失，并回顾了从 GAN-TTS 中采用的组件。</p>
<h3 id="aligner">2.1 Aligner</h3>
<p>给定一个长度为 <span class="math inline">\(N\)</span> 的标记序列 <span class="math inline">\(x = (x_1,...,x_N)\)</span>，我们首先计算标记表示 <span class="math inline">\(h = f (x, z, s)\)</span>，其中 <span class="math inline">\(f\)</span> 是一个膨胀卷积的堆栈中穿插着批量归一化和 ReLU 激活。潜变量 <span class="math inline">\(z\)</span> 和说话人嵌入 <span class="math inline">\(s\)</span> 调节批量归一化层的尺度和移位参数。<strong>然后我们分别预测每个输入标记的长度：<span class="math inline">\(l_n = g(h_n, z, s)\)</span>，其中 <span class="math inline">\(g\)</span> 是一个 MLP。我们在输出端使用 ReLU 非线性来确保预测长度是非负的。</strong>然后，我们可以找到作为令牌长度累积和的预测令牌结束位置：<span class="math inline">\(e_n = 􏰄\sum^{n}_{m=1} l_m\)</span>，令牌中心位置为 <span class="math inline">\(c_n = e_n − \frac{1}{2} l_n\)</span>。基于这些预测位置，我们可以将标记表示插入到 200 Hz 的音频对齐表示中，<span class="math inline">\(a = (a_1 , . . , a_S )\)</span>，其中 <span class="math inline">\(S = ⌈e_N ⌉\)</span> 是输出的总数时间步骤。为了计算 <span class="math inline">\(a_t\)</span>​，我们使用 softmax 在 <span class="math inline">\(t\)</span> 和 <span class="math inline">\(c_n\)</span> 之间的平方距离上获得标记表示 <span class="math inline">\(h_n\)</span> 的插值权重，由温度参数 <span class="math inline">\(σ^2\)</span>​ 缩放，我们将其设置为 10.0（即高斯核）： <span class="math display">\[
\begin{equation} \label{eqa2}
w_t^n = \frac {\text{exp} 􏰀−σ^{−2}(t − c_n)^2􏰁} {\sum^N_{m=1} \text{exp} (􏰀−σ^{−2}(t−c_m)^2􏰁}
\end{equation}
\]</span> 使用这些权重，我们可以计算 <span class="math inline">\(a_t = 􏰄\sum^N_{n=1} w_t^nh_n\)</span>​，这相当于非均匀插值。 通过使用累积求和来预测令牌长度和获得位置，而不是直接预测位置，我们隐式地强制对齐的单调性。 请注意，对韵律具有非单调影响的标记（例如标点符号）仍然可以影响整个话语，这要归功于扩张卷积 <span class="math inline">\(f\)</span> 的堆栈，其感受野足够大，可以在整个标记序列中传播信息。 卷积还确保跨不同序列长度的泛化。 附录 B 包括对准器的伪代码。</p>
<h3 id="windowed-generator-training">2.2 Windowed Generator training</h3>
<p>训练样本的长度差异很大，从大约 1 到 20 秒不等。 我们不能在训练期间将所有序列填充到最大长度，因为这会浪费并且非常昂贵：24 kHz 下的 20 秒音频对应于 480,000 个时间步长，这会导致高内存要求。 <strong>相反，我们从每个样本中随机提取一个 2 秒的窗口，我们将其称为训练窗口，通过对随机偏移 <span class="math inline">\(η\)</span> 进行均匀采样。</strong> 对齐器为此窗口生成 200 Hz 音频对齐表示，然后将其馈送到解码器（见图 1）。 请注意，我们只需要计算落在采样窗口内的时间步长 <span class="math inline">\(t\)</span> 对应的<span class="math inline">\(a_t\)</span>​，但我们必须计算整个输入序列的预测标记长度 <span class="math inline">\(l_n\)</span>​。 在评估期间，我们简单地为完整话语生成音频对齐的表示并在其上运行解码器，这是可能的，因为它是完全卷积的。</p>
<h3 id="adversial-discriminators">2.3 Adversial discriminators</h3>
<p><strong>随机窗口鉴别器</strong> 我们使用从 GAN-TTS 中采用的一组随机窗口鉴别器（RWD）。 每个 RWD 对不同长度的音频片段进行操作，从训练窗口随机采样。 我们使用窗口大小为 240、480、960、1920 和 3600 的五个 RWD。这使每个 RWD 能够以不同的分辨率运行。 请注意，24 kHz 的 3600 个样本对应于 150 ms 的音频，因此所有 RWD 都在短时间尺度上运行。 我们模型中的所有 RWD 对文本都是无条件的：它们无法访问文本序列或对齐器输出。 （GAN-TTS 使用 10 个 RWD，其中 5 个以我们忽略的语言特征为条件。）然而，它们通过投影嵌入以说话者为条件。</p>
<p><strong>频谱图鉴别器。</strong> 我们使用一个额外的鉴别器，它在频谱图域的完整训练窗口上运行。 我们从音频信号中提取对数缩放的梅尔频谱图，并使用 BigGAN 深度架构，基本上将频谱图视为图像。 频谱图鉴别器还通过投影嵌入使用说话者身份。 频谱图鉴别器架构的详细信息包含在附录 C 中。</p>
<h3 id="频谱预测损失">2.4 频谱预测损失</h3>
<p>在初步实验中，我们发现对抗性反馈不足以学习对齐。在训练开始时，对齐器不会产生准确的对齐，因此输入令牌中的信息在时间上不正确分布。<strong>这鼓励解码器忽略对齐器输出。无条件鉴别器没有提供有用的学习信号来纠正这个问题。</strong>如果我们想改用条件鉴别器，我们将面临一个不同的问题：我们没有对齐的Ground-truth。条件判别器还需要一个 aligner 模块，该模块在训练开始时无法正常运行，从而有效地将它们变成了无条件判别器。尽管理论上应该可以对抗性地训练鉴别器的对齐器模块，但我们发现这在实践中不起作用，并且训练会卡住。</p>
<p>相反，我们建议通过在频谱图域中使用显式预测损失来指导学习：<strong>我们最小化生成器输出的对数缩放梅尔频谱图与相应的Ground-truth训练窗口之间的 L1 损失。</strong>这有助于训练起飞，并使条件鉴别器变得不必要，从而简化了模型。设 <span class="math inline">\(S_{gen}\)</span> 是生成音频的频谱图，<span class="math inline">\(S_{gt}\)</span> 是相应Ground-truth的频谱图，<span class="math inline">\(S[t,f]\)</span> 是时间步 <span class="math inline">\(t\)</span> 和 mel-frequency bin <span class="math inline">\(f\)</span> 的对数缩放幅度。那么预测损失为： <span class="math display">\[
\begin{equation} \label{eqa3}
L_{pred} = \frac{1}{F} \sum^T_{t=1} \sum^{􏰄F}_{f=1} |S_{gen}[t,f]−S_{gt}[t,f]|.
\end{equation}
\]</span> <span class="math inline">\(T\)</span> 和 <span class="math inline">\(F\)</span>​​ 分别是时间步长和梅尔频率箱的总数。 <strong>在频谱图域而不是时域中计算预测损失的优点是增加了生成的信号和Ground-truth信号之间的相位差的不变性</strong>，这在感知上并不显着。 鉴于频谱图提取操作有多个超参数且其实现未标准化，因此我们在附录 D 中提供了用于此的代码。我们对Ground-truth应用了少量抖动（在 24 kHz 时高达 ±60 个样本） 计算 <span class="math inline">\(S_{gt}\)</span> 之前的波形，这有助于减少生成的音频中的伪影。</p>
<p>无法仅从对抗性反馈中学习对齐是值得扩展的：基于似然的自回归模型在学习对齐方面没有问题，因为它们能够在训练期间受益于教师强迫：模型被训练为执行给定前面的ground-truth，对每个序列步骤的下一步预测，并且预计一次仅推断对齐一个步骤。然而，这与前馈对抗模型不兼容，因此预测损失对于我们模型的引导对齐学习是必要的。</p>
<p>请注意，尽管我们在 <span class="math inline">\(L_{pred}\)</span> 中使用 mel 谱图进行训练（并计算谱图鉴别器的输入，第 2.3 节），但生成器本身不会生成谱图作为生成过程的一部分。相反，它的输出是原始波形，我们将这些波形转换为频谱图仅用于训练（通过波形反向传播梯度到梅尔频谱图转</p>
<h3 id="dynamic-time-warping">2.5 Dynamic time warping</h3>
<div class="figure">
<img src="/images/eats-fig2.png" alt="eats-fig2" />
<p class="caption">eats-fig2</p>
</div>
<p>频谱图预测损失错误地假设令牌长度是确定性的。 通过结合动态时间扭曲 (DTW)，我们可以放宽对生成的和Ground-truth频谱图精确对齐的要求。 我们通过在生成的和目标频谱图 <span class="math inline">\(S_{gen}\)</span>​ 和 <span class="math inline">\(S_{gt}\)</span> 之间迭代找到最小成本对齐路径 <span class="math inline">\(p\)</span> 来计算预测损失。 我们从两个频谱图中的第一个时间步开始：<span class="math inline">\(p_{gen,1} = p_{gt,1} = 1\)</span>。在每次迭代 <span class="math inline">\(k\)</span>​ 中，我们采取三种可能的行动之一：</p>
<ol style="list-style-type: decimal">
<li><p>在<span class="math inline">\(S_{gen}\)</span>​​，<span class="math inline">\(S_{gt}\)</span>​​中都进入下一个时间步： <span class="math inline">\(p_{gen,k+1} = p_{gen,k} + 1, p_{gt,k+1} = p_{gt,k} + 1\)</span>；</p></li>
<li>仅在 <span class="math inline">\(S_{gt}\)</span>​ 中进入下一个时间步： <span class="math inline">\(p_{gen,k+1} = p_{gen,k}, p_{gt,k+1} = p_{gt,k} + 1\)</span>；</li>
<li><p>仅在 <span class="math inline">\(S_{gen}\)</span> 中转到下一个时间步：<span class="math inline">\(p_{gen,k+1} = p_{gen,k} + 1, p_{gt,k+1} = p_{gt,k}\)</span>。</p></li>
</ol>
<p>结果路径是 <span class="math inline">\(p = ⟨(p_{gen,1}, p_{gt,1}), . . . , (p_{gen,K_p} , p_{gt,K_p} )⟩\)</span>​​​​​​，其中 <span class="math inline">\(K_p\)</span>​​​​ 是长度。 每个动作都根据 <span class="math inline">\(S_{gen}[p_{gen,k}]\)</span>​​​ 和 <span class="math inline">\(S_{gt}[p_{gt,k}]\)</span>​​​ 之间的 <span class="math inline">\(L_1\)</span>​ 距离分配一个成本。如果我们选择不是固定步骤推进两个频谱图（即我们正在扭曲 通过采取行动 2 或 3 来绘制频谱图；我们使用 <span class="math inline">\(w = 1.0\)</span>​​），那么会产生一个扭曲惩罚<span class="math inline">\(w\)</span>。 因此，扭曲惩罚鼓励不会偏离身份对齐太远的对齐路径。 令 <span class="math inline">\(δ_k\)</span> 是一个指标，对于发生扭曲的迭代为 1，否则为 0。 那么总路径成本<span class="math inline">\(c_p\)</span>为： <span class="math display">\[
\begin{equation}\label{eqa4}
c_p = \sum^{􏰄K_p}_{k=1} 􏰂w·δ_k + \frac{1}{􏰄F} \sum^{F}_{f=1} |S_{gen}[p_{gen,k},f]−S_{gt}[p_{gt,k},f]|􏰃
\end{equation}
\]</span> <span class="math inline">\(K_p\)</span> 取决于扭曲程度（<span class="math inline">\(T ≤ K_p ≤ 2T − 1\)</span>）。 DTW 预测损失则为： <span class="math display">\[
\begin{equation}\label{eqa5}
L^′_{pred} = min_{p∈P} c_p,
\end{equation}
\]</span> 其中 <span class="math inline">\(P\)</span>​​ 是所有有效路径的集合。 <span class="math inline">\(p ∈ P\)</span>​​ 仅当 <span class="math inline">\(p_{gen,1} = p_{gt,1} = 1\)</span>​​ 和 <span class="math inline">\(p_{gen,K_p} = p_{gt,K_p} = T\)</span>​​ 时，即谱图的第一个和最后一个时间步是对齐的。 为了找到最小值，我们使用动态规划。 图 2 显示了两个序列之间的最佳比对路径图。</p>
DTW 是可微的，但所有路径的最小值使得优化变得困难，因为梯度仅通过最小路径传播。 我们改为使用 DTW 的软版本，它用软最小值替换最小值： $$
<span class="math display">\[\begin{equation} \label{eqa6}
L^{′′}_{pred} = −τ · log 􏰄\sum_{ p∈P } \text{exp} (􏰀− \frac{c_p}{τ} ) , 

\end{equation}\]</span>
<p>$$ 其中 <span class="math inline">\(τ = 0.01\)</span> 是温度参数，损耗比例因子 <span class="math inline">\(λ_{pred} = 1.0\)</span>。 请注意，通过让 <span class="math inline">\(τ → 0\)</span> 来恢复最小操作。由此产生的损失是所有路径上的加权聚合成本，使梯度传播能够通过所有可行路径。 这就产生了一个权衡：较高的 <span class="math inline">\(τ\)</span> 使优化更容易，但由此产生的损失不太准确地反映了最小路径成本。 附录 E 中提供了软 DTW 程序的伪代码。</p>
<p>通过放松预测损失中的对齐，生成器可以生成不完全对齐的波形，而不会因此受到严重惩罚。 这与对抗性损失产生了协同作用：而不是因为预测损失的刚性而相互对抗，损失现在合作来奖励具有随机对齐的真实音频生成。 请注意，<strong>预测损失是在训练窗口上计算的，而不是在全长话语上计算的</strong>，因此我们仍然假设窗口的起点和终点完全对齐。 虽然这可能是不正确的，但在实践中似乎并不是什么大问题。</p>
<h3 id="对齐器长度损失">2.6 对齐器长度损失</h3>
<p>为了确保模型产生真实的标记长度预测，我们添加了一个损失，鼓励预测的话语长度接近真实长度。 该长度是通过对所有令牌长度预测求和得出的。 设 <span class="math inline">\(L\)</span> 为 200 Hz 时训练话语的时间步数，<span class="math inline">\(l_n\)</span> 为第 <span class="math inline">\(n\)</span> 个标记的预测长度，<span class="math inline">\(N\)</span>​ 为标记数，则长度损失为： <span class="math display">\[
\begin{equation} \label{eqa7}
L_{length} = \frac{1}{2} ( L − \sum^N_{n=1} l_n )^2
\end{equation}
\]</span> 我们使用比例因子<span class="math inline">\(λ_{length} = 0.1\)</span>​。 请注意，我们不能单独将预测长度 <span class="math inline">\(l_n\)</span>​ 与真实长度匹配，因为后者不可用。</p>
<h3 id="文本预处理">2.7 文本预处理</h3>
<p>尽管我们的模型在字符输入上运行良好，但我们发现使用音素输入显着提高了样本质量。考虑到拼写映射到音素的异类方式，尤其是在英语中，这并不奇怪。许多字符序列也有特殊的发音，例如数字、日期、度量单位和网站域，并且模型需要非常大的训练数据集来学习正确发音。可以预先应用文本规范化来拼出这些序列，因为它们通常是发音的，然后可能会转换为音素。我们使用开源工具 phonemizer，它执行部分标准化和音素化。最后，无论是对文本还是音素输入序列进行训练，我们都会使用特殊的静音标记（用于训练和推理）对序列进行前后填充，以允许对齐器在每个话语的开头和结尾处考虑静音。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Wavegrad2</title>
    <url>/2021/08/10/Wavegrad2/</url>
    <content><![CDATA[<h1 id="wavegrad2-文本到语音合成的迭代细化">WaveGrad2: 文本到语音合成的迭代细化</h1>
<h2 id="摘要">摘要</h2>
<p>本文介绍了 WaveGrad 2，一种用于文本到语音合成的非自回归生成模型。 <strong>WaveGrad 2 被训练来估计给定音素序列的波形的对数条件密度的梯度。</strong>该模型采用输入音素序列，并通过迭代细化过程生成音频波形。这与原始的 WaveGrad 声码器形成对比，后者以由单独模型生成的梅尔频谱图特征为条件。迭代细化过程从高斯噪声开始，通过一系列细化步骤（例如 50 步），逐步恢复音频序列。 WaveGrad 2 通过调整细化步骤的数量，提供了一种在推理速度和样本质量之间进行权衡的自然方式。实验表明，该模型可以生成高保真音频，接近最先进的神经 TTS 系统的性能。我们还报告了对不同模型配置的各种消融研究。</p>
<h2 id="introduction">Introduction</h2>
<p>深度学习彻底改变了文本到语音 (TTS) 合成。 文本到语音是一个多模态生成问题，它将输入文本序列映射到具有许多可能变化的语音序列，例如韵律、说话风格和发音模式。 大多数神经 TTS 系统遵循两阶段生成过程。 在第一步中，特征生成模型从文本或音素序列生成中间表示，通常是线性或梅尔谱图。 中间表示控制波形的结构，通常由自回归架构生成以捕获丰富的分布。 接下来，声码器将中间特征作为输入并预测波形。 因为它在推理过程中将来自特征生成模型的预测特征作为输入，所以声码器通常使用预测特征作为输入进行训练。</p>
<p>尽管这种两级 TTS 管道可以产生高保真音频，但部署可能很复杂，因为它使用了级联的学习模块。 另一个问题与通常主要根据经验选择的中间特征有关。 例如，mel-spectrogram 功能通常运行良好，但可能不是所有应用程序的最佳选择。 相比之下，数据驱动的端到端方法的好处已在机器学习的各种任务领域中得到广泛观察。 端到端方法能够从训练数据中自动学习最好的中间特征，这些特征通常是特定于任务的。 它们也更容易训练，因为它们不需要在不同阶段的监督和Ground-truth信号。</p>
<p>有两种主要的端到端 TTS 模型。自回归模型提供易于处理的似然计算，但它们需要在推理时反复生成波形样本，这可能很慢。相比之下，非自回归模型能够实现高效的并行生成，但它们需要令牌持续时间信息。用于训练的参考标记持续时间通常由离线强制对齐模型计算。为了预测生成的持续时间，训练了一个额外的模块来预测真实的持续时间。最近的工作侧重于将非自回归模型应用于端到端 TTS。然而，他们仍然依赖谱损失和梅尔谱图进行对齐，并且没有充分利用端到端的训练。 Fast Speech 2 需要额外的调节信号，例如音调和能量，以减少候选输出序列的数量。 EATS 使用对抗训练以及频谱图损失来处理一对多映射问题，这使得架构更加复杂。</p>
<p>在这项工作中，我们提出了 WaveGrad 2，<strong>这是一种非自回归音素到波形模型，不需要中间特征或专门的损失函数。</strong>为了使架构更加端到端，WaveGrad 解码器被集成到 Tacotron 2 式非自回归编码器中。 WaveGrad 解码器从随机噪声开始迭代优化输入信号，并且可以产生具有足够步长的高保真音频。一对多映射问题由分数匹配目标score matching objective处理，该目标优化对数似然的加权变分下界。</p>
<p>非自回归编码器遵循最近提出的非注意力 Tacotron，<strong>它结合了文本编码器和高斯重采样层以合并持续时间信息。</strong>在训练期间使用Ground-truth持续时间信息，并训练持续时间预测器来估计它。在推理过程中，持续时间预测器预测每个输入标记的持续时间。与基于注意力的模型相比，这种持续时间预测器对注意力失败的弹性明显更强，并且由于计算位置的方式而保证了单调对齐。本文的主要贡献如下：</p>
<ul>
<li>一种完全可微且高效的架构，可直接生成波形而无需显式生成频谱图等中间特征；</li>
<li>通过改变细化步骤的数量在保真度和生成速度之间提供自然权衡的模型；</li>
<li>端到端非自回归模型达到 4.43 平均意见得分 (MOS)，接近最先进的神经 TTS 系统的性能。</li>
</ul>
<h2 id="score-matching">Score matching</h2>
<p>与最初的 WaveGrad 类似，WaveGrad 2 建立在分数匹配 和扩散概率模型 (diffusion probabilistic models) 的先前工作之上。 在 TTS 的情况下，得分函数被定义为对数条件分布 <span class="math inline">\(p(y | x)\)</span> 相对于输出 <span class="math inline">\(y\)</span>​ 的梯度为 <span class="math display">\[
\begin{equation}\label{eqa1}
s(y | x) = ∇_y \text{ log } p(y | x),
\end{equation}
\]</span> 其中 <span class="math inline">\(y\)</span> 是波形，<span class="math inline">\(x\)</span> 是conditioning signal。 为了在给定条件信号的情况下合成语音，可以通过Langevin dynamics从初始化 <span class="math inline">\(y ̃_0\)</span> 开始迭代地绘制波形为 <span class="math display">\[
\begin{equation}\label{eqa2}
y ̃_{i + 1} = y ̃_i + \frac{η}{2} s ( y ̃_i | x ) + \sqrt η z_i,
\end{equation}
\]</span> 其中 <span class="math inline">\(η &gt; 0\)</span> 是步长，<span class="math inline">\(z_i ∼ N(0,I)\)</span>，<span class="math inline">\(I\)</span> 表示单位矩阵。</p>
<p>继之前的工作之后，我们采用了一种特殊的参数化方法，称为扩散模型(diffusion model)。 评分网络 <span class="math inline">\(s(y ̃ | x, α ̄)\)</span>​ 被训练通过最小化模型预测和真实值 <span class="math inline">\(\epsilon\)</span>​ 之间的距离来预测缩放导数，如下所示 <span class="math display">\[
\begin{equation}\label{eqa3}
E_{α ̄,\epsilon} [|| \epsilon_{\theta}(y ̃, x, \sqrt  α ̄ ) - \epsilon||_1]
\end{equation}
\]</span> 其中 <span class="math inline">\(\epsilon ∼ N (0, I )\)</span>​ 是通过应用&quot;重新参数化技巧&quot;引入的噪声项，<span class="math inline">\(α ̄\)</span> 是噪声水平，<span class="math inline">\(y ̃\)</span>​​ 是根据以下公式采样的 <span class="math display">\[
\begin{equation}\label{eqa4}
 y ̃ = \sqrt α ̄ y_0 + \sqrt {1-α ̄} \epsilon
\end{equation}
\]</span> 在训练过程中，<span class="math inline">\(α ̄\)</span>'s是从区间<span class="math inline">\([α ̄_n, α ̄_{n+1}]\)</span>​​中基于一个预定义的<span class="math inline">\(\beta\)</span>​​​的线性计划采样的，公式如下： <span class="math display">\[
\begin{equation}\label{eqa5}
α ̄_n:=􏰅\prod_{s=1}^n(1−β_s).
\end{equation}
\]</span> 在每次迭代中，更新后的波形按照以下随机过程进行估计： <span class="math display">\[
\begin{equation}\label{eqa6}
 y_{n−1}=\frac{1}{\sqrt \alpha_n} ( y_n− \frac{\beta_n} {\sqrt {1-α ̄_n}} \epsilon_θ(y_n,x, \sqrt α ̄_n) +σ_nz.
\end{equation}
\]</span></p>
<h2 id="wavegrad-2">WaveGrad 2</h2>
<div class="figure">
<img src="/images/wavegrad2-fig1.png" alt="wavegrad2-fig1" />
<p class="caption">wavegrad2-fig1</p>
</div>
<p>所提出的模型包括三个模块，如图1：</p>
<ul>
<li>编码器将音素序列作为输入并从输入上下文中提取抽象的隐藏表示。</li>
<li>重采样层改变编码器输出的分辨率以匹配输出波形时间尺度，量化为 10ms 段（类似于典型的梅尔频谱图特征）。 这是通过在训练期间调节目标持续时间来实现的。 在推理期间利用持续时间预测器模块预测的持续时间。</li>
<li>WaveGrad 解码器通过迭代改进有噪声的波形来预测原始波形。 在每次迭代中，解码器逐渐细化信号并添加细粒度的细节。</li>
</ul>
<h3 id="encoder">3.1 Encoder</h3>
<p>编码器的设计遵循 Tacotron 2 的设计。 音素标记用作输入，在单词边界插入静音标记。 在每个句子之后添加一个序列结束标记。 标记首先转换为学习嵌入learned embedding，然后通过 3 个含dropout 的卷积层和批量归一化层。 最后，通过将输出传递到具有 ZoneOut 正则化的单个双向长短期记忆 (LSTM) 层，对长期上下文信息进行建模。</p>
<h3 id="重采样">3.2 重采样</h3>
<p>输出波形序列的长度与编码器表示的长度非常不同。 在 Tacotron 2 中，这是通过注意力机制解决的。 为了使结构非自回归和加速推理，我们采用了非注意力 Tacotron 中引入的高斯上采样。 高斯上采样不是根据其持续时间重复每个标记，而是同时预测持续时间和影响范围。 这些参数用于注意力权重计算，它完全依赖于预测的位置。 在训练期间，改为使用Ground-truth持续时间，并测量额外的均方损失以训练持续时间预测器。 这在图 1 中被标记为 Duration Loss。推理期间不需要真实持续时间，而是采用预测持续时间。</p>
<h3 id="采样窗口">3.3 采样窗口</h3>
<p>由于波形分辨率非常高（在我们的例子中为每秒 24,000 个样本），由于高计算成本和内存限制，计算一次话语中所有波形样本的损失是不可行的。 相反，在学习了整个输入序列的表示之后，我们对一个小片段进行采样以合成波形。 由于重采样层，编码器表示和波形样本已经对齐。 在每个小批量中单独采样随机段，并根据上采样率（在我们的设置中为 300）提取相应的波形段。 在推理期间使用完整的编码器序列（重采样后），这会在训练和推理之间引入小的不匹配。 我们进行消融研究以研究采样窗口大小如何影响第 4.1 节中的保真度。</p>
<h3 id="解码器">3.4 解码器</h3>
<div class="figure">
<img src="/images/wavegrad2-fig2.png" alt="wavegrad2-fig2" />
<p class="caption">wavegrad2-fig2</p>
</div>
<p>解码器逐渐对隐藏的表示进行上采样以匹配波形分辨率。 在我们的例子中，波形以 24 kHz 采样，我们需要上采样 300 倍。 这是使用 WaveGrad 解码器实现的，如图 2 所示。该架构包括 5 个上采样块 (UBlock) 和 4 个下采样块 (DBlock)。 在生成过程的每次迭代中，网络通过预测包含的噪声项 <span class="math inline">\(\epsilon_n\)</span> 来对噪声输入波形估计 <span class="math inline">\(y_n\)</span> 去噪，条件是隐藏表示遵循等式 6。如第 2 节所述，生成过程从 随机噪声估计 <span class="math inline">\(y_N\)</span> ，并在 <span class="math inline">\(N\)</span>​（通常设置为 1000)步上迭代细化它以生成波形样本。 在我们之前的工作之后，训练目标是预测和真实噪声项之间的 L1 损失。 在训练期间，使用单个随机采样迭代计算此损失。</p>
<h2 id="结论">结论</h2>
<p>在本文中，我们提出了 WaveGrad 2，这是一种端到端的非自回归 TTS 模型，它以音素序列作为输入并直接合成波形，而不像大多数 TTS 系统那样使用手工设计的中间特征（例如，频谱图）。 与先前的工作类似，输出波形是通过从随机噪声开始的迭代细化过程生成的。 生成过程通过改变细化步骤的数量来提供保真度和速度之间的权衡。 实验表明，WaveGrad 2 能够生成与强基线相当的高保真音频。 探索不同模型配置的消融研究发现，增加模型大小是决定 WaveGrad 2 综合质量的最重要因素。 未来的工作包括在有限的细化迭代次数下提高性能。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Reinforce-aligner Reinforcement Alignment Search for Robust End-to-End Text-to-Speech</title>
    <url>/2021/08/10/reinforce-aligner/</url>
    <content><![CDATA[<h1 id="Reinforce-aligner-增强对齐搜索以实现稳健的端到端文本到语音"><a href="#Reinforce-aligner-增强对齐搜索以实现稳健的端到端文本到语音" class="headerlink" title="Reinforce-aligner: 增强对齐搜索以实现稳健的端到端文本到语音"></a>Reinforce-aligner: 增强对齐搜索以实现稳健的端到端文本到语音</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>文本到语音 (TTS) 合成是从文本或音素输入生成合成语音的过程。传统的 TTS 模型包含多个处理步骤，需要外部对齐器，提供音素到帧序列的注意力对齐。随着每增加一个步骤的复杂性增加和效率降低，对具有高效内部对准器的端到端 TTS 的现代合成管道的需求不断扩大。<strong>在这项工作中，我们提出了一种端到端的文本到波形网络，该网络具有一种新颖的基于强化学习的持续时间搜索方法。</strong>我们提出的生成器是前馈的，对齐器通过从为最大化累积奖励而采取的行动中接收主动反馈来训练代理做出最佳持续时间预测。我们展示了由受过训练的代理生成的音素到帧序列的准确对齐增强了合成音频的保真度和自然度。实验结果还表明，与具有内部和外部对准器的其他最先进的 TTS 模型相比，我们提出的模型具有优越性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>自回归模型启动了文本转语音 (TTS) 的快速进展。这些模型取代了传统方法，并且通常在具有注意力机制的编码器-解码器框架中进行序列到序列。编码器的目的是从音素序列中提取隐藏的表征特征向量，解码器从产生的向量中生成梅尔谱图。尽管有这些优点，但自回归模型中的端到端注意力也有局限性，例如推理速度慢、跳词和阅读。作为解决这个问题的一种方法，非自回归模型被提议用于从文本或音素并行生成梅尔谱图。尽管新架构减轻了自回归模型的一些缺点，但非自回归模型的持续时间对齐器仍然需要外部对齐器的指导。使用外部矫正器的最关键问题是增加了训练过程的复杂性。在训练之前，自回归模型需要正确对齐的文本和语音注意力图。这会延迟训练过程，并且非自回归模型变得依赖于外部对齐器生成的对齐质量。因此，最近的合成管道的设计目标是强大的端到端内部对准器。</p>
<p>最近与我们的工作相关的 TTS 工作是 EATS 和 HiFi-GAN 。 EATS 是一种端到端的文本到波形网络，具有内部对齐器，该对齐器将音素与高斯核的梅尔谱序列对齐近似。尽管该模型提出了强大的文本到波形合成，但没有额外训练对齐以确保改进的持续时间对齐。 Hifi-GAN 通过多尺度和多周期鉴别器生成高质量的音频波形，但该模型仅限于仅考虑梅尔谱图作为输入。我们提出的文本到波形网络代表了两全其美的优点和对缺点的改进。</p>
<p>在本文中，我们提出了一种具有强化对齐器的端到端文本到波形网络，这是一种基于强化学习的对齐搜索方法，用于鲁棒语音合成。我们的代理通过一系列步骤与环境交互，以选择给定当前状态的最佳动作。然后，环境对动作应用更新并返回动作的奖励，以便代理在下一步考虑。重复这个训练过程直到收敛，并使网络能够在内部学习自己的对齐方式。我们的实验结果显示了强化对齐器对生成的音频波形的持续时间对齐和质量的积极影响。我们的在线演示网页上提供了合成音频样本。 </p>
<h2 id="Model-Architecture"><a href="#Model-Architecture" class="headerlink" title="Model Architecture"></a>Model Architecture</h2><p>我们提出的全卷积生成器利用文本或音素作为输入并生成原始音频波形作为输出。编码器包含一个改进的多感受野融合模块（MRF）。 MRF 的原始实现使用梅尔频谱图作为输入，并通过转置卷积将梅尔频谱图上采样为原始波形。我们的实现利用音素嵌入作为输入和输出隐藏表示，而无需上采样。基于 MRF 的音素编码器包含多个具有多个内核大小和扩张率的残差块，这是我们网络的重要组成部分，因为各种感受野能够从音素嵌入中提取不同的上下文特征。然后，编码器输出通过强化对齐器进行输出以产生输出帧。输出帧由 $γ$​ 随机分割。最后，解码器将 $γ$ 上采样 256 以产生原始音频波形。</p>
<p><img src="/images/ra-fig1.png" alt="ra-fig1"></p>
<p>我们的模型在训练期间包含针对不同目标的两个判别器。第一个鉴别器是中提出的多尺度鉴别器。该鉴别器通过音阶的变化有效地学习音频的不同频率分量。三个子鉴别器中的每一个都包含不同尺度的卷积：原始音频，按因子 2 下采样，按因子 4 下采样。第二个鉴别器是 Multi-Period Discriminator，它通过卷积捕获音频的不同特征在周期性变化中。每个不同的周期值考虑输入音频的不同周期段。</p>
<h2 id="Reinforce-Aligner"><a href="#Reinforce-Aligner" class="headerlink" title="Reinforce-Aligner"></a>Reinforce-Aligner</h2><h3 id="3-1-Reinforcement-learning-setup"><a href="#3-1-Reinforcement-learning-setup" class="headerlink" title="3.1 Reinforcement learning setup"></a>3.1 Reinforcement learning setup</h3><p><img src="/images/ra-fig2.png" alt="ra-fig2"></p>
<p>强化对齐器的一般架构如图 1 所示。持续时间预测器是在训练过程的每个步骤中与环境交互的代理。 环境是整个文本到波形网络。 如图 2(a) 的生成器训练过程所示，每个持续时间预测都被上采样为波形，并通过各个 melspectrogram 的 mel-spectrogram 损失计算奖励。 奖励反馈返回给代理以进行最终动作选择。</p>
<h3 id="3-2-Agent"><a href="#3-2-Agent" class="headerlink" title="3.2 Agent"></a>3.2 Agent</h3><p>持续时间预测器为每个音素持续时间预测返回一个标量值。 Predictor 由两个 1D 卷积层组成，每个层都有层归一化、ReLU 激活和跟随的 dropout。最后一个线性层将卷积输出重塑为单个标量值。代理有两种可用的操作：</p>
<ul>
<li>保持：保持音素持续时间预测而不对预测输出进行任何更改。</li>
<li>移位：移位音素持续时间预测，移位值应用于交替符号。</li>
</ul>
<p>应用于每个音素持续时间的移位由交替符号组成，这对于保持持续时间预测输出的总和很重要。此外，我们设计了两种类型的转换：分段转换和音素转换。分段移位对应于应用于整个音素序列段的移位。音素移位是应用于音素序列的每个音素持续时间的移位。我们在消融研究中检查了班次类型和值对对齐结果和语音质量的影响。</p>
<h3 id="3-3-Environment"><a href="#3-3-Environment" class="headerlink" title="3.3 Environment"></a>3.3 Environment</h3><p>在这个基于强化学习的设置中，环境是经过训练的文本到波形网络，它从音素输入输出音频波。 强化对齐器中的环境有两个主要目标：</p>
<ol>
<li>在代理采取行动之前向代理提供输入音素序列信息，</li>
<li>在考虑两种可能的行动中的每一个之后向代理提供反馈。</li>
</ol>
<h4 id="3-3-1-State"><a href="#3-3-1-State" class="headerlink" title="3.3.1 State"></a>3.3.1 State</h4><p>环境在每个训练步骤中为代理生成音素嵌入输出。 从音素序列输入，基于多感受野融合的音素编码器通过多个残差块生成音素嵌入输出。 这些编码器输出是代理决定动作的当前状态输入。</p>
<h4 id="3-3-2-Rewards"><a href="#3-3-2-Rewards" class="headerlink" title="3.3.2 Rewards"></a>3.3.2 Rewards</h4><p>根据shift的类型，我们有两种不同的奖励。 两种奖励都考虑了梅尔谱图损失，即真实数据和生成波形的梅尔谱图之间的 L1 损失。 如图 2(a) 所示，每个质谱图都是由预测 (KEEP) 和移位 (SHIFT) 持续时间合成的波形产生的。 分段奖励比较用于训练的整个波段的损失值。 较低的损失值意味着生成的波形与真实情况的相似度较高。 音素方面的奖励考虑了音素方面的梅尔谱损失值。 具体来说，梅尔谱图损失值通过下采样插入到音素持续时间序列的形状中。 将预测时长的音素时长序列表示为 $D_k = [d_1,d_2,d_3,…,d_j]$ 并将移位shifted时长表示为 $D_s =[d_1 +α,d_2 −α,d_3 +α…,d_j ±α]$​，其中$α$​ 表示移位shift值。 然后，我们的奖励被公式化为：</p>
<p><img src="/images/ra-al1.png" alt="ra-al1"></p>
<p>这里，$L_k$ 表示预测和真实梅尔谱图之间的 L1 损失，$L_s$​ 是移位shifted和真实梅尔谱图之间的 L1 损失。 $r_k$​ 和 $r_s$​ 分别是保持奖励和转移奖励值。 每个$j$​是音素时长序列中总共$N$​个索引的音素时长索引值。 对于分段奖励，保持奖励值和转移奖励值均具有相等的值 $∀j$​。 对于音素方面的奖励，每个 $j$​ 都有唯一的保持和转移奖励值。</p>
<h3 id="3-4-Gaussian-upsampling"><a href="#3-4-Gaussian-upsampling" class="headerlink" title="3.4 Gaussian upsampling"></a>3.4 Gaussian upsampling</h3><p>预测的持续时间被缩放到帧序列输出的长度。 如 [21] 中所介绍的，每个缩放预测用于查找缩放令牌长度及其中心位置的累积总和。 我们首先计算权重：</p>
<p><img src="/images/ra-al2.png" alt="ra-al2"></p>
<p>给定固定温度参数 $σ^2$、缩放标记中心位置 $c$ 和时间步长 $t$​。 我们通过在编码器输出和权重之间产生加权和来完成上采样。 输出特征用作解码器输入，转置卷积将特征上采样为原始波形。</p>
<h3 id="3-5-Reinforced-Duration-Loss"><a href="#3-5-Reinforced-Duration-Loss" class="headerlink" title="3.5 Reinforced Duration Loss"></a>3.5 Reinforced Duration Loss</h3><p>我们通过包含奖励和持续时间预测动作的损失为代理提供适当的反馈。 对于每个音素序列索引 $j$，在原始持续时间预测和所选动作的持续时间之间比较持续时间值。 损失定义为：</p>
<p><img src="/images/ra-al3.png" alt="ra-al3"></p>
<p>给定 $N$ 个total tokens，每个 $D_k$、$r_k$ 对和 $D_s$、$r_s$​ 对分别代表保持、转移动作的持续时间和奖励值。 $D_{pred}$ 等于 $D_k$​，因为保持动作不会改变预测的持续时间值。 因此，该损失为 shift 动作返回正损失，而为 keep 动作返回零损失。</p>
<h2 id="Auxiliary-loss"><a href="#Auxiliary-loss" class="headerlink" title="Auxiliary loss"></a>Auxiliary loss</h2><p>我们利用 GAN 损失 和reinforcement 持续时间损失。 此外，辅助损失用于支持我们的文本到波形网络的训练。</p>
<h3 id="4-1-Total-duration-loss"><a href="#4-1-Total-duration-loss" class="headerlink" title="4.1 Total duration loss"></a>4.1 Total duration loss</h3><p>对齐器的主要目的是产生音素到帧序列的准确对齐。 但是，对齐器在训练期间没有“正确”的持续时间对齐可供参考，因此不确定持续时间输出是否准确。 因此，总持续时间损失被用作模型的指导。 我们参考了中的对准器长度损失。 令 $m_{length}$ 是真实谱图的长度，$l_j$ 是第 $j$​​ 个标记的预测长度。 总持续时间损失为：</p>
<p><img src="/images/ra-al4.png" alt="ra-al4"></p>
<h3 id="4-2-Mel-spectrogram-loss"><a href="#4-2-Mel-spectrogram-loss" class="headerlink" title="4.2 Mel-spectrogram loss"></a>4.2 Mel-spectrogram loss</h3><p>之前的研究提到梅尔谱损失能够优化发生器并提高生成波形的质量。 此外，强化对齐器的奖励设计取决于梅尔谱图损失值，以在我们的模型中为代理产生质量反馈。 损失公式为：</p>
<p><img src="/images/ra-al5.png" alt="ra-al5"></p>
<p>其中 $mel_{gt}、mel_{pred}$ 是 $T$​ 时间步长的真值和合成波形的梅尔谱图。</p>
<h3 id="4-3-Soft-DTW"><a href="#4-3-Soft-DTW" class="headerlink" title="4.3 Soft DTW"></a>4.3 Soft DTW</h3><p>我们通过在Ground-truth和具有动态时间扭曲 (DTW)  的合成频谱图之间迭代地找到对齐路径，使梅尔频谱图有容错空间。 这种方法的主要目标是减轻两个频谱图必须完全对齐的要求。</p>
<p>总共的loss定义如下：</p>
<p><img src="/images/ra-al6.png" alt="ra-al6"></p>
<p>其中 $L$ 是梅尔谱图长度，$ω$​ 是动作 2 和 3 发生的扭曲惩罚。$δ$​ 是一个二进制指示符，当扭曲惩罚大于零时为 1。 对于我们的损失，我们使用 [29] 中的软最小值来产生 Soft-DTW。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们提出了一种端到端的文本到波形网络，该网络具有一种新颖的基于强化学习的持续时间对齐搜索方法。 该模型的优势在于代理能够通过基于奖励反馈的动作主动搜索最佳持续时间对齐。 我们进行了一系列实验来为我们的强化对齐器选择最佳奖励。 我们提出的模型能够以更准确的持续时间对齐和增强的合成音频自然度优于其他最先进的方法。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>nvc-net End-to-End Adversarial Voice Conversion</title>
    <url>/2021/08/11/nvc-net/</url>
    <content><![CDATA[<h1 id="nvc-net-端到端对抗性音色转换任务">NVC-Net 端到端对抗性音色转换任务</h1>
<h2 id="摘要">摘要</h2>
<p>语音转换在语音合成的许多应用中越来越受欢迎。这个想法是在保持语言内容不变的情况下将语音身份从一个说话者更改为另一个说话者。许多语音转换方法依赖于使用声码器从声学特征重建语音，因此，语音质量在很大程度上取决于这种声码器。在本文中，我们提出了一种端到端的对抗网络 NVC-Net，它可以直接对任意长度的原始音频波形进行语音转换。通过从语音内容中分离说话者身份，<strong>NVC-Net 能够执行非并行的传统多对多语音转换以及从看不见的目标说话者的简短话语中进行零样本语音转换。</strong>重要的是，NVC-Net 是非自回归和完全卷积的，可以实现快速推理。我们的模型能够在 NVIDIA V100 GPU 上以超过 3600 kHz 的速率生成样本，在相同硬件配置下比最先进的方法快几个数量级。对非并行多对多语音转换任务的客观和主观评估表明，NVC-Net 以更少的参数获得了有竞争力的结果。</p>
<h2 id="introduction">Introduction</h2>
<p>语音转换包括改变源说话者的语音，使其听起来像目标说话者的语音，同时保持语言信息不变。语音转换的典型应用包括说话人转换、电影和视频中的配音、语音辅助系统和发音转换。尽管在该领域取得了很大成功，但语音转换仍有一些问题需要解决。</p>
<p>早期的语音转换方法需要并行训练数据，其中包含不同说话者所说的相同语言内容的话语。这些方法的一个缺点是它们对源话语和目标话语之间的错位敏感。时间对齐和手动校正通常作为预处理步骤执行，以使这些方法更可靠地工作。尽管如此，收集并行训练数据仍然是一项耗时的工作，在许多情况下甚至是一项不可行的任务。这激发了越来越多的研究兴趣，即通过使用生成对抗网络 (GAN) 及其变体在非并行训练数据上开发语音转换方法。尽管基于 GAN 的方法通过非并行训练数据学习映射提供了一种有前景的解决方案，但设计这种与并行方法相比具有竞争力的方法仍有待解决。</p>
<p>语音转换中的另一个常见问题是对原始音频波形进行建模。一个好的模型应该捕获音频信号中的短期和长期时间依赖性，由于语音的高时间分辨率（例如，每秒 16k 个样本），这并非微不足道。一个简单的解决方案是将时间分辨率降低为较低维的表示（例如，声学特征）。在这种情况下，人们需要一个声码器来从声学特征重建波形表示。像 WORLD 或 STRAIGHT这样的传统声码器通常会引入伪像（金属声音）。为了克服这个问题，已经提出了更复杂的神经声码器，例如自回归和非自回归模型。 WaveNet和 WaveRNN 等自回归模型推理速度较慢，因此它们不太适合实际应用。尽管像 WaveGlow 这样的非自回归模型产生更快的推理，但波形重建仍然主导着计算工作和内存需求。像 MelGAN 或 Parallel WaveGAN这样的轻量级声码器可以缓解内存问题，但是，训练和推理之间的特征不匹配问题仍未解决。事实上，正如吴等人所观察到的那样。 特征不匹配问题可能导致声码器在输出中产生嘈杂或塌陷的语音，尤其是在训练数据有限的情况下。</p>
<p>虽然许多研究工作都集中在在训练期间看到的说话者之间转换音频，但很少关注将音频从训练期间看不见的说话者转换为音频的问题（即零样本语音转换）。钱等人建议在具有许多说话人的大数据集上使用预先训练的说话人编码器。通过精心设计的瓶颈层，内容信息可以从说话者信息中分离出来，从而实现零样本语音转换。然而，在数据和计算资源有限的情况下，说话人编码器不能很好地概括不可见的说话人，因为说话人嵌入往往是分散的。</p>
<p>在本文中，我们以端到端的方式使用对抗训练来解决从原始音频波形转换语音的问题。本文的主要贡献总结如下：</p>
<ol style="list-style-type: decimal">
<li>我们介绍了 NVC-Net，这是一种经过对抗训练的多对多语音转换方法，不需要并行数据。我们的方法旨在将说话者身份与语音内容分开。据我们所知，这是第一个基于 GAN 的方法，它直接在原始音频波形上明确地执行语音转换的解缠结。</li>
<li>与其他基于 GAN 的语音转换方法不同，NVC-Net 可以直接生成原始音频，而无需训练额外的声码器。因此，可以避免使用独立声码器时的特征不匹配问题。声码器的缺失也使得 NVC-Net 在推理时非常高效。它能够在 NVIDIA V100 GPU 上以超过 3600 kHz 的速率生成音频。</li>
<li>NVC-Net 通过限制说话者的表示来解决零样本语音转换问题。我们在说话人潜在空间上应用 Kullback-Leibler 正则化，<strong>使说话人表示对输入的微小变化具有鲁棒性。这也允许在生成时进行随机采样。</strong>在推理过程中，可以从该空间随机采样说话人嵌入以生成不同的输出或从给定的参考话语中提取说话人嵌入。</li>
</ol>
<h2 id="相关工作">相关工作</h2>
<p>由于难以收集并行训练数据，当前的语音转换方法求助于非并行训练数据。他们中的许多人都受到了计算机视觉领域的最新进展的启发。例如，许等人通过根据说话者身份对解码器进行调节，引入了基于变分自动编码器 (VAE) 的语音转换框架。作为扩展，Kameoka 等人对 VAE 使用了一个辅助分类器，迫使转换后的音频被正确分类。尽管取得了成功，但众所周知，条件 VAE 具有生成过度平滑音频的局限性。这可能是有问题的，因为该模型可能会产生质量较差的音频。</p>
<p>解决这个问题的一个有吸引力的解决方案是基于 GAN，因为它们没有对数据分布做出任何明确的假设。他们学习捕捉真实音频的分布，使生成的音频听起来更自然。第一项研究基于 CycleGAN。这个想法是通过结合对抗性损失、循环一致性损失和身份映射损失来学习源和目标说话者之间声学特征的映射。最近，受到 Choi 等人工作的启发StarGAN-VC及其改进版本在语音转换方面表现出非常有希望的结果。许等人证明 GAN 可以改进基于 VAE 的模型。然而，尽管在生成逼真的图像方面取得了成功，但构建 GAN 以直接生成高保真音频波形是一项具有挑战性的任务。<strong>原因之一是音频是高度周期性的，人耳对不连续性很敏感。</strong>此外，与图像不同，音频通常会引起高时间分辨率要求。</p>
<p>在语音转换的背景下，我们将语音视为说话者身份和语言内容的组合。将说话者身份与语言内容分开允许独立地改变说话者身份。有几种方法利用自动语音识别 (ASR) 模型的潜在表示进行语音转换。 ASR 模型用于从源语音中提取语言特征，然后使用另一个依赖说话人的模型将这些特征转换为目标说话人。这些模型可以进行非并行语音转换，但它们的性能高度依赖于 ASR 模型的准确性。用于语音转换的其他解开技术包括自动编码器、矢量量化和实例归一化。</p>
<p>许多上述方法依赖于语音的中间表示，例如频谱图和梅尔频率倒谱系数。尽管它们能够产生良好的感知质量的语音，但有些仍然需要额外的监督来训练强大的声码器。只有几种方法可以处理原始音频波形。例如，恩格尔等人提出了一种 WaveNet 风格的自动编码器，用于在不同乐器之间转换音频。最近，塞拉等人介绍了 Blow，一种用于原始音频信号语音转换的标准化流网络。基于流的模型具有高效采样和精确似然计算的优点，但是，与自回归模型相比，它们的表现力较差。</p>
<h2 id="nvc-net">NVC-Net</h2>
<p>我们的提议旨在实现以下目标：（i）从潜在嵌入重建高度感知相似的音频波形，（ii）在转换过程中保留说话人不变的信息，以及（iii）为目标说话人生成高保真音频。特别是，NVC-Net 由一个内容编码器 <span class="math inline">\(E_c\)</span>​​​​​、一个说话人编码器 <span class="math inline">\(E_s\)</span>​​​​​、一个生成器 <span class="math inline">\(G\)</span>​​​​​ 和三个鉴别器 <span class="math inline">\(D(k)\)</span>​​​​​ 组成，其中 <span class="math inline">\(k = 1、2、3\)</span>​​​​​，它们用于输入的不同时间分辨率。图 1 说明了整体架构。我们假设话语 <span class="math inline">\(x\)</span>​​​​​ 是从两个潜在嵌入生成的，说话者身份<span class="math inline">\(z∈R^{d_{spk}}\)</span>​​​​​，语音内容信息<span class="math inline">\(c∈R^{d_{con}×L_{con}}\)</span>​​​​​，即，<span class="math inline">\(x=G(c,z)\)</span>​​​​​。内容描述了不同说话者之间不变的信息，例如语音和其他韵律信息。为了将话语 <span class="math inline">\(x\)</span>​​​​​ 从说话人 <span class="math inline">\(y\)</span>​​​​​ 转换为说话人 <span class="math inline">\(\tilde y􏰈\)</span>​​​​​​􏰈 的话语 <span class="math inline">\(\tilde x\)</span>​​​​，我们通过内容编码器 <span class="math inline">\(c = E_c(x)\)</span>​​​​ 将 <span class="math inline">\(x\)</span>​​​ 映射到内容嵌入中。此外，目标说话人嵌入 <span class="math inline">\(\tilde z\)</span>​​​ 是从扬声器编码器 <span class="math inline">\(E_s(x􏰈)\)</span> 的输出分布中采样的。最后，我们从内容嵌入 <span class="math inline">\(c\)</span> 生成原始音频，该内容嵌入 <span class="math inline">\(c\)</span> 以目标说话人嵌入 <span class="math inline">\(􏰈\tilde z\)</span> 为条件，即 <span class="math inline">\(\tilde x􏰈 = G(c, 􏰈\tilde z)\)</span>。</p>
<h3 id="训练目标">3.1 训练目标</h3>
<p>我们的目标是训练内容编码器 <span class="math inline">\(E_c\)</span>、说话者编码器 <span class="math inline">\(E_s\)</span> 和学习多个说话者之间映射的生成器 <span class="math inline">\(G\)</span>​​。 下面，我们将详细解释训练的目标函数。</p>
<p><strong>对抗性损失</strong> 为了使合成语音与真实语音无法区分，鉴别器的损失函数 <span class="math inline">\(L_{adv}(D^{(k)})\)</span>​​​ 和生成器的损失函数 <span class="math inline">\(L_{adv}(E_c, E_s, G)\)</span>​​​​​ 定义为：</p>
<div class="figure">
<img src="/images/nvc-net-al1.png" alt="nvc-net-al1" />
<p class="caption">nvc-net-al1</p>
</div>
<p>编码器和生成器被训练来欺骗鉴别器，而鉴别器被训练来同时解决多个二元分类任务。 每个鉴别器有多个输出分支，每个分支对应一个任务。 每项任务包括为一个特定说话者确定输入话语是真实的还是由生成器转换的。 分支的数量等于说话者的数量。 当为 <span class="math inline">\(y\)</span>​ 类的话语 <span class="math inline">\(x\)</span>​ 更新鉴别器 <span class="math inline">\(D^{(k)}\)</span>​ 时，我们只惩罚它的第 <span class="math inline">\(y\)</span>​ 个分支输出 <span class="math inline">\(D^{(k)}(x)[y]\)</span> 不正确，同时保持其他分支输出不变。 在我们的实现中，源话语 <span class="math inline">\(\tilde x\)</span> 的参考话语 <span class="math inline">\(x\)</span>​􏰈 是从同一个小批量中随机抽取的。</p>
<p><strong>重构损失</strong> 在生成音频时，我们通过从内容和说话人嵌入重构输入来强制生成器 G 使用说话人嵌入。 可以使用原始波形上的逐点损失来测量原始音频和生成的音频之间的差异。 然而，逐点损失无法正确捕捉它们之间的差异，因为两个感知相同的音频可能不具有相同的音频波形。 相反，我们使用以下特征匹配损失，</p>
<div class="figure">
<img src="/images/nvc-net-al2.png" alt="nvc-net-al2" />
<p class="caption">nvc-net-al2</p>
</div>
<p>其中 <span class="math inline">\(D^{(k)}\)</span> 表示来自第 <span class="math inline">\(i\)</span> 层鉴别器 <span class="math inline">\(D^{(k)}\)</span>​ 的 <span class="math inline">\(N_i\)</span>​ 单元的特征图输出，<span class="math inline">\(∥.∥_1\)</span>表示L1范数，<span class="math inline">\(L\)</span>​表示层数。 差异是通过鉴别器的特征图来衡量的。 为了进一步提高语音的保真度，我们添加了以下频谱损失</p>
<div class="figure">
<img src="/images/nvc-net-al3.png" alt="nvc-net-al3" />
<p class="caption">nvc-net-al3</p>
</div>
<p>其中 <span class="math inline">\(θ(.,w)\)</span>​ 计算梅尔谱图的对数幅度，FFT 大小为 <span class="math inline">\(w\)</span>​​，<span class="math inline">\(∥.∥_2\)</span> 表示 L2 范数。 在频谱域上测量生成的信号和Ground-truth信号之间的差异具有增加相位不变性的优点。 继恩格尔等人之后使用不同的 FFT 大小 <span class="math inline">\(w ∈ W = {2048, 1024, 512}\)</span> 在不同的时空分辨率下计算频谱损失。 最后，总重建损失是所有谱损失和特征匹配损失的总和，即</p>
<div class="figure">
<img src="/images/nvc-net-al4.png" alt="nvc-net-al4" />
<p class="caption">nvc-net-al4</p>
</div>
<p>其中 <span class="math inline">\(β ≥ 0\)</span> 是一个加权项。 在我们的实验中，我们简单地设置 <span class="math inline">\(β = 1\)</span>。</p>
<p><strong>内容保存损失</strong> 为了鼓励转换后的话语保留其输入音频的说话人不变特征，我们最小化以下内容保留损失</p>
<div class="figure">
<img src="/images/nvc-net-al5.png" alt="nvc-net-al5" />
<p class="caption">nvc-net-al5</p>
</div>
<p>我们鼓励内容编码器 <span class="math inline">\(E_c\)</span> 保留输入 <span class="math inline">\(x\)</span> 的基本特征，同时在转换期间更改其说话人身份 􏰈<span class="math inline">\(z\)</span>。添加此损失有两个潜在的好处。首先，它允许循环转换，因为如果我们转换，例如，从说话者 A 到说话者 B 的话语，然后将其从 B 转换回 A，我们应该获得原始话语，前提是重构也被最小化。其次，最小化方程 (3) 还导致从语音内容中分离出说话者身份。可以看出，如果来自不同说话人的话语的内容嵌入相同，则说话人信息无法嵌入到内容嵌入中。与之前的作品不同，我们不对内容嵌入执行任何域分类损失，从而使训练过程更简单。有趣的是，我们观察到方程中 l2-norm 的数值。 (3) 会受到 <span class="math inline">\(E_c\)</span>​ 输出规模的影响。通过简单地缩小任何 <span class="math inline">\(E_c(x)\)</span>，将减少内容保存损失。因此，内容嵌入的量级将相对较小，使得两个内容嵌入之间的距离变得毫无意义。为了避免这种情况，我们将内容嵌入正则化为在空间维度上有一个单位 l2-范数，即 <span class="math inline">\(c_{ij} ← c_{ij}/(􏰅\sum_k c^2_{kj})^{1/2}\)</span>。在附录 A.4.2 中，我们对这种归一化进行了实证分析。</p>
<p><strong>Kullback-Leibler 损失</strong> 为了从说话人潜在空间执行随机采样，我们惩罚说话人输出分布与先验零均值单位方差高斯分布的偏差，即，</p>
<div class="figure">
<img src="/images/nvc-net-al6.png" alt="nvc-net-al6" />
<p class="caption">nvc-net-al6</p>
</div>
<p>其中 <span class="math inline">\(D_{KL}\)</span> 表示 Kullback-Leibler (KL) 散度，<span class="math inline">\(p(z|x)\)</span> 表示 <span class="math inline">\(E_s(x)\)</span>​ 的输出分布。 约束说话人潜在空间提供了两种简单的方法来在推理时对说话人嵌入进行采样：（i）来自先验分布 <span class="math inline">\(N (z|0, I)\)</span> 的样本或（ii）来自 <span class="math inline">\(p(z|x)\)</span> 的样本，用于参考 <span class="math inline">\(x\)</span>。 一方面，该term强制说话者嵌入平滑且不那么分散，从而对看不见的样本进行泛化。 另一方面，我们隐式地最大化对数似然 <span class="math inline">\(log p(x)\)</span> 的下界近似值（参见附录 A.1 中的推导）。</p>
<p><strong>最后损失</strong> 从方程 (1) 到 (4)，编码器、生成器和鉴别器的最终损失函数可以总结如下</p>
<div class="figure">
<img src="/images/nvc-net-al7.png" alt="nvc-net-al7" />
<p class="caption">nvc-net-al7</p>
</div>
<p>其中 <span class="math inline">\(λ_{rec} ≥ 0\)</span>​、<span class="math inline">\(λ_{con} ≥ 0\)</span> 和 <span class="math inline">\(λ_{kl} ≥ 0\)</span> 控制客观项的权重。 对于我们的实验，这些超参数设置为 <span class="math inline">\(λ_{rec} = 10\)</span>、<span class="math inline">\(λ_{con} = 10\)</span>​​ 和 <span class="math inline">\(λ_{kl} = 0.02\)</span>。</p>
<h3 id="model-architectures">3.2 Model architectures</h3>
<p>我们描述了 NVC-Net 的主要组件。 NVC-Net 的一些结构继承自 MelGAN 和 WaveNet。有关网络配置的更多详细信息，请参见附录 A.2。</p>
<p><strong>内容编码器</strong> 内容编码器是一个全卷积神经网络，可以应用于任何输入序列长度。它将原始音频波形映射到编码的内容表示。内容嵌入的时间分辨率比其输入低 256 倍。该网络由 4 个下采样块组成，后跟两个带有 GELU 激活的卷积层。下采样块由 4 个残差块的堆栈组成，后跟一个跨步卷积。残差块包含具有门控 tanh 非线性和残差连接的扩张卷积。这类似于 WaveNet，但这里我们只是使用反射填充reflection padding而不应用因果卷积。对于内容编码器，没有条件输入被馈送到残差块。通过增加每个残差块中的膨胀，我们的目标是捕捉音频信号的长期时间依赖性，因为感受野随着块的数量呈指数增长。</p>
<p><strong>说话人编码器</strong> 说话者编码器从话语中产生编码的说话者表示。我们假设 <span class="math inline">\(p(z|x)\)</span>​ 是一个条件独立的高斯分布。网络输出均值向量 <span class="math inline">\(μ\)</span> 和对角协方差 <span class="math inline">\(σ^2I\)</span>​，其中 <span class="math inline">\(I\)</span>​ 是单位矩阵。因此，说话人嵌入是通过从输出分布中采样得到的，即 <span class="math inline">\(z ∼ N (μ, σ^2I)\)</span>。尽管采样操作是不可微的，但可以使用重新参数化技巧将其重新参数化为可微操作，即 <span class="math inline">\(z = μ + σ ⊙ ε\)</span>​，其中 <span class="math inline">\(ε ∼ N (0, I)\)</span>​。我们从音频信号中提取 Melspectrograms，并将它们用作说话人编码器的输入。该网络由 5 个残差块和扩张的 1D 卷积组成。我们使用具有 0.2 负斜率的 Leaky ReLU 非线性进行激活。最后，平均池化用于去除时间维度，然后是两个密集层，输出均值和协方差。</p>
<p><strong>生成器</strong> 生成器将内容和说话人嵌入映射回原始音频。我们的网络架构继承自 MelGAN，但它的输入来自内容编码器，而不是对预先计算的梅尔谱图进行上采样。更具体地说，该网络由 4 个上采样块组成。每个上采样由转置卷积层执行，然后是带有 GELU 激活的 4 个残差块的堆栈。为了避免伪影，内核大小被选择为步长的倍数。与 MelGAN 不同，我们的模型在残差块中使用了门控 tanh 非线性。为了对说话人身份进行调节，我们将说话人嵌入提供给残差块，如 WaveNet 中所示（另请参见图 1）。说话人嵌入首先通过一个 1 × 1 的卷积层来减少维数以匹配膨胀层中使用的特征图的数量。然后，它的输出在时间维度上broadcast。最后，我们通过在末尾应用带有 tanh 激活的卷积层来生成输出音频波形。</p>
<p><strong>鉴别器</strong> 鉴别器架构类似于 MelGAN 中使用的架构。特别是，三个具有相同网络架构的鉴别器应用于不同的音频分辨率，即输入的下采样版本，分别具有 1、2 和 4 的不同尺度。我们使用内核大小为 4 的跨步平均池化来对音频比例进行下采样。与 MelGAN 不同的是，我们的鉴别器是一个多任务鉴别器，它包含多个线性输出分支。输出分支的数量对应于说话人的数量。每个分支都是一个二元分类任务，确定输入是源说话者的真实样本还是来自生成器的转换输出。我们还利用 PatchGAN 的思想，而不是对整个音频序列进行分类，它使用基于窗口的鉴别器网络来分类本地音频块是真实还是虚假。</p>
<h3 id="数据增强">3.3 数据增强</h3>
<p>众所周知，用有限的数据训练 GAN 可能会导致过拟合。为了学习语义信息而不是记住输入信号，我们应用了一些数据增强策略。将信号的相位移动 180 度时，人类的听觉感知不会受到影响，因此，我们可以通过乘以 -1 来翻转输入的符号以获得不同的输入。类似于 Serra 等人，还执行了随机幅度缩放。特别是，输入的幅度由一个随机因子缩放，在 [0.25, 1] 的范围内均匀绘制。为了减少伪影，我们在计算等式中的重建损失时将少量时间抖动（在 [−30, 30] 范围内）应用于真实波形。 请注意，这些数据增强策略不会改变说话者身份或信号的内容信息。此外，我们引入了一种随机洗牌策略 (random shuffle strategy)，该策略应用于说话人编码器网络。首先，输入信号被分成从 0.35 到 0.45 秒的均匀随机长度的片段。然后我们打乱这些段的顺序，然后将它们连接起来形成一个新的输入，其中的语言信息以随机顺序打乱。我们观察到随机洗牌策略有助于避免内容信息泄漏到说话人嵌入中，从而实现更好的解开。</p>
<h2 id="结论">结论</h2>
<p>在本文中，我们介绍了 NVC-Net，这是一种对抗性神经网络，以端到端的方式进行语音转换训练。 NVC-Net 不是限制模型使用典型的中间表示（例如，频谱图、语言特征等），而是能够通过对抗性反馈和重建损失的组合来利用良好的内部表示。特别是，NVC-Net 旨在将说话者身份与语言内容分开。我们的实证研究证实，NVC-Net 在传统语音转换设置以及零样本语音转换设置中产生了非常有竞争力的结果。与其他语音转换方法相比，NVC-Net 在推理方面非常高效。尽管 NVC-Net 可以合成不同的样本，但它仍然缺乏对语音其他方面的明确控制，例如节奏和韵律。在未来的工作中，我们将重点解决这个问题。另一个有前途的方向是将 NVC-Net 扩展到其他语音转换任务，例如音乐和歌声。</p>
<h3 id="影响广泛">影响广泛</h3>
<p>语音转换，更一般地说，语音合成模型具有广泛的应用，对社会产生各种影响。在娱乐行业，自动内容生成可能会催生新类型的个性化游戏，并在此过程中生成场景。它还具有使配音更容易、更便宜的潜力，允许社区访问以前无法用他们的语言提供的大量内容。在医学领域，本文提出的模型可以为语言障碍者带来新型的交流辅助工具。特别是，我们设想了一种能够保留失去说话能力的人的原始声音的交流辅助工具。在具有积极社会影响的应用程序中，语音转换可用于说话人去标识化，这有助于保护隐私。另一方面，它也可以用于生成深度伪造并促进“假新闻”的传播。出于这个原因，我们认为研究界也必须对操纵图像和音频的检测进行研究。提高语音转换模型的计算效率激发了本文提出的工作。特别是，所提出的端到端模型不需要在推理时使用声码器，这通常占大部分计算的量。主观评估表明，我们的模型可以通过最先进的 AutoVC 获得有竞争力的结果（见图 2 和图 3），同时在 GPU 上的速度提高四个数量级（见表 3）。这些结果表明，语音转换推理可以以更少的硬件和更少的能耗进行大规模执行。总的来说，我们认为在开发新模型时，能源消耗和环境影响应该是一个重要的标准。虽然我们都对巨大模型的性能感到兴奋和印象深刻，但研究界还应该考虑权衡这些模型的积极社会影响与使用它们对环境的影响。我们向读者推荐 Strubell 等人的工作关于这方面的进一步讨论。最后，表 3 中的结果表明，我们还在单个 CPU 内核上进行实时推理，而无需任何特定的代码优化。这表明语音转换应用程序将能够在本地设备上执行，有助于保护用户的匿名性和隐私。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Glow-TTS A Generative Flow for Text-to-Speech via Monotonic Alignment Search</title>
    <url>/2021/08/17/glow-tts/</url>
    <content><![CDATA[<h1 id="glow-tts-通过单调对齐搜索生成文本到语音的流程">Glow-TTS 通过单调对齐搜索生成文本到语音的流程</h1>
<h2 id="摘要">摘要</h2>
<p>最近，已经提出了诸如 FastSpeech 和 ParaNet 之类的文本到语音 (TTS) 模型来并行地从文本生成梅尔谱图。尽管具有优势，但如果没有自回归 TTS 模型作为外部对齐器的指导，则无法训练并行 TTS 模型。在这项工作中，我们提出了 Glow-TTS，这是一种基于流的并行 TTS 生成模型，不需要任何外部对准器。通过结合流和动态规划的特性，所提出的模型自行搜索文本和语音的潜在表示之间最可能的单调对齐。我们证明了执行硬单调对齐可以实现稳健的 TTS，它可以推广到长话语，并且采用生成流可以实现快速、多样化和可控的语音合成。 Glow-TTS 在合成时与自回归模型 Tacotron 2 相比获得了数量级的加速，并具有可比的语音质量。我们进一步表明，我们的模型可以轻松扩展到多说话人设置。</p>
<h2 id="intruduction">Intruduction</h2>
<p>文本转语音 (TTS) 是一项从文本生成语音的任务，基于深度学习的 TTS 模型已成功生成自然语音。在神经 TTS 模型中，自回归模型，如 Tacotron 2 和 Transformer TTS，已经显示出最先进的性能。尽管自回归 TTS 模型的综合质量很高，但在实时服务中直接部署它们仍存在一些困难。由于模型的推理时间随输出长度呈线性增长，因此无需设计复杂的框架即可将生成长话语引起的不良延迟传播到 TTS 系统的多个管道。此外，大多数自回归模型在某些情况下缺乏稳健性。例如，当输入文本包含重复的单词时，自回归 TTS 模型有时会产生严重的注意力错误。</p>
<p>为了克服自回归 TTS 模型的这种局限性，已经提出了并行 TTS 模型，例如 FastSpeech。这些模型可以比自回归模型更快地合成梅尔谱图。除了快速采样之外，FastSpeech 还通过将其对齐限制为单调来减少合成失败的情况，例如发音错误、跳过或重复单词。然而，为了训练并行 TTS 模型，文本和语音之间对齐良好的注意力图是必要的。最近提出的并行模型从它们的外部对齐器中提取注意力图，预训练的自回归 TTS 模型。因此，模型的性能关键取决于外部矫正器的性能。</p>
<p>在这项工作中，我们消除了任何外部对准器的必要性，并简化了并行 TTS 模型的训练过程。在这里，我们提出了 Glow-TTS，这是一种基于流的并行 TTS 生成模型，可以在内部学习自己的对齐方式。 通过结合流和动态规划的特性，Glow-TTS 有效地搜索文本和语音潜在表示之间最可能的单调对齐。所提出的模型被直接训练以最大化语音与对齐的对数似然。我们证明了强制执行硬单调对齐可以实现稳健的 TTS，它可以推广到长话语，并且使用流可以实现快速、多样化和可控的语音合成。</p>
<p>Glow-TTS 生成梅尔谱图的速度比自回归 TTS 模型 Tacotron 2 快 15.7 倍，同时获得可比的性能。至于鲁棒性，当输入的话语很长时，所提出的模型明显优于 Tacotron 2。通过改变语音的潜在表征，我们可以合成具有各种语调模式的语音并调节语音的音调。我们进一步表明，我们的模型只需稍作修改即可扩展到多说话人设置。我们的源代码和合成音频样本是公开可用的。</p>
<h2 id="相关工作">相关工作</h2>
<p><strong>文本和语音之间的对齐估计 </strong>传统上，隐马尔可夫模型 (HMM) 已被用于估计文本和语音之间的未知对齐。在语音识别中，CTC 已被提议作为一种减轻 HMM 缺点的方法，例如通过一个判别神经网络模型后，假设样本点之间的条件独立性。上述两种方法都可以通过具有动态规划的前向后向算法有效地估计对齐。在这项工作中，我们引入了一种类似的动态规划方法来搜索文本和语音的潜在表示之间最可能的对齐方式，我们的建模与 CTC 的不同之处在于它是生成式的，而与 HMMs 的不同之处在于它可以并行采样序列，而没有假设样本点之间的条件独立性。</p>
<p><strong>文本到语音模型</strong> TTS 模型是一系列从文本合成语音的生成模型。 TTS 模型，例如 Tacotron 2、Deep Voice 3 和 Transformer TTS，从文本生成梅尔谱图，可与人类语音相媲美。近期也有些研究致力于增强 TTS 模型的表达能力。辅助嵌入方法被提出，通过控制语调和节奏等因素来生成不同的语音，并且一些工作旨在在各种说话者的声音中合成语音。最近，一些工作提出了并行生成梅尔谱图帧的方法。 FastSpeech 和 ParaNet 显着加快了自回归 TTS 模型的梅尔谱图生成，同时保持合成语音的质量。然而，两种并行 TTS 模型都需要从预训练的自回归 TTS 模型中提取对齐，以缓解文本和语音之间的长度不匹配问题。我们的 Glow-TTS 是一个独立的并行 TTS 模型，它通过利用流和动态规划的特性在内部学习对齐文本和语音。</p>
<p><strong>基于流的生成模型</strong> 基于流的生成模型因其优点而受到了很多关注。他们可以通过应用可逆变换来估计数据的确切可能性。生成流被简单地训练以最大化可能性。除了有效的密度估计之外，提出的转换保证了快速有效的采样。普雷格等人 [18] 和 Kim 等人 [10] 为语音合成引入了这些转换，以克服自回归声码器 WaveNet [29] 采样速度慢的问题。他们提出的模型都比 WaveNet 更快地合成原始音频。通过应用这些转换，Glow-TTS 可以并行合成给定文本的梅尔谱图。</p>
<p>并行于我们的工作，已经提出了 AlignTTS [34]、Flowtron [28] 和 Flow-TTS [15]。 AlignTTS 和 Flow-TTS 是不需要外部对齐器的并行 TTS 模型，Flowtron 是基于流的模型，展示了风格转移和语音变化可控性的能力。 但是，AlignTTS 不是基于流的模型而是前馈网络，Flowtron 和 Flow-TTS 使用软注意力模块。 通过同时采用硬单调对齐和生成流，我们的模型在稳健性、多样性和可控性方面结合了两全其美。</p>
<h2 id="glow-tts">Glow-TTS</h2>
<div class="figure">
<img src="/images/glowtts-fig1.png" alt="glowtts-fig1" />
<p class="caption">glowtts-fig1</p>
</div>
<p>受人类按顺序读出文本而不跳过任何单词这一事实的启发，我们设计了 Glow-TTS 以生成基于文本和语音表示之间单调和非跳过对齐的梅尔谱图。 在第 3.1 节中，我们制定了所提出模型的训练和推理过程，如图 1 所示。我们在第 3.2 节中介绍了我们的对齐搜索算法，该算法从训练中消除了外部对齐器的必要性，以及所有组件的架构 Glow-TTS（即文本编码器、持续时间预测器和基于流的解码器）在第 3.3 节中介绍。</p>
<h3 id="训练和推理过程">3.1 训练和推理过程</h3>
<p>Glow-TTS 通过基于流的解码器 <span class="math inline">\(f_{dec}: z \rightarrow x\)</span>​​ 转换条件先验分布 <span class="math inline">\(P_Z (z|c)\)</span>​​​ 来模拟梅尔谱图 <span class="math inline">\(P_X(x|c)\)</span>​ 的条件分布：<span class="math inline">\(z → x\)</span>​，其中 <span class="math inline">\(x\)</span>​ 和 <span class="math inline">\(c\)</span>​​ 分别表示输入梅尔谱图和文本序列。 通过使用变量的变化，我们可以计算数据的准确对数似然如下：</p>
<div class="figure">
<img src="/images/glowtts-al1.png" alt="glowtts-al1" />
<p class="caption">glowtts-al1</p>
</div>
<p>我们使用网络参数 <span class="math inline">\(θ\)</span> 和对齐函数 <span class="math inline">\(A\)</span> 参数化数据和先验分布。先验分布 <span class="math inline">\(P_Z\)</span> 是各向同性多元高斯分布，先验分布 <span class="math inline">\(μ\)</span> 和 <span class="math inline">\(σ\)</span> 的所有统计数据都由文本编码器 <span class="math inline">\(f_{enc}\)</span> 获得。 文本编码器将文本条件 <span class="math inline">\(c = c_{1:T_{text}}\)</span> 映射到统计信息 <span class="math inline">\(μ = μ_{1:T_{text}}\)</span> 和 <span class="math inline">\(σ = σ_{1:T_{text}}\)</span> 中，其中 <span class="math inline">\(T_{text}\)</span> 表示文本输入的长度。 在我们的公式中，对齐函数 <span class="math inline">\(A\)</span> 代表从语音的潜在表示的索引到来自 <span class="math inline">\(f_{enc}\)</span> 的统计的索引的映射：<span class="math inline">\(A(j) = i \text{ if } z_j ∼ N(z_j;μ_i,σ_i)\)</span>​. 我们假设对齐函数是单调和投影，以确保 Glow-TTS 不会跳过或重复文本输入。 那么，先验分布可以表示如下：</p>
<div class="figure">
<img src="/images/glowtts-al2.png" alt="glowtts-al2" />
<p class="caption">glowtts-al2</p>
</div>
<p>其中 <span class="math inline">\(T_{mel}\)</span> 表示输入梅尔谱图的长度。</p>
<p>我们的目标是找到使数据的对数似然最大化的参数 <span class="math inline">\(θ\)</span> 和对齐 <span class="math inline">\(A\)</span>，如公式 3 中所示。 然而，在计算上很难找到全局解。为了解决棘手问题，我们通过将目标分解为两个后续问题来减少参数和对齐的搜索空间：（i）针对当前参数 <span class="math inline">\(θ\)</span> 搜索最可能的单调对齐 <span class="math inline">\(A^*\)</span>​，如等式 4 所示，以及(ii) 更新参数 <span class="math inline">\(θ\)</span> 以最大化对数似然 <span class="math inline">\(\text{log } p_X (x|c; θ, A^∗)\)</span>​​。在实践中，我们使用迭代方法处理这两个问题。在每个训练步骤，我们首先找到 <span class="math inline">\(A^∗\)</span>，然后使用梯度下降更新 <span class="math inline">\(θ\)</span>​。迭代过程实际上是广泛使用的维特比训练的一个例子，它最大化最可能隐藏对齐的对数似然。修改后的目标不保证方程 3 的全局解，但它仍然提供了一个很好的全局解的下界。</p>
<div class="figure">
<img src="/images/glowtts-al34.png" alt="glowtts-al34" />
<p class="caption">glowtts-al34</p>
</div>
<p>为了解决对齐搜索问题 (i)，我们引入了一种对齐搜索算法，即单调对齐搜索 (MAS)，我们在 3.2 节中对其进行了描述。</p>
<p>为了在推理时估计最可能的单调对齐 <span class="math inline">\(A^*\)</span>，我们还训练持续时间预测器 <span class="math inline">\(f_{dur}\)</span> 以匹配从对齐 <span class="math inline">\(A^*\)</span>​计算出的持续时间标签，如公式 5 所示。遵循 FastSpeech 的架构，我们附加了持续时间预测器 在文本编码器之上，并在对数域中使用均方误差损失 (MSE) 对其进行训练。 我们还将停止梯度算子 <span class="math inline">\(sg[·]\)</span>，它去除了反向传播中输入的梯度，用于持续时间预测器的输入，以避免影响最大似然目标。 持续时间预测器的损失在公式 6 中描述。</p>
<div class="figure">
<img src="/images/glowtts-al56.png" alt="glowtts-al56" />
<p class="caption">glowtts-al56</p>
</div>
<p>在推理过程中，如图 1b 所示，先验分布和对齐的统计数据由文本编码器和持续时间预测器预测。 然后，从先验分布中采样潜在变量，并通过基于流的解码器转换潜在变量来并行合成梅尔谱图。</p>
<h3 id="单调对齐搜索">3.2 单调对齐搜索</h3>
<div class="figure">
<img src="/images/glowtts-fig2.png" alt="glowtts-fig2" />
<p class="caption">glowtts-fig2</p>
</div>
<p>如第 3.1 节所述，<strong>MAS 搜索潜在变量和先验分布的统计数据之间最可能的单调对齐</strong>，这些统计数据分别来自输入语音和文本。 图 2a 显示了可能的单调对齐的一个示例。</p>
<p>我们在算法 1 中展示了我们的对齐搜索算法。我们首先推导出部分对齐的递归解决方案，然后找到整个对齐。</p>
<p>设 <span class="math inline">\(Q_{i,j}\)</span>​​ 是最大对数似然，其中先验分布和潜在变量的统计量分别部分地分配给第 <span class="math inline">\(i\)</span>​​ 个和第 <span class="math inline">\(j\)</span>​​ 个元素。 然后，<span class="math inline">\(Q_{i,j}\)</span>​​​ 可以用 <span class="math inline">\(Q_{i−1,j−1}\)</span>​​ 和 <span class="math inline">\(Q_{i,j−1}\)</span>​​​ 递归公式化，如公式 7 所示，因为如果部分序列的最后元素 <span class="math inline">\(z_j\)</span>​​ 和 <span class="math inline">\({μ_i, σ_i}\)</span>​​​ 对齐，则 先前的潜在变量 <span class="math inline">\(z_{j−1}\)</span>​​​ 应该与 <span class="math inline">\(\{μ_{i−1}, σ_{i−1}\}\)</span>​​ 或 <span class="math inline">\(\{μ_i, σ_i\}\)</span>​​ 对齐以满足单调性和投影。</p>
<div class="figure">
<img src="/images/glowtts-al7.png" alt="glowtts-al7" />
<p class="caption">glowtts-al7</p>
</div>
<p>该过程如图 2b 所示。 我们迭代计算 <span class="math inline">\(Q\)</span> 的所有值，直到 <span class="math inline">\(Q_{T_{text} ,T_{mel}}\)</span> 。</p>
<p>类似地，最可能的对齐 <span class="math inline">\(A^*\)</span> 可以通过确定递推关系中哪个 <span class="math inline">\(Q\)</span>​ 值更大来获得，如公式 7。因此，可以通过缓存所有 <span class="math inline">\(Q\)</span> 值使用动态规划有效地找到 <span class="math inline">\(A^*\)</span>​； <span class="math inline">\(A^∗\)</span>​ 的所有值都从对齐的末尾回溯，<span class="math inline">\(A^∗(T_{mel}) = T_{text}\)</span>​，如图 2c 所示。</p>
<p>该算法的时间复杂度为 <span class="math inline">\(O(T_{text} × T_{mel})\)</span>​。 尽管该算法难以并行化，但它可以在 CPU 上高效运行，无需 GPU 执行。 在我们的实验中，每次迭代花费的时间不到 20 毫秒，这不到总训练时间的 2%。 此外，我们在推理过程中不需要 MAS，因为持续时间预测器用于估计对齐。</p>
<div class="figure">
<img src="/images/glowtts-al.png" alt="glowtts-al" />
<p class="caption">glowtts-al</p>
</div>
<h3 id="模型架构">3.3 模型架构</h3>
<p>本节简要介绍 Glow-TTS 的每个组件，整体模型架构和模型配置见附录 A。</p>
<p><strong>解码器</strong> <strong>Glow-TTS 的核心部分是基于流的解码器。在训练期间，我们需要有效地将梅尔谱图转换为latent representation 以用于最大似然估计和我们内部对齐搜索。</strong>在推理过程中，需要将先验分布有效地转换为梅尔谱分布以进行并行解码。因此，我们的解码器由一系列可以并行执行正向和逆向变换的流组成。具体来说，我们的解码器是多个块的堆栈，每个块由一个激活归一化层、可逆 1x1 卷积层和仿射耦合层组成。我们遵循 WaveGlow 的仿射耦合层架构，除了我们不使用局部条件。为了计算效率，我们将 80 通道梅尔频谱图帧沿时间维度分成两半，并在流操作之前将它们分组为一个 160 通道特征图。我们还修改了 1x1 卷积以减少计算雅可比行列式的耗时。在每一次 1x1 卷积之前，我们将特征图沿通道维度分成 40 组，并分别对它们进行 1x1 卷积。为了允许每组中的通道混合，分别从耦合层分隔的特征图的一半和另一半中提取相同数量的通道。详细说明可在附录 A.1 中找到。</p>
<p><strong>编码器和持续时间预测器</strong> 我们遵循 Transformer TTS 的编码器结构，并稍作修改。我们移除位置编码并将相对位置表示添加到自注意力模块中。我们还向编码器预网络添加了一个残差连接。为了估计先验分布的统计数据，我们在编码器的末尾附加了一个线性投影层。持续时间预测器由两个带有 ReLU 激活、层归一化和 dropout 的卷积层组成，后跟一个投影层。持续时间预测器的架构和配置与 FastSpeech的架构和配置相同。</p>
<h2 id="结论">结论</h2>
<p>在本文中，我们提出了一种新型的并行 TTS 模型 Glow-TTS。 Glow-TTS 是一种基于流的生成模型，直接用最大似然估计训练。 由于所提出的模型自己找到了文本和语音潜在表示之间最可能的单调对齐方式，因此整个训练过程得以简化，无需外部对齐器。 除了简单的训练程序外，我们还表明 Glow-TTS 合成梅尔谱图的速度比自回归基线 Tacotron 2 快 15.7 倍，同时表现出相当的性能。 我们还展示了 Glow-TTS 的其他优势，例如控制合成语音的语速或音调、鲁棒性和多说话人设置的可扩展性的能力。 由于这些优势，我们相信所提出的模型可以应用于各种 TTS 任务，例如韵律转换或风格建模。</p>
<h2 id="更广泛的影响">更广泛的影响</h2>
<p>在本文中，研究人员介绍了 Glow-TTS，这是一种多样化、强大且快速的文本到语音 (TTS) 合成模型。包括 Glow-TTS 在内的神经 TTS 模型可应用于许多需要自然合成语音的应用。其中一些应用是人工智能语音助手服务、有声读物服务、广告、汽车导航系统和自动应答服务。因此，通过利用合成自然发声语音的模型，此类应用程序的提供者可以提高用户满意度。此外，所提出模型的快速合成速度可能有利于一些提供实时语音合成服务的服务提供商。然而，由于能够合成自然语音，TTS 模型也可能被假新闻或网络钓鱼等网络犯罪所滥用。这意味着 TTS 模型可用于模仿名人的声音来操纵人们的行为，或模仿某人的朋友或家人的声音以达到欺诈目的。随着语音合成技术的发展，从合成语音中检测真人语音的研究越来越多。神经 TTS 模型有时会合成带有泥浆或错误发音的不良语音。因此，在新闻广播等即使是单个发音错误也很严重的领域，应谨慎使用。另一个问题是关于训练数据。许多用于语音合成的语料库都包含由少数说话者发出的语音数据。如果没有对 TTS 模型的使用范围进行详细的考虑和限制，说话人的声音可能会超出他们的预期。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Hifi-gan</title>
    <url>/2021/08/17/Hifi-gan/</url>
    <content><![CDATA[<h1 id="hifi-gan-用于高效和高保真语音合成的生成对抗网络">Hifi-GAN: 用于高效和高保真语音合成的生成对抗网络</h1>
<h2 id="摘要">摘要</h2>
<p>最近关于语音合成的几项工作采用了生成对抗网络 (GAN) 来生成原始波形。虽然这些方法提高了采样效率和内存使用率，但它们的样本质量还没有达到自回归和基于流的生成模型的质量。在这项工作中，我们提出了 HiFi-GAN，它实现了高效和高保真语音合成。<strong>由于语音音频由具有不同周期的正弦信号组成，我们证明了对音频的周期性模式进行建模对于提高样本质量至关重要。</strong>单个说话者数据集的主观人类评估（平均意见得分，MOS）表明，<strong>我们提出的方法证明了与人类质量的相似性，同时在单个 V100 GPU 上生成 22.05 kHz 高保真音频的速度比实时快 167.9 倍。</strong>我们进一步展示了 HiFi-GAN 对看不见的说话者的 vocoder音频生成 和端到端语音合成的泛化能力。最后，HiFi-GAN 的小尺寸版本在 CPU 上生成样本的速度比实时快 13.4 倍，质量与自回归对应物相当。</p>
<h2 id="introduction">Introduction</h2>
<p>语音是人类最常用和最自然使用的通信接口之一。随着最近科技的发展，语音被用作亚马逊Alexa等人工智能（AI）语音助手服务的主要接口，也被广泛应用于汽车、智能家居等领域。因此，随着人们与机器对话的需求增加，正在积极研究合成人类语音等自然语音的技术。 近年来，随着神经网络的发展，语音合成技术有了飞速的发展。大多数神经语音合成模型使用两阶段管道：1）从文本中预测低分辨率中间表示，例如梅尔谱图或语言特征，以及 2) 从中间表示合成原始波形音频。第一阶段是从文本中对人类语音的低级表示进行建模，而第二阶段模型以每秒高达 24,000 个样本和高达 16 位保真度的方式合成原始波形。在这项工作中，我们专注于设计第二阶段模型，该模型可以有效地从梅尔频谱图合成高保真波形。</p>
<p>已经进行了各种工作来提高第二阶段模型的音频合成质量和效率。 WaveNet 是一种自回归 (AR) 卷积神经网络，它展示了基于神经网络的方法在质量上超越传统方法的能力。但是，由于 AR 结构，WaveNet 在每个前向操作中生成一个样本；合成高时间分辨率音频的速度非常慢。提出了基于流的生成模型来解决这个问题。由于它们能够通过并行转换相同大小的噪声序列来对原始波形进行建模，因此基于流的生成模型充分利用现代并行计算处理器来加速采样。 Parallel WaveNet 是一种逆自回归流 (IAF)，经过训练以最小化其与称为教师的预训练 WaveNet 的 Kullback-Leibler 散度。与教师模型相比，它将合成速度提高到 1,000 倍或更多，而不会降低质量。 WaveGlow 消除了对教师模型进行提炼的需要，并通过采用基于 Glow 的高效双射流通过最大似然估计简化了学习过程。与 WaveNet 相比，它还可以产生高质量的音频。然而，它的深度架构需要很多参数，超过 90 层。</p>
<p>生成对抗网络 (GAN) 是最主要的深度生成模型之一，也已应用于语音合成。库马尔等人(2019) 提出了一种多尺度架构，用于在多个原始波形尺度上运行的鉴别器。考虑到复杂的架构，MelGAN 生成器足够紧凑，可以在 CPU 上进行实时合成。山本等人 (2020) 提出了多分辨率 STFT 损失函数来改进和稳定 GAN 训练，并且比 IAF 模型 ClariNet 实现了更好的参数效率和更少的训练时间。 GAN-TTS 通过在不同窗口大小上运行的多个鉴别器，成功地根据语言特征生成了高质量的原始音频波形，而不是梅尔谱图。与 Parallel WaveNet 相比，该模型还显示出更少的 FLOP。尽管有这些优势，但 GAN 模型与 AR 或基于流的模型之间的样本质量仍然存在差距。</p>
<p>我们提出了 HiFi-GAN，它比 AR 或基于流的模型实现了更高的计算效率和样本质量。由于语音音频由具有不同周期的正弦信号组成，因此对周期模式进行建模对于生成逼真的语音音频很重要。<strong>因此，我们提出了一个由小的子鉴别器组成的鉴别器，每个子鉴别器只获得原始波形的特定周期部分。</strong>这种架构是我们模型成功合成逼真语音音频的基础。当我们为鉴别器提取音频的不同部分时，我们还设计了一个模块，该模块放置多个残差块，每个残差块并行观察不同长度的模式，并将其应用于生成器。</p>
<p>HiFi-GAN 的 MOS 分数比最好的公开可用模型 WaveNet 和 WaveGlow 更高。它在单个 V100 GPU 上以 3.7 MHz 的速度合成人类质量的语音音频。我们进一步展示了 HiFi-GAN 对看不见的说话者的梅尔谱图反演和端到端语音合成的普遍性。最后，HiFi-GAN 的小尺寸版本只需要 0.92M 参数，同时超越了最好的公开可用模型和最快版本的 HiFi-GAN 样本，CPU 实时速度比实时快 13.44 倍，单个实时比实时快 1,186 倍。 V100 GPU 具有与自回归对应物相当的质量。</p>
<p>我们的音频样本可在演示网站上获得，并且我们将实施作为可重复性和未来工作的开源提供。</p>
<h2 id="hifi-gan">Hifi-GAN</h2>
<h3 id="overview">2.1 Overview</h3>
<p>HiFi-GAN 由一个生成器和两个判别器组成：多尺度和多周期判别器。 生成器和鉴别器进行对抗训练，还有两个额外的损失以提高训练稳定性和模型性能。</p>
<h3 id="generator">2.2 Generator</h3>
<div class="figure">
<img src="/images/hifigan-fig1.png" alt="hifigan-fig1" />
<p class="caption">hifigan-fig1</p>
</div>
<p>生成器是一个完全卷积的神经网络。 它使用梅尔谱图作为输入，并通过转置卷积对其进行上采样，直到输出序列的长度与原始波形的时间分辨率相匹配。 每个转置卷积后跟一个多感受野融合 (MRF) 模块，我们将在下一段中描述。 图 1 显示了生成器的架构。 与之前的工作一样，噪声没有作为额外输入提供给生成器。</p>
<p><strong>多感受野融合</strong> 我们为我们的生成器设计了多感受野融合（MRF）模块，它并行观察各种长度的模式。 具体来说，MRF 模块返回多个残差块的输出总和。 为每个残差块选择不同的内核大小和扩张率，以形成不同的感受野模式。 MRF 模块和残差块的架构如图 1 所示。我们在生成器中留下了一些可调参数； 可以调节隐藏维度 <span class="math inline">\(h_u\)</span>、转置卷积的内核大小 <span class="math inline">\(k_u\)</span>​、内核大小 <span class="math inline">\(k_r\)</span> 和 MRF 模块的膨胀率 <span class="math inline">\(D_r\)</span>，以在合成效率和样本质量之间进行权衡，以匹配自己的要求。</p>
<h3 id="discriminator">2.3 Discriminator</h3>
<p>识别长期依赖关系是模拟真实语音音频的关键。例如，音素持续时间可能长于 100 毫秒，从而导致原始波形中超过 2,200 个相邻样本之间的高度相关性。这个问题在之前的工作中已经通过增加生成器和鉴别器的感受野得到了解决。我们专注于另一个尚未解决的关键问题；由于语音音频由不同周期的正弦信号组成，因此需要识别音频数据中的各种周期模式。</p>
<p>为此，我们提出了<strong>多周期鉴别器（MPD）</strong>，它由几个子鉴别器组成，每个子鉴别器处理输入音频的一部分周期信号。此外，为了捕获连续模式和长期依赖关系，我们使用 MelGAN 中提出的<strong>多尺度鉴别器 (MSD)</strong>，它连续评估不同级别的音频样本。我们进行了简单的实验来展示 MPD 和 MSD 捕获周期性模式的能力，结果可以在附录 B 中找到。</p>
<div class="figure">
<img src="/images/hifigan-fig2.png" alt="hifigan-fig2" />
<p class="caption">hifigan-fig2</p>
</div>
<p><strong>Multi-Period Discriminator</strong> MPD 是子鉴别器的混合体，<strong>每个子鉴别器只接受输入音频的等距样本；空间以周期 <span class="math inline">\(p\)</span> 给出</strong>。子鉴别器旨在通过查看输入音频的不同部分来捕获彼此不同的隐式结构。我们将周期设置为 <span class="math inline">\([2, 3, 5, 7, 11]\)</span> 以尽可能避免重叠。如图 2b 所示，我们首先将长度为 <span class="math inline">\(T\)</span> 的一维原始音频重塑为高度为 <span class="math inline">\(T /p\)</span> 和宽度为 <span class="math inline">\(p\)</span> 的二维数据，然后对重塑后的数据应用二维卷积。在 MPD 的每个卷积层中，我们将宽度轴上的内核大小限制为 1，以独立处理周期性样本。每个子鉴别器都是一堆带有泄漏校正线性单元 (ReLU) 激活的跨步卷积层。随后，将权重归一化应用于 MPD。通过将输入音频重塑为 2D 数据而不是采样音频的周期性信号，MPD 的梯度可以传递到输入音频的所有时间步长。</p>
<p><strong>Multi-Scale Discriminator</strong> 因为 MPD 中的每个子鉴别器只接受不相交的样本，所以我们添加了 MSD 来连续评估音频序列。 MSD 的架构源自 MelGAN 的架构（Kumar 等，2019）。 MSD 是在不同输入尺度上运行的三个子鉴别器的混合：原始音频、×2 平均合并音频和 ×4 平均合并音频，如图 2a 所示。 MSD 中的每个子鉴别器都是一堆具有泄漏 ReLU 激活的跨步和分组卷积层。 通过减少步幅和添加更多层来增加鉴别器的大小。 除了对原始音频进行操作的第一个子鉴别器之外，还应用了权重归一化。 相反，应用了频谱归一化（Miyato 等人，2018 年）并按照其报道稳定了训练。</p>
<p><strong>请注意，MPD 对原始波形的不相交样本进行操作，而 MSD 对平滑波形进行操作。</strong></p>
<p>对于之前使用 MPD 和 MSD 等多鉴别器架构的工作，Bińkowski 等人 (2019)的作品也可以参考。工作中提出的鉴别器架构与 MPD 和 MSD 的相似之处在于它是鉴别器的混合，但 MPD 和 MSD 是基于马尔可夫窗口的完全无条件鉴别器，而它平均输出并具有条件鉴别器。此外，MPD 和 RWD 之间的相似性（Bin ́kowski et al., 2019）可以在重塑输入音频的部分考虑，但 MPD 使用设置为质数的周期来区分尽可能多周期的数据，而 RWD 使用重塑重叠周期的因素，并且不单独处理重构数据的每个通道，这与MPD的目标不同。 RWD 的一种变体可以执行与 MPD 类似的操作，但它在参数共享和对相邻信号的跨步卷积方面也与 MPD 不同。有关架构差异的更多详细信息，请参见附录 C。</p>
<h3 id="训练损失函数">2.4 训练损失函数</h3>
<p><strong>GAN 损失</strong> 为简洁起见，我们将我们的判别器 MSD 和 MPD 描述为贯穿第 2.4 节中的一个判别器。 对于生成器和鉴别器，训练目标遵循 LS-GAN，用最小二乘损失函数替换原始 GAN 目标的二元交叉熵项 对于非零梯度流。 训练判别器将真实样本分类为 1，将生成器合成的样本分类为 0。训练生成器通过将要分类的样本质量更新为几乎等于 1 的值来伪造判别器。 生成器 G 和鉴别器 D 的损失定义为</p>
<div class="figure">
<img src="/images/hifigan-al12.png" alt="hifigan-al12" />
<p class="caption">hifigan-al12</p>
</div>
<p>，其中 <span class="math inline">\(x\)</span> 表示真实音频，<span class="math inline">\(s\)</span>​ 表示输入条件，真实音频的梅尔谱图。</p>
<p><strong>Mel-Spectrogram Loss</strong> 除了 GAN 目标函数之外，我们添加了 mel-spectrogram loss 以提高生成器的训练效率和生成的音频的保真度。 参考之前的工作，将重建损失应用于 GAN 模型有助于产生真实的结果，在 Yamamoto 等人 (2020) 的工作，通过联合优化多分辨率频谱图和对抗性损失函数，有效地捕获了时频分布。 我们根据输入条件使用了mel-spectrogram loss，由于人类听觉系统的特性，也可以预期它具有更加专注于提高感知质量的效果。 <strong>梅尔谱图损失是由生成器合成的波形的梅尔谱图与真实波形的梅尔谱图之间的 L1 距离。</strong> 它被定义为</p>
<div class="figure">
<img src="/images/hifigan-al3.png" alt="hifigan-al3" />
<p class="caption">hifigan-al3</p>
</div>
<p>，其中 <span class="math inline">\(φ\)</span> 是将波形转换为相应梅尔谱图的函数。 mel-spectrogram loss 帮助生成器合成与输入条件对应的真实波形，并从早期阶段稳定对抗训练过程。</p>
<p><strong>特征匹配损失</strong> 特征匹配损失是一种学习的相似性度量，通过真实样本和生成样本之间鉴别器的特征差异来衡量。 由于它已成功用于语音合成，我们将其用作训练生成器的额外损失。 提取判别器的每一个中间特征，计算每个特征空间中一个ground truth样本和一个条件生成的样本之间的L1距离。 特征匹配损失定义为</p>
<div class="figure">
<img src="/images/hifigan-al4.png" alt="hifigan-al4" />
<p class="caption">hifigan-al4</p>
</div>
<p>，其中 <span class="math inline">\(T\)</span>​ 表示鉴别器的层数； <span class="math inline">\(D_i\)</span>​ 和 <span class="math inline">\(N_i\)</span> 分别表示鉴别器第 <span class="math inline">\(i\)</span> 层的特征和特征数量。</p>
<p><strong>最终损失 </strong> 我们对生成器和鉴别器的最终目标是</p>
<div class="figure">
<img src="/images/hifigan-al56.png" alt="hifigan-al56" />
<p class="caption">hifigan-al56</p>
</div>
<p>，其中我们设置 <span class="math inline">\(λ_{fm} = 2\)</span> 和 <span class="math inline">\(λ_{mel} = 45\)</span>。因为我们的判别器是一组 MPD 和 MSD 的子判别器，等式 5 和 6 可以相对于子判别器转换为</p>
<p><img src="/images/hifigan-al78.png" alt="hifigan-al78" />，其中 <span class="math inline">\(D_k\)</span> 表示 MPD 和 MSD 中的第 <span class="math inline">\(k\)</span> 个子鉴别器。</p>
<h2 id="结论">结论</h2>
<p>在这项工作中，我们引入了 HiFi-GAN，它可以有效地合成高质量的语音音频。最重要的是，我们提出的模型在合成质量方面优于性能最佳的公开可用模型，甚至可与人类水平相媲美。此外，它在合成速度方面显示出显着提高。我们从由不同周期的模式组成的语音音频的特征中汲取灵感并将其应用于神经网络，并通过消融研究验证了所提出的鉴别器的存在对语音合成的质量有很大影响。此外，这项工作提出了几个在语音合成应用中具有重要意义的实验。 HiFi-GAN 展示了从端到端设置中的嘈杂输入中概括看不见的扬声器并合成与人类质量相当的语音音频的能力。此外，我们的小尺寸模型展示了与最佳公开可用的自回归对应物相当的样本质量，同时在 CPU 上以比实时更快的数量级生成样本。这显示了设备上自然语音合成的进展，这需要低延迟和内存占用。最后，我们的实验表明，可以使用相同的判别器和学习机制训练各种配置的生成器，这表明可以根据目标规范灵活选择生成器配置，而无需耗时的超参数搜索鉴别器。</p>
<p>我们将 HiFi-GAN 作为开源发布。我们设想我们的工作将作为未来语音合成研究的基础。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Blow a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion</title>
    <url>/2021/08/18/blow/</url>
    <content><![CDATA[<h1 id="blow-用于非并行原始音频语音转换的单尺度超条件流">Blow 用于非并行原始音频语音转换的单尺度超条件流</h1>
<h2 id="摘要">摘要</h2>
<p>原始音频生成的端到端模型是一个挑战，特别是如果它们必须处理非并行数据，这在许多情况下都是理想的设置。 语音转换是其中一种情况，模型必须在录音中模拟说话者。 在本文中，我们提出了 Blow，一种使用超网络条件 (hypernetwork conditioning) 在原始音频之间执行多对多语音转换的单尺度归一化流。 Blow 是使用单个说话者标识符逐帧使用非并行数据进行端到端训练的。 我们表明 Blow 与现有的基于流的架构和其他竞争基线相比具有优势，在客观和主观评估中获得相同或更好的性能。 我们通过消融研究进一步评估其主要组成部分的影响，并量化许多属性，例如必要的训练数据量或对源或目标说话者的偏好。</p>
<h2 id="introduction">Introduction</h2>
<p>原始音频波形的端到端生成仍然是当前神经系统的挑战。处理原始音频比处理中间表示要求更高，因为它需要更高的模型容量和通常更大的感受野。事实上，即使在 16 kHz 的采样率下，生成高级波形结构也一直被认为是棘手的，并且只是在自回归模型、生成对抗网络以及最近的，标准化流（normalizing flow）。尽管如此，没有长期上下文信息的生成仍然会导致次优结果，因为现有架构难以捕获此类信息，即使它们采用理论上足够大的感受野。</p>
<p>语音转换是一项任务，即在保留口语内容的同时，将源说话者身份替换为有针对性的不同身份。它有多种应用，主要应用在医疗、娱乐和教育领域。语音转换系统通常是一对一或多对一的，从某种意义上说，它们只能从单个或最多少数几个源说话人转换为唯一的目标说话人。虽然这对于某些情况可能已经足够了，但它限制了它们的适用性，同时也阻止了它们从多个目标中学习。此外，语音转换系统通常以严格监督的方式使用并行数据进行训练。为此，需要输入/输出录音对，相应的源/目标说话人以相对准确的时间对齐方式发音相同的基本内容。收集此类数据是不可扩展的，并且在最好的情况下会出现问题。因此，研究人员正在转向使用非平行数据。然而，非并行语音转换仍然是一个悬而未决的问题，其结果与使用并行数据的结果相差甚远。</p>
<p>在这项工作中，我们探索使用标准化流（normalizing flow）进行非并行、多对多、原始音频语音转换。我们提出了 Blow，这是一种规范化流（normalizing flow）架构，可以学习在最少的监督下端到端地转换录音。它仅使用单独的音频帧，以及在此类帧中表示说话者身份的标识符或标签。 Blow 继承了 Glow 的一些结构，但引入了一些改进，除了产生更好的可能性之外，还证明对有效的语音转换至关重要。改进包括使用单尺度结构、每个块中少量的流、前向-后向转换机制、基于超网络的条件模块、共享说话人嵌入以及许多原始音频的数据增强策略。我们客观和主观地量化 Blow 的有效性，获得与许多基线相当甚至更好的性能。我们还进行了一项消融研究，以量化每个新组件的相对重要性，并评估其他方面，例如对源/目标说话者的偏好或目标分数与训练音频量之间的关系。</p>
<p>## 相关工作</p>
<p>据我们所知，没有使用标准化流进行语音转换的已发表作品，一般只有三个使用标准化流进行音频转换。Waveglow和FloWavenet 同时提出使用归一化流作为从梅尔谱图到原始音频的解码器。 他们的模型基于 Glow，但在仿射耦合网络中具有 WaveNet 结构。 AdaFlow采用标准化流进行音频异常检测和跨域图像转换。 他们建议使用与类别相关的统计数据来自适应地标准化流激活，就像 AdaBN 对常规网络所做的那样。</p>
<h3 id="归一化流">2.1 归一化流</h3>
<p>基于 Barlow 的冗余减少原理，一些学者已经使用了可逆体积保留神经架构。最近，Dinh 等人提出通过图像生成的最大似然执行阶乘学习，仍然使用体积保留变换。 Rezende 和 Mohamed 以及 Dinh 等人介绍了非体积保持变换的使用，前者采用标准化流的术语以及仿射和径向变换的使用。 Kingma 和 Dhariwal 提出了一种利用 1×1 可逆卷积的有效图像生成和处理架构。尽管与生成对抗网络、自回归模型或变分自编码器相比很少受到关注，但基于流的模型具有许多使它们特别有吸引力的优点，包括精确推理和似然评估、高效合成、有用的潜在空间，以及一些节省梯度内存的潜力。</p>
<h3 id="非并行语音转换">2.2 非并行语音转换</h3>
<p>非并行语音转换在使用经典机器学习技术的方法方面有着悠久的传统。然而，今天，神经网络在该领域占据主导地位。一些方法利用自动语音识别或文本表示来从声学中分离内容。这很容易去除源说话者的特征，但进一步挑战了生成器，它需要额外的上下文来正确定义目标语音。许多方法采用声码器来获得中间表示并作为生成模块。这些通常使用变分自编码器、生成对抗网络或两者在中间表示之间进行转换。最后，有一些作品在原始音频上采用了全神经架构。在这种情况下，架构的某些部分可能是经过预训练的，也可能不是端到端学习的。除了语音转换，还有一些处理非并行音乐或音频转换的作品：Engel 等提出了一种用于音符合成和乐器音色转换的 WaveNet 自动编码器；莫尔等人在一般音乐翻译中加入了领域混淆损失，而 Nachmani 和 Wolf 在歌声转换中加入了身份不可知损失；哈克等人使用序列到序列模型进行音频风格转换。</p>
<h2 id="flow-based-生成模型">Flow-based 生成模型</h2>
<p>基于流的生成模型学习从输入样本 <span class="math inline">\(x ∈ X\)</span>​ 到潜在表示 <span class="math inline">\(z ∈ Z\)</span>​ 的双射映射，使得 <span class="math inline">\(z = f(x)\)</span>​ 和 <span class="math inline">\(x = f−1(z)\)</span>​。 这个映射 <span class="math inline">\(f\)</span>​，通常称为归一化流，是一个由神经网络参数化的函数，由 <span class="math inline">\(k\)</span> 个可逆变换 <span class="math inline">\(f = f_1 ◦ · · · ◦ f_k\)</span> 组成。 因此，具有相同维度的 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(z\)</span>​ 之间的关系可以表示为</p>
<div class="figure">
<img src="/images/blow-al1.png" alt="blow-al1" />
<p class="caption">blow-al1</p>
</div>
<p>对于生成方法，我们希望对概率密度 <span class="math inline">\(p(X)\)</span> 进行建模，以便能够生成真实的样本。 这通常是难以直接处理的，但我们现在可以使用 <span class="math inline">\(f\)</span> 来模拟精确的对数似然</p>
<div class="figure">
<img src="/images/blow-al2.png" alt="blow-al2" />
<p class="caption">blow-al2</p>
</div>
<p>对于单个样本 <span class="math inline">\(x\)</span>，并使用变量的变化、反函数定理、组合性和对数属性，我们可以写出</p>
<div class="figure">
<img src="/images/blow-al3.png" alt="blow-al3" />
<p class="caption">blow-al3</p>
</div>
<p>其中<span class="math inline">\(∂f_i(h_{i-1})/∂h_{i-1}\)</span>​​ 是<span class="math inline">\(h_{i-1}\)</span>​ 处<span class="math inline">\(f_i\)</span>​ 的雅可比矩阵，对数行列式测量由<span class="math inline">\(f_i\)</span> 引起的对数密度的变化。 在实践中，人们选择带有三角雅可比矩阵的变换 <span class="math inline">\(f_i\)</span>​ 来实现行列式的快速计算并确保可逆性，尽管这些可能不像更复杂的那样具有表现力。 类似地，为 <span class="math inline">\(p(z)\)</span>​ 选择各向同性单位高斯以允许快速采样和直接操作。</p>
<p>已经为图像生成提出了 <span class="math inline">\(f\)</span> 和 <span class="math inline">\(f_i\)</span>​ 的许多结构和参数化，最流行的是 RealNVP 和 Glow 。最近，其他工作提出了在多个上下文中进行更好的密度估计和图像生成的改进。 RealNVP 使用具有批量归一化、屏蔽卷积和仿射耦合层的块结构。它结合了 2×2 挤压操作和交替的棋盘和通道掩码。 Glow 更进一步，除了用激活归一化 (ActNorm) 代替批量归一化之外，还通过可逆的 1×1 卷积引入了通道混合。其架构由 3 到 6 个块组成，由 2×2 挤压操作和 32 到 64 步流程组成，包括 ActNorm、1×1 可逆卷积和仿射耦合的序列。对于仿射耦合，使用了三个带有整流线性单元 (ReLU) 的卷积层。 Glow 和 RealNVP 都具有多尺度结构，可以分解出不同分辨率下 z 的分量，目的是定义不同粒度的中间表示级别。这也是其他图像生成流程和两个现有的音频生成流程所遵循的策略。</p>
<h2 id="blow">Blow</h2>
<p>Blow 从 Glow 继承了一些结构，但结合了我们展示的一些修改，这些修改是有效语音转换的关键。主要是使用（1）单尺度结构，（2）每个块中的flow较少，（3）前向后向转换机制，（4）超调节模块，（5）共享说话人嵌入，以及 (6) 一些原始音频的数据增强策略。我们现在提供一般结构的概述（图 1）。</p>
<div class="figure">
<img src="/images/blow-fig1.png" alt="blow-fig1" />
<p class="caption">blow-fig1</p>
</div>
<p>我们使用具有替代模式和一系列流的步骤的一维 2x 挤压操作（图 1，左）。流的步骤由作为通道混合器的线性可逆层（类似于二维情况下的 1×1 可逆卷积）、ActNorm 和具有仿射耦合的耦合网络组成（图 1，中心）。耦合网络由具有 ReLU 激活的一维卷积和超卷积形成（图 1，右）。耦合网络的最后一个卷积和超卷积的核宽度为3，而中间卷积的核宽度为1（我们使用512×512通道）。相同的说话人嵌入馈送所有耦合网络，并且独立地适用于每个超卷积。按照惯例，我们将输出 z 与单位各向同性高斯进行比较，并优化由 z 的维数归一化的对数似然 L（方程 1）。</p>
<h3 id="单尺度结构">4.1 单尺度结构</h3>
<p>除了上述处理中间级别表示的能力外，多尺度结构被认为可以促进梯度流，因此有助于训练非常深的模型，如归一化流。在这里，在初步分析中，我们观察到说话者身份特征几乎只出现在较粗略的表示水平上。此外，我们发现，通过去除多尺度结构并跨块携带相同的输入维度，不仅梯度流动没有问题，而且还获得了更好的对数似然（见下文）。</p>
<p>我们认为，梯度仍然流动而不分解块激活的事实是因为损失函数中的对数行列式项仍然在每个流程步骤中分解（附录 A）。因此，一些梯度仍然穿梭回到相应的层和下面。我们以某种方式获得了具有单尺度结构的更好对数似然的事实，因为块激活现在在后续块中进行进一步处理。然而，据我们所知，在当前图像生成流程的基于可能性的评估中似乎忽略了这一方面。</p>
<h3 id="many-blocks">4.2 Many blocks</h3>
<p>基于流的图像生成模型处理 32×32 和 256×256 像素之间的图像。对于原始音频，16 kHz 下 256 个样本的一维输入对应于 16 ms，这不足以捕获任何有趣的语音结构。<strong>音素持续时间可以在 50 到 180 毫秒之间</strong>，我们需要更多的长度来模拟一些音素转换。因此，我们需要增加模型的输入和感受野。为此，基于流的音频生成模型选择了更积极的压缩因子，以及膨胀高达 28 的 WaveNet 式耦合网络。相比之下，在 Blow 中，我们选择使用许多具有相对每个流程步骤很少。特别是，我们使用 8 个块，每个块有 12 个流（8×12 结构）。由于每个块都有 2 次挤压操作，这意味着总共挤压 28 个样本。</p>
<p>考虑内核宽度为 3 的两个卷积，一个 8×12 结构产生大约 12500 个样本的感受野，在 16 kHz 下，对应于 781 ms。但是，为了允许更大的批量大小，我们使用 4096 个样本的输入帧大小（16 kHz 时为 256 毫秒）。如果我们在单词中间切入，这至少足以容纳一个音素和一个音素转换，并且与 WaveNet 等其他成功模型的感受野相当。 Blow 在没有上下文的情况下逐帧运行；我们承认这可能不足以对依赖说话人的长距离韵律进行建模，但我们相信这足以对核心说话人身份特征进行建模。</p>
<h3 id="前向后向转换">4.3 前向后向转换</h3>
<p>在基于 Glow 的模型中执行图像处理 或类条件 的默认策略是在 <span class="math inline">\(z\)</span> 空间中工作。这具有许多有趣的特性，包括执行渐进式更改或插值的可能性，以及基于小数据的小样本学习或操作的潜力。然而，我们观察到，对于语音转换，遵循此策略的结果在很大程度上并不令人满意（附录 B）。</p>
<p>我们不使用 <span class="math inline">\(z\)</span> 来执行身份操作，而是将其视为与身份无关的表示。我们的想法是，指定 <span class="math inline">\(x\)</span> 的某些实际输入特征的任何提供条件都应该有助于将 <span class="math inline">\(x\)</span> 转换为 <span class="math inline">\(z\)</span>，特别是如果我们考虑最大似然目标。也就是说，了解输入的条件/特征应该有助于发现被所述条件/特征隐藏的进一步相似性，从而有助于学习。按照这个思路，如果在从 <span class="math inline">\(x\)</span> 到 <span class="math inline">\(z\)</span> 的流中的多个层次的条件化逐渐使我们进入一个无条件的 <span class="math inline">\(z\)</span> 空间（附录 C.3），那么，当用不同的条件从 <span class="math inline">\(z\)</span> 转换回 <span class="math inline">\(x\)</span>​ 时，那还应该逐步将这个新条件的特征印在输出 <span class="math inline">\(x\)</span>​ 上。 Blow 使用源说话者标识符 <span class="math inline">\(y_S\)</span> 将 <span class="math inline">\(x(S)\)</span> 转换为 <span class="math inline">\(z\)</span>​，使用目标说话者标识符 <span class="math inline">\(y_T\)</span>​ 将 <span class="math inline">\(z\)</span> 转换为转换后的音频帧 <span class="math inline">\(x(T)\)</span>。</p>
<h3 id="hyperconditioning">4.4 Hyperconditioning</h3>
<p>在基于流的模型中引入条件的一个简单的地方是耦合网络，因为不需要计算雅可比矩阵，也没有可逆性约束。 此外，在仿射通道耦合的情况下，耦合网络负责执行大部分转换，因此我们希望它具有强大的表示能力，可能会通过进一步的调节信息来增强。 调节耦合网络的常用方法是向其输入层添加或连接一些表示。 然而，根据我们观察到的连接往往被忽略并且加法不够强大，我们决定直接使用卷积核的权重进行调节。 也就是说，条件表示决定了卷积算子采用的权重，就像超网络所做的那样。 我们在耦合网络的第一层进行（图 1，右）。</p>
<p>使用一维卷积，并给定一个输入激活矩阵 <span class="math inline">\(H\)</span>，对于第 <span class="math inline">\(i\)</span>​ 个卷积过滤器，我们有：</p>
<div class="figure">
<img src="/images/blow-al4.png" alt="blow-al4" />
<p class="caption">blow-al4</p>
</div>
<p>其中 <span class="math inline">\(∗\)</span>​​​ 是一维卷积算子，<span class="math inline">\(W_y^{(i)}\)</span>​​ 和 <span class="math inline">\(b_y^{(i)}\)</span>​​​ 分别表示条件 <span class="math inline">\(y\)</span>​​​ 施加的第 <span class="math inline">\(i\)</span> 个内核权重和偏差。 一组 <span class="math inline">\(n\)</span>​ 个条件相关的核和偏置 <span class="math inline">\(K_y\)</span> 可以通过以下方式获得</p>
<div class="figure">
<img src="/images/blow-al5.png" alt="blow-al5" />
<p class="caption">blow-al5</p>
</div>
<p>其中 <span class="math inline">\(g\)</span> 是一个适配器网络，它将条件表示 <span class="math inline">\(e_y\)</span>​ 作为输入，而这又取决于条件标识符 <span class="math inline">\(y\)</span>（在我们的例子中是说话者身份）。 向量 <span class="math inline">\(e_y\)</span> 是一种嵌入，可以在说话者的某些预先计算的特征表示中固定或初始化，或者如果我们需要独立模型，则可以从头开始学习。 在本文中，我们选择独立版本。</p>
<h3 id="structure-wise-shared-embeddings">4.5 Structure-wise shared embeddings</h3>
<p>我们发现每个耦合网络学习一个 <span class="math inline">\(e_y\)</span> 通常会导致次优结果。 我们假设，给定大量的流程步骤（或耦合网络），独立条件表示不需要关注条件的本质（说话者身份），因此可以自由学习数字的任何组合 无论它们与条件的关系如何，都可以最小化负对数似然。 因此，为了减少模型的自由度，我们决定约束这种表示。 受到 StyleGAN 架构的松散启发，我们设置了一个单一的可学习嵌入 <span class="math inline">\(e_y\)</span>，它在流程的所有步骤中由每个耦合网络共享（图 1，左）。 这减少了参数的数量和模型的自由度，结果证明会产生更好的结果。 遵循类似的推理，我们还使用了最小可能的适配器网络 <span class="math inline">\(g\)</span>（图 1，右）：具有偏差的单个线性层，仅执行维度调整。</p>
<h3 id="数据增强">4.6 数据增强</h3>
<p>为了训练 Blow，我们丢弃无声帧（附录 B），然后用 4 种数据增强策略增强剩余的帧。 首先，我们应用时间抖动。 我们将每个帧 <span class="math inline">\(x\)</span>​ 的起始 <span class="math inline">\(j\)</span>​ 移动为<span class="math inline">\(j^′ =j+⌊U(−ξ,ξ)⌉\)</span>​​，其中 <span class="math inline">\(U\)</span>​ 是统一随机数生成器和 <span class="math inline">\(ξ\)</span>​ 为帧大小。 其次，我们使用随机预加重/去加重滤波器。 由于说话者的身份不会随着简单的过滤策略而改变，我们应用了一个系数 <span class="math inline">\(α = U (-0.25, 0.25)\)</span> 的强调滤波器。 第三，我们执行随机幅度缩放。 说话人身份也将通过缩放保留，另外我们希望模型能够处理 -1 和 1 之间的任何幅度。我们使用 <span class="math inline">\(x^′ = U (0, 1) · x/ max(|x|)\)</span> . 最后，我们随机翻转帧中的值。 听觉感知与平均压力水平有关，因此我们可以翻转 <span class="math inline">\(x\)</span>​ 的符号以获得具有相同感知质量的不同输入：<span class="math inline">\(x&#39; = sgn(U (-1, 1)) · x\)</span>​。</p>
<h3 id="实现细节">4.7 实现细节</h3>
<p>我们现在概述了与基于流的生成模型的常见实现不同的细节，并进一步让感兴趣的读者参考所提供的代码以全面了解它们。我们还想指出，我们没有对 Blow 执行任何超参数调整。</p>
<p><strong>通用</strong>——我们使用 <span class="math inline">\(10^{−4}\)</span> 的学习率和 114 的批量大小来训练 Blow。如果 10 个时期已经过去，验证集没有改进，我们将学习率退火 5 倍，并在第三个阶段停止训练发生这种情况的时候。我们使用 8×12 结构，具有 2× 交替模式挤压操作。对于耦合网络，我们将通道分成两半，并使用具有 512 个滤波器和内核宽度 3、1 和 3 的一维卷积。嵌入的维度为 128。我们在 16 kHz 下以 4096 帧大小进行训练，没有重叠，并用一个数据增强批次（批次包含来自所有说话者的帧的随机混合）初始化 ActNorm 权重。我们使用 Hann 窗口和 50% 的重叠进行合成，将 -1 和 1 之间的整个话语标准化。我们使用 PyTorch 实现 Blow。</p>
<p><strong>耦合</strong>——正如官方 Glow 代码中所做的那样（但在论文中没有提到），我们发现约束来自耦合网络的缩放因子可以提高训练的稳定性。对于具有通道级联的仿射耦合</p>
<div class="figure">
<img src="/images/blow-al6.png" alt="blow-al6" />
<p class="caption">blow-al6</p>
</div>
<p>其中 2c 是通道总数，我们使用</p>
<div class="figure">
<img src="/images/blow-al7.png" alt="blow-al7" />
<p class="caption">blow-al7</p>
</div>
<p>其中 <span class="math inline">\(σ\)</span> 对应于 sigmoid 函数，<span class="math inline">\(ε\)</span> 是一个小常数，以防止无限对数 -行列式（并在反向传递中除以 0）。</p>
<p><strong>超调节</strong>——如果我们严格遵循方程2 和 3，超调节操作可以涉及大量 GPU 内存占用（每个批次元素有 n 个不同的内核）和耗时计算（每个内核和批处理元素的双循环）。 在实践中，这可能会使像 Blow 这样的基于流的非常深的架构无法执行操作。 然而，通过限制核 <span class="math inline">\(W(i)\)</span>​ 的维数，使得每个通道都与它自己的一组 <span class="math inline">\(y\)</span> 卷积内核，我们可以实现较小的 GPU 占用空间和每个适应网络的易于处理的参数数量。 这对应于深度可分离卷积，并且可以通过分组卷积来实现，在大多数深度学习库中都可用。</p>
<h2 id="结论">结论</h2>
<p>在这项工作中，我们提出了基于流的生成模型用于原始音频合成的潜力，特别是对于非并行语音转换的挑战性任务。 我们提出了 Blow，一种单尺度超条件流，具有共享嵌入的多块结构，并以向前向后的方式执行转换。 由于 Blow 在这些方面与现有的基于流的生成模型不同，因此它能够胜过那些模型，并与现有的非并行语音转换系统竞争甚至改进。 我们还量化了所提议改进的影响，并评估了训练数据量和源/目标说话者的选择对最终结果的影响。 作为未来的工作，我们希望改进模型，看看我们是否可以处理其他任务，例如语音增强或乐器转换，可能是通过进一步增强超调节机制，或者简单地通过调整其结构或超参数。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>i-vector/d-vector/x-vector</title>
    <url>/2021/08/19/i-vector-d-vector-x-vector/</url>
    <content><![CDATA[<h1 id="声纹识别中的vectors"><a href="#声纹识别中的vectors" class="headerlink" title="声纹识别中的vectors"></a>声纹识别中的vectors</h1><p>一、i-vector</p>
<p>i-vector 模型是输出一个400维的向量<br>二、d-vector</p>
<p><img src="/images/sv-fig1.png" alt="sv-fig1"></p>
<p>DNN 会输入一个固定长度的语音，对它做 Speaker Recognition。然后我们把这个模型的最后一层隐层抽取出来，它就是这段语音的 d-vector。不用 output layer 中的最后一层输出，因为它的维度是和训练时语者数目有关的。而是它前面的那一层隐层输出。</p>
<p>在实际预测的时候，输入语音是不等长的，会把语音截成多段，然后取这几段特征的d-vector的平均值作为最后的speaker embedding。</p>
<p><img src="/images/sv-fig2.png" alt="sv-fig2"></p>
<p>三、x-vector</p>
<p>x-vector 是d-vector的升级版，它不像 d-vector 那样简单的取平均，而是把每一小段的声音信号输出的特征，算一个 mean 和 variance，然后concat起来，再放进一个DNN里去来判断是哪个说话人说的。其他的部分和d-vector一致。<br>————————————————<br>版权声明：本文为CSDN博主「weixin_39087379」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。<br>原文链接：<a href="https://blog.csdn.net/weixin_39087379/article/details/112795796">https://blog.csdn.net/weixin_39087379/article/details/112795796</a></p>
<p><img src="/images/vectors-fig3.png" alt="vectors-fig3"></p>
<p><img src="/images/vectors-fig4.png" alt="vectors-fig4"></p>
<p><strong>因子分析：</strong><br>信息冗余是高维数据分析常见的问题，使用因子分析方法，可以将一些信息重叠和复杂的关系变量简化为较少的足够描述原有观测信息的几个因子，是一种数据降维的统计方法。本文介绍JFA和I-vector都为因子分析方法。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>SYNTACTIC REPRESENTATION LEARNING FOR NEURAL NETWORK BASED TTS WITH SYNTACTIC PARSE TREE TRAVERSAL</title>
    <url>/2021/08/20/gnn-tts2/</url>
    <content><![CDATA[<h1 id="基于神经网络的-tts-与句法解析树遍历的句法表示学习">基于神经网络的 TTS 与句法解析树遍历的句法表示学习</h1>
<h2 id="摘要">摘要</h2>
<p>句子文本的句法结构与语音的韵律结构相关，这对于提高文本到语音 (TTS) 系统的韵律和自然度至关重要。如今，TTS 系统通常尝试将句法结构信息与基于专家知识的手动设计特征结合起来。在本文中，我们提出了一种基于句法解析树遍历的句法表示学习方法，以自动利用句法结构信息。两个成分标签序列通过来自成分分析树的左先和右先遍历线性化。然后通过相应的单向门控循环单元 (GRU) 网络从每个组成标签序列中在单词级别提取句法表示。同时，引入了核范数最大化损失以增强组成标签嵌入的可辨别性和多样性。上采样的句法表示和音素嵌入被连接起来作为 Tacotron2 的编码器输入。实验结果证明了我们提出的方法的有效性，与基线相比，平均意见得分 (MOS) 从 3.70 增加到 3.82，ABX 偏好超过 17%。此外，对于具有多个句法分析树的句子，从合成语音中可以清楚地感知韵律差异。</p>
<h2 id="introduction">Introduction</h2>
<p>最近，基于神经网络的文本到语音 (TTS) 系统在合成语音的韵律和自然度方面取得了一定的成功，而不是传统方法。通过应用带有注意力的编码器-解码器框架，这些系统可以通过从语言空间到声学空间的灵活映射来学习声学和韵律模式，从而直接从字素或音素预测语音参数。然而，学习到的韵律模式仅包含部分韵律结构信息，导致韵律和自然性表现不佳，甚至不正确的韵律。</p>
<p>为了进一步提高合成语音的韵律和自然度，提出了在基于神经网络的 TTS 模型的输入序列中添加韵律结构注释，如音调和中断索引 (ToBI) 标签或其他韵律结构标签。韵律结构标注需要从语音中主观标注，耗时较长。</p>
<p>虽然这些注释可以通过训练另一个韵律结构预测模型自动注释，但预测的韵律结构标签的准确性仍然受到使用主观标记的注释作为基本事实的限制。成功的句法到韵律映射证明了句法结构和韵律信息之间的高度相关性。提出了一组基于规则的句法特征，例如词性 (POS) 和当前词在父短语中的位置，并用于基于隐马尔可夫模型 (HMM) 的声学模型。为了利用更多的句法结构信息，在基于神经网络的 TTS 中提出了基于短语结构的特征（PSF）和基于词关系的特征（WRF）。 PSF 和 WRF 扩展了 HMM 模型中使用的句法特征集。进一步引入了更多特征，例如以当前词开头的最高级别短语（HBCW）和最低共同祖先（LCA），以对句法结构进行建模。</p>
<p>然而，扩展的特征仍然是手动设计的特征，而不是自动学习的高级表示。 PSF 仅包含来自整个句法树结构的有限层的特征。 WRF 仅从整个句法分析树中暴露部分节点和边的信息。</p>
<p><strong>为了更好地利用句法信息，受神经机器翻译中句法解析树遍历方法的启发</strong>，我们提出了一种句法表示学习方法，以进一步提高基于神经网络的 TTS 中合成语音的韵律和自然度。句法分析树通过左先和右先遍历线性化为两个组成标签序列。然后对每个序列使用不同的单向 GRU 网络从组成标签序列中提取句法表示。之后，句法表示从词级到音素级上采样，并与音素嵌入连接。 Tacotron 2 用于从连接的句法表示和音素嵌入生成频谱图，并使用 Griffin-Lim 来重建波形。将核范数最大化损失 (NML) 引入组成标签嵌入层以增强可辨别性和多样性。与仅使用左先遍历 相比，提出了右先遍历以减轻歧义。</p>
<p>实验结果表明，我们提出的模型在韵律和自然度方面优于基线。与基线方法相比，平均意见得分 (MOS) 从 3.70 增加到 3.82（t 检验，p=0.0079）。 ABX 偏好率超过基线方法 17%。对于具有多个不同句法解析树的句子，可以从相应的合成语音中清楚地感知韵律差异。</p>
<h2 id="方法论">方法论</h2>
<div class="figure">
<img src="/images/bi-traversal-fig1.png" alt="bi-traversal-fig1" />
<p class="caption">bi-traversal-fig1</p>
</div>
<p>图 1 显示了我们提出的方法的框架。 我们的工作主要集中在引入可训练的句法结构信息提取器作为基于神经网络的 TTS 系统的一部分，以提高合成语音的韵律和自然度。</p>
<h3 id="句法表征学习">2.1 句法表征学习</h3>
<p>为了向基于神经网络的 TTS 系统提供具有丰富句法信息的高级句法表示，我们提出了一种基于句法分析树遍历的句法表示学习网络。 提取成分分析树，包括成分的标签和树结构。</p>
<p>为了表示基于神经网络的 TTS 的树结构，可以使用深度优先遍历将句法分析树线性化为组成序列。 由于任何单树遍历算法都会将多个句法分析树映射到同一序列，因此建议使用左优先和右优先来减轻歧义。 两次遍历生成的组成标签序列可以表示为以下等式：</p>
<div class="figure">
<img src="/images/bi-traversal-al1.png" alt="bi-traversal-al1" />
<p class="caption">bi-traversal-al1</p>
</div>
<p>其中 <span class="math inline">\(C_l\)</span> 和 <span class="math inline">\(C_r\)</span> 分别是从左先遍历和右先遍历生成的组成标签序列，<span class="math inline">\(c^i_l\)</span> 和 <span class="math inline">\(c^i_r\)</span> 是组成标签，<span class="math inline">\(m\)</span> 是序列的长度。 然后，组成标签由共享嵌入层嵌入，并由两个不同的单向 GRU 网络建模，每个序列一个 GRU 网络。 该过程可以表示为：</p>
<div class="figure">
<img src="/images/bi-traversal-al2.png" alt="bi-traversal-al2" />
<p class="caption">bi-traversal-al2</p>
</div>
<p>其中<span class="math inline">\(\hat C_l\)</span>和<span class="math inline">\(\hat C_r\)</span>是组成标签的嵌入序列，<span class="math inline">\(GRU_l(·)\)</span>和<span class="math inline">\(GRU_r(·)\)</span>是两个不同的单向GRU，<span class="math inline">\(O_l\)</span>和<span class="math inline">\(O_r\)</span>分别是<span class="math inline">\(GRU_l(·)\)</span>和<span class="math inline">\(GRU_r(·)\)</span>的输出。</p>
<p>句法特征是每个单词的 GRU 输出的串联，可以表示为：</p>
<div class="figure">
<img src="/images/bi-traversal-al3.png" alt="bi-traversal-al3" />
<p class="caption">bi-traversal-al3</p>
</div>
<p>其中 <span class="math inline">\(p^i_l\)</span> 和 <span class="math inline">\(p^i_r\)</span> 是第 <span class="math inline">\(i\)</span> 个单词在 <span class="math inline">\(C_l\)</span> 和 <span class="math inline">\(C_r\)</span> 中的位置，<span class="math inline">\(w\)</span> 是输入文本的单词数，<span class="math inline">\(f_i\)</span> 是学习到的句法表示。</p>
<h3 id="核范数最小化损失">2.2 核范数最小化损失</h3>
<p>为了提高句法标签嵌入的可辨别性和多样性，提出了全局核范数最大化损失（NML）来增加所有可能的组成标签的嵌入的等级。 NML 定义为：</p>
<div class="figure">
<img src="/images/bi-traversal-al4.png" alt="bi-traversal-al4" />
<p class="caption">bi-traversal-al4</p>
</div>
<p>其中 <span class="math inline">\(C\)</span> 是所有可能的组成标签的集合，<span class="math inline">\(\hat C\)</span> 和 <span class="math inline">\(N\)</span> 分别是 <span class="math inline">\(C\)</span> 的嵌入和长度。 <span class="math inline">\(∥\hat C∥_∗\)</span> 计算如下：</p>
<div class="figure">
<img src="/images/bi-traversal-al5.png" alt="bi-traversal-al5" />
<p class="caption">bi-traversal-al5</p>
</div>
<p>其中 <span class="math inline">\(σ_i\)</span> 是 <span class="math inline">\(\hat C\)</span> 的第 <span class="math inline">\(i\)</span> 个奇异值。</p>
<h3 id="带有句法表示的-tts">2.3 带有句法表示的 TTS</h3>
<p>学习到的句法表示与单词相关，它们被上采样到音素级别并与音素嵌入连接。 复制句法表示以匹配当前单词的音素序列长度。 Tacotron 2 用于从连接的句法表示和音素嵌入生成频谱图，并进一步利用 Griffin-Lim 来重建波形。 整个模型使用损失函数进行训练，损失函数可以表示为：</p>
<div class="figure">
<img src="/images/bi-traversal-al6.png" alt="bi-traversal-al6" />
<p class="caption">bi-traversal-al6</p>
</div>
<p>其中 <span class="math inline">\(L_{TTS}\)</span> 是 Tacotron 2 中定义的损失函数，<span class="math inline">\(λ\)</span> 是 NML 的损失权重。</p>
<h2 id="实验">实验</h2>
<p>实验部分挺有趣的，只展示两张频谱图</p>
<div class="figure">
<img src="/images/bi-traversal-fig3.png" alt="bi-traversal-fig3" />
<p class="caption">bi-traversal-fig3</p>
</div>
<div class="figure">
<img src="/images/bi-traversal-fig4.png" alt="bi-traversal-fig4" />
<p class="caption">bi-traversal-fig4</p>
</div>
<h2 id="结论">结论</h2>
<p>在这项研究中，我们研究了一种句法表示学习方法，以自动利用基于神经网络的 TTS 的句法结构信息。 引入核范数最大化损失以增强合成语音韵律的可辨别性和多样性。 实验结果证明了我们提出的方法的有效性。 对于具有多个句法分析树的句子，从合成的语音中可以清楚地观察到韵律差异。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Dependency Parsing based Semantic Representation Learning with Graph Neural Network for Enhancing Expressiveness of Text-to-Speech</title>
    <url>/2021/08/20/gnn-tts/</url>
    <content><![CDATA[<h1 id="基于依赖解析的语义表示学习与图神经网络增强文本到语音的表达能力">基于依赖解析的语义表示学习与图神经网络增强文本到语音的表达能力</h1>
<h2 id="摘要">摘要</h2>
<p>句子的语义信息对于提高文本转语音 (TTS) 系统的表达能力至关重要，但仅凭借当今的编码器结构无法从有限的训练 TTS 数据集中很好地学习。随着大规模预训练文本表示的发展，<strong>来自转换器 (BERT) 的双向编码器表示已被证明可以体现文本上下文语义信息</strong>，并作为附加输入应用于 TTS。然而，BERT 不能从句子中的依赖关系点明确关联语义标记。在本文中，为了增强表达能力，我们提出了一种基于图神经网络的语义表示学习方法，考虑了句子的依赖关系。输入文本的依赖图由考虑正向和反向的依赖树结构的边组成。然后通过关系门控图网络 (RGGN) 在词级提取语义表示，<strong>该网络以来自 BERT 的特征作为节点输入</strong>。上采样语义表示和字符级嵌入被连接起来作为 Tacotron-2 的编码器输入。实验结果表明，我们提出的方法在 LJSpeech 和 Blizzard Challenge 2013 数据集中都优于使用 vanilla BERT 特征的基线，并且从相反方向学习的语义表示对于增强表达能力更有效。</p>
<h2 id="introduction">Introduction</h2>
<p>文本转语音 (TTS) 作为人机交互框架的一个组成部分，旨在从给定的文本中生成具有丰富表现力的自然语音。韵律是决定合成语音表达能力的关键因素。传统的统计参数 TTS 系统由<strong>文本分析模块、持续时间模型、声学模型和声码器</strong>组成，其中文本分析模块通常利用专家开发的人工语言特征，如词性(POS) 标签或自然语言解析相关结构，例如解析树，以从输入文本中捕获韵律线索。最近，端到端 TTS (E2E-TTS) 系统通过编码器-解码器结构简化了合成管道，并且在合成语音的自然性和可理解性方面取得了显着的成功。</p>
<p>由于这些 E2E-TTS 技术，例如 Tacotron-2 和 Deep Voice 3，使用编码器仅从文本（字符或音素）生成语言特征，并利用基于注意力的解码器直接生成原始频谱图，因此它们不需要复杂的文本分析或持续时间建模。 E2E-TTS 系统的输入通常通过使用标记器和语法解析器的上下文无关句法信息来丰富。但是，关于语义的部分信息仍可用于这些 E2E-TTS 系统，因为仅凭借当今的编码器结构，无法从训练 TTS 数据集中很好地学习此类信息。</p>
<p>由于预训练文本表示的发展，来自转换器 (BERT) 的双向编码器表示已被证明不仅体现了无文本的句法信息，还体现了文本上下文语义信息。现有的 E2E-TTS 扩展使用 BERT 来提高韵律性能。预训练的 BERT 首先作为附加输入引入 Tacotron-2，并显示平均意见得分的收益。类似的方法进一步验证了 BERT 在中文多说话者 TTS 任务中改善韵律的能力。沿着不同的路线，CHiVE-BERT 在基于 RNN 的语音合成模型中结合了 BERT 模型。这些方法通过利用来自 BERT 的短语和单词的语义信息改进了合成语音的韵律。</p>
<p>显式地结合依赖关系等句子结构有助于有效提高合成语音的自然度和表现力。然而，作为基于transformer的模型，由于训练任务的限制，BERT并没有明确地对依赖结构进行建模。结果，话语的呈现受到不利影响，尤其是对于长句。</p>
<p>为了在句子中明确地合并不同词汇标记之间的依赖关系，GraphSpeech 将输入词汇标记的关系从依赖解析引入到图注意力机制，这被证明对表示语义关联很有用。 然而，Graph-Speech 使用基于循环神经网络 (RNN) 的关系编码器来建模依赖关系，考虑到复杂的句子结构，这不足以捕获信息。 图神经网络 (GNN) 被进一步引入 TTS，因为它能够通过图节点之间的消息传递来学习表示，如 GraphTTS 和 GraphPB ，但所有的他们使用仅从文本中设计的简单结构，而没有考虑更深层次的语义。</p>
<p>为了更好地利用依存分析的句子级语义信息，我们提出了一种基于图神经网络的语义表示学习方法，考虑了句子的依存关系。输入文本的依赖图由考虑正向和反向的依赖树结构的边组成。然后通过关系门控图网络（RGGN）在单词级别提取语义表示，其中输入来自 BERT 的单词特征作为节点输入。上采样语义表示和字符级嵌入被连接起来作为 Tacotron-2 的编码器输入，然后是多频段 MelGAN 声码器 以生成合成语音。</p>
<p>实验结果表明，我们提出的方法在 LJSpeech 和 Blizzard Challenge 2013 数据集的自然度和表现力方面都优于使用 vanilla BERT 特征的基线，并且从相反方向学习的语义表示对于增强表现力更有效。还通过案例研究探索和证明了从依赖图中学习的语义表示的有效性。</p>
<h2 id="methodology">2. Methodology</h2>
<p>为了提高合成语音的表达能力，我们提出了一种基于图神经网络的语义表示学习方法，考虑了句子的依赖关系。 所提出方法的主要框架如图 1 所示。 在构建输入文本的依赖图后，使用两个单独的关系门控图网络 (RGGN) 来学习语义表示，这些表示作为附加输入传递给 Tacotron-2。</p>
<div class="figure">
<img src="/images/gnn-tts-fig1.png" alt="gnn-tts-fig1" />
<p class="caption">gnn-tts-fig1</p>
</div>
<h3 id="依赖图构建">2.1 依赖图构建</h3>
<div class="figure">
<img src="/images/gnn-tts-fig2.png" alt="gnn-tts-fig2" />
<p class="caption">gnn-tts-fig2</p>
</div>
<p>在依存树中，使用双词化依存关系来描述词之间的关系进行语义分析，如图2所示。 我们使用图来表示语义标记的依赖关系，定义为 <span class="math inline">\(G = (V, E)\)</span>。 节点 <span class="math inline">\(v ∈ V\)</span> 表示句子中带有语义信息的标记，通常是一个词。 边是一对 <span class="math inline">\(e = (v_i,v_j) ∈ E\)</span>，它表示从节点 <span class="math inline">\(v_i\)</span> 到节点 <span class="math inline">\(v_j\)</span> 的有向边，具有特定的依赖关系。 并且不同类型的边代表不同类型的依赖关系。 一个句子可以表示为一个单词序列 <span class="math inline">\(W = [w_1 , w_2 , ..., w_n ]\)</span>，其中 <span class="math inline">\(n\)</span> 是句子的单词数。 我们从 <span class="math inline">\(W\)</span> 中提取语义表示和依赖树的结构，可以表述为：</p>
<div class="figure">
<img src="/images/gnn-tts-al1.png" alt="gnn-tts-al1" />
<p class="caption">gnn-tts-al1</p>
</div>
<p>其中<span class="math inline">\(v_i\)</span>是第<span class="math inline">\(i\)</span>个词的BERT特征，组成节点集<span class="math inline">\(V_{bert}\)</span>，<span class="math inline">\(E_{dep}\)</span>是依赖解析的关系边集，可以看作是<span class="math inline">\(W\)</span>的依赖结构。</p>
<p>如图2所示，节点“jumped”指向原始依赖树中的节点“quickly”。 但考虑到“quickly”修饰“jumped”，子节点的语义信息应该同时影响父节点。 因此，我们使用两个信息流方向来构建依赖图。 前向定义为信息从父节点流向子节点，与依赖树一致。 反向定义为信息从子节点流向父节点，反向流向依赖树。 正反方向对应的结构可以表示为：</p>
<div class="figure">
<img src="/images/gnn-tts-al2.png" alt="gnn-tts-al2" />
<p class="caption">gnn-tts-al2</p>
</div>
<p>其中 <span class="math inline">\(E_{fwd}\)</span> 是前向依赖结构，<span class="math inline">\(Direction_{rev}\)</span> 是方向反转操作，<span class="math inline">\(E_{rev}\)</span> 是 <span class="math inline">\(W\)</span> 的反向依赖结构。 因此前向依赖图和反向依赖图可以表示为：</p>
<div class="figure">
<img src="/images/gnn-tts-al3.png" alt="gnn-tts-al3" />
<p class="caption">gnn-tts-al3</p>
</div>
<p>单词的语义信息会通过 <span class="math inline">\(G_{fwd}\)</span> 和 <span class="math inline">\(G_{rev}\)</span> 的拓扑结构直接或间接流向依赖节点。 这样，一个句子的依赖图就由依赖树构造出来，进一步传递给关系门控图网络进行语义表示学习。</p>
<h3 id="关系图网络">2.2 关系图网络</h3>
<p>关系门控图网络（RGGN）旨在从第 2.1 节中的依赖图学习语义表示。 门控图神经网络（GGNN）因其信息流的长期传播而被采用。 此外，受关系图卷积网络的启发，将对应于不同类型边的不同权重矩阵引入到GGNN。</p>
<p>RGGN 通过信息传播将依赖图映射到语义表示。 首先，隐藏状态 <span class="math inline">\(h^0_i\)</span> 由节点 <span class="math inline">\(i\)</span> 的词级 BERT 特征 <span class="math inline">\(v_i ∈ V_{bert}\)</span> 发起。 然后传播步骤通过多次迭代计算每个节点的节点表示，表示为：</p>
<div class="figure">
<img src="/images/gnn-tts-al4.png" alt="gnn-tts-al4" />
<p class="caption">gnn-tts-al4</p>
</div>
<p>其中 <span class="math inline">\(N(i)\)</span> 是节点 <span class="math inline">\(i\)</span> 的邻居集，<span class="math inline">\(W_{eij}\)</span> 表示传播中从节点 <span class="math inline">\(i\)</span> 到 <span class="math inline">\(j\)</span> 的边的权重参数，<span class="math inline">\(h^t_i\)</span> 和 <span class="math inline">\(h^{t+1}_i\)</span> 表示节点 <span class="math inline">\(i\)</span> 在第 <span class="math inline">\(t\)</span> 和 <span class="math inline">\(t+1\)</span>次迭代中的隐藏状态，<span class="math inline">\(a_i^t\)</span> 是节点 <span class="math inline">\(i\)</span> t的邻居的隐藏状态 <span class="math inline">\(h^t_j\)</span> 的加权和，<span class="math inline">\(GRU(·)\)</span> 表示门控聚合器，它更新隐藏状态 <span class="math inline">\(h^t_i\)</span> 合并来自邻居和节点的信息。 节点 <span class="math inline">\(i\)</span> 的前一个迭代步骤。 在 RGGN 中，由于门控循环单元，节点可以在传播过程中保留长期信息。</p>
<p>多次迭代后图中节点的隐藏状态被视为图嵌入，通过显式连接收集相关节点的信息。 根据预先标记的节点顺序，我们将图嵌入用于与原始句子一致的词级语义表示序列。 因此，考虑到句子的依赖关系，我们可以获得每个单词的语义表示。</p>
<h3 id="带有语义表示的-tts">2.3 带有语义表示的 TTS</h3>
<p>如图 1 所示，我们设计了两个 RGGNs <span class="math inline">\(RGGN_{fwd}\)</span> 和 <span class="math inline">\(RGGN_{rev}\)</span> 分别处理前向和反向依赖图，并且它们的参数不共享。 词级语义表示可以通过以下方式获得：</p>
<div class="figure">
<img src="/images/gnn-tts-al5.png" alt="gnn-tts-al5" />
<p class="caption">gnn-tts-al5</p>
</div>
<p>其中<span class="math inline">\(r^i_{fwd}\)</span>是从前向依赖图中学习到的<span class="math inline">\(W\)</span>中第<span class="math inline">\(i\)</span>个单词的语义表示，<span class="math inline">\(r^i_{rev}\)</span>是从反向依赖图中学习到的，<span class="math inline">\(R_{fwd}\)</span>和<span class="math inline">\(R_{rev}\)</span>是两个依赖图的语义表示序列。 此外，我们结合 <span class="math inline">\(R_{fwd}\)</span>和 <span class="math inline">\(R_{rev}\)</span> 来考虑正向和反向的依赖信息：</p>
<div class="figure">
<img src="/images/gnn-tts-al6.png" alt="gnn-tts-al6" />
<p class="caption">gnn-tts-al6</p>
</div>
<p>其中 <span class="math inline">\(⊕\)</span> 是 <span class="math inline">\(r^i_{fwd}\)</span> 和 <span class="math inline">\(r^i_{rev}\)</span> 的元素相加操作，以获得第 <span class="math inline">\(i\)</span> 个单词的语义表示 <span class="math inline">\(r^i_{bi}\)</span>，<span class="math inline">\(R_{bi}\)</span> 是 <span class="math inline">\(r^i_{bi}\)</span> 的序列。</p>
<p>然后将单词级语义表示上采样到字符级并与字符嵌入连接。 也就是说，我们复制单词级别的语义表示以匹配单词的字符长度。 然后将串联传递给 Tacotron 2 的编码器，然后是基于注意力的解码器以生成梅尔谱图。 波形最终使用以梅尔频谱图为条件的多频段 MelGAN 声码器合成。</p>
<h2 id="结论">结论</h2>
<p>在这项研究中，我们提出了一种基于图神经网络的语义表示学习方法，考虑句子的依赖关系以增强表达能力。 构建依赖图时，正向和反向都考虑在内。 实验结果表明，我们提出的方法在两个数据集中都优于基线，并且从相反方向学习的语义表示对于增强表达能力更有效。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</title>
    <url>/2021/08/21/syntactic-tree-tts/</url>
    <content><![CDATA[<h1 id="利用解析树中的句法特征来改进端到端-tts">利用解析树中的句法特征来改进端到端 TTS</h1>
<h2 id="摘要">摘要</h2>
<p>端到端 TTS 可以直接从给定的字素或音素序列预测语音，其性能优于传统 TTS。 然而，它的预测能力仍然受到训练数据的声学/语音覆盖范围的限制，通常受训练集大小的限制。 为了进一步提高发音、韵律和感知自然度的 TTS 质量，我们建议利用嵌入在句法分析树中的信息，其中句子的短语/单词信息组织在多级树结构中。 具体来说，研究了两个关键特征：短语结构和相邻词之间的关系。 在三个测试集上测量的主观听力实验结果表明，所提出的方法可有效提高基线系统合成语音的发音清晰度、韵律和自然度。</p>
<h2 id="introduction">Introduction</h2>
<p>文本到语音 (TTS) 系统的评估侧重于测量可懂度、自然度、韵律和说话人相似度等几个因素。传统的基于语音参数的 TTS 系统已经实现了高可懂度，例如，基于 GMM-HMM 的和基于 NN 的统计语音合成。最近，基于 WaveNet 或 WaveRNN 的神经声码器的语音质量也有了很大的提高，它可以通过从生成的声学特征中预测语音样本来产生高质量的语音。然而，它在发音、韵律和自然度方面的预测能力仍然受到其声学/语音覆盖范围和可用于训练的数据量的限制。</p>
<p>在传统的英语 TTS 中，通常使用 ToBI 标签来转录韵律变化，包括：重音、强调、停顿等。训练数据的语音韵律可以在 ToBI 中手动或自动进行注释，以训练一个 ToBI 韵律模型来预测给定文本中的 ToBI 标签。注释基于文本和音频，但在预测中只有文本可用。需要考虑长期上下文来预测韵律的高质量 ToBI 韵律模型可能难以使用有限的文本-语音配对数据进行训练。此外，需要预测持续时间、中断、发声和 F0 轮廓的模型可以使训练更具挑战性。韵律模型的预测误差可以累积，然后降低对 TTS 频谱参数的预测，并导致合成语音中出现意外故障。此外，ToBI 标签序列不能完全表征韵律信息。</p>
<p>最近，提出了端到端的 TTS 训练，例如，char2wav、Tacotron和 Tacotron2以统一的方式直接从字素或音素预测语音参数，无需手动注释语音数据来训练模型。它可以通过最小化迭代训练循环中的预测误差，通过语言空间到声学空间之间的灵活映射来学习各种声学模式。端到端模型中的所有模块都是联合训练的，因此可以避免因训练模块分离而导致的累积误差。实验结果表明，端到端模型的性能优于传统的统计 TTS。但是，仍然会出现问题，例如错误的重音模式、不合理的中断、错误的发音，特别是对于训练数据覆盖的域之外的长而复杂的测试句子。由此产生的较差的泛化会严重降低相应的 TTS 性能。</p>
<p>端到端模型是一种高度依赖于序列信息的序列到序列模型。但是训练句子的大小不足以覆盖目标（文本）域，包括不同的长度和上下文。为了提高模型的泛化能力，我们需要尽可能提高我们的数据在文本域上的覆盖率，最好的方法是提高数据的泛化能力。仅由字素或音素组成的序列泛化性低，因为每个序列都指特定的案例，不能代表一些具有共同特征的案例。这就导致了很多情况在训练集中没有很好覆盖的问题。所以我们可以使用更高级、更抽象的特征来描述序列，以提高输入数据的覆盖率和泛化能力。语义信息和句法信息正是我们所需要的。</p>
<p>在本文中，我们将尝试利用句法信息，特别是从用于端到端 TTS 的句法解析树中得出的语言特征。 句法解析是一种广泛使用的句法分析工具。 它也称为“短语结构解析”，可以描述句子中单词之间的短语结构和短语级关系。 我们从 TTS 的角度对句法解析进行了系统的研究，并从不同的角度提出了一系列基于句法解析的特征，以帮助优化端到端的 TTS 模型。 我们使用三个不同的测试集从三个方面评估我们的模型，在普通测试集上的性能，在复杂句子上的性能和在病理测试集上的泛化。 实验结果表明，这些特征有助于提高韵律和泛化能力。</p>
<h2 id="句法解析衍生的语言特征">句法解析衍生的语言特征</h2>
<h3 id="句法解析">2.1 句法解析</h3>
<p>句法解析将句子分解为其句法短语树结构。树中的组件有其对应的级别和语法角色，例如名词短语和动词短语。包含多个单词的短语也可以进一步解析为子短语，直到到达单词的末端叶子为止。句法解析可以递归地对一个例句进行，如图1所示。近年来，解析技术的研究取得了长足的进步，出现了许多新的解析算法，如概率上下文无关文法（PCFG）、分解解析器、Shift-Reduce解析器和直接到树。改进的解析性能带来了更少的歧义和更稳定的句法分析。用具有丰富语法结构的大型文本数据库训练的句法解析模型可以为 TTS 提供有用的句法特征。</p>
<p>早些年，句法解析主要用于帮助构建更好的基于规则的韵律预测模块。例如，描述了一个基于规则的系统，通过句法解析来推断韵律短语。然后随着统计参数语音合成的发展，句法解析衍生的特征可以用作韵律预测的前端。在一些文献中，提出了从句法分析中提取的特征以改进韵律预测。在某文献中，作者的目的是建立一个模型，该模型可以从句法树映射到韵律树，以改进中断索引标记。新研究还尝试使用句法解析来改进基于 HMM 或 DNN 的声学模型。实验结果表明，句法解析可以改善 TTS 的韵律。</p>
<p>在本节中，我们将尝试从短语结构和词关系两个方面对句法解析进行系统分析，并测试深层句法解析衍生的语言特征以提高 TTS 性能。</p>
<h3 id="基于短语结构的特征">2.2 基于短语结构的特征</h3>
<p>当我们以宏观的方式研究句法树结构时，树在多个层次上描述了一个短语结构。 短语结构控制着句子的句法框架。 句子的节奏和语调本质上嵌入在基于树的短语结构中。 我们采用从短语结构中导出的特征来表征句法信息：</p>
<ul>
<li><p>词性 (POS: Part-of-speech)</p></li>
<li><p>短语标签，例如 S、NP、VP 和 PP。</p></li>
<li><p>短语第一个词的短语边界标签</p></li>
<li><p>词在短语中的相对位置。 <span class="math inline">\(𝑅_𝑤 = 𝑃_𝑤/𝑁_𝑝\)</span> <span class="math inline">\(𝑃_𝑤\)</span>：当前词在当前词组中的位置</p></li>
</ul>
<p><span class="math inline">\(𝑁_𝑝\)</span>：当前词组词数</p>
<div class="figure">
<img src="/images/syntactic-tree-tts-fig1.png" alt="syntactic-tree-tts-fig1" />
<p class="caption">syntactic-tree-tts-fig1</p>
</div>
<p>例如，图1中的“like”是其父节点的边界，即<span class="math inline">\(VP\)</span>； 它的POS是<span class="math inline">\(VBP\)</span>； 它在 <span class="math inline">\(S\)</span> 中的相对位置是 <span class="math inline">\(𝑅_𝑤 = 5/9\)</span>（“.”也是一个词）； 它属于高级短语，<span class="math inline">\(S\)</span> 和 <span class="math inline">\(VP\)</span>。</p>
<p>这些特征可以捕获句子的结构信息，然后用该信息影响每个单词。 当我们使用基于短语结构的特征时，我们需要固定层数和选择特定层的方式。 不同的大小和方法会对韵律产生不同的影响。 在3.2.1中，我们将讨论层的选择。</p>
<h3 id="基于词关系的特征">2.3 基于词关系的特征</h3>
<div class="figure">
<img src="/images/syntactic-tree-tts-fig2.png" alt="syntactic-tree-tts-fig2" />
<p class="caption">syntactic-tree-tts-fig2</p>
</div>
<p>短语结构中的高冗余使得仅使用有限的文本数据更难提取用于韵律预测的有用信息。 出于这个原因，我们建议改进功能。 我们都知道 ToBI 中的特征是基于单词或较低级别的音素，如重音、重音和中断。 因此，我们希望通过提取一些既可以描述单词的句法属性又可以描述两个相邻单词之间关系的特征来关注树中单词之间的关系，以帮助学习这些韵律特征。 我们定义特征并将它们用图 2 解释为：</p>
<ul>
<li><p>一个词的词性。 (橙色部分)</p></li>
<li><p>与两个相邻词的结合点相关的短语。中断或停顿只发生在两个相邻词之间。我们在本文中对其进行了扩展，如果我们将 NONE 设置为每个单词的第一个短语，则会在两个相邻的短语之间发生中断。例如，“boys”和“like”交界处的短语是<span class="math inline">\(NP\)</span>和<span class="math inline">\(VP\)</span>，“like”和“eating”交界处的短语是<span class="math inline">\(NONE\)</span>和<span class="math inline">\(VP\)</span>。为了找到这两个短语，我们采用以下两个标准：</p></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>当前单词开头的最高级别短语 (HBCW)。</p></li>
<li><p>以前一个词结尾的最高级别短语 (HEPW)。</p></li>
</ol>
<ul>
<li><p>最低共同祖先（LCA），由两个相邻词组成的最低级节点。在图2中，<span class="math inline">\(S\)</span>是“boys”和“like”的<span class="math inline">\(LCA\)</span>，第二级<span class="math inline">\(VP\)</span>是“eating”和“apples”的<span class="math inline">\(LCA\)</span>。</p></li>
<li><p>句法距离。它显示了树中两个相邻单词之间的距离。更长的距离可能导致更高的概率中断更长的停顿。我们定义了以下与句法距离相关的特征：</p></li>
</ul>
<ol style="list-style-type: decimal">
<li>高度 (<span class="math inline">\(𝐻\)</span>)：树中的水平。 <span class="math inline">\(𝐻_𝑙\)</span> , <span class="math inline">\(𝐻_𝑐\)</span> , <span class="math inline">\(𝐻_𝑝\)</span> 分别表示 <span class="math inline">\(LCA\)</span> 的级别，当前词的 POS 和前一个词的 POS。</li>
<li>距离（<span class="math inline">\(𝐷\)</span>）：树中节点之间的最短路径长度（不包括单词）。 <span class="math inline">\(𝐷_{𝑐𝑙}\)</span>, <span class="math inline">\(𝐷_{𝑝𝑙}\)</span>, <span class="math inline">\(𝐷_{𝑐𝑝}\)</span> 是指 LCA 与当前 POS、LCA 与前一个 POS、当前 POS 和前一个 POS 之间的距离。</li>
</ol>
<p>“like”的<span class="math inline">\(𝐷_{𝑐𝑝}\)</span>是从NNS到VBP的最短路径的长度。 我们可以添加 <span class="math inline">\(𝐷_{𝑐𝑙}\)</span> 和 <span class="math inline">\(𝐷_{𝑝𝑙}\)</span> 来得到它的值。 (<span class="math inline">\(𝐷_{𝑐𝑙} = 𝐻_𝑙 -𝐻_𝑐;𝐷_{𝑝𝑙} = 𝐻_𝑙 -𝐻_𝑝;𝐷_{𝑐𝑝} = 𝐷_{𝑐𝑙} +D_{pl}\)</span>)</p>
<p>模型结构图见如下图3</p>
<div class="figure">
<img src="/images/syntactic-tree-tts-fig3.png" alt="syntactic-tree-tts-fig3" />
<p class="caption">syntactic-tree-tts-fig3</p>
</div>
<p>实验结果见如下图4，其中本文提出的模型对于4个had进行了较好程度的建模，建模出了不同的音高和韵律。</p>
<div class="figure">
<img src="/images/syntactic-tree-tts-fig4.png" alt="syntactic-tree-tts-fig4" />
<p class="caption">syntactic-tree-tts-fig4</p>
</div>
<h2 id="结论">结论</h2>
<p>在这项研究中，我们研究了嵌入在解析树中的句法解析派生特征，以提高端到端 TTS 合成性能。 两个特定的特征，短语结构和词关系，被有利地选择来测试它们对韵律预测、发音清晰度、自然性和端到端 TTS 合成的泛化的影响。 实验结果表明，句法特征确实可以提高合成语音在韵律、可理解性和概括性方面的质量。 基于词关系的特征 (WRF) 在检查的三个测试集上产生最佳性能。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>SC-GlowTTS an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model</title>
    <url>/2021/08/21/SC-GlowTTS/</url>
    <content><![CDATA[<h1 id="sc-glowtts-一种高效的零样本多说话者文本到语音模型">SC-GlowTTS: 一种高效的零样本多说话者文本到语音模型</h1>
<h2 id="摘要">摘要</h2>
<p>在本文中，我们提出了 SC-GlowTTS：一种高效的零样本多说话者文本到语音模型，可提高训练期间未见过的说话者的相似性。 我们提出了一种<strong>说话人条件架构，该架构探索了一种在零样本场景中工作的基于流的解码器。</strong> 作为文本编码器，我们探索了基于空洞残差卷积的编码器、基于门控卷积的编码器和基于transformer的编码器。 此外，我们已经表明，为训练数据集上的 TTS 模型预测的频谱图调整基于 GAN 的声码器可以显着提高新说话者的相似性和语音质量。 我们的模型仅使用 11 个说话者进行收敛，在与新说话者的相似性以及高语音质量方面达到了最先进的结果。</p>
<h2 id="introduction">Introduction</h2>
<p>近年来，由于深度学习的巨大进步，文本到语音 (TTS) 系统受到了很多关注，这使得虚拟助手等语音应用程序得以普及。大多数 TTS 系统是根据单个说话者的声音量身定制的，但目前有兴趣为新说话者合成语音，这在训练期间没有看到，仅使用几秒钟的语音样本。这种方法被称为零样本多说话人 TTS (ZS-TTS)。</p>
<p>ZS-TTS 最初是通过扩展 Deep-Voice 3 提出的。此外，一些学者使用广义端到端损失 (GE2E) 使用从经过训练的说话人编码器提取的外部嵌入探索了 Tacotron 2，从而产生了一个可以生成语音的模型，类似于目标说话人。同样，一些学者使用不同的说话人嵌入方法探索了 Tacotron 2。作者表明，与 X-vector嵌入相比，LDE 嵌入提高了相似性，并为新说话者合成了更自然的语音。 某作者还表明，训练性别相关模型可以提高未见过说话者的相似性。</p>
<p>在这种情况下，一个主要问题是训练期间观察到的和未观察到的说话者之间的相似性差距。为了缩小这种差距，Attentron 提出了一种细粒度编码器，它具有从各种参考样本中提取详细样式的注意力机制和粗粒度编码器。由于使用了多个参考样本而不是一个，他们对看不见的说话者获得了更好的相似性。</p>
<p>尽管最近有结果，但零样本多说话人TTS 仍然是一个悬而未决的问题，特别是关于可见和不可见说话人的质量差异。此外，当前的方法严重依赖 Tacotron 2，而使用基于流的方法有可能改善结果。在这种情况下，FlowTron允许操纵语音的多个方面，例如音调、语气、语速、节奏和口音。此外， 提出 GlowTTS 达到与 Tacotron 2 相似的质量，但速度提高了 15.7 倍，同时允许语音速度操纵。</p>
<p>在本文中，我们提出了一种新方法，Speaker Conditional GlowTTS (SC-GlowTTS)，用于对未见过的说话者进行零样本学习。我们的模型依赖 GlowTTS 作为将输入字符转换为频谱图的部分。 SC-GlowTTS 使用基于 Angular Prototype loss 的外部说话人编码器来学习说话人嵌入向量，并调整 HiFi-GAN 声码器以将输出频谱图转换为波形。我们的贡献如下：</p>
<ul>
<li>一种新颖的零样本多说话人 TTS 方法，仅在训练集中 11 名说话人的情况下即可获得最先进的结果；</li>
<li>在零样本多说话人 TTS 设置中实现高质量和比实时语音合成更快的架构；</li>
<li>为训练数据集上的 TTS 模型预测的频谱图调整基于 GAN 的声码器，以显着提高新说话者的相似度和语音质量。</li>
</ul>
<p>我们每个实验的音频样本都可以在演示网站上找到。 此外，为了重现性，Coqui TTS 提供了实现，并且所有实验的checkpoints都可以在 Github 存储库中找到。</p>
<h2 id="说话人相关的glowtts模型">说话人相关的GlowTTS模型</h2>
<p>Speaker Conditional Glow-TTS (SC-GlowTTS) 建立在 GlowTTS 的基础上，但包括几个新颖的修改。除了 GlowTTS 的基于Transformer的编码器网络之外，我们还探索了残差扩张卷积网络 (Residual dilated convolutional network) 和门控卷积网络 (gated convolutional network)；据我们所知，在这种情况下首次使用。我们的卷积残差编码器基于speedyspeech，但是我们使用 Mish 而不是 ReLU 激活函数。另一方面，我们的门控卷积网络由 9 个卷积块组成，每个块包括一个 dropout 层、一个 1D 卷积和一个层归一化。我们在所有卷积层中使用内核大小 5、扩张率 1 和 192 个通道。<strong>基于流的解码器使用与 GlowTTS 模型相同的架构和配置</strong>。然而，为了将其转换为零样本 TTS 模型，我们在所有 12 个解码器块的仿射耦合层中包含了说话人嵌入。我们还使用 FastSpeech 的持续时间预测器网络来预测字符持续时间。为了捕捉不同说话者的不同语音特征，我们在持续时间预测器的输入中添加了说话者嵌入。最后，HiFi-GAN 用作声码器。</p>
<p>SC-GlowTTS 模型在推理过程中如图 1 所示，其中 (<span class="math inline">\(+\)</span>) 表示串联。 在训练期间，该模型使用单调对齐搜索 (MAS)，其中解码器的目标是调节梅尔谱图和嵌入在 <span class="math inline">\(P_Z\)</span> 先验分布中的输入说话人。 MAS 的目的是将 <span class="math inline">\(P_Z\)</span> 先验分布与编码器的输出对齐。 在推理过程中，不使用 MAS，而是由文本编码器和持续时间预测器网络预测 <span class="math inline">\(P_Z\)</span> 先验分布和对齐。 最后，潜在变量 <span class="math inline">\(Z\)</span> 从先验分布 <span class="math inline">\(P_Z\)</span> 中采样。 使用反向解码器和说话人嵌入，并行合成梅尔谱图，通过基于流的解码器转换潜在变量 <span class="math inline">\(Z\)</span>。</p>
<p>为简洁起见，我们将带有变压器、残差卷积和基于门控卷积的编码器的 SC-GlowTTS 模型分别命名为 SC-GlowTTS-Trans、SC-GlowTTS-Res 和 SC-GlowTTS-Gated 模型。</p>
<h2 id="sc-glowtts-在少量说话人下的性能">SC-GlowTTS 在少量说话人下的性能</h2>
<p>为了模拟只有少数说话者的场景，我们通过选择 VCTK 数据集训练集的子集来反映我们的测试集。 这个新的训练集由 11 说话人（每个人约400句）组成，7F/4M。 我们为每个口音选择了 1 个代表，除了“新西兰”口音只有一个说话者并且在我们的测试集中，所以我们添加了一个“美国”说话者，选择的说话者是 229、249、293、313 、301、374、304、316、251、297 和 323。从这个新的训练集中，我们选择了随机样本用作验证集。 作为测试集，我们使用 3.3 节中定义的相同的测试集。</p>
<p><strong>我们使用 SC-GlowTTS-Trans 模型并使用 LJSpeech 数据集对其进行 290k 步训练。在单说话人数据集中进行这种预训练是为了在更大的词汇表中准备模型的编码器。</strong>我们对新训练集中的 SC-GlowTTS-Trans 模型进行了微调，<strong>其中只有 11 个说话者进行了 70k 步</strong>，并且使用验证集，我们选择了最佳检查点作为第 66k 步。此外，使用在 LibriTTS 数据集中训练 75k 步的 HiFi-GAN 模型，我们使用相同的技术调整其他 95k 步。这个新实验的结果是 SECS 为 0.7707，MOS 为 3.71 ± 0.07，Sim-MOS 为 3.93 ± 0.08。这些结果与 Tacotron 2 实现的 0.7791、MOS 3.74 和 Sim-MOS 3.951 ± 0.07 的 SECS 兼容，后者使用了更大的 98 个说话人组。因此，我们的 SC-GlowTTS-Trans 模型与一个小 9.8 倍的数据集收敛，性能与 Tacotron 2 相当。我们认为这是向前迈出的重要一步，尤其是对于低资源语言中的 ZS-TTS。</p>
<h2 id="zero-shot-voice-conversion">Zero-shot Voice Conversion</h2>
<p>与原始 GlowTTS 模型一样，我们不向模型编码器提供任何有关说话人身份的信息，因此编码器预测的分布被迫独立于说话人身份。 因此，与 GlowTTS 一样，SC-GlowTTS 可以仅使用模型的解码器来转换语音。 然而，在我们的工作中，我们使用外部扬声器嵌入来调节 SC-GlowTTS。 通过执行零样本语音转换，它使我们的模型能够模拟训练中未见过的说话者的语音。 演示页面上提供了零样本语音转换的示例。</p>
<h2 id="结论和未来工作">结论和未来工作</h2>
<p>在这项工作中，我们提出了一种新方法 SC-GlowTTS，实现了最先进的 ZS-TTS 结果。 我们为 SC-GlowTTS 模型探索了三种不同的编码器，并表明基于transformer的编码器为训练中未见过的说话者提供了最佳的相似性。 我们的 SC-GlowTTS 模型优于 Tacotron 2。此外，当与外部扬声器编码器结合使用时，SC-GlowTTS 模型可以在训练集中仅 11 个扬声器的情况下执行 ZS-TTS。 最后，我们发现对训练和验证集中的 TTS 模型预测的频谱图中 HiFi-GAN 声码器的调整可以显着提高未见过说话者的合成语音（MOS）的相似度和质量。 在训练中。 作为未来的工作，继的工作之后，我们打算将 SC-GlowTTS 扩展为一种少量的方法。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Parrotron An End-to-End Speech-to-Speech Conversion Model and its Applications to Hearing-Impaired Speech and Speech Separation</title>
    <url>/2021/08/22/Parrotron/</url>
    <content><![CDATA[<h1 id="Parrotron-端到端语音到语音转换模型及其在听障语音和语音分离中的应用"><a href="#Parrotron-端到端语音到语音转换模型及其在听障语音和语音分离中的应用" class="headerlink" title="Parrotron: 端到端语音到语音转换模型及其在听障语音和语音分离中的应用"></a>Parrotron: 端到端语音到语音转换模型及其在听障语音和语音分离中的应用</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Project Parrotron 致力于开发可以省略中间步骤，不参考视觉提示（如嘴部运动），直接将非标准语音转化为标准语音的技术，以帮助语言障碍者与人和设备都能更好的交流。</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们描述了 Parrotron，这是一种端到端训练的语音到语音转换模型，可将输入频谱图直接映射到另一个频谱图，而无需使用任何中间离散表示。该网络由编码器、频谱图解码器和音素解码器组成，最后接上一个声码器以合成时域波形。我们证明，该模型可以被训练，以归一化来自任何说话者的语音为具有固定口音和一致发音和韵律的单个规范目标说话者的语音，不管其口音、韵律和背景噪声如何。我们进一步表明，这种归一化模型可以适用于对聋哑人的高度非典型语音进行归一化，从而显着提高可懂度和自然度，通过语音识别器和听力测试进行测量。最后，展示了该模型在其他语音任务上的实用性，我们表明可以训练相同的模型架构来执行语音分离任务。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>具有注意力的编码器-解码器模型最近在建模各种复杂的序列到序列问题方面取得了相当大的成功。 这些模型已成功用于处理语音和自然语言处理中的各种任务，例如机器翻译、语音识别，甚至组合语音翻译。 他们还在端到端文本到语音 (TTS) 合成和自动语音识别 (ASR) 方面取得了最先进的结果，在给定几乎原始输入的情况下，使用单个神经网络直接生成目标序列。</p>
<p>在本文中，我们结合基于注意力的语音识别和合成模型来构建直接的端到端语音到语音序列转换器。 该模型根据不同的输入频谱图生成语音频谱图，没有中间离散表示。</p>
<p>我们测试这样一个统一模型是否足够强大，可以将来自多种口音、缺陷（可能包括背景噪声）的任意语音归一化，并在单个预定义目标说话者的语音中生成相同的内容。 任务是投射掉所有非语言信息，包括说话者特征，只保留所说的内容，而不是说的人、地点或方式。 <strong>这相当于一个独立于文本的多对一语音转换任务。</strong> 我们使用 ASR 和听力研究在此语音归一化任务上评估模型，验证它是否能够按预期保留基础语音内容并投射其他信息。</p>
<p>我们证明了预训练的归一化模型可以适应执行更具挑战性的任务，将聋哑人的高度非典型语音转换为流利的语音，显着提高可懂度和自然度。最后，我们评估同一网络是否能够执行语音分离任务。鼓励读者在配套网站上收听声音示例。 </p>
<p>已经提出了多种语音转换技术，包括映射码本、神经网络、动态频率扭曲和高斯混合模型。最近的工作还涉及口音转换。在本文中，我们提出了一种直接生成目标信号的端到端架构，从头开始合成它。它与最近关于序列到序列语音转换的工作最相似。 使用类似的端到端模型，以说话人身份为条件，将来自多个说话人的词段转换为多个目标语音。与为每个源-目标说话人对训练单独模型的不同，我们专注于多对一转换。我们的模型在源-目标频谱图对上进行训练，没有使用来自预训练语音识别器的瓶颈特征来增强输入，以更明确地捕获源语音中的音素信息。然而，我们确实发现多任务训练模型来预测源语音音素很有帮助。最后，相比来说，我们在没有辅助对齐或自动编码损失的情况下训练模型。</p>
<p>类似的语音转换技术也已应用于提高有声乐障碍的说话者的可懂度，尤其是听力受损的说话者。 我们将更现代的机器学习技术应用于这个问题，并证明，如果有足够的训练数据，端到端训练的一对一转换模型可以显着提高聋哑人的清晰度和自然度。</p>
<h2 id="模型架构"><a href="#模型架构" class="headerlink" title="模型架构"></a>模型架构</h2><p>我们使用端到端的序列到序列模型架构，该架构采用输入源语音并生成/合成目标语音作为输出。 这种模型的唯一训练要求是成对的输入-输出语音的平行语料库。 我们将此语音到语音模型称为 Parrotron。</p>
<p><img src="/images/paratron-fig1.png" alt="paratron-fig1"></p>
<p>如图 1 所示，网络由一个编码器和一个带注意力的解码器组成，然后是一个声码器来合成时域波形。 编码器将一系列声学帧转换为隐藏特征表示，解码器使用该表示来预测频谱图。 核心架构基于最近基于注意力的端到端 ASR 模型和 TTS 模型，例如 Tacotron。</p>
<h3 id="2-1-频谱编码器"><a href="#2-1-频谱编码器" class="headerlink" title="2.1 频谱编码器"></a>2.1 频谱编码器</h3><p>基本编码器配置类似于Tacotron-2中的编码器，并且在第 3.1 节中评估了一些变化。从以 16 kHz 采样的输入语音信号中，我们提取了 125-7600 Hz 范围内的 80 维 log-mel 频谱图特征，使用 Hann 窗口、50 ms 帧长、12.5 ms 帧偏移和 1024 点计算短时傅立叶变换 (STFT)。</p>
<p>输入特征被传递到具有 ReLU 激活的两个卷积层的堆栈中，每个卷积层由 32 个内核组成，在时间 × 频率维度上，形状为 3 × 3 ，步幅为 2 × 2，按总因子4按时间对序列进行下采样，减少以下层的计算量。在每一层之后应用批量标准化。</p>
<p>使用 1 × 3 滤波器将此下采样序列传递到双向卷积 LSTM (CLSTM) 层，即在每个时间步长内仅在频率轴上进行卷积。最后，将其传递到每个方向上大小为 256 的三个双向 LSTM 层的堆栈中，与 512 维线性投影交错，然后是批处理规范和 ReLU 激活，以计算最终的 512 维编码器表示。</p>
<h3 id="2-2-频谱解码器"><a href="#2-2-频谱解码器" class="headerlink" title="2.2 频谱解码器"></a>2.2 频谱解码器</h3><p>解码器目标是 1025-dim STFT 幅度，使用与输入特征相同的帧和 2048 点 FFT 计算。我们使用Tacotron-2中描述的解码器网络，该网络由一个自回归 RNN 组成，以一次一帧地预测来自编码输入序列的输出频谱图。来自前一个解码器时间步长的预测首先通过一个包含 2 个全连接层的小型预网络，其中包含 256 个 ReLU 单元，这有助于学习注意力。 pre-net 输出和注意力上下文向量被连接起来，并通过 2 个单向 LSTM 层的堆栈，具有 1024 个单元。 LSTM 输出和注意力上下文向量的串联然后通过线性变换进行投影，以产生对目标频谱图帧的预测。最后，这些预测通过 5 层卷积 post-net，它预测要添加到初始预测的残差。每个 post-net 层都有 512 个过滤器，形状为 5 × 1，然后是批量归一化和 tanh 激活。</p>
<p>为了从预测的频谱图合成音频信号，我们主要使用 Griffin-Lim 算法来估计与预测幅度一致的相位，然后是逆 STFT。然而，在进行人类听力测试时，我们改为使用 WaveRNN神经声码器，该声码器已被证明可以显着提高合成保真度。</p>
<h3 id="2-3-使用-ASR-解码器进行多任务训练"><a href="#2-3-使用-ASR-解码器进行多任务训练" class="headerlink" title="2.3 使用 ASR 解码器进行多任务训练"></a>2.3 使用 ASR 解码器进行多任务训练</h3><p>由于这项工作的目标是仅生成语音而不是任意音频，因此联合训练编码器网络以同时学习底层语言的高级表示有助于使频谱图解码器预测偏向于相同底层语音的表示内容。我们通过添加一个辅助 ASR 解码器来预测输出语音的（字素或音素）转录本，以编码器潜在表示为条件来实现这一点。这种经过多任务训练的编码器可以被认为是学习输入的潜在表示，该表示维护有关底层转录本的信息，即更接近在 TTS 序列到序列网络中学习的潜在表示。</p>
<p>解码器输入是通过将上一步发出的字素的 64 维嵌入和 512 维注意力上下文连接起来创建的。这被传递到一个 256 单元的 LSTM 层。最后，将注意力上下文和 LSTM 输出的串联传递到 softmax 中，以预测输出词汇表中每个字素的概率。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们描述了 Parrotron，这是一种端到端的语音到语音模型，可将输入频谱图直接转换为另一个频谱图，无需中间符号表示。我们发现可以训练该模型将来自不同说话者的语音标准化为单个目标说话者的语音，同时保留语言内容并投射掉非语言内容。然后我们表明，该模型可以成功地适用于改善失聪者语音的 WER 和自然度。我们最终证明，可以训练相同的模型来成功识别、分离和重建重叠语音混合中最响亮的说话者，从而提高 ASR 性能。 Parrotron 系统还有其他潜在的应用，例如通过将重口音或其他非典型语音转换为标准语音来提高可懂度。将来，我们计划在其他语言障碍上对其进行测试，并采用中的技术来保留说话者的身份。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Vocoder-free End-to-End Voice Conversion with Transformer Network</title>
    <url>/2021/08/23/end2endVC-Transformer/</url>
    <content><![CDATA[<h1 id="使用-transformer-网络进行无声码器的端到端语音转换">使用 Transformer 网络进行无声码器的端到端语音转换</h1>
<h2 id="摘要">摘要</h2>
<p>与原始频谱相比，基于梅尔频率滤波器组 (MFB) 的方法具有学习语音的优势，因为 MFB 具有较小的特征尺寸。然而，使用 MFB 方法的语音生成器需要额外的声码器，这需要大量的训练过程计算开销。附加的前/后处理（例如 MFB 和声码器）对于将真实的人类语音转换为其他语音不是必不可少的。<strong>可以只使用原始频谱和相位来生成具有清晰发音的不同风格的声音。</strong>在这方面，我们提出了一种快速有效的方法，以并行方式使用原始频谱转换逼真的声音。我们的基于转换器的模型架构没有任何 CNN 或 RNN 层，显示了快速学习的优势，并解决了传统 RNN 顺序计算的局限性。在本文中，我们介绍了一种使用 Transformer 网络的无声码器的端到端语音转换方法。所提出的转换模型也可用于语音识别的说话人自适应。我们的方法可以在不使用 MFB 和声码器的情况下将源语音转换为目标语音。我们可以通过将转换后的幅度乘以相位来获得用于语音识别的自适应 MFB。我们分别使用自然度、相似度和清晰度等指标和平均意见得分对 TIDIGITS 数据集进行语音转换实验。</p>
<h2 id="introduction">Introduction</h2>
<p>语音转换在各个工业领域都获得了相当大的关注。最近，使用循环神经网络 (RNN) 构建的编码器-解码器模型，例如长短期记忆 (LSTM) 、双向长短期记忆 (BiLSTM) 和门控循环单元 (GRU) 已被广泛用于序列建模。有很多基于 RNN 编码器-解码器结构的神经网络模型，也称为序列到序列（Seq2Seq），它们在语音转换任务中取得了良好的效果。</p>
<p>然而，RNN 为每个序列一个一个地处理单词。 RNN 的这种顺序特性可能成为 GPU 并行计算的障碍，并使训练速度变慢。最重要的是，如果这些时间信息变长，模型往往会忘记远处位置的内容或将其与下一个位置的内容混合。 Transformer 网络通过使用注意力机制来推导输入和输出之间的全局依赖性，部分解决了 RNN 的这些问题，在许多领域达到了最先进的性能。没有任何卷积神经网络 (CNN) 或 RNN 层的 Transformer 显示了快速学习的优势，并解决了传统 RNN 顺序计算的局限性。</p>
<p>将波形语音作为语音转换的模型输入，短时傅立叶变换 (STFT) 将其转换为时频域形式的原始频谱。用 STFT 计算的频谱可以提供比波形语音更有用的信息。特别是，文本到语音 (TTS)、语音转换和语音识别中使用的传统方法通过原始频谱传递梅尔滤波器组以生成梅尔频率滤波器组（MFB，也称为梅尔谱图）。在 MFB 中，频谱的频率分量是在 STFT 之后获得的。之后，根据反映人耳耳蜗特性的梅尔曲线对其进行压缩。<strong>在 MFB 中，相位信息在压缩时会被删除。</strong></p>
<p>MFB 由每个时间步长 40 到 80 个特征维度组成，与原始频谱相比具有学习速度的优势，因为 MFB 的特征尺寸较小。<strong>但是，由于丢失了相位信息，它不能直接转换为波形语音。</strong>因此，采用 MFB 方法的语音生成器需要额外的声码器，这需要大量的训练过程计算开销。换句话说，<strong>馈入 Seq2Seq 的 MFB 应该在合成线性标度谱的声码器的帮助下通过相位估计合成自然语音。</strong>然后，它可以将模型的最终输出转化为波形语音。</p>
<p>因此，采用 MFB 方法的语音生成器需要额外的声码器，这需要大量的训练计算过程。使用 Griffin-Lim 和 WaveNet 等声码器，可以通过合成获得更好的语音质量。相反，不可避免地要避免由于额外的计算而带来的复杂性问题。</p>
<p>然而，为了避免额外的前/后处理，如 MFB 和声码器，我们提出了一种快速有效的方法，以并行方式使用原始频谱转换逼真的声音，以生成具有清晰发音的不同风格的声音。在本文中，我们介绍了一种使用 Transformer 网络的无声码器端到端语音转换方法。我们专注于转换由 STFT 获得的原始频谱，而无需借助需要迭代合成的声码器。此外，还可以利用相位信息通过逆 STFT 还原波形语音。</p>
<p>我们提出的转换模型也可以用于语音识别的说话人适应。 我们的方法可以在不使用 MFB 和声码器的情况下将源语音转换为目标语音。 我们可以通过将转换后的幅度乘以相位来获得用于语音识别的自适应 MFB。 此外，还可以将语音识别性能较差的少数民族（老人、儿童、方言、残疾说话者）的声音转换为普通成年人的声音。 通过说话人自适应替代少数族裔和普通成年人的特征，可以实现更好的语音识别性能。 我们分别使用自然度、相似度和清晰度等指标和平均意见得分 (MOS) 在 TIDIGITS 数据集上执行我们的语音转换实验。</p>
<h2 id="相关工作">相关工作</h2>
<p>在本节中，我们首先介绍我们在本文中使用的声码器、语音转换和transformer网络的先前研究。</p>
<h3 id="vocoder">Vocoder</h3>
<p>声码器用于将线性尺度频谱合成为语音信号，通过相位估计合成自然语音。在 Griffin-Lim 算法中，计算上一步输出的语音信号的 STFT，并将幅度替换为作为输入的修正 STFT 幅度。该算法通过恢复原始信号的迭代过程来恢复STFT幅度与给定的修正STFT最相似的语音信号，以最小化新STFT和输入修正STFT幅度的平方误差。</p>
<p>WaveNet 是一种自回归模型，它使用语音样本之间的顺序特征，并通过使用先前的样本预测下一个样本，成功地合成了高质量的语音。但是，生成速率的速度很慢，因为下一个样本是从前一个样本中一个一个地生成的。 Parallel WaveNet 旨在解决 WaveNet 的样本生成速度慢的问题，它使用逆自回归流来合成语音。由于逆自回归流在学习过程中不知道目标语音数据集的分布，所以通过使用训练有素的WaveNet（教师网络）提取目标数据集的分布信息并与结果进行比较来进行学习逆自回归流。它的优点是语音合成速度比 WaveNet 快，但合成语音的质量较低。与 Parallel WaveNet 不同，WaveGlow 不需要预训练的教师网络，并且具有快速语音合成的优势。但是，由于 WaveGlow 使用基于分布的损失函数，因此合成语音的质量很差。此外，当与 TTS 结合时，会出现合成语音的质量取决于从文本中预测的 MFB 质量的问题。</p>
<h3 id="语音转换">语音转换</h3>
<p>在Parrotron中，将残疾说话者的声音转换为一般声音。编码器由 CNN 和三个 BiLSTM 组成，解码器由两个 LSTM 组成。使用编码器-解码器之间的注意力。为了解决信号到信号的转换问题，语音识别解码器连接到编码器输出以进行多任务学习，并且仅用于训练任务。</p>
<p>要在不同语言的语音之间进行翻译并将翻译后的输出合成为语音，通常必须通过语音识别、翻译和 TTS 任务。然而，在另一篇文章中，他们将不同语言的语音转换为基于端到端注意力的 Seq2Seq 网络。无需经过其他步骤，它可以直接将另一种语言的语音翻译成一种。 Encoder由8个BiLSTMs组成，编码器输出用于通过辅助任务预测输入和目标的音素时间信息。同样，这些辅助解码器仅用于学习。此外，解码器可以根据输入说话人进行任意调整。因此，可以通过使用预先训练的说话人编码器将语音转换为所需说话人的语音。他们考虑使用 WaveRNN 声码器而不是 Griffin-Lim，因为它可以显着提高语音质量。</p>
<h3 id="transformer">Transformer</h3>
<div class="figure">
<img src="/images/end2end-vc-transformer-fig1.png" alt="end2end-vc-transformer-fig1" />
<p class="caption">end2end-vc-transformer-fig1</p>
</div>
<p>RNN 广泛用于序列建模任务，例如神经翻译和语言建模。然而，由于 RNN 对每个序列一个一个地处理单词，这个顺序过程可能成为并行化和缓慢学习的障碍。最重要的是，如果这些时间信息变长，模型往往会忘记远处位置的内容，以便或与下一个位置的内容混合。 Transformer 网络是一种模型架构，它完全依赖于注意力机制来推导出输入和输出之间的全局依赖关系。如图 1 所示，没有 CNN 和 RNN 的 Transformer 模型架构显示了快速学习时间的优势。传统RNN由于在时间信息上表现不佳的缺点，已经通过self-attention解决了。 BERT是由transformer 演化而来的，不仅用于翻译，还包括句子相关性的总结和预测等许多自然语言处理（NLP）领域。BERT 与NLP 一起被广泛应用于其他领域。 VideoBERT 学习了从向量双向和语音识别导出的视觉和语言标记序列的双向联合分布，并输出视频数据。这导致了对各种任务的研究，包括动作分类和视频字幕。在Transformer-TTS中，将 Transformer 网络与 TTS 模型称为 Tacotron2，用于呈现语音合成的结果。在Transformer-VC中，基于 Transformer 网络执行语音转换，使用预训练的 TTS。他们使用基于声码器的合成使用预先训练的模型参数执行语音转换。</p>
<p>因此，声码器有助于提高语音合成的质量，但合成需要时间。 我们使用Transformer网络是因为它通过自我注意以及快速有效的并行学习技术的泛化性能。 此外，我们通过专注于原始频谱阶段的转换而不采用通过声码器的语音合成方法来执行我们的实验。 更多细节在第 3 节中给出。</p>
<h2 id="本文方法">本文方法</h2>
<p>本节介绍在没有声码器帮助的情况下使用原始频谱而不是 MFB 进行端到端语音转换。</p>
<h3 id="原始频谱">原始频谱</h3>
<div class="figure">
<img src="/images/end2end-vc-transformer-fig2.png" alt="end2end-vc-transformer-fig2" />
<p class="caption">end2end-vc-transformer-fig2</p>
</div>
<p>图 2 显示了将波形语音转换为频谱、MFB 并返回波形语音的流程图。 给定一个连续的音频信号 <span class="math inline">\(x[n]\)</span>，这可以表示为：</p>
<div class="figure">
<img src="/images/end2end-vc-transformer-al1.png" alt="end2end-vc-transformer-al1" />
<p class="caption">end2end-vc-transformer-al1</p>
</div>
<p>其中 <span class="math inline">\(A\)</span> 是振幅，<span class="math inline">\(ω\)</span>是径向角频率以radians/seconds为单位, <span class="math inline">\(f\)</span> 是 <span class="math inline">\(ω/2π\)</span>, <span class="math inline">\(φ\)</span> 是弧度的初始相位, <span class="math inline">\(n\)</span> 是时间index， T 是 <span class="math inline">\(\frac{1}{f_s}\)</span> 。 下一个过程是应用预加重滤波器到<span class="math inline">\(x\)</span>以放大高频。预加重滤波器有多种用途。 高频一般小于低频。 因此，使用预加重滤波器有助于避免 STFT 期间的数值问题并提高信噪比。</p>
<p>随着信号频率随时间变化，经过预加重后，信号被分成短时间帧。 由于信号的频率轮廓随时间丢失，因此执行傅立叶变换时假设信号的频率在很短的时间内是静止的，而不是在整个信号上。 语音处理的典型帧大小为 20ms 到 40ms，有 50% 的重叠。 通常帧大小使用 25ms，步幅重叠大小使用 10ms（15ms 重叠）。</p>
<p>下一步是将信号切割成帧，并对每一帧应用汉明、汉宁窗函数。 可以通过对每一帧执行 N 点 FFT (NFFT) 来计算频谱。 这里，NFFT一般使用256（16ms）或512（32ms）。 最后，通过 STFT 获得的频谱可以用幅度和相位由以下等式表示：</p>
<div class="figure">
<img src="/images/end2end-vc-transformer-al2.png" alt="end2end-vc-transformer-al2" />
<p class="caption">end2end-vc-transformer-al2</p>
</div>
<p>其中 <span class="math inline">\(D\)</span> 是复值频谱，<span class="math inline">\(S\)</span> 是幅度，<span class="math inline">\(P\)</span> 是相位。</p>
<p>总之，原始频谱可以直接从语音波形中恢复，如图 2 所示。因此，我们使用频谱以有效的方式进行语音转换，无需任何后处理。</p>
<h3 id="提出的模型结构">提出的模型结构</h3>
<div class="figure">
<img src="/images/end2end-vc-transformer-fig3.png" alt="end2end-vc-transformer-fig3" />
<p class="caption">end2end-vc-transformer-fig3</p>
</div>
<ol style="list-style-type: decimal">
<li>模型数据流：第 2 节中提到的声码器复杂且计算量大，需要大量重复工作来恢复音频波形。 为了解决这个问题，我们专注于频谱级别的转换。 图 3 显示了在上半部分使用 MFB 的传统方法，在下半部分显示了本文提出的Transformer网络。 传统方法使用MFB的输出表示为<span class="math inline">\(M_1，M_2，...，M_n\)</span>作为Seq2Seq的输入，通过声码器获得输出。 Seq2Seq 中的编码器输入考虑了所有时间信息。 它与我们的模型没有什么不同。 然而，解码器一次预测 n 帧 MFB，从而将解码器步骤的数量减少到 <span class="math inline">\(n/γ\)</span>，其中 <span class="math inline">\(γ\)</span> 是缩减因子。 使用 CBHG（一维卷积组、高速公路网络、双向门控循环单元）模块对线性尺度谱 <span class="math inline">\(F\)</span> 进行后处理，结果为 <span class="math inline">\(F_1 , F_2 , ..., F_n\)</span> 。 声码器对于将 <span class="math inline">\(F\)</span> 转换为表示为 <span class="math inline">\(S_1^{&#39;} , S_2^{&#39;} , ..., S_n^{&#39;}\)</span> 的波形至关重要。 使用先前输入来预测当前步骤的自回归声码器。 一旦我们得到 <span class="math inline">\(S_1^{&#39;}\)</span> ，使用 <span class="math inline">\(S_1^{&#39;}\)</span> 来预测 <span class="math inline">\(S_2^{&#39;}\)</span> ，最终得到 <span class="math inline">\(S_n^{&#39;}\)</span> 。 然而，这并不能降低计算成本。</li>
</ol>
<p>另一方面，在图 3 中提出的模型中，幅度 <span class="math inline">\(S\)</span> 和相位 <span class="math inline">\(P\)</span> 是使用等式(2)获得的，来自经过 STFT 后的原始光谱。 然后，我们使用 <span class="math inline">\(S\)</span> 作为模型编码器的输入。 解码器并行转换频谱。 在模型的最终输出 <span class="math inline">\(\hat x\)</span> 和输入相位 <span class="math inline">\(P\)</span> 之间进行元素乘法之后，可以通过逆 STFT 获得转换后的目标语音。 我们可以使用转换后的源幅度和相位立即恢复预测的语音，而无需声码器的帮助。 我们提出的模型是一种以并行方式使用原始频谱转换真实声音的快速有效方法。 我们的方法不依赖于后处理。</p>
<ol start="2" style="list-style-type: decimal">
<li>标记和零填充：语料库的模型输入通常是从词嵌入矩阵中向量化的。 与语料库不同，频谱由连续值组成。 频谱由时间 <span class="math inline">\(T\)</span> 的 <span class="math inline">\(N\)</span> 维组成。这些值不是稀疏表示。 语料库设置最大长度，将句首（SOS）放在语料库前面，句末（EOS）放在语料库末尾。</li>
</ol>
<p>组合序列的 SOS 用作解码器输入。 Seq2Seq 需要通过教师强制使用真实值进行训练。 因此，在推理阶段，解码器的输入仅使用 SOS 令牌。</p>
<p>通过这个，自回归变换器使用波束搜索或贪婪搜索进行预测。 我们将 EOS 令牌放入我们的解码器输入中并执行语音转换。 另外，由于beam search是基于beam depth和softmax，所以我们使用贪心搜索。</p>
<p>我们对所有频谱都使用了零填充。 使用零填充的原因是Transformer网络考虑整个序列并并行学习。 即使语音脚本相同，每个说话者的特征长度也不同。 出于这个原因，我们使用了零填充。</p>
<p>为了避免关注零值和实向量，当每个时间步的维度上有零值时，我们乘以<span class="math inline">\(-1e-9\)</span>。 零填充将在下一节中描述。</p>
<div class="figure">
<img src="/images/end2end-vc-transformer-fig4.png" alt="end2end-vc-transformer-fig4" />
<p class="caption">end2end-vc-transformer-fig4</p>
</div>
<ol start="3" style="list-style-type: decimal">
<li>基于 Transformer 的模型架构：图 4 显示了我们基于 Transformer 的模型架构。首先，我们获得依赖于 <span class="math inline">\(NFFT\)</span> 系数的频谱，然后通过等式 (2) 将 <span class="math inline">\(S\)</span> 和 <span class="math inline">\(P\)</span> 分开(2)。<span class="math inline">\(S\)</span> 用作编码器输入。在这种情况下，我们不使用词嵌入，因为 <span class="math inline">\(S\)</span> 是一个时频域，它由沿时间轴的频率采样组成。最终输入是 <span class="math inline">\(S\)</span> 加上通过 <span class="math inline">\(PE\)</span> 的位置向量。然后通过 N$$ 编码器执行多头注意。多头注意力结果通过包含整流线性单元（ReLU）的两层前馈网络来重建未被清理的信息。到目前为止的过程是通过组合每个时间步的整个时间信息来制作新的上下文信息。然后我们执行一个残差连接，将输入数据添加到到目前为止获得的值中。这意味着未包括在输入时间信息中的上下文信息由输入处理并添加。编码器查看整个给定的时间信息，并将每个时间步信息编码为更好的表示。</li>
</ol>
<p>解码器与编码器方法一样，仅使用从目标 <span class="math inline">\(y\)</span> 的频谱信号中通过 STFT 的 <span class="math inline">\(S\)</span>，并根据已知信息创建新信息。然而，解码器的不同之处在于它在执行自注意力时使用了掩码多头注意力。使用 masked multi-head attention 的原因是为了通过在 self-attention 的 time step 之后覆盖特征来防止 self-attention。这表明transformer网络是自回归模型。之后，注意力被连接在编码器输出和解码器输出之间。这个过程决定了多少解码器使用输入频谱时间信息的 <span class="math inline">\(x\)</span> 来表示 <span class="math inline">\(y_i\)</span>。编码器-解码器注意力的结果被添加到解码器的屏蔽多头注意力结果中。然后将它们放入前馈网络。输出终于出来了。到目前为止，输出 <span class="math inline">\(\hat x\)</span> 与输入 <span class="math inline">\(x\)</span> 和目标 <span class="math inline">\(y\)</span> 具有相同的维度 <span class="math inline">\(d_{model}\)</span>，只是幅度时间长度不同。最后，<span class="math inline">\(\hat x\)</span> 目前只有从源 <span class="math inline">\(x\)</span> 转换为目标 <span class="math inline">\(y\)</span> 的幅度。然后我们将此值乘以 <span class="math inline">\(P\)</span> 以制作包含复数的频谱。最后，可以使用逆 STFT 将其恢复为波形语音。</p>
<h2 id="结论">结论</h2>
<p>我们在原始频谱级别提出了具有自注意力机制的语音变换，而传统方法在 MFB 级别使用声码器。与原始频谱相比，基于 MFB 的方法具有计算学习方便的优势。然而，使用 MFB 方法的语音生成器需要声码器，这需要大量的训练过程计算开销。使用声码器，可以通过合成获得更好的语音质量。相反，由于额外计算而导致的复杂性问题是不可避免的。附加的前/后处理（例如 MFB 和声码器）对于将真实的人类语音转换为其他语音不是必不可少的。在本文中，我们提出了一种无声码器的端到端语音转换方法，该方法使用可以并行转换频谱的快速高效的Transformer网络。在没有重复声码器的帮助下用原始频谱获得转换结果的优点是使用原始相位信息来提供结果。我们收集了 38 名参与者，对转换后的语音的自然度、相似度和清晰度进行了 MOS 评估。在整体说话人平均 MOS 中，我们的实验结果得分分别为自然度 3.40±0.31、相似度 3.82±0.25 和清晰度 3.93±0.25。我们的结果表明，所提出的方法可以在保持自然性和相似性的适当性的同时，以良好的清晰度进行变换。</p>
<h2 id="未来工作">未来工作</h2>
<p>在评估阶段，<span class="math inline">\(\hat x\)</span> 有一个不自然的转换部分。这似乎是由错位引起的，因为 <span class="math inline">\(\hat x\)</span> 和 <span class="math inline">\(phase_x\)</span> 的长度显着偏离。这是转换为最大长度的基于Transformer的模型的一个特征。换句话说，由于零填充，所有数据集的长度都相同。但是，如果<span class="math inline">\(phase_x\)</span> 的实际向量长度小于<span class="math inline">\(\hat x\)</span>，则会导致严重的错位问题。在上述情况下，恢复波形的质量可能很差。因此，音高被打破，自然度被削弱。因此，我们的模型需要进行相位变换来解决错位问题。该发现出乎意料，表明输入频谱长度存在问题。</p>
<p>我们发现了相位在研究中的重要性。如果 <span class="math inline">\(phase_x\)</span> 和转换后的 <span class="math inline">\(\hat x\)</span> 相互对齐，问题就可以解决。为了解决这个问题，我们必须使用复杂的神经网络来对齐原始频谱中包含的幅度和相位。如果可以根据转换后的幅度进行相位对齐，将提高人声的质量。将有可能将语音识别性能较差的少数民族的声音转换为普通成年人的声音。可以通过说话人自适应替代少数民族和普通成年人的特征来实现更好的语音识别性能。我们将研究相位适应和与量级的对齐作为我们的下一个任务。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Towards end-to-end F0 voice conversion based on Dual-GAN with convolutional wavelet kernels</title>
    <url>/2021/08/23/f0-vc/</url>
    <content><![CDATA[<h1 id="基于带卷积小波核的-dual-gan-实现端到端的-f0-语音转换">基于带卷积小波核的 Dual-GAN 实现端到端的 F0 语音转换</h1>
<h2 id="摘要">摘要</h2>
<p>本文提出了一种在表达性语音转换的背景下进行 F0 转换的端到端框架。 提出了一个单一的神经网络，其中第一个模块用于学习不同时间尺度上的 F0 表示，第二个对抗模块用于学习从一种情绪到另一种情绪的转换。 第一个模块由带有小波内核的卷积层组成，因此可以有效地编码 F0 变化的各种时间尺度。 单个分解/转换网络允许以端到端的方式直接从原始 F0 信号中学习对于转换而言最佳的 F0 分解。</p>
<h2 id="introduction">Introduction</h2>
<p>基频 (F0) 是人类语音交流和人机交互中必不可少的声学特征。 F0作为语音韵律的一个关键特征，在语音交际的各个方面都发挥着重要作用：它传达语言信息（F0有助于阐明话语的句法结构或用于语义强调），情感或社会态度等副语言信息，甚至通过他的说话风格成为说话者身份的一部分。因此，因此，生成式 F0 建模在文本到语音、语音身份转换 (VC) 和表达性语音转换领域非常有用，通过允许直接和参数控制F0 来操纵声音的表现力（例如说话风格或情绪）。从本质上讲，F0 变化发生在不同的时间尺度上，每个尺度都与特定功能相关，从微观变化到宏观轮廓，如强调、情绪和模式。为了涵盖 F0 建模的这些特性，已经提出了风格化方法和多级建模。</p>
<p>值得注意的是，在 VC 中，诸如高斯混合模型 和基于 LSTM 的序列到序列模型等生成模型被用于学习从中性语音到表达性语音的 F0 转换。 最近，各种工作集中在使用连续小波变换 (CWT) 作为 F0 的中间表示，在其上使用生成对抗网络 (GAN) 模型，例如 Dual-GAN、Cycle-GAN、VAW -GAN 或 VA-GAN 被训练来学习转换。 这些模型中的大多数是在并行数据和情感对上学习的，这允许学习话语的两种不同情感版本之间的直接映射，同时保留固定和受控的语言内容。</p>
<p>罗等人提出了一种称为 CWT 自适应尺度（CWT-AS）的有前途的方法。 CWT 在小波核上计算 F0 信号的分解，这允许在不同时间尺度上表示 F0，在表达性语音转换中具有各种应用。 使用 CWT 的 F0 建模最近被指定为可以在任意语言尺度（例如，音素、音节、单词和话语）上计算分解的可能性。 自适应尺度 (AS) 算法被描述为通过选择使 CWT 空间中情绪之间的平均距离最大化的尺度，为每对情绪选择最佳 CWT 表示。</p>
<p>根据这些选定的尺度，计算 F0 轮廓的 CWT 分解。 最后，使用 Dual-GAN 从这些表示中学习每对情感之间的转换函数。 尽管这种方法看起来很有前景，但它有两个主要局限性：1）尺度选择仅基于情绪之间距离的最大化，而忽略了它们对 F0 信号的重建能力。 这可能会导致 F0 重建不佳，进而降低转换的质量和自然度； 2）F0信号的CWT-AS分解和dual-GAN是独立优化的，这构成了训练的瓶颈。 因此，就双 GAN 目标而言，CWT 分解可能不是最佳的。</p>
<p>为了克服这些限制，我们提出了一种端到端架构来有效地学习情绪之间的 F0 转换。 所提出的神经架构将 F0 分解和双 GAN 结合到一个网络中，从而在双 GAN 目标意义上优化 CWT 分解，并结合所得分解的分离和重建损失。 对社会态度语音转换的应用表明，与 CWT-AS 方法相比，所提出的方法显着提高了转换的质量。</p>
<h2 id="提案方法">提案方法</h2>
<p>在本节中，我们将介绍我们基于 CWT-AS 的提案，并通过在转换学习过程之上集成 F0 风格化部分来展示它的不同之处，我们将其称为语音 f0 转换的端到端方法 Ⅱ-A。 我们的贡献的概念和技术细节在 II-B 和 II-C 中给出。</p>
<h3 id="框架概述">框架概述</h3>
<p>由于我们提出的 VC 系统需要并行数据，因此考虑了分别与表达性 <span class="math inline">\(a\)</span> 和 <span class="math inline">\(b\)</span> 相关的话语集 <span class="math inline">\(X_a\)</span> 和 <span class="math inline">\(X_b\)</span>。 然后对一对话语进行采样并提取 <span class="math inline">\(F_0\)</span> 序列，源 <span class="math inline">\(x_a\)</span> 和目标 <span class="math inline">\(x_b\)</span>。 除了表达性之外，一对中的每个话语都具有相同的内容（语言内容、说话者身份）。 源和目标 <span class="math inline">\(F_0\)</span> 被赋予我们称为 Wevelet 小波核卷积编码器 (WKCE)。 一个分类器，表示为 C，其目标是预测表达能力，由 WKCE 输出提供。 如图 1 所示，这两个模块必须被视为 Dual-GAN (DG) 的预网络 (<span class="math inline">\(pN\)</span>)，可以进行预训练，也可以与 Dual-GAN 一起训练形成端到端系统 用于 f0 转换。</p>
<h3 id="小波核卷积自编码器">小波核卷积自编码器</h3>
<p>作为一种多尺度建模方法，CWT 在尝试表示长期和短期依赖性时完全适合，韵律受其影响。 由于 CWT 只能应用于连续函数，因此需要在浊音 F0 段之间进行简单的线性插值以获得与短语相关的连续 F0 函数，然后可以在向量 <span class="math inline">\(x ∈ [0, 1]^T\)</span> 中对其进行采样。</p>
<p>我们的 WKCE 基于为时间向量 <span class="math inline">\(t ∈ R^T\)</span> 定义的母小波 <span class="math inline">\(ψ_s ∈ R^T\)</span> 在 F0 信号 <span class="math inline">\(x\)</span> 和小波核之间执行卷积</p>
<div class="figure">
<img src="/images/f0-vc-al1.png" alt="f0-vc-al1" />
<p class="caption">f0-vc-al1</p>
</div>
<p>考虑到小波内核依赖于控制组成内核的每个小波的宽度的 <span class="math inline">\(N\)</span> 个可学习参数 <span class="math inline">\(s\)</span>，时间级别 <span class="math inline">\(s\)</span> 对 F0 信号 <span class="math inline">\(x\)</span>的贡献 <span class="math inline">\(h^s_x\)</span> 是 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(ψ_s\)</span> 之间的卷积。 因此，输入 <span class="math inline">\(x\)</span>，我们的 WKCE 模块将输出 <span class="math inline">\(W_e (x) = [h^{s_0}_x,...,h^{s_N}_x ] ∈ R^{N×T}\)</span>。 如果我们将 <span class="math inline">\(W_r\)</span> 表示为重建操作，则重建信号 <span class="math inline">\(\hat x\)</span> 由下式给出</p>
<div class="figure">
<img src="/images/f0-vc-al2.png" alt="f0-vc-al2" />
<p class="caption">f0-vc-al2</p>
</div>
<p><span class="math inline">\(x ̄\)</span> 是 <span class="math inline">\(x\)</span> 的平均值，<span class="math inline">\(d_t = 1.2\)</span>，<span class="math inline">\(d_j = 0.125\)</span>，<span class="math inline">\(C_d = 3.541\)</span> 和 <span class="math inline">\(Y_0 = 0.867\)</span>（详情参见 [17]）</p>
<p>如果我们表示 <span class="math inline">\(E\)</span>，数学期望并考虑分别从源分布和目标分布 <span class="math inline">\(P (x^a )\)</span> 和 <span class="math inline">\(P (x^b )\)</span> 采样的 <span class="math inline">\(x^a\)</span> 和 <span class="math inline">\(x^b\)</span>，则可以针对 L1 损失训练该模块以实现重建目标，公式如下</p>
<div class="figure">
<img src="/images/f0-vc-al3.png" alt="f0-vc-al3" />
<p class="caption">f0-vc-al3</p>
</div>
<p>可以添加对 CWTs 潜在空间的分类约束，<span class="math inline">\(W_e\)</span> 和 <span class="math inline">\(C\)</span> 是针对 <span class="math inline">\(L_{cl}\)</span>、预测源表达性 <span class="math inline">\(\hat a = C(W_e(x^a))\)</span> 和真实值 <span class="math inline">\(a\)</span> 之间的交叉熵 (CE) 损失进行训练的 与 <span class="math inline">\(\hat b\)</span> 和 <span class="math inline">\(b\)</span> 之间的 CE 相加。</p>
<div class="figure">
<img src="/images/f0-vc-al4.png" alt="f0-vc-al4" />
<p class="caption">f0-vc-al4</p>
</div>
<h3 id="模型">模型</h3>
<p>在本文中，我们专注于称为 Dual-GAN 的特定 GAN 网络，它能够学习并行数据对之间的映射。该网络基于两个概念：1）对抗性学习，即训练生成模型在两个神经网络（称为生成器 G 和判别器 D）之间的最小-最大博弈中找到解决方案。2）双重监督学习即同时训练两个双重任务的模型，利用它们之间的概率相关性来规范训练过程。结合这些突破，可以利用 GAN 产生现实转换的能力以及由于双重监督学习带来的显着改进。</p>
<p>这第二点意味着正向和逆向变换，分别是 <span class="math inline">\(G_{a→b} : (W_e(x^a), z^a) → x^b\)</span> 和 <span class="math inline">\(G_{b→a} : (W_e(x^b, z^b) → x^a\)</span>, 是联合学习的，其中 <span class="math inline">\(z^a\)</span> 和 <span class="math inline">\(z^b\)</span> 是在 <span class="math inline">\(G_a\)</span> 和 <span class="math inline">\(G_b\)</span> 的每一层以 dropout 形式提供的随机独立噪声。需要第一个损失 <span class="math inline">\(L_{a↔b}\)</span> 来训练 <span class="math inline">\(G_{a→b}\)</span>、<span class="math inline">\(G_{b→a}\)</span> 和 <span class="math inline">\(W_e\)</span>。</p>
<div class="figure">
<img src="/images/f0-vc-al5.png" alt="f0-vc-al5" />
<p class="caption">f0-vc-al5</p>
</div>
<p>同时，<span class="math inline">\(D_a\)</span> 区分 <span class="math inline">\(G_{a→b}\)</span> 的转换输出 <span class="math inline">\(\hat X^b\)</span> 和域 <span class="math inline">\(X_b\)</span> 的真实样本，<span class="math inline">\(D_b\)</span> 类似地完成对抗机制。 训练 <span class="math inline">\(G_{a→b}\)</span>、<span class="math inline">\(G_{b→a}\)</span>、<span class="math inline">\(D_a\)</span>、<span class="math inline">\(D_b\)</span> 和 <span class="math inline">\(W_e\)</span> 需要对抗性损失 <span class="math inline">\(L_{ADV}\)</span></p>
<div class="figure">
<img src="/images/f0-vc-al6.png" alt="f0-vc-al6" />
<p class="caption">f0-vc-al6</p>
</div>
<p>添加了第三个约束，称为 Dual loss，以加强 <span class="math inline">\(G_{a→b}\)</span> 和 <span class="math inline">\(G_{b→a}\)</span>之间的内在联系，可以理解为过程的正则化。</p>
<div class="figure">
<img src="/images/f0-vc-al7.png" alt="f0-vc-al7" />
<p class="caption">f0-vc-al7</p>
</div>
<p>因此，可以为 pre-Net 预训练和适当的 Dual-GAN 训练制定两个最终损失，分别是 <span class="math inline">\(L_{pN}\)</span> 和 <span class="math inline">\(L_{DG}\)</span>，分别具有 <span class="math inline">\(α\)</span>、<span class="math inline">\(β\)</span>、<span class="math inline">\(λ\)</span> 和 <span class="math inline">\(γ\)</span> 加权重建、分类、转换和双重目标。</p>
<div class="figure">
<img src="/images/f0-vc-al8.png" alt="f0-vc-al8" />
<p class="caption">f0-vc-al8</p>
</div>
<h2 id="结论">结论</h2>
<p>在本文中，我们提出了一种在表达性语音转换的上下文中进行 F0 转换的端到端框架，将不同时间级别的 F0 分解及其在单个网络中的转换结合在一起。 客观和主观评估都表明我们的方法可以实现比基线更好的性能。 我们旨在推广多说话人 F0 转换，并通过构建表达性嵌入来避免配对学习。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>gnn-tts-papers</title>
    <url>/2021/08/24/gnn-tts-papers/</url>
    <content><![CDATA[<h1 id="graph-neural-network-tts-paper-list">Graph neural network TTS paper list</h1>
<ol style="list-style-type: decimal">
<li><a href="https://arxiv.org/pdf/1904.04764.pdf">Exploiting Syntactic Features in a Parsed Tree to Improve End-to-End TTS</a> | Interspeech 2019 | 西北工业大学谢磊团队</li>
<li><a href="https://arxiv.org/pdf/2003.01924.pdf">GraphTTS</a> | ICASSP 2020 | 平安科技TTS团队</li>
<li><a href="https://arxiv.org/pdf/2010.12423.pdf">GraphSpeech</a> | ICASSP 2021 | 新加坡国立大学 李海洲团队</li>
<li><a href="https://arxiv.org/pdf/2012.02626.pdf">GraphPB</a> | SLT 2021 | 平安科技TTS团队</li>
<li><a href="https://www1.se.cuhk.edu.hk/~hccl/publications/pub/202106_ICASSP_ChangheSONG.pdf">SYNTACTIC REPRESENTATION LEARNING FOR NEURAL NETWORK BASED TTS WITH SYNTACTIC PARSE TREE TRAVERSAL</a> | ICASSP 2021 | 清华大学深研院 吴致勇团队</li>
<li><a href="https://arxiv.org/pdf/2104.06835v3.pdf">Enhancing Word-Level Semantic Representation via Dependency Structure for Expressive Text-to-Speech Synthesis</a> | Interspeech 2021? | 清华大学深研院 吴致勇团队</li>
<li><a href="https://arxiv.org/pdf/2204.11792.pdf">SyntaSpeech: Syntax-Aware Generative Adversarial Text-to-Speech</a> | IJCAI 2022 | 浙江大学 赵洲 任意团队</li>
<li><a href="https://arxiv-export1.library.cornell.edu/pdf/2203.15276v1">Applying Syntax–Prosody Mapping Hypothesis and Prosodic Well-Formedness Constraints to Neural Sequence-to-Sequence Speech Synthesis</a> | Japan</li>
</ol>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ADVERSARIALLY LEARNING DISENTANGLED SPEECH REPRESENTATIONS FOR ROBUST MULTI-FACTOR VOICE CONVERSION</title>
    <url>/2021/08/24/multi-factor-vc/</url>
    <content><![CDATA[<h1 id="对抗性学习分离的语音表示，用于稳健的多因素语音转换"><a href="#对抗性学习分离的语音表示，用于稳健的多因素语音转换" class="headerlink" title="对抗性学习分离的语音表示，用于稳健的多因素语音转换"></a>对抗性学习分离的语音表示，用于稳健的多因素语音转换</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>将语音分解为解开的语音表示对于在语音转换 (VC) 中实现高度可控的风格转换至关重要。 VC中传统的语音表征学习方法仅将语音分解为说话者和内容，对其他与韵律相关的因素缺乏可控性。针对更多语音因素的最先进的语音表示学习方法正在使用主要的解缠结算法，例如随机重采样（random resampling）和临时瓶颈层大小调整，但是很难确保稳健的语音表示解缠结。为了提高 VC 中多因素高度可控风格迁移的鲁棒性，我们提出了一种基于对抗性学习的解开语音表示学习框架。提取表征内容、音色、节奏和音高的四种语音表示，并通过受 BERT 启发的对抗网络进一步解开。对抗网络用于通过从其他表示中随机屏蔽和预测一个表示来最小化语音表示之间的相关性。还采用单词预测网络来学习更多信息的内容表示。实验结果表明，与最先进的方法相比，所提出的语音表示学习框架通过将转换率从 48.2% 提高到 57.1% 和 ABX 偏好超过 31.2%，显着提高了 VC 在多个因素上的鲁棒性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>语音转换 (VC) 旨在将源说话者的输入语音转换为目标说话者所说的声音，而不改变语言内容 。 除了音色的转换外，还可以在韵律、音高、节奏或其他非语言领域等各种领域进行转换。 针对这些语音因素的表征学习方法已经被提出并应用于语音处理的许多研究领域。但是，将这些方法提取的语音表征直接应用到 VC 中 可能会导致其他语音因素的意外转换，因为它们可能不一定是正交的。 因此，解开语音信号中各种信息因素混合的表示对于实现高度可控的 VC 至关重要。</p>
<p>传统上，只有说话人和内容信息在 VC 中被分解。由编码器和解码器组成的自动编码器被提出并广泛用于 VC 。在训练期间，解码器从说话者和从编码器或其他预训练提取器提取的内容表示中重建语音。<strong>基于变分自编码器的方法将内容信息的潜在空间建模为高斯分布</strong>以追求正则化特性。进一步提出了基于矢量量化的方法，将内容信息建模为与语音信息分布更相关的离散分布。采用辅助对抗说话人分类器，通过最小化表示之间的互信息，鼓励编码器从内容信息中丢弃说话人信息。</p>
<p>为了克服在传统 VC 中替换说话人表示时韵律也被转换的情况，应用不同的信息瓶颈将说话人信息分解为音色和其他韵律相关因素，如节奏和音高。为了改善解纠缠，瓶颈层的受限尺寸鼓励编码器丢弃可以从其他瓶颈中学习的信息。还建议在信息瓶颈中使用随机重采样以从内容和音高表示中去除节奏信息。</p>
<p>然而，如果没有明确的解缠建模，随机重采样和限制瓶颈层的大小只能获得有限的语音表示解缠。 <strong>随机重采样通常被实现为使用时间维度上的线性插值分割和重采样语音段，只能用于去除与时间相关的信息，例如节奏。</strong> 此外，随机重采样被证明是一种部分解缠结算法，它只能污染节奏信息的随机部分。 此外，瓶颈层的大小需要仔细设计，以提取临时且可能不适合其他数据集的解开语音表示。 而内容编码器实际上是一个残差编码器，不能保证内容信息只在内容表示中建模。</p>
<p>在本文中，为了实现多因素 VC 的鲁棒性和高度可控的风格迁移，我们提出了一种基于对抗性学习的解开语音表示学习框架。 所提出的框架通过受 BERT 启发的对抗性网络明确地消除了表征不同语音因素的语音表示之间的相关性。 语音首先被分解为四个语音表示，分别代表内容、音色和另外两个与韵律相关的因素，节奏和音高。在训练期间，对抗性掩码和预测 (MAP: mask-and-predict) 网络将随机掩蔽其中一个语音表示并从其余表示中推断出来。 MAP 网络被训练以最大化掩码和剩余表示之间的相关性，而语音表示编码器被训练以通过采用 MAP 网络的反向梯度来最小化相关性。通过这种方式，表示学习框架以对抗方式进行训练，语音表示编码器试图解开表示，而 MAP 网络试图最大化表示相关性。单词预测网络用于从内容表示中预测单词存在向量，内容表示指示每个词汇是否存在于参考语音中。解码器在训练期间根据表示重构语音，并通过替换相应的语音表示在多个因素上实现 VC。</p>
<p>实验结果表明，所提出的语音表示学习框架显着提高了 VC 在多个因素上的鲁棒性，与最先进的语音表示学习相比，转换率从 48.2% 提高到 57.1%，ABX 偏好超过 31.2% 多因素的方法。 此外，所提出的框架还避免了用于复杂瓶颈调整的费力手动工作。</p>
<h2 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h2><p><img src="/images/multi-factor-vc-fig1.png" alt="multi-factor-vc-fig1"></p>
<p>我们提出的解开语音表示学习框架，如图 1 所示，由三个子网络组成：（i）(multiple speech representation encoders) 多个语音表示编码器，将语音编码为不同的语音表示，表征内容、音色、节奏和音调，（ii) (Adversial mask-and-predict network) 一个对抗性 MAP 网络，它被训练来基于掩码和预测操作 (mask-and-predict) 来捕获不同语音表示之间的相关性，(iii) (Auxilliary word prediction network)一个辅助词预测网络，它预测一个二进制词存在向量，指示内容表示是否包含相应的词汇的话。 最后，使用解码器从这些解开的语音表示中合成语音。</p>
<h3 id="2-1-语音表示学习"><a href="#2-1-语音表示学习" class="headerlink" title="2.1 语音表示学习"></a>2.1 语音表示学习</h3><p>SpeechFlow 中的三个编码器经过微调，可以从帧级的参考语音中提取节奏、音调和内容表示。 One-hot 说话者标签（ID）嵌入在话语级别并用作音色表示。</p>
<h3 id="2-2-语音表示解纠缠的对抗学习"><a href="#2-2-语音表示解纠缠的对抗学习" class="headerlink" title="2.2 语音表示解纠缠的对抗学习"></a>2.2 语音表示解纠缠的对抗学习</h3><p>受 BERT 启发的对抗性 MAP 网络旨在明确解开提取的语音表示。 在训练期间，这四种语音表示中的一种被随机屏蔽，对抗网络从其他表示中推断出屏蔽后的表示。 对抗网络由梯度反向层 和一堆预测头层 组成，它们也已用于掩蔽声学建模。 每个预测头层由一个全连接层、GeLU 激活层、层归一化 和另一个全连接层组成，如图 1(b) 所示。 在反向传播到语音表示编码器之前，对抗网络的梯度被梯度反转层 [20] 反转。 这里采用 L1 损失来衡量以下等式中展示的对抗性损失：</p>
<p><img src="/images/multi-factor-vc-al1.png" alt="multi-factor-vc-al1"></p>
<p>其中 $⊙$ 是逐元素乘积运算，$L_{adversarial}$ 是对抗性损失，$Z$ 是 $Z_r$、$Z_c$、$Z_f$、$Z_u$ 的串联，分别表示<strong>节奏、内容、音高和音色表示，</strong>M 是随机选择的对应于丢弃的二进制掩码 在表示被丢弃的地方值为 0 的区域，对于未屏蔽的表示，值为 1。</p>
<p>MAP 网络被训练为通过最小化对抗性损失来尽可能准确地预测掩码表示，而在反向传播中，梯度是反向的，这鼓励编码器学习的表示包含尽可能少的互信息。</p>
<h3 id="2-3-辅助词预测网络"><a href="#2-3-辅助词预测网络" class="headerlink" title="2.3 辅助词预测网络"></a>2.3 辅助词预测网络</h3><p>为了避免内容信息被编码到其他表示中，我们设计了一个辅助词预测网络来从内容表示中预测每个词汇表的存在。 词预测网络是一堆预测头层，它产生一个二进制的 词汇表尺寸 向量，其中每个维度表示该句子中是否存在相应的词汇词。 词存在向量表示为 $V_{word} =[v_1,v_2,…,v_n]$ 其中 $v_i = 1$ 如果词 $i $在语音中，否则 $v_i = 0$。这里应用交叉熵损失以强制内容预测尽可能准确：</p>
<p><img src="/images/multi-factor-vc-al2.png" alt="multi-factor-vc-al2"></p>
<p>其中 $v_i^{‘}$ 是预测的单词存在指示符，$n$ 是词汇量的大小。 $v_i^{‘} = 1$ 如果单词 $i $被预测存在，否则 $v_i^{‘} = 0$。它旨在确保内容表示更具信息性并避免内容信息泄漏到其他表示中。 在语音转换和文本到语音系统中使用了类似的内容保留策略，这被证明是有效的并且可以提高性能。</p>
<h3 id="2-4-具有解纠缠语音表示的-VC"><a href="#2-4-具有解纠缠语音表示的-VC" class="headerlink" title="2.4. 具有解纠缠语音表示的 VC"></a>2.4. 具有解纠缠语音表示的 VC</h3><p>SpeechFlow 中的解码器用于从解开的语音表示中生成梅尔谱图。 在训练期间，从相同的话语中提取四个语音表示，并训练解码器从语音表示重建梅尔谱图，损失函数定义为以下等式：</p>
<p><img src="/images/multi-factor-vc-al3.png" alt="multi-factor-vc-al3"></p>
<p>其中 $S$ 和 $\hat S$ 分别是输入和重建语音的梅尔谱图。 整个模型使用定义为以下等式的损失进行训练：</p>
<p><img src="/images/multi-factor-vc-al4.png" alt="multi-factor-vc-al4"></p>
<p>其中$α$、$β$、$γ$分别是对抗性损失、词预测损失和重建损失的损失权重。 为了提高我们提出的框架的鲁棒性，重建损失的损失权重被设计为指数衰减。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>为了提高 VC 中多因素高度可控风格迁移的鲁棒性，我们提出了一种基于对抗性学习的解开语音表示学习框架。 我们提取了四种表征内容、音色、节奏和音调的语音表示，我们采用了一个受 BERT 启发的对抗网络来进一步解开语音表示。 我们使用单词预测网络来学习更多信息的内容表示。 实验结果表明，所提出的语音表示学习框架显着提高了 VC 在多个因素上的鲁棒性。 在未来的工作中将探索不同的掩蔽策略。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>readling_list</title>
    <url>/2021/08/26/readling-list/</url>
    <content><![CDATA[<h1 id="reading-list">Reading list</h1>
<p>2021-8-26</p>
<ol style="list-style-type: decimal">
<li>LVCNET: EFFICIENT CONDITION-DEPENDENT MODELING NETWORK FOR WAVEFORM GENERATION</li>
<li>Graph Neural Networks for Natural Language Processing: A Survey</li>
</ol>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>interspeech2021</title>
    <url>/2021/08/31/interspeech2021/</url>
    <content><![CDATA[<h1 id="interspeech-2021">Interspeech 2021</h1>
<h2 id="清华深研院---吴致勇教授团队">清华深研院 - 吴致勇教授团队</h2>
<h3 id="语音合成">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/li21r_interspeech.html">Towards Multi-Scale Style Control for Expressive Speech Synthesis</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/lu21d_interspeech.html">VAENAR-TTS: Variational Auto-Encoder Based Non-AutoRegressive Text-to-Speech Synthesis</a></p>
<h3 id="音色转换">音色转换</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21h_interspeech.html">Adversarially Learning Disentangled Speech Representations for Robust Multi-Factor Voice Conversion</a></p>
<h3 id="其他方向">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wu21i_interspeech.html">Voting for the Right Answer: Adversarial Defense for Speaker Verification</a></p>
<h2 id="西北工业大学---谢磊教授团队">西北工业大学 - 谢磊教授团队</h2>
<h3 id="语音合成-1">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/cong21_interspeech.html">Glow-WaveGAN: Learning Speech Representations from GAN-Based Variational Auto-Encoder for High Fidelity Flow-Based Speech Synthesis</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/cong21b_interspeech.html">Controllable Context-Aware Conversational Speech Synthesis</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/an21b_interspeech.html">Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS</a></p>
<h3 id="音色转换-1">音色转换</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21g_interspeech.html">Enriching Source Style Transfer in Recognition-Synthesis Based Non-Parallel Voice Conversion</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/du21_interspeech.html">Improving Robustness of One-Shot Voice Conversion with Deep Discriminative Speaker Encoder</a></p>
<h3 id="其他方向-1">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/lv21_interspeech.html">DCCRN+: Channel-Wise Subband DCCRN with SNR Estimation for Speech Enhancement</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/fu21b_interspeech.html">AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/guo21_interspeech.html">Multi-Speaker ASR Combining Non-Autoregressive Conformer CTC and Conditional Speaker Chain</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/yao21_interspeech.html">WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21ea_interspeech.html">Auto-KWS 2021 Challenge: Task, Datasets, and Baselines</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21ha_interspeech.html">Efficient Conformer with Prob-Sparse Attention Mechanism for End-to-End Speech Recognition</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21ia_interspeech.html">F-T-LSTM Based Complex Network for Joint Acoustic Echo Cancellation and Speech Enhancement</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21g_interspeech.html">Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification</a></p>
<h2 id="microsoft---谭旭团队">Microsoft - 谭旭团队</h2>
<h3 id="语音合成-2">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/yan21d_interspeech.html">Adaptive Text to Speech for Spontaneous Style</a></p>
<h3 id="音色转换-2">音色转换</h3>
<h3 id="其他方向-2">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/hou21b_interspeech.html">Cross-Domain Speech Recognition with Unsupervised Character-Level Distribution Matching</a></p>
<h2 id="google---heiga-zen-团队">Google - Heiga Zen 团队</h2>
<h3 id="语音合成-3">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/chen21p_interspeech.html">WaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/elias21_interspeech.html">Parallel Tacotron 2: A Non-Autoregressive Neural TTS Model with Differentiable Duration Modeling</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/jia21_interspeech.html">PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS</a></p>
<h3 id="音色转换-3">音色转换</h3>
<h3 id="其他方向-3">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/chen21c_interspeech.html">Semi-Supervision in ASR: Sequential MixMatch and Factorized TTS-Based Augmentation</a></p>
<h2 id="新加坡国立大学---李海洲团队">新加坡国立大学 - 李海洲团队</h2>
<h3 id="语音合成-4">语音合成</h3>
<h3 id="音色转换-4">音色转换</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhou21b_interspeech.html">Limited Data Emotional Voice Conversion Leveraging Text-to-Speech: Two-Stage Sequence-to-Sequence Training</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhou21c_interspeech.html">Cross-Lingual Voice Conversion with a Cycle Consistency Loss on Linguistic Representation</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/liu21p_interspeech.html">Reinforcement Learning for Emotional Text-to-Speech Synthesis with Improved Emotion Discriminability</a></p>
<h3 id="其他方向-4">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhu21c_interspeech.html">Serialized Multi-Layer Multi-Head Attention for Neural Speaker Embedding</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21b_interspeech.html">Temporal Convolutional Network with Frequency Dimension Adaptive Attention for Speech Enhancement</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/yue21_interspeech.html">Phonetically Motivated Self-Supervised Speech Representation Learning</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/das21_interspeech.html">Diagnosis of COVID-19 Using Auditory Acoustic Cues</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/borsdorf21_interspeech.html">Universal Speaker Extraction in the Presence and Absence of Target Speakers for Speech of One and Two Talkers</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21aa_interspeech.html">Neural Speaker Extraction with Speaker-Speech Cross-Attention Network</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/borsdorf21b_interspeech.html">GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/jiang21c_interspeech.html">Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification</a></p>
<h3 id="合作论文">合作论文</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhang21g_interspeech.html">Multi-Level Transfer Learning from Near-Field to Far-Field Speaker Verification</a></p>
<h2 id="北京大学-深研院---邹月娴团队">北京大学 深研院 - 邹月娴团队</h2>
<h3 id="spoken-dialogue-systems">Spoken Dialogue Systems</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/chen21_interspeech.html">Self-Supervised Dialogue Learning for Spoken Conversational Question Answering</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/xu21_interspeech.html">Semantic Transportation Prototypical Network for Few-Shot Intent Detection</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21d_interspeech.html">SpecAugment++: A Hidden Space Data Augmentation Method for Acoustic Scene Classification</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/yang21b_interspeech.html">Unsupervised Multi-Target Domain Adaptation for Acoustic Scene Classification</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/you21c_interspeech.html">Contextualized Attention-Based Knowledge Transfer for Spoken Conversational Question Answering</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21da_interspeech.html">Text Anchor Based Metric Learning for Small-Footprint Keyword Spotting</a></p>
<h2 id="ustc-中科大---凌震华团队">USTC 中科大 - 凌震华团队</h2>
<h3 id="语音合成-5">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/zhou21f_interspeech.html">UnitNet-Based Hybrid Speech Synthesis</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/chen21s_interspeech.html">A Neural-Network-Based Approach to Identifying Speakers in Novels</a></p>
<h3 id="音色转换-5">音色转换</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/ding21_interspeech.html">Adversarial Voice Conversion Against Neural Spoofing Detectors</a></p>
<h2 id="台湾国立大学---李宏毅团队">台湾国立大学 - 李宏毅团队</h2>
<h3 id="音色转换-6">音色转换</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/lin21b_interspeech.html">S2VC: A Framework for Any-to-Any Voice Conversion with Self-Supervised Pretrained Representations</a></p>
<h3 id="其他方向-5">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/yang21c_interspeech.html">SUPERB: Speech Processing Universal PERformance Benchmark</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/chang21b_interspeech.html">Towards Lifelong Learning of End-to-End ASR</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wang21ea_interspeech.html">Auto-KWS 2021 Challenge: Task, Datasets, and Baselines</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/huang21h_interspeech.html">Stabilizing Label Assignment for Speech Separation by Self-Supervised Pre-Training</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/wu21i_interspeech.html">Voting for the Right Answer: Adversarial Defense for Speaker Verification</a></p>
<h3 id="metric">Metric</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/tseng21b_interspeech.html">Utilizing Self-Supervised Representations for MOS Prediction</a></p>
<h2 id="uoe-爱丁堡大学---simon-king-团队">UoE 爱丁堡大学 - Simon King 团队</h2>
<h3 id="语音合成-6">语音合成</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/valentinibotinhao21_interspeech.html">Detection and Analysis of Attention Errors in Sequence-to-Sequence Text-to-Speech</a></p>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/mohan21_interspeech.html">Ctrl-P: Temporal Control of Prosodic Variation for Speech Synthesis</a></p>
<h3 id="其他方向-6">其他方向</h3>
<p><a href="https://www.isca-speech.org/archive/interspeech_2021/torresquintero21_interspeech.html">ADEPT: A Dataset for Evaluating Prosody Transfer</a></p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>MOSNet Deep Learning-based Objective Assessment for Voice Conversion</title>
    <url>/2021/09/07/mosnet/</url>
    <content><![CDATA[<h1 id="mosnet基于深度学习的语音转换目标评估">MOSNet：基于深度学习的语音转换目标评估</h1>
<h2 id="摘要">摘要</h2>
<p>现有的语音转换（VC）客观评估指标并不总是与人类感知相关。因此，使用此类标准训练 VC 模型可能无法有效提高转换后语音的自然度和相似度。在本文中，我们提出了基于深度学习的评估模型来预测人类对转换语音的评分。我们采用卷积和循环神经网络模型来构建平均意见得分 (MOS) 预测器，称为 MOSNet。提出的模型在 Voice Conversion Challenge (VCC) 2018 的大规模听力测试结果上进行了测试。实验结果表明，所提出的 MOSNet 的预测分数与系统级别的人类 MOS 评分高度相关，同时在话语级别与人类 MOS 评分相当相关。同时，我们修改了 MOSNet 来预测相似度分数，初步结果表明预测分数也与人类评分相当相关。这些结果证实，所提出的模型可以用作计算评估器来测量 VC 系统的 MOS，以减少对昂贵的人工评估的需求。</p>
<h2 id="introduction">Introduction</h2>
<p>生成语音的质量量化一直是语音合成、语音增强和语音转换 (VC) 系统中长期存在的问题。这些系统的评估报告了客观和主观测量结果。在 VC 社区中，诸如梅尔倒谱距离 (MCD) 之类的客观度量被广泛用于自动测量转换语音的质量。然而，这些指标并不总是与人类感知相关，因为它们主要测量声学特征的失真。诸如平均意见分数 (MOS) 和相似度分数等主观衡量标准可以代表 VC 系统的内在自然性和相似性，但这些类型的评估既耗时又昂贵，因为它们需要大量参与者进行听力测试并提供感性评分。</p>
<p>已经提出了许多评估算法和模型来克服上述问题。例如，在语音增强领域，ITU-T 发布的语音质量感知评估（PESQ）是衡量增强语音质量的侵入性评估，因为评估需要黄金参考。有几个非侵入式评估指标 用于评估增强语音和合成语音的质量。例如，傅等人提出的 Quality-Net 作为基于双向长短期记忆 (BLSTM) 的质量评估模型，以帧方式预测增强语音的话语级质量。预测分数与 PESQ 分数之间的高度相关性证实了其作为语音增强的非侵入性评估模型的有效性。吉村等人提出了一种基于全连接神经网络和卷积神经网络（CNN）的合成语音自然度预测器来预测话语级和系统级 MOS。该模型是在手工制作的特征上进行的，并使用大规模听力测试评级进行训练。值得注意的是，他们都报告了话语级别的人类评分存在很大差异，因为听力测试是主观的，听众可能会对同一话语提供不同的评分。因此，评估模型很难获得与人类话语级别评级的高度相关性。尽管如此，系统级预测还是相对可靠的。以前的工作已经展示了神经网络在模拟人类感知以增强合成语音方面的能力。但是，对于 VC 系统没有这样的评估模型。我们的目标是使用语音转换挑战赛 (VCC) 2018 的大规模听力测试结果为 VC 系统开发语音自然度和相似性评估模型。</p>
<p>在本文中，我们提出了一种新的基于深度学习的端到端语音自然度评估模型，称为 MOSNet。为了开发这样的客观措施来建模并与人类主观评级保持一致，我们研究了卷积神经网络 (CNN)、双向长短期记忆 (BLSTM) 和 CNN-BLSTM，因为这些架构已经显示出它们的建模能力人类的感知。我们使用这样的架构来提取有价值的特征，然后使用全连接（FC）层来预测相应的自然性分数。凭借神经网络的能力和VCC 2018的大规模人类自然性评估，我们的自然性评估模型的MOS预测在系统层面实现了与人类MOS评级的高度相关性和话语层面的公平相关性。此外，我们修改了 MOSNet 来预测相似度分数，初步结果表明，预测的相似度分数与人类相似度评分相当相关。实验结果表明，我们提出的模型具有很高的能力来衡量 VC 系统的语音自然度和相似度。据我们所知，这是第一个基于深度学习的 VC 语音质量和相似性评估模型。</p>
<p>本文的组织如下：第 2 节描述了来自 VCC 2018 的数据及其分布。第 3 节介绍了所提出的模型。第 4 节讨论了实验和结果。最后，结论和未来的工作是 在第 5 节中介绍。</p>
<h2 id="vcc评估数据">VCC评估数据</h2>
<h3 id="vcc-2018">2.1 VCC 2018</h3>
<p>语音转换挑战赛 (VCC) 2018 是 VCC 的第二版，是一项大规模的语音转换挑战赛。 VCC 2018 语料库是通过从设备和生产语音 (DAPS) 数据集中选择一部分说话者来准备的，这些数据集由美国专业英语演讲者在干净无噪音的环境中记录。挑战的参与者需要使用他们的 VC 系统通过并行或非并行训练数据进行训练，将语音信号从源说话者转换为目标说话者。所有参与挑战的并行和非并行 VC 系统都通过众包听力测试在自然度和相似度方面进行了评估。</p>
<p>VCC 2018 的评估如下：有 2,572 个评估集，每个由 44 个话语组成。总共 113,168 次人工评估完全涵盖了 28,292 个提交的音频样本。每个音频样本由 4 位听众评分。 113,168 项评估分为 82,304 项自然度评估和 30,864 项说话人相似性评估。 82,304 条自然度评估涵盖了 20,580 条提交的话语，MOS 范围为 1 到 5，最低得分为 1，最高得分为 5。语料库、听者和评估方法的详细说明可以在 中找到。我们将每个话语的四个 MOS 评分的平均分数作为其真实分数。</p>
<h3 id="数据及其分布和可预测性">2.2. 数据及其分布和可预测性</h3>
<p>每个话语的四个 MOS 评分的均值和标准差的直方图如图 1 所示，可以看出均值 MOS 的分布更接近于高斯分布，均值 MOS 值集中在 3.0 左右。然而，对于大约一半的提交话语，四个 MOS 评分的标准偏差大于 1，表明分数的变化程度更高。这是意料之中的，因为在进行听力测试时，同一话语的感知评级取决于听众的个人经历和偏好。因此，我们确定考虑数据的内在可预测性和听众之间的内在相关性很重要。在本研究中，我们使用 bootstrap 方法来验证 VCC 2018 中人类评估的固有可预测性。</p>
<p>我们进行了 1,000 次复制来估计每个子集与整个数据集之间的 MOS 相关性。请注意，自然语音的 MOS 评估被排除在外。对于每次复制，我们从总共 267 个听众中随机抽取了 134 个听众，以测量他们的平均 MOS 作为 MOSsub。然后，根据线性相关系数 (LCC) 、Spearman 秩相关系数 (SRCC) 和均方误差 (MSE)，将 MOSsub 与 MOSall（使用整套 MOS 评级计算）进行比较。平均 LCC、SRCC 和 MSE 值如表 1 所示。因此，很明显 LCC 和 SRCC 在系统级别相当高，但在话语级别较低。 MSE 显示出类似的趋势。结果表明，虽然不同听者的主观感知评分在话语层面存在差异，但在系统层面具有良好的一致性。分析表明，虽然系统层面的MOS是可预测的，但是话语层面的MOS只能在一定程度上进行预测，虽然不如系统层面的预测。</p>
<h2 id="mosnet">MOSNet</h2>
<p>本文提出了一种基于深度学习的客观评估，以根据 MOS 对人类感知进行建模，称为 MOSNet。 使用原始幅度谱图作为输入特征，并使用三个基于神经网络的模型，即 CNN、BLSTM 和 CNN-BLSTM 从全连接 (FC) 层和池化机制的输入特征中提取有价值的特征以生成预测的 MOS . 在以下部分中，我们将详细介绍 MOSNet 的每个组件。</p>
<h3 id="型号详情">3.1 型号详情</h3>
<p>MOSNet 不同架构的详细配置如表 2 所示，包括 CNN、BLSTM 和 CNN-BLSTM。 BLSTM 架构与 Quality-Net 中使用的架构相同。凭借通过时间的前向和后向处理，BLSTM 能够将长期时间依赖性和顺序特征整合到代表性特征中。 CNN 已被广泛用于对时间序列数据进行建模并取得了令人满意的性能。 CNN 通过堆叠更多的卷积层来扩展其感受野。本研究中使用的 CNN 架构有 12 个卷积层，最后一个卷积层中每个神经元的感受野为 25 帧（时间尺度约为 400 毫秒）。我们相信，通过考虑 25 帧的片段，MOSNet 可以捕获更多的时间信息来预测质量分数。最近的研究已经证实了将 CNN 和 RNN (BLSTM) 结合用于增强、分类 和识别任务的有效性。因此，我们还研究了用于 MOSNet 中特征提取的 CNN + BLSTM 架构，在表 2 和后续讨论中将其称为 CNN-BLSTM。使用提取的特征，我们使用两个 FC 层将逐帧特征回归为帧级标量，以指示每帧的自然度得分。最后，对帧级分数应用全局平均操作以获得话语级自然度分数。</p>
<h3 id="目标函数">3.2 目标函数</h3>
<p>如上一节所述，我们将 MOS 预测制定为回归任务。 MOSNet 的输入是从语音中提取的频谱特征序列。 VCC 2018 的 MOS 评估被用作训练模型的真实情况。 傅等人指出通过在目标函数中使用帧级预测误差，话语级预测将与人类评分更加相关。 因此，我们将训练 MOSNet 的目标函数公式化为：</p>
<p>其中 <span class="math inline">\(\hat Q_s\)</span> 和 <span class="math inline">\(Q_s\)</span> 分别表示第 <span class="math inline">\(s\)</span> 个话语的真实 MOS 和预测 MOS，<span class="math inline">\(α\)</span> 是权重因子，<span class="math inline">\(q_{s,t}\)</span> 表示时间 <span class="math inline">\(t\)</span> 的帧级预测，<span class="math inline">\(T_s\)</span> 是总帧数在第<span class="math inline">\(s\)</span>个话语中，<span class="math inline">\(S\)</span>表示训练话语的数量。值得注意的是，方程中的目标函数。 (1) 结合了话语级 MSE 和帧级 MSE。在 [7] 中，加权因子 (<span class="math inline">\(α\)</span>) 用于减轻语音增强任务中跨帧的严重 MSE 变化。我们的初步实验表明，与增强语音的质量相比，转换后的语音质量在帧间更加稳定。具体来说，从高 MOS VC 系统生成的转换语音通常会产生高帧级 MOS，反之亦然。因此，我们在本研究中将权重因子 <span class="math inline">\(α\)</span> 设置为 1。为了计算帧级 MSE，地面实况 MOS 用于语音中的所有帧。从实验中可以确定，帧级 MSE 有助于 MOSNet 以更好的预测精度收敛，这将在下一节中讨论。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Wav2vec Unsupervised Pre-training for Speech Recognition</title>
    <url>/2021/09/17/wav2vec/</url>
    <content><![CDATA[<h1 id="Wav2vec-无监督语音识别预训练"><a href="#Wav2vec-无监督语音识别预训练" class="headerlink" title="Wav2vec 无监督语音识别预训练"></a>Wav2vec 无监督语音识别预训练</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们通过学习原始音频的表示来探索无监督的语音识别预训练。 wav2vec 在大量未标记的音频数据上进行训练，然后使用得到的表示来改进声学模型训练。 我们预训练了一个简单的多层卷积神经网络，该网络通过噪声对比二元分类任务进行了优化。 当只有几个小时的转录数据可用时，我们在 WSJ 上的实验将基于强字符的 log-mel 滤波器组基线的 WER 降低了 32%。 我们的方法在 nov92 测试集上实现了 2.78% 的 WER。 这优于 Deep Speech 2，这是文献中报道最好的基于字符的系统，同时使用的标记训练数据少了三个数量级。 </p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>当前最先进的语音识别模型需要大量转录的音频数据才能获得良好的性能。 最近，神经网络的预训练已成为标记数据稀缺的设置的有效技术。关键思想是在有大量标记或未标记数据可用的设置中学习一般表示，并利用学习到的表示来提高数据量有限的下游任务的性能。这对于需要大量努力来获取标记数据的任务尤其有趣，例如语音识别。</p>
<p>在计算机视觉中，ImageNet (Deng et al., 2009) 和 COCO (Lin et al., 2014) 的表征已被证明可用于初始化诸如图像字幕 (Vinyals et al., 2016) 或姿态估计等任务的模型（帕夫洛等人，2019 年）。计算机视觉的无监督预训练也显示出前景（Doersch 等，2015）。在自然语言处理 (NLP) 中，语言模型的无监督预训练（Devlin 等人，2018 年；Radford 等人，2018 年；Baevski 等人，2019 年）改进了许多任务，例如文本分类、短语结构解析和机器学习翻译（Edunov 等人，2019 年；Lample &amp; Conneau，2019 年）。在语音处理中，预训练侧重于情绪识别（Lian 等人，2018 年）、说话人识别（Ravanelli 和 Bengio，2018 年）、音素辨别（Synnaeve 和 Dupoux，2016a；van den Oord 等人，2018 年） ) 以及将 ASR 表示从一种语言转移到另一种语言 (Kunze et al., 2017)。已经有针对语音的无监督学习的工作，但结果表示尚未应用于改进有监督的语音识别（Synnaeve &amp; Dupoux，2016b；Kamper 等，2017；Chung 等，2018；Chen 等，2018； Chorowski 等人，2019 年）。</p>
<p>在本文中，我们应用无监督预训练来改进有监督的语音识别。这使得能够利用比标记数据更容易收集的未标记音频数据。我们的模型 wav2vec 是一个卷积神经网络，它将原始音频作为输入并计算可以输入到语音识别系统的一般表示。目标是一个对比损失，需要将真实的未来音频样本与负样本区分开来（Collobert 等人，2011；Mikolov 等人，2013；van den Oord 等人，2018）。与之前的工作（van den Oord 等人，2018 年）不同，我们超越了逐帧音素分类，并将学习到的表示应用于改进强监督 ASR 系统。 wav2vec 依赖于完全卷积的架构，与之前工作中使用的循环自回归模型相比，该架构可以随着时间的推移在现代硬件上轻松并行化（第 2 节）。</p>
<p>我们在 WSJ 基准测试中的实验结果表明，基于大约 1,000 小时未标记语音的预训练表示可以显着改善基于字符的 ASR 系统，并优于文献 Deep Speech 2 中基于字符的最佳结果。 TIMIT 任务，预训练使我们能够匹配文献中报告的最佳结果。 在仅具有 8 小时转录音频数据的模拟低资源设置中，与仅依赖标记数据的基线模型（§3 和 §4）相比，wav2vec 将 WER 降低了 32%。</p>
<h2 id="预训练方法"><a href="#预训练方法" class="headerlink" title="预训练方法"></a>预训练方法</h2><p>给定一个音频信号作为输入，我们优化我们的模型（第 2.1 节）以从给定的信号上下文中预测未来的样本。 这些方法的一个常见问题是需要对数据分布 $p(x)$ 进行准确建模，这具有挑战性。 我们通过首先将原始语音样本 $x$ 编码为较低时间频率的特征表示 $z$ 然后隐式建模类似于 van den Oord 等人的密度函数 $p(z_{i+k} |z_i . . . z_{i-r} )$ 来避免这个问题。</p>
<h3 id="2-1-Model"><a href="#2-1-Model" class="headerlink" title="2.1 Model"></a>2.1 Model</h3><p><img src="/images/wav2vec-fig1.png" alt="wav2vec-fig1"></p>
<p>我们的模型将原始音频信号作为输入，然后应用两个网络。编码器网络将音频信号嵌入到潜在空间中，上下文网络结合编码器的多个时间步长来获得上下文化表示（图 1）。然后使用两个网络来计算目标函数（第 2.2 节）。</p>
<p>给定原始音频样本 $x_i ∈ X$，我们应用编码器网络 $f : X 􏰀→ Z$，我们将其参数化为类似于 van den Oord 等人的五层卷积网络。 或者，可以使用其他架构，例如 Zeghidour 等人的可训练前端等。编码器层具有内核大小 (5, 4, 2, 2, 2) 和步幅 (10, 8, 4, 4, 4)。编码器的输出是一个低频特征表示 $z_i ∈ Z$，它对 $16KHz$ 音频的大约 $30ms$ 进行编码，并且跨步导致每 $10ms$ 表示一次 $z_i$。</p>
<p>接下来，我们将上下文网络 $g : Z 􏰀→ C$ 应用于编码器网络的输出，以混合多个潜在表示 $z_i . . . z_{i-v}$ 转换为单个上下文化张量 $c_i = g(z_i . . z_{i-v} )$ 以获得大小为 $v$的感受野。上下文网络有七层，每层内核大小为 3，步幅为 1。上下文网络的总感受野约为 180ms。</p>
<p>两个网络的层都由一个具有 512 个通道的因果卷积、一个组归一化层和一个 ReLU 非线性组成。我们对每个样本的特征和时间维度进行归一化，这相当于使用单个归一化组进行组归一化 。我们发现选择对输入数据的缩放和偏移不变的归一化方案很重要。这种选择导致表示可以很好地跨数据集泛化。</p>
<h3 id="2-2-目标函数"><a href="#2-2-目标函数" class="headerlink" title="2.2 目标函数"></a>2.2 目标函数</h3><p>我们训练模型，通过最小化每个步骤 k = 1,…,K 的对比损失，将未来 k 步的样本 zi+k 与从提议分布 pn 中抽取的干扰样本 ̃z 区分开来：</p>
<p><img src="/images/wav2vec-al1.png" alt="wav2vec-al1"></p>
<p>其中我们表示 $sigmoid σ(x) = 1/(1+exp(−x))$，其中 $σ(z^⊤_{i+k}h_k(c_i))$ 是 zi+k 是真实样本的概率。 我们考虑对每个步骤 $k$ 进行特定于步骤的仿射变换 $h_k(c_i) = W_kc_i+b_k$，这适用于 $c_i$（van den Oord 等人，2018 年）。 我们优化损失 $L = 􏰃\sum^K_{k=1} L_k$，对不同步长求和 (1)。 在实践中，我们通过从每个音频序列中统一选择干扰项（即 $p_n(z) = \frac{1} {T}$，其中 $T$ 是序列长度，我们将 $λ$设置为负数的数量），通过对 10 个负样本进行采样来近似期望值。 </p>
<p>训练后，我们将上下文网络 $c_i$ 产生的表示输入到声学模型中而不是 log-mel 滤波器组功能。</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h3 id="3-1-数据"><a href="#3-1-数据" class="headerlink" title="3.1 数据"></a>3.1 数据</h3><p>我们考虑以下语料库：对于 TIMIT (Garofolo et al., 1993) 上的音素识别，我们使用标准的训练、开发和测试分割，其中训练数据包含三个多小时的音频数据。 华尔街日报 (WSJ; Woodland et al., 1994) 包含大约 81 小时的转录音频数据。 我们在 si284 上训练，在 nov93dev 上验证并在 nov92 上测试。 Librispeech (Panayotov et al., 2015) 包含总共 960 小时的干净和嘈杂的训练语音。 对于预训练，我们使用 WSJ 语料库的完整 81 小时、干净的 Librispeech 的 80 小时子集、完整的 960 小时 Librispeech 训练集，或所有这些的组合。</p>
<p>为了训练基线声学模型，我们为步长为 10 毫秒的 25 毫秒滑动窗口计算了 80 个对数梅尔滤波器组系数。 最终模型根据单词错误率 (WER) 和字母错误率 (LER) 进行评估。</p>
<h3 id="3-2-声学模型"><a href="#3-2-声学模型" class="headerlink" title="3.2 声学模型"></a>3.2 声学模型</h3><p>我们使用 wav2letter++ 工具包来训练和评估声学模型（Pratap 等人，2018 年）。对于 TIMIT 任务，我们遵循 Zeghidour 等人的基于字符的 wav2letter++ 设置。 (2018a) 使用七个连续的卷积块（内核大小为 5，具有 1,000 个通道），然后是 PReLU 非线性和 0.7 的丢失率。最终表示被投影到 39 维音素概率。该模型使用自动分割标准 (ASG; Collobert et al., 2016)) 使用动量 SGD 进行训练。</p>
<p>我们 WSJ 基准的基线是 Collobert 等人中描述的 wav2letter++ 设置。 (2019) 这是一个带有门控卷积的 17 层模型 (Dauphin et al., 2017)。该模型预测了 31 个字素的概率，包括标准英文字母、撇号和句点、两个重复字符（例如，单词 ann 被转录为 an1）以及用作单词边界的静音标记 (|)。</p>
<p>所有声学模型都使用 fairseq 和 wav2letter++ 的分布式训练实现在 8 个 NVIDIA V100 GPU 上进行训练。在 WSJ 上训练声学模型时，我们使用学习率为 5.6 的普通 SGD 以及梯度剪裁（Collobert 等人，2019 年）并训练 1,000 个时期，总批次大小为 64 个音频序列。在使用 4-gram 语言模型评估检查点后，我们使用提前停止并根据验证 WER 选择模型。对于 TIMIT，我们使用 0.12 的学习率、0.9 的动量并在 8 个 GPU 上训练 1000 个时期，批量大小为 16 个音频序列。</p>
<p><img src="/images/wav2vec-tab1.png" alt="wav2vec-tab1"></p>
<h3 id="3-3-解码"><a href="#3-3-解码" class="headerlink" title="3.3 解码"></a>3.3 解码</h3><p>为了解码来自声学模型的输出，我们使用词典以及仅在 WSJ 语言建模数据上训练的单独语言模型。 我们考虑了一个 4-gram KenLM 语言模型（Heafield et al., 2013）、一个基于词的卷积语言模型（Collobert et al., 2019）和一个基于字符的卷积语言模型（Likhomanenko et al., 2019）。 我们使用 Collobert 等人的波束搜索解码器从上下文网络 c 或 log-mel 滤波器组的输出中解码单词序列 y，通过最大化</p>
<p><img src="/images/wav2vec-al2.png" alt="wav2vec-al2"></p>
<p>其中 $f_{AM}$ 是声学模型，$p_{LM}$ 是语言模型，$π = π_1, …, π_L$ 是 $y $的字符。 超参数 $α$、$β$ 和 $γ$ 是语言模型、词惩罚和沉默惩罚的权重。</p>
<p>为了解码 WSJ，我们使用随机搜索调整超参数 $α$、$β$ 和 $γ$。 最后，我们使用 $α$、$β$ 和 $γ$ 的最佳参数设置以及 4000 的波束大小和 250 的波束分数阈值来解码来自声学模型的发射。</p>
<h3 id="3-4-预训练模型"><a href="#3-4-预训练模型" class="headerlink" title="3.4 预训练模型"></a>3.4 预训练模型</h3><p>预训练模型在 Fairseq 工具包 Ott 等人的 PyTorch 中实现。 我们使用 Adam Kingma &amp; Ba (2015) 和余弦学习率计划 Loshchilov &amp; Hutter (2016) 对它们进行了优化，该计划对 WSJ 和干净的 Librispeech 训练数据集进行了超过 40K 的更新步骤。我们从 1e-7 的学习率开始，逐渐将其预热 500 次更新至 0.005，然后按照余弦曲线将其衰减至 1e-6。我们为完整的 Librispeech 训练 400K 步。为了计算目标，我们对 10 个负样本进行抽样，并使用 K = 12 个任务。</p>
<p>我们在 8 个 GPU 上进行训练，并在每个 GPU 上放置可变数量的音频序列，每个 GPU 的预定义限制为 1.5M 帧。序列按长度分组，我们将它们裁剪为每个最大 150K 帧，或批次中最短序列的长度，以较小者为准。裁剪从序列的开头或结尾删除语音信号，我们随机决定每个样本的裁剪偏移量；我们重新采样每个时期。这是数据增强的一种形式，但也确保了 GPU 上所有序列的长度相等，并平均删除了 25% 的训练数据。裁剪后，GPU 的总有效批量大小约为 556 秒的语音信号（对于可变数量的音频序列）。</p>
<p><img src="/images/wav2vec-fig2.png" alt="wav2vec-fig2"></p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>与 van den Oord 等人不同。 (2018)，我们直接在下游语音识别任务上评估预训练的表示。 我们在 WSJ 基准上测量语音识别性能并模拟各种低资源设置（第 4.1 节）。 我们还评估了 TIMIT 音素识别任务（第 4.2 节）并消除了各种建模选择（第 4.3 节）。</p>
<h3 id="4-1-WSJ基准的预训练"><a href="#4-1-WSJ基准的预训练" class="headerlink" title="4.1 WSJ基准的预训练"></a>4.1 WSJ基准的预训练</h3><p>我们考虑对 WSJ 的音频数据（无标签）、干净的 Librispeech 的一部分（大约 80 小时）和完整的 Librispeech 以及所有数据集的组合（§3.1）进行预训练。对于预训练实验，我们将上下文网络的输出提供给声学模型，而不是 log-mel 滤波器组特征。</p>
<p>表 1 显示，对更多数据进行预训练可以提高 WSJ 基准的准确性。预训练的表示可以大大提高我们基于字符的基线的性能，该基线是在 log-mel 滤波器组特征上训练的。这表明对未标记音频数据进行预训练可以比基于字符的最佳方法 Deep Speech 2（Amodei 等人，2016 年）在 11 月 92 日提高 0.3 WER。我们最好的预训练模型的性能与 Hadian 等人的基于音素的模型一样好。 (2018)。 Ghahremani 等人 (2017) 是一种基于音素的方法，它对转录的 Libirspeech 数据进行预训练，然后在 WSJ 上进行微调。相比之下，我们的方法只需要未标记的音频数据和 Ghahremani 等人 (2017) 还依赖于比我们的设置更强大的基线模型。</p>
<p>具有较少转录数据的预训练表示有什么影响？为了更好地理解这一点，我们使用不同数量的标记训练数据训练声学模型，并在使用和不使用预训练表示（对数梅尔滤波器组）的情况下测量准确性。预训练的表示在完整的 Librispeech 语料库上进行训练，我们在使用 4-gram 语言模型解码时根据 WER 测量准确性。图 2 显示，当只有大约 8 小时的转录数据可用时，预训练将 nov93dev 上的 WER 降低了 32%。与更大的 Librispeech (wav2vec Libri) 相比，仅对 WSJ (wav2vec WSJ) 的音频数据进行预训练的性能更差。这进一步证实了对更多数据进行预训练对于获得良好性能至关重要。</p>
<p><img src="/images/wav2vec-tab23.png" alt="wav2vec-tab23"></p>
<h3 id="4-2-TIMIT预训练"><a href="#4-2-TIMIT预训练" class="headerlink" title="4.2 TIMIT预训练"></a>4.2 TIMIT预训练</h3><p>在 TIMIT 任务中，我们使用具有高辍学率的 7 层 wav2letter++ 模型（§3；Synnaeve 等人，2016 年）。 表 2 显示，当我们对 Librispeech 和 WSJ 音频数据进行预训练时，我们可以匹配最先进的技术。 随着更多数据用于预训练，准确率稳步提高，当我们使用最大量的数据进行预训练时，可以获得最佳准确率。</p>
<h3 id="4-3-消融"><a href="#4-3-消融" class="headerlink" title="4.3 消融"></a>4.3 消融</h3><p><img src="/images/wav2vec-tab45.png" alt="wav2vec-tab45"></p>
<p>在本节中，我们分析了我们为 wav2vec 所做的一些设计选择。我们对干净的 Librispeech 的 80 小时子集进行预训练，并在 TIMIT 上进行评估。表 3 显示增加负样本的数量最多只能帮助十个样本。此后，随着训练时间的增加，性能趋于平稳。我们怀疑这是因为正样本的训练信号随着负样本数量的增加而减少。在这个实验中，除了负样本的数量外，一切都保持不变。<br>接下来，我们通过裁剪音频序列来分析数据增强的效果（第 3.4 节）。创建批次时，我们将序列裁剪为预定义的最大长度。表 4 显示 150K 帧的裁剪大小会产生最佳性能。不限制最大长度（无）给出了大约 207K 帧的平均序列长度，并导致最差的准确性。这很可能是因为该设置提供的数据增强量最少。</p>
<p>表 5 显示，预测未来超过 12 步不会带来更好的性能，并且增加步数会增加训练时间。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我们介绍了 wav2vec，这是第一个使用完全卷积模型将无监督预训练应用于语音识别的应用。 我们的方法在 WSJ 的测试集上实现了 2.78 WER，这一结果优于文献中下一个最著名的基于字符的语音识别模型（Amodei 等，2016），同时使用的转录训练数据少了三个数量级。 我们表明，更多的预训练数据可以提高性能，而且这种方法不仅可以改进资源匮乏的设置，还可以改进使用所有《华尔街日报》训练数据的设置。 在未来的工作中，我们将研究可能进一步提高性能的不同架构和微调。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>Adaspeech Adaptive text to speech for custom voice</title>
    <url>/2021/10/14/adaspeech/</url>
    <content><![CDATA[<h1 id="ADASPEECH-ADAPTIVE-TEXT-TO-SPEECH-FOR-CUSTOM-VOICE"><a href="#ADASPEECH-ADAPTIVE-TEXT-TO-SPEECH-FOR-CUSTOM-VOICE" class="headerlink" title="ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE"></a>ADASPEECH: ADAPTIVE TEXT TO SPEECH FOR CUSTOM VOICE</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>自定义语音是商业语音平台中的一种特定文本到语音 (TTS) 服务，旨在调整源 TTS 模型以使用来自她/他的少量语音合成目标说话者的个人语音。自定义语音对 TTS 自适应提出了两个独特的挑战：1）为了支持不同的客户，自适应模型需要处理可能与源语音数据非常不同的各种声学条件，以及 2）为了支持大量客户，每个目标说话者的自适应参数需要足够小，以减少内存使用，同时保持高语音质量。在这项工作中，我们提出了 AdaSpeech，这是一种自适应 TTS 系统，用于高质量和高效地定制新语音。我们在 AdaSpeech 中设计了几种技术来解决自定义语音中的两个挑战：1）为了处理不同的声学条件，我们在话语和音素级别对声学信息进行建模。具体来说，在预训练和微调期间，我们使用一个声学编码器来提取话语级（utterance-level vector）向量，另一个从目标语音中提取一系列音素级向量（phoneme-level vector）；在推理中，我们从参考语音中提取话语级别向量，并使用声学预测器来预测音素级别向量。 2）为了更好地权衡自适应参数和语音质量，我们在 AdaSpeech 的梅尔谱图解码器中引入了条件层归一化，并在自适应的说话人嵌入之外对这部分进行了微调。我们在 LibriTTS 数据集上预训练源 TTS 模型，并在 VCTK 和 LJSpeech 数据集（与 LibriTTS 具有不同的声学条件）上对其进行微调，自适应数据很少，例如，20 个句子，大约 1 分钟的演讲。实验结果表明，AdaSpeech 实现了比基线方法更好的自适应质量，每个说话者只有大约 5K 的特定参数，这证明了其对自定义语音的有效性。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>文本转语音 (TTS) 旨在从文本中合成自然和可理解的语音，并在机器学习社区中引起了很多兴趣。 TTS 模型在使用大量高质量和单说话人录音进行训练时可以合成自然的人声，并且已经扩展到多说话人场景使用多说话人语料库。 然而，这些语料库包含一组固定的说话者，其中每个说话者仍然有一定数量的语音数据。</p>
<p>如今，自定义语音在个人助理、新闻广播和音频导航等不同应用场景中越来越受到关注，并在商业语音平台（部分自定义语音服务包括微软Azure、亚马逊AWS和谷歌云）中得到广泛支持。在自定义语音中，源TTS模型通常适用于自适应数据很少的个性化语音，因为自定义语音的用户为了方便起见，更喜欢记录尽可能少的自适应数据（几分钟或几秒）。很少有自适应数据对自适应语音的自然性和相似性提出了很大的挑战。此外，自定义语音还存在几个独特的挑战：1）自定义用户的录音通常与源语音数据（用于训练源 TTS 模型的数据）具有不同的声学条件。例如，适应数据通常记录有不同的说话韵律、风格、情绪、口音和录音环境。这些声学条件的不匹配使得源模型难以概括并导致较差的适应质量。 2) 在将源 TTS 模型适配到新语音时，需要在微调参数和语音质量之间进行权衡。一般来说，更多的适配参数通常会带来更好的语音质量，从而增加内存存储和服务成本。</p>
<p>虽然之前在 TTS 适配方面的工作已经很好地考虑了自定义语音中为数不多的适配数据设置，但还没有完全解决上述挑战。他们对整个模型或解码器部分进行了微调，实现了良好的质量，但导致了太多的自适应参数。商业化定制语音的部署需要减少适配参数的数量。否则，内存存储会随着用户的增加而爆炸。有些作品仅微调说话人嵌入，或训练说话人编码器模块在适应过程中不需要微调。虽然这些方法导致了轻量级和高效的适应，但它们导致了较差的适应质量。而且，之前的大部分工作都假设源语音数据和自适应数据在同一个域中，没有考虑不同声学条件的设置，这在自定义语音场景中是不切实际的。</p>
<p>在本文中，我们提出了 AdaSpeech，这是一种自适应 TTS 模型，用于高质量和高效的新语音定制。 AdaSpeech 为自定义语音采用三阶段管道：1) 预训练； 2）微调； 3) 推理。在预训练阶段，TTS模型在大规模多说话人数据集上进行训练，可以确保TTS模型覆盖多样化的文本和说话声音，有利于适应。在微调阶段，源 TTS 模型通过在具有不同声学条件的有限自适应数据上微调（部分）模型参数来适应新的语音。在推理阶段，TTS模型的未适配部分（所有自定义语音共享的参数）和适配部分（每个自定义语音都有特定的适配参数）都用于推理请求。我们基于流行的非自回归 TTS 模型构建 AdaSpeech，并进一步设计了几种技术来应对挑战在自定义语音中：</p>
<ul>
<li>声学条件建模。为了处理不同的声学条件以进行适应，我们在预训练和微调中对话语和音素级别的声学条件进行建模。具体来说，我们使用两个声学编码器从目标语音中提取一个话语级向量和一系列音素级向量，作为梅尔谱解码器的输入，分别表示全局和局部声学条件。这样，解码器就可以根据这些声学信息预测不同声学条件下的语音。否则，模型会记住声学条件，不能很好地概括。在推理中，我们从参考语音中提取话语级别向量，并使用另一个建立在音素编码器上的声学预测器来预测音素级别向量。</li>
<li>条件层标准化。为了在确保自适应质量的同时微调尽可能少的参数，我们在预训练中修改了 Melspectrogram 解码器中的层归一化，通过使用说话人嵌入作为条件信息在层归一化中生成尺度和偏置向量。在微调中，我们只调整与条件层归一化相关的参数。通过这种方式，与微调整个模型相比，我们可以大大减少自适应参数，从而大大减少内存存储，但由于条件层归一化的灵活性，我们可以保持高质量的自适应语音。</li>
</ul>
<p>为了评估我们提出的 AdaSpeech 对自定义语音的有效性，我们进行了实验以在 LibriTTS 数据集上训练 TTS 模型，并在具有不同自适应设置的 VCTK 和 LJSpeech 数据集上调整模型。 实验结果表明，AdaSpeech 在 MOS（平均意见得分）和 SMOS（相似性 MOS）方面比基线方法实现了更好的适应质量，每个说话者只有大约 5K 的特定参数，证明了其对自定义语音的有效性。</p>
<h2 id="AdaSpeech"><a href="#AdaSpeech" class="headerlink" title="AdaSpeech"></a>AdaSpeech</h2><p>在本节中，我们首先描述我们提出的 AdaSpeech 的整体设计，然后介绍解决自定义语音挑战的关键技术。 最后，我们列出了用于自定义语音的 AdaSpeech 的预训练、微调和推理管道。</p>
<p><img src="/images/adaspeech-fig1.png" alt="adaspeech-fig1"></p>
<p>AdaSpeech 的模型结构如图 1 所示。我们采用 FastSpeech 2作为模型主干，因为 FastSpeech系列是非自回归 TTS。基本模型主干由音素编码器、梅尔谱解码器和方差适配器组成，该适配器将包括持续时间、音调和能量在内的方差信息提供到 Ren 等人的音素隐藏序列中。如图 1 所示，我们设计了两个额外的组件来解决自定义语音中的独特挑战：1）为了支持不同的客户，我们使用声学条件建模（Acoustic condition modeling）来捕获不同粒度的自适应语音的不同声学条件； 2）为了以负担得起的内存存储支持大量客户，我们在解码器中使用条件层归一化（Conditional LayerNorm），以在高语音质量的同时以较少的参数进行高效适应。在接下来的小节中，我们将分别介绍这些组件的详细信息。</p>
<h3 id="2-1-声学条件建模"><a href="#2-1-声学条件建模" class="headerlink" title="2.1 声学条件建模"></a>2.1 声学条件建模</h3><p>在自定义语音中，自适应数据可以用不同的韵律、风格、口音说话，并且可以在各种环境下记录，这可以使声学条件与源语音数据中的声学条件大不相同。这对适应源 TTS 模型提出了巨大的挑战，因为源语音不能涵盖自定义语音中的所有声学条件。缓解这个问题的一个实用方法是提高源 TTS 模型的适应性（通用性）。在文本到语音中，由于输入文本缺乏足够的声学条件（例如说话者音色、韵律和录音环境）来预测目标语音，因此模型倾向于记忆和过拟合训练数据，并且在适应过程中泛化能力较差。解决此类问题的一种自然方法是提供相应的声学条件作为输入，使模型学习合理的文本到语音映射，以实现更好的泛化而不是记忆。</p>
<p><img src="/images/adaspeech-fig2.png" alt="adaspeech-fig2"></p>
<p>为了更好地模拟不同粒度的声学条件，我们对不同级别的声学条件进行分类，如图 2a 所示： 1) 说话人级别（speaker level），粗粒度的声学条件以捕获扬声器的整体特征； 2) 话语级别（utterance level），说话人每次话语的细粒度声学条件； 3）音素水平（phoneme level），话语的每个音素中更细粒度的声学条件，例如特定音素的重音、音高、韵律和时间环境噪声。由于说话人 ID（嵌入）被广泛用于捕捉多说话人场景中说话人级别的声学条件，因此默认情况下使用说话人嵌入，我们描述了话语级和音素级声学条件建模如下：</p>
<ul>
<li>话语水平。 我们使用声学编码器从参考语音中提取向量，然后将其扩展并添加到音素隐藏序列中，以提供话语级声学条件。 如图 2b 所示，声学编码器由几个卷积层和一个平均池化层组成，以获得单个向量。 参考语音是训练时的目标语音，而推理时该说话者的随机选择语音。</li>
<li>音素水平。 我们使用另一个声学编码器（如图 2c 所示）从目标语音中提取一系列音素级向量并将其添加到音素隐藏序列中以提供音素级声学条件。 为了从语音中提取音素级信息，我们首先根据音素和梅尔谱序列之间的对齐（如图2a所示）对同一音素对应的语音帧进行平均，将语音帧序列的长度转换为长度音素序列。 在推理过程中，我们使用另一个基于原始音素编码器的音素级声学预测器（如图 2d 所示）来预测音素级向量。</li>
</ul>
<p>使用语音编码器提取单个向量或向量序列来表示语音序列的特征在以前的工作中已经被采用。 他们通常利用它们来改善 TTS 模型的说话者音色或韵律，或提高模型的可控性。 在这项工作中，我们的声学条件建模的关键贡献是以新的视角对不同粒度的不同声学条件进行建模，从而使源模型更适应不同的适应数据。 如 4.2 节所分析，话语级和音素级声学建模确实可以帮助学习声学条件，并且对于确保适应质量至关重要。</p>
<h3 id="2-2-CONDITIONAL-LAYER-NORMALIZATION"><a href="#2-2-CONDITIONAL-LAYER-NORMALIZATION" class="headerlink" title="2.2 CONDITIONAL LAYER NORMALIZATION"></a>2.2 CONDITIONAL LAYER NORMALIZATION</h3><p>在使用小自适应参数的同时实现高自适应质量具有挑战性。 以前的工作使用说话人编码器的零样本自适应或仅微调扬声器嵌入无法达到满意的质量。 我们能否以略多但可以忽略不计的参数为代价大大提高语音质量？ 为此，我们分析了 FastSpeech 2 的模型参数，它基本上建立在 Transformer 的结构之上，在每个 Transformer 块中都有一个自注意力网络和一个前馈网络。 self-attention 和两层前馈网络的 query、key、value 和 output 中的矩阵乘法都是参数密集型的，自适应效率不高。 我们发现解码器中的每个自注意力和前馈网络都采用层归一化，这可以极大地影响隐藏激活和最终预测的轻量级可学习尺度向量 $γ$ 和偏置向量 $β：LN (x) = γ \frac{x−μ}{\sigma} + β$，其中 $μ$ 和 $σ$ 是隐藏向量 $x$ 的均值和方差。</p>
<p><img src="/images/adaspeech-fig3.png" alt="adaspeech-fig3"></p>
<p>如果我们可以使用一个小的条件网络，根据相应的说话人特征确定层归一化中的尺度和偏置向量，那么我们就可以在适应新的语音时对这个条件网络进行微调，在保证适应质量的同时大大减少适应参数 . 如图 3 所示，条件网络由两个简单的线性层 $W_c^γ$ 和 $W_c^β$ 组成，它们以扬声器嵌入 $E^s$ 作为输入，分别输出尺度和偏置向量：</p>
<p><img src="/images/adaspeech-math1.png" alt="adaspeech-math1"></p>
<p>其中 $s$ 表示说话者 ID，$c ∈ [C]$ 表示解码器中有 $C$ 个条件层归一化（解码器层数为 $(C − 1)/2$，因为每层有两个条件层归一化对应于自注意力 和 Transformer 中的前馈网络，并且在最终输出处有一个额外的层归一化）并且每个都使用不同的条件矩阵。</p>
<h3 id="2-3-AdaSpeech-的pipeline"><a href="#2-3-AdaSpeech-的pipeline" class="headerlink" title="2.3 AdaSpeech 的pipeline"></a>2.3 AdaSpeech 的pipeline</h3><p><img src="/images/adaspeech-al1.png" alt="adaspeech-al1"></p>
<p>我们在算法 1 中列出了 AdaSpeech 的预训练、微调和推理管道。 在微调期间，我们只对解码器和说话人嵌入 $E_s$ 中每个条件层归一化中的两个矩阵 $W_c^γ$ 和 $W_c^β$ 进行微调，其他 模型参数包括话语级和音素级声学编码器以及音素级声学预测器，如第 2.1 节所述。 在推理过程中，我们不直接在每个条件层归一化中使用两个矩阵 $W_c^γ$ 和 $W_c^β$，因为它们仍然有很大的参数。 取而代之的是，考虑到 $E_s$ 在推理中是固定的，我们使用这两个矩阵根据等式 1 从说话人嵌入 $E_s$ 计算每个尺度和偏置向量 $γ_c^s$ 和 $β_c^s$。 这样，我们可以节省大量的内存storage。</p>
<h2 id="实验设置"><a href="#实验设置" class="headerlink" title="实验设置"></a>实验设置</h2><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p>我们在 LibriTTS 数据集上训练 AdaSpeech 源模型，该数据集是源自 LibriSpeech的多说话人语料库（2456 名说话人），包含 586 小时的语音数据。 为了在自定义语音场景中评估 AdaSpeech，我们使源模型适应其他数据集中的语音，包括 VCTK（具有 108 个扬声器和 44 小时语音数据的多扬声器数据集）和 LJSpeech（具有 24 小时语音数据的单扬声器高质量数据集），与 LibriTTS 具有不同的声学条件。 作为比较，我们还使源模型适应同一 LibriTTS 数据集中的语音。</p>
<p>我们从 LibriTTS 和 VCTK 的训练集中随机选择几个说话者（包括男性和女性）和 LJSpeech 训练集中唯一的单个说话者进行适应。对于每个选择的说话者，我们随机选择 K = 20 个句子进行适应，并在实验部分研究较小 K 的影响。我们使用 LibriTTS 训练集中的所有说话者（不包括选择进行适配的说话者）来训练源 AdaSpeech 模型，并使用这些数据集中与适应说话者对应的原始测试集来评估适应语音质量。</p>
<p>我们对这些语料库中的语音和文本数据进行如下预处理：1）将所有语音数据的采样率转换为16kHz； 2）按照Shen等人的常见做法提取具有12.5ms跳跃大小和50ms窗口大小的mel谱图。 3）通过字素到音素的转换将文本序列转换为音素序列，并以音素作为编码器输入。</p>
<h3 id="模型设置"><a href="#模型设置" class="headerlink" title="模型设置"></a>模型设置</h3><p>AdaSpeech 的模型遵循 FastSpeech 2 中的基本结构，它由 4 个用于音素编码器和 Melspectrogram 解码器的前馈 Transformer 块组成。隐藏维度（包括音素嵌入、说话者嵌入、self-attention中的隐藏、前馈网络的输入和输出隐藏）设置为256。注意力头的数量、前馈滤波器大小和内核大小分别设置为 2、1024 和 9。输出线性层将 256 维隐藏转换为 80 维梅尔谱图。其他模型配置遵循 Fastspeech 除非另有说明。</p>
<p>音素级声学编码器（图 2c）和预测器（图 2d）共享相同的结构，由 2 个卷积层组成，滤波器大小和内核大小分别为 256 和 3，还有一个线性层将隐藏层压缩到一个维度4（我们根据我们的初步研究选择了 4 的维度，也与之前的工作一致。我们使用 MFA来提取音素和梅尔谱序列之间的对齐，用于准备音素级声学编码器的输入。我们还尝试将 VQ-VAE 用于音素级声学编码器，但没有发现明显的收益。话语级声学编码器由 2 个卷积层组成，滤波器大小、内核大小和步长大小分别为 256、5 和 3，以及一个池化层以获得单个向量。</p>
<h3 id="训练、适配和推理"><a href="#训练、适配和推理" class="headerlink" title="训练、适配和推理"></a>训练、适配和推理</h3><p>在源模型训练过程中，我们首先对 AdaSpeech 进行 60,000 步的训练，除音素级声学预测器的参数外，所有模型参数都进行了优化。然后我们对剩余的 40,000 步联合训练 AdaSpeech 和音素级声学预测器，其中音素级声学编码器的隐藏输出用作标签（停止梯度以防止回流到音素级声学编码器）以训练具有均方误差 (MSE) 损失的音素级声学预测器。我们在 4 个 NVIDIA P40 GPU 上训练 AdaSpeech，每个 GPU 的批量大小约为 12,500 个语音帧。 Adam 优化器与 $β_1 = 0.9、β_2 = 0.98、ε = 10^{−9}$ 一起使用。在</p>
<p>适配过程中，我们在 1 个 NVIDIA P40 GPU 上对 AdaSpeech 进行了 2000 步的微调，其中仅优化了扬声器嵌入和条件层归一化的参数。在推理过程中，从说话人的另一个参考语音中提取话语级声学条件，从音素级声学预测器预测音素级声学条件。我们使用 MelGAN 作为声码器从生成的梅尔频谱图合成波形。</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>在本节中，我们首先评估 AdaSpeech 的自适应语音质量，并进行消融研究以验证 AdaSpeech 中每个组件的有效性，最后我们展示了我们的方法的一些分析。</p>
<h3 id="4-1-自适应语音质量"><a href="#4-1-自适应语音质量" class="headerlink" title="4.1 自适应语音质量"></a>4.1 自适应语音质量</h3><p>我们根据自然度（合成声音听起来像人类的自然声音）和相似度（合成声音听起来如何与这个说话者相似）来评估自适应声音的质量。因此，我们使用 MOS（平均意见得分）对自然度和 SMOS（相似度 MOS）进行人工评估以进行相似性评估。每个句子由 20 名评委听。对于 VCTK 和 LibriTTS，我们将多个适配说话人的 MOS 和 SMOS 分数取平均值作为最终分数。我们将 AdaSpeech 与以下几种设置进行比较：1) GT，真实录音； 2）GT mel + Vocoder，使用ground-truth mel-spectrogram与MelGAN声码器合成波形； 3）Baseline（spk emb），一个基于FastSpeech2的baseline系统，它只在适应时微调说话人的嵌入，可以看作我们的下界； 4）Baseline（decoder），另一个基于FastSpeech2的baseline系统，在适配时对整个解码器进行微调，由于适配时使用了更多的参数，可以认为是一个很强的可比系统； 5) AdaSpeech，我们提出的 AdaSpeech 系统，在适应期间具有话语/音素级声学条件建模和条件层归一化。</p>
<p><img src="/images/adaspeech-tab1.png" alt="adaspeech-tab1"></p>
<p>MOS 和 SMOS 结果如表 1 所示。我们有几个观察结果：1）将模型（在 LibriTTS 上训练）适应跨域数据集（LJSpeech 和 VCTK）比适应域内数据集（LibriTTS）更困难，因为在跨域数据集上，自适应模型（两个基线和 AdaSpeech）与真实 mel + 声码器设置之间的 MOS 和 SMOS 差距更大。这也证实了在自定义语音场景中建模不同声学条件的挑战。 2) 与仅微调说话人嵌入，即基线 (spk emb) 相比，AdaSpeech 在三个自适应数据集中在 MOS 和 SMOS 方面都实现了显着改进，仅在条件层归一化中利用了更多的参数。我们还在下一小节（表 3）中分析，即使我们增加基线的自适应参数以匹配或超过 AdaSpeech，它的性能仍然比 AdaSpeech 差很多。 3）与微调整个解码器，即基线（解码器）相比，AdaSpeech 在 MOS 和 SMOS 中都实现了稍微更好的质量，重要的是具有更小的自适应参数，这证明了我们提出的声学条件建模和条件的有效性和效率层归一化。请注意，对整个解码器进行微调会导致自适应参数过多，无法满足自定义语音场景。</p>
<h3 id="4-2-方法分析"><a href="#4-2-方法分析" class="headerlink" title="4.2 方法分析"></a>4.2 方法分析</h3><p>在本节中，我们首先进行消融研究以验证 AdaSpeech 中每个组件的有效性，包括话语级和音素级声学条件建模以及条件层归一化，然后对我们提出的 AdaSpeech 进行更详细的分析。</p>
<h3 id="Ablation-Study"><a href="#Ablation-Study" class="headerlink" title="Ablation Study"></a>Ablation Study</h3><p><img src="/images/adaspeech-tab2.png" alt="adaspeech-tab2"></p>
<p>我们在 VCTK 测试集上比较了AdaSpeech 中去除每个组件时自适应语音质量的CMOS（比较MOS）（每个句子由20 个评委听）。 具体来说，当去除条件层归一化时，我们只微调说话人嵌入。 从表 2 中我们可以看出，去除话语级和音素级声学建模以及条件层归一化都会导致语音质量的性能下降，证明了 AdaSpeech 中每个组件的有效性。</p>
<h3 id="声学条件建模分析"><a href="#声学条件建模分析" class="headerlink" title="声学条件建模分析"></a>声学条件建模分析</h3><p>我们分析了从 LibriTTS 数据集上的几个说话者的话语级声学编码器中提取的向量。我们在图 4a 中使用 t-SNE 来说明它们，其中每个点代表一个话语级别向量，每种颜色都属于同一个说话者。 可以看出，同一个说话者的不同话语聚集在一起，但在声学条件上存在差异。 也有一些例外，例如棕色实心圆圈中的两个粉红色点和一个蓝色点。 根据我们对相应语音数据的调查，这些点对应于声音短而情绪化的话语，因此虽然属于不同的说话者，但彼此接近。</p>
<h3 id="条件层归一化分析"><a href="#条件层归一化分析" class="headerlink" title="条件层归一化分析"></a>条件层归一化分析</h3><p>我们进一步将条件层归一化 (CLN) 与其他两种设置进行比较：1) LN + 微调尺度/偏差：去除扬声器嵌入条件，仅微调层归一化和扬声器嵌入中的尺度/偏差； 2) LN + 微调其他：去除说话人嵌入的条件，而是微调解码器中的其他（类似甚至更多）参数。 CMOS 评估如表 3 所示。可以看出，与条件层归一化相比，这两种设置导致的质量较差，这验证了其有效性。</p>
<h3 id="变化的适应数据"><a href="#变化的适应数据" class="headerlink" title="变化的适应数据"></a>变化的适应数据</h3><p>我们在 VCTK 和 LJSpeech 上研究了不同数量的自适应数据（少于默认设置）的语音质量，并进行了如图 4b 所示的 MOS 评估。 可以看出，当自适应数据减少时，语音质量继续下降，当自适应数据少于10个句子时，语音质量下降很快。</p>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>在本文中，我们开发了 AdaSpeech，这是一种自适应 TTS 系统，用于支持自定义语音中的独特要求。我们提出声学条件建模，使源 TTS 模型更适合具有各种声学条件的自定义语音。我们进一步设计了条件层归一化以提高适应效率：微调少量模型参数以实现高语音质量。我们最终展示了 AdaSpeech 中用于自定义语音的预训练、微调和推理管道。实验结果表明，AdaSpeech 可以支持具有不同声学条件的自定义语音，内存存储量少，同时语音质量高。对于未来的工作，我们将进一步改进源 TTS 模型中声学条件的建模，并研究更多样化的声学条件，例如自定义语音中的嘈杂语音。我们还将研究未转录数据的自适应设置并进一步压缩模型大小以支持更多自定义语音。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>patches_are_all_you_need</title>
    <url>/2021/10/20/patches-are-all-you-need/</url>
    <content><![CDATA[<h1 id="Patches-are-all-you-need"><a href="#Patches-are-all-you-need" class="headerlink" title="Patches are all you need"></a>Patches are all you need</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>尽管多年来卷积网络一直是视觉任务的主要架构，但最近的实验表明，基于 Transformer 的模型，尤其是 Vision Transformer (ViT)，在某些情况下可能会超过其性能。然而，由于 Transformer 中自注意力层的二次运行时间，ViT 需要使用补丁嵌入，将图像的小区域组合成单个输入特征，以便应用于更大的图像尺寸。这就提出了一个问题：ViT 的性能是由于固有的更强大的 Transformer 架构，还是至少部分是由于使用补丁作为输入表示？在本文中，我们为后者提供了一些证据：具体而言，我们提出了 ConvMixer，这是一个极其简单的模型，在精神上与 ViT 和更基本的 MLP-Mixer 相似，因为它直接将补丁作为输入进行操作，分离空间和通道维度的混合，并在整个网络中保持相同的大小和分辨率。然而，相比之下，ConvMixer 仅使用标准卷积来实现混合步骤。尽管它很简单，但我们表明，除了优于 ResNet 等经典视觉模型之外，ConvMixer 在类似的参数计数和数据集大小方面也优于 ViT、MLP-Mixer 及其一些变体。我们的代码可在 <a href="https://github.com/tmp-iclr/convmixer">https://github.com/tmp-iclr/convmixer</a> 获得。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>多年来，卷积神经网络一直是应用于计算机视觉任务的深度学习系统的主要架构。但最近，基于 Transformer 模型的架构，例如所谓的 Vision Transformer 架构，在其中许多任务中都表现出了引人注目的性能，通常优于经典卷积架构，尤其是对于大型数据集。一个可以理解的假设是，Transformer 成为视觉领域的主导架构只是时间问题，就像它们在语言处理中一样。然而，为了将 Transformers 应用于图像，必须改变表示：因为如果在每个像素级别上天真地应用，Transformers 中使用的自注意力层的计算成本将与每个图像的像素数成二次方缩放，妥协是首先将图像拆分为多个“补丁”，线性嵌入它们，然后将转换器直接应用于这个补丁集合。</p>
<p>在这项工作中，我们从根本上探讨了视觉转换器的强大性能是否更多地来自这种基于补丁的表示，而不是来自转换器架构本身。 我们开发了一个非常简单的卷积架构，我们将其称为“ConvMixer”，因为它与最近提出的 MLP-Mixer（Tolstikhin 等人，2021）相似。 这种架构在很多方面类似于 Vision Transformer（和 MLP-Mixer）：它直接对补丁进行操作，它在所有层中保持相同的分辨率和大小表示，它不会对连续层的表示进行下采样， 它将“通道混合”与信息的“空间混合”分开。 但与 Vision Transformer 和 MLP-Mixer 不同，我们的架构仅通过标准卷积来完成所有这些操作。</p>
<p>我们在本文中展示的主要结果是，这种 ConvMixer 架构尽管极其简单（它可以在大约 6 行密集 PyTorch 代码中实现），但优于“标准”计算机视觉模型，例如具有类似参数计数的 ResNets 和一些 相应的 Vision Transformer 和 MLP-Mixer 变体，即使添加了一系列旨在使这些架构在较小的数据集上具有更高性能的功能。 这表明，至少在某种程度上，补丁表示本身可能是实现 Vision Transformers 等新架构“卓越”性能的最关键组成部分。 虽然这些结果自然只是一个快照，但我们相信这提供了一个强大的“基于卷积但基于补丁”的基线，以便与未来更高级的架构进行比较。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>python-import</title>
    <url>/2021/10/25/python-import/</url>
    <content><![CDATA[<h1 id="python-import"><a href="#python-import" class="headerlink" title="python import"></a>python import</h1><p>被python的import坑了n次了，每次项目套子项目的时候import就会开始逐渐混乱，因此特此开这个日志来简要总结一下python的import，希望今后不再踩坑python的import</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>multiple_gpu</title>
    <url>/2021/10/26/multiple-gpu/</url>
    <content><![CDATA[<h1 id="多gpu并行计算训练和推理">多GPU并行计算，训练和推理</h1>
<p>先来回顾一下Python的多进程multiprocessing和多线程Threading</p>
<h2 id="多进程">多进程</h2>
<p>https://zhuanlan.zhihu.com/p/85821779</p>
<h2 id="多线程">多线程</h2>
<p>适合IO密集型任务，不适合CPU密集型任务（如循环，计数）</p>
<p>https://python.land/python-concurrency/the-python-gil</p>
<h2 id="多线程-vs-多进程">多线程 VS 多进程</h2>
<p>https://zhuanlan.zhihu.com/p/20953544</p>
<p>接下来再看下torch实现多GPU并行计算的几种方法</p>
<h2 id="ddp-distributeddataparallel-vs-dataparallel">DDP DistributedDataParallel VS DataParallel</h2>
<ol style="list-style-type: decimal">
<li>DDP 多进程，多线程； DP 单进程，多线程</li>
<li>DDP 可以分布式计算，DP只能单台机器计算</li>
<li>DDP 可以model paralle，DP不可</li>
</ol>
<p>https://blog.csdn.net/qq_37541097/article/details/109736159</p>
<p>在Pytorch中使用多GPU的常用启动方式一种是<code>torch.distributed.launch</code>一种是<code>torch.multiprocessing</code>模块。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>ClsVC</title>
    <url>/2021/11/03/ClsVC/</url>
    <content><![CDATA[<h1 id="clsvc-learning-speech-representations-with-two-different-classification-tasks.">CLSVC: LEARNING SPEECH REPRESENTATIONS WITH TWO DIFFERENT CLASSIFICATION TASKS.</h1>
<h2 id="abstract">Abstract</h2>
<p>语音转换（VC）旨在将一个说话者的声音转换成另一个说话者所说的新语音。以前的工作侧重于通过应用两个不同的编码器分别从输入语音中学习内容信息和音色信息来学习潜在表示。然而，无论是应用瓶颈网络还是矢量量化技术，都很难从语音信号中完美地分离说话者和内容信息。在本文中，我们提出了一种新颖的语音转换框架“ClsVC”来解决这个问题。它仅使用一个编码器通过划分潜在空间来获得音色和内容信息。此外，提出了一些约束以确保潜在空间的不同部分分别只包含分离的内容和音色信息。我们已经展示了设置这些约束的必要性，我们也通过实验证明，即使我们改变了潜在空间的划分比例，内容和音色信息也将始终很好地分离。在 VCTK 数据集上的实验表明，就转换后语音的自然性和相似性而言，ClsVC 是最先进的框架。</p>
<h2 id="introduction">Introduction</h2>
<p>语音转换（VC）是一个令人兴奋的话题，致力于通过保留原始话语中的内容，同时用目标说话者的声音特征替换它，将源说话者的一种话语转换为目标人的另一种话语。 到目前为止，许多方法已经成功地应用于VC。 通常，这些方法可以粗略地分为两类，即并行 VC 和非并行 VC。 具体来说，并行 VC 意味着模型训练需要并行语料库，这对于非并行 VC 来说是不必要的。 最近，越来越多的研究人员将注意力集中在非并行 VC 的解决方案上，因为我们不容易收集这么多成对的源目标语音数据集。</p>
<p>早期的 VC 系统，如高斯混合模型需要大量并行数据进行模型训练，生成的语音质量不够好。 随着深度学习的进步，近年来提出了多种新颖的 VC 方法。 其中，基于 GAN 的模型是最流行的方法之一，可以在没有明确近似的情况下学习目标语音的全局生成分布。 这些基于 GAN 的模型联合训练生成器和鉴别器。 来自鉴别器的对抗性损失用于鼓励生成器输出构建与真实语音无法区分的输出。 由于循环一致性训练，基于 GAN 的 VC 模型可以使用非并行语音数据集进行训练。</p>
<p>此外，学习离散语音表示也引起了很多关注。 矢量量化（VQ），一种极其重要的信号压缩方法，可以将连续数据量化为离散数据。 先前的研究已经证实，连续语音数据产生的量化离散数据与音素信息密切相关。 最近，VQVC被提议学习仅用重建损失来解开内容和说话者信息。 然后，VQVC+ 很快被提出通过在基于自动编码器的 VC 系统中添加 U-Net 架构来提高 VQVC 的转换性能。 为了大大提高解开内容和说话人信息的性能，引入了许多其他现有研究与 VQ 相结合，例如 VQ-Wav2Vec、VQ-VAE 和 VQ-CPC。</p>
<p>还有另一项研究重点是使用自动编码器学习潜在表示。 尤其是变分自动编码器（VAE）最为著名。 VAE 的网络结构包含一个编码器和一个解码器，核心思想非常明确：编码器从输入语音中学习特定的潜在空间，解码器从该潜在空间输出重构的语音。 在这个过程中，VAE 侧重于如何强制编码器学习特定的潜在空间。 到目前为止，许多基于 VAE 的模型已成功应用。 此外，AutoVC 是 Autoencoder 的另一个成功应用。 通过巧妙的实验设计，AutoVC 使用两种不同的编码器分别学习内容和说话人信息，使该模型仅靠自重构损失就可以实现分布匹配风格迁移。</p>
<p>不幸的是，在 VC 领域，上面提到的所有模型都有其固有的缺点。 例如，基于 GAN 的模型通常可以达到很好的转换效果，并保证生成数据和输入数据的匹配，但是公认 GAN 的训练非常不稳定。 相反，VQVC的训练简单且足够快，但是这种方法产生的音频量很差。 这可能是因为离散的语音表示不可避免地会丢失一些内容信息。 另外，虽然基于VAE的模型也有很大的转换效果，但不能保证分布匹配。 AutoVC 是一项伟大的研究，训练非常简单并且达到了最先进的结果。 然而，为了实现风格转换，它必须引入一个预先训练好的扬声器编码器。</p>
<p>基于这些现有的方法，我们自然想知道是否有一种新的解决方案可以像 AutoVC 和 GAN 一样实现分布匹配，像 VQVC 和 VAE 一样容易训练，可以像 VQ 一样仅通过一个编码器解开内容和说话者信息， 并且在语音转换或从语音中分离语言和音色信息方面也有更好的表现？</p>
<p>在本文中，我们提出了一种新颖的语音转换框架来满足上述所有要求。 具体来说，我们的模型类似于 VAE，Autoencoder 是我们模型的主要框架，应用了两种不同类型的分类任务来强制我们的模型正确分离内容和说话者信息。 这里，两个分类任务分别是指一般分类任务和对抗性分类任务。 一般分类任务的目标是尽可能准确地识别与说话人相关的特征，即说话人信息。 而后者旨在消除潜在空间中的说话人信息以获得独立于说话人的特征，即内容信息。 实验结果在 VCTK 数据集上进行。 客观和主观评估表明，所提出的方法在自然度和说话人相似度方面优于 VQVC、AutoVC、VQ-VAE 和 StarGAN-VC。</p>
<h2 id="background">Background</h2>
<p>在数理统计中，如果我们已经知道 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的联合概率密度函数，我们就可以很容易地分别求出 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的边际概率密度函数。 形式上，如果 <span class="math inline">\((x, y) ∼ p(x, y)\)</span> 已知，我们可以通过以下公式得到 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> 的边缘分布：</p>
<p>进一步地，在一些设置条件的约束下，虽然方程中联合概率密度函数的封闭形式为 <span class="math inline">\(p(x, y)\)</span>。 (1) 通常是未知的，当每个 <span class="math inline">\(z\)</span> 对应于唯一的 <span class="math inline">\((x, y)\)</span> 对时，神经网络从输入样本 <span class="math inline">\(z\)</span> 中学习 <span class="math inline">\(x\)</span> 和 <span class="math inline">\(y\)</span> 的边缘分布仍然是可行的。</p>
<p><strong>互信息 (MI)</strong>，衡量两个不同变量之间依赖性的关键指标。 最近，许多 MI 估计器已成功应用于约束神经网络以解开输入数据的不同分量。 可以表述为</p>
<p>其中<span class="math inline">\(P(X)\)</span>和<span class="math inline">\(P(Y)\)</span>分别是<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的边际分布，<span class="math inline">\(P(X,Y)\)</span>表示<span class="math inline">\(X\)</span>和<span class="math inline">\(Y\)</span>的联合分布。 由于很难获得所需的分布公式<span class="math inline">\(P(X,Y)\)</span>，因此许多研究集中于提出基于样本的MI下限（或上限）以获得可计算的近似值。</p>
<p>最近，引入了一种新的 MI 估计器，用于从语音中学习内容和风格信息以进行语音转换。 具体来说，他们提出了一种新颖的基于 MI 的学习目标，以鼓励内容编码器输出内容嵌入并引导说话人编码器输出说话人嵌入。 受此启发，我们提出了一种新的、简单且更有效的学习潜在语音表示的框架。</p>
<p><strong>梯度反转层 (GRL)</strong> 梯度反转层 (GRL) 最初是为了解决域适应问题提出的，旨在强制模型输出与域无关的域共享特征。 具体来说，GRL 通常位于编码器和域分类器之间。 在前向传播期间，GRL 充当身份变换。 在反向传播过程中，GRL 从后续层获取梯度，将其乘以 -1 并将其传递给前一层，以便编码器和域分类器具有完全相反的优化目标。</p>
<h2 id="method">Method</h2>
<p>首先，对于每个语音 x ，我们使用内容嵌入 Cx 来表示语言信息，并提出说话人嵌入 Sx 来表示音色和风格信息。 并且，U 表示扬声器组。 以下两个定理是我们框架的前提：</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>vscode</title>
    <url>/2022/05/23/vscode/</url>
    <content><![CDATA[<h1 id="VSCode-踩坑指南"><a href="#VSCode-踩坑指南" class="headerlink" title="VSCode 踩坑指南"></a>VSCode 踩坑指南</h1><h2 id="VSCode-连接内网环境下的远程服务器"><a href="#VSCode-连接内网环境下的远程服务器" class="headerlink" title="VSCode 连接内网环境下的远程服务器"></a>VSCode 连接内网环境下的远程服务器</h2><p>Version 1.54.3</p>
<ul>
<li><p>Step 1 下载安装包，安装VS Code，无坑</p>
</li>
<li><p>Step 2 terminal 配置，settings里搜索shell:windows，然后选择想要的terminal，也可以在settings.json上编辑。</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;terminal.integrated.profiles.windows&quot;: &#123;</span><br><span class="line">    &quot;PowerShell&quot;: &#123;</span><br><span class="line">        &quot;source&quot;: &quot;PowerShell&quot;,</span><br><span class="line">        &quot;icon&quot;: &quot;terminal-powershell&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;Command Prompt&quot;: &#123;</span><br><span class="line">        &quot;path&quot;: [</span><br><span class="line">            &quot;$&#123;env:windir&#125;\\Sysnative\\cmd.exe&quot;,</span><br><span class="line">            &quot;$&#123;env:windir&#125;\\System32\\cmd.exe&quot;</span><br><span class="line">        ],</span><br><span class="line">        &quot;args&quot;: [],</span><br><span class="line">        &quot;icon&quot;: &quot;terminal-cmd&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;gitBash&quot;: &#123;</span><br><span class="line">        &quot;path&quot;:&quot;S:\\Git\\bin\\bash.exe&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;terminal.integrated.defaultProfile.windows&quot;: &quot;gitBash&quot;,</span><br></pre></td></tr></table></figure>
<ul>
<li>Step 3 配置远程服务器</li>
</ul>
<p>本地下载 Remote - SSH, Remote - SSH: Editing Configuration Files 两个插件</p>
<p>远程下载和配置暂时掠过，即可完成ssh配置</p>
<p>The SSH installation couldn’t be found 但是git bash 有安装了ssh。所以需要在vs code 配置ssh隧道</p>
<p>​    “remote.SSH.path”: “ssh.exe的路径”</p>
<ul>
<li>Step 4 配置远程Debug断点调试代码</li>
</ul>
<p><strong>Mac/Linux: Add “pythonPath”: “python3”<code>into</code>.vscode/launch.json` to the same place 注意每一个debug的项目都要重复添加一次。</strong></p>
<p>总算是解决了VS Code 远程连接的各种坑，希望对大家有用。</p>
<h2 id="其他BUGS"><a href="#其他BUGS" class="headerlink" title="其他BUGS"></a>其他BUGS</h2><ul>
<li>BUG 1: Setting up SSH Host xxxx: Downloading VS Code Server locally</li>
</ul>
<p>一直卡在”正在打开远程”的状态</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>few-shot-TTS</title>
    <url>/2022/05/27/few-shot-TTS/</url>
    <content><![CDATA[<h1 id="Zero-One-Few-shot-TTS的所有论文"><a href="#Zero-One-Few-shot-TTS的所有论文" class="headerlink" title="Zero/One/Few-shot TTS的所有论文"></a>Zero/One/Few-shot TTS的所有论文</h1><p>研究Few-shot TTS有一段时间了，想要系统化的了解一下这个行业的发展情况，因此在这里总结一下所有有关Zero/One/Few-shot TTS / Voice clone的相关论文。</p>
<p>Motivation: 采集尽可能少量目标说话人的声音，克隆目标说话人的音色，制作tts引擎，重点需支持中文。</p>
<p>Github List:</p>
<ol>
<li>[x] 实时中文语音克隆 Mocking Bird <a href="https://github.com/babysor/MockingBird/blob/main/README-CN.md">Code</a></li>
<li>[x] Neural Voice Cloning with a Few Samples (Sercan Ö. Arık, Baidu, 2018 Dec.) <a href="https://github.com/SforAiDl/Neural-Voice-Cloning-With-Few-Samples">Code1</a> | <a href="https://github.com/IEEE-NITK/Neural-Voice-Cloning">Code2</a> | <a href="https://arxiv.org/pdf/1802.06006.pdf">Paper</a></li>
<li>[x] Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis (Jia Ye, Google, 2019 Jan.) <a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Code</a> | <a href="https://arxiv.org/pdf/1806.04558.pdf">Paper</a></li>
<li>[x] One model to speak them all (Mutian He, 港科技 &amp; 微软 2021 Jul.)  <a href="https://github.com/mutiann/few-shot-transformer-tts">Code</a> | <a href="https://arxiv.org/abs/2103.03541">Paper</a></li>
<li>[x] Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation (Dongchan Min, KAIST, 2021 Jun, ICML) <a href="https://github.com/keonlee9420/StyleSpeech">Code</a> | <a href="https://arxiv.org/abs/2106.03153">Paper</a></li>
<li>[x] SC-GlowTTS: an Efficient Zero-Shot Multi-Speaker Text-To-Speech Model (Edresson Casanova, University of Sa ̃o Paulo, 2021 Jun, Interspeech 2021)</li>
<li>[x] YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone (Edresson Casanova, Universidade de Sa ̃o Paulo, 2022 Feb ) <a href="https://github.com/Edresson/YourTTS">Code</a> | <a href="https://arxiv.org/abs/2112.02418">Paper</a></li>
<li>[x] Zero-shot VITS Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus <a href="https://github.com/hcyspeech/TransferTTS">Code</a> | <a href="https://arxiv.org/pdf/2203.15447.pdf">Paper</a></li>
<li>[x] AdaSpeech <a href="https://arxiv.org/pdf/2103.00993.pdf">Code</a> | <a href="https://arxiv.org/pdf/2103.00993.pdf">Paper</a> || AdaSpeech2 <a href="https://github.com/rishikksh20/AdaSpeech2">Code</a> | <a href="https://arxiv.org/pdf/2104.09715.pdf">Paper</a></li>
<li>Deepfakes for Video Conferencing Using General Adversarial Networks (GANs) and Multilingual Voice Cloning <a href="https://github.com/ghatoledipak/Deepfakes-for-Video-Conferencing-Using-General-Adversarial-Networks-GANs-and-Voice-Cloning">Code</a></li>
<li>[x] Unet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning (Rui Li, CloudMinds Inc, ICASSP 2022) <a href="https://github.com/CMsmartvoice/One-Shot-Voice-Cloning">Code</a> | <a href="https://arxiv.org/pdf/2109.11115.pdf">Paper</a></li>
<li>[x] TorToiSe (jbetker, April 2022) <a href="https://github.com/neonbjb/tortoise-tts/tree/main/tortoise">Code</a> | <a href="https://nonint.com/2022/04/25/tortoise-architectural-design-doc/">Intro</a> | No training methodology</li>
</ol>
<p>Other Large Audio Models:<br><a href="https://github.com/liusongxiang/Large-Audio-Models">https://github.com/liusongxiang/Large-Audio-Models</a></p>
<h3 id="Prompt-based-Audio-Synthesis"><a href="#Prompt-based-Audio-Synthesis" class="headerlink" title="Prompt-based Audio Synthesis"></a>Prompt-based Audio Synthesis</h3><ul>
<li>[x] NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers(2023), Kai Shen et al. <a href="https://arxiv.org/pdf/2304.09116.pdf">[PDF]</a></li>
<li>[x] FoundationTTS: Text-to-Speech for ASR Customization with Generative Language Model(2023), Ruiqing Xue et al. <a href="https://arxiv.org/pdf/2303.02939v3.pdf">[PDF]</a></li>
<li>[x] VALL-E X: Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling (2023), Ziqiang Zhang et al. <a href="https://arxiv.org/pdf/2303.03926.pdf">[PDF]</a></li>
<li>Noise2Music: Text-conditioned Music Generation with Diffusion Models(2023), Qingqing Huang et al. <a href="https://arxiv.org/pdf/2302.03917">[PDF]</a></li>
<li>[x] Spear-TTS: Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision(2023), Eugene Kharitonov et al. <a href="https://arxiv.org/abs/2302.03540">[PDF]</a></li>
<li>MusicLM: Generating Music From Text(2023), Andrea Agostinelli et al. <a href="https://arxiv.org/pdf/2301.11325">[PDF]</a></li>
<li>[x] InstructTTS: Modelling Expressive TTS in Discrete Latent Space with Natural Language Style Prompt (2023), Dongchao Yang et al. <a href="https://arxiv.org/pdf/2301.13662.pdf">[PDF]</a></li>
<li>[x] AudioLDM: Text-to-Audio Generation with Latent Diffusion Models(2023), Haohe Liu et al. <a href="https://arxiv.org/pdf/2301.12503">[PDF]</a></li>
<li>Moûsai: Text-to-Music Generation with Long-Context Latent Diffusion(2023), Flavio Schneider et al. <a href="https://arxiv.org/pdf/2301.11757">[PDF]</a></li>
<li>[x] Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models(2023), Rongjie Huang et al. <a href="https://text-to-audio.github.io/paper.pdf">[PDF]</a></li>
<li>[x] VALL-E: Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers (2023), Chengyi Wang et al. <a href="https://arxiv.org/pdf/2301.02111.pdf">[PDF]</a></li>
<li>[x] Diffsound: Discrete Diffusion Model for Text-to-sound Generation (2022), Dongchao Yang et al.<a href="https://arxiv.org/pdf/2207.09983.pdf">[PDF]</a></li>
<li>[x] VQTTS: High-Fidelity Text-to-Speech Synthesis with Self-Supervised VQ Acoustic Feature (2022), Chenpeng Du. <a href="https://arxiv.org/pdf/2204.00768.pdf">[PDF]</a></li>
<li>[x] DiscreTalk: Text-to-Speech as a Machine Translation Problem (2020), Tomoki Hayashi. <a href="https://arxiv.org/pdf/2005.05525.pdf">[PDF]</a></li>
<li>[x] DelightfulTTS 2: End-to-End Speech Synthesis with Adversarial Vector-Quantized Auto-Encoders (2022), Yanqing Liu <a href="https://arxiv.org/pdf/2207.04646.pdf">[PDF]</a></li>
<li>[x] PromptTTS: Controllable Text-to-Speech with Text Descriptions (2022), Zhifang Guo <a href="https://arxiv.org/pdf/2211.12171.pdf">[PDF]</a></li>
</ul>
<h3 id="Audio-Language-Models"><a href="#Audio-Language-Models" class="headerlink" title="Audio Language Models"></a>Audio Language Models</h3><ul>
<li>[x] AudioLM: a Language Modeling Approach to Audio Generation(2022), Zalán Borsos et al. <a href="https://arxiv.org/pdf/2209.03143">[PDF]</a></li>
</ul>
<h3 id="Audio-SSL-and-UL-models"><a href="#Audio-SSL-and-UL-models" class="headerlink" title="Audio SSL and UL models"></a>Audio SSL and UL models</h3><ul>
<li>[x] vq-wav2vec: Self-Supervised Learning of Discrete Speech Representations(2019), Alexei Baevski et al. <a href="https://arxiv.org/pdf/1910.05453.pdf">[PDF]</a></li>
<li>MuLan: A Joint Embedding of Music Audio and Natural Language (2022) Qingqing Huang et al. <a href="https://arxiv.org/pdf/2208.12415">[PDF]</a></li>
<li>[x] W2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training (2021) <a href="https://arxiv.org/pdf/2108.06209">[PDF]</a></li>
<li>[x] HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units (2021) Wei-Ning Hsu et al. <a href="https://arxiv.org/pdf/2106.07447.pdf">[PDF]</a>,</li>
<li>[x] wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (2020), Alexei Baevski et al. <a href="https://arxiv.org/pdf/2006.11477.pdf">[PDF]</a></li>
<li>[x] Data2vec: A general framework for self-supervised learning in speech, vision and language (2022), Alexei Baevski et al. <a href="https://arxiv.org/pdf/2202.03555.pdf">[PDF]</a></li>
<li>[x] SpeechT5: Unified-Modal Encoder-Decoder Pre-Training for Spoken Language Processing (2022), Junyi Ao. <a href="https://arxiv.org/pdf/2110.07205.pdf">[PDF]</a></li>
<li>[x] CLAP: Learning Audio Concepts From Natural Language Supervision (2022), Benjamin Elizalde <a href="https://arxiv.org/pdf/2206.04769.pdf">[PDF]</a></li>
<li>[x] AudioGen: Textually Guided Audio Generation (2023), Felix Kreuk. <a href="https://arxiv.org/pdf/2209.15352.pdf">[PDF]</a></li>
<li>[x] WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing (2022), Sanyuan Chen <a href="https://arxiv.org/pdf/2110.13900.pdf">[PDF]</a></li>
<li>[x] Wav2vec: Unsupervised Pre-training for Speech Recognition (2019), Steffen Schneider <a href="https://arxiv.org/pdf/1904.05862.pdf">[PDF]</a></li>
<li>[x] A Brief Overview of Unsupervised Neural Speech Representation Learning (2022), Lasse Borgholt <a href="https://arxiv.org/pdf/2203.01829.pdf">[PDF]</a></li>
<li>[x] ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers (2022), Kaizhi Qian <a href="https://arxiv.org/abs/2204.09224">[PDF]</a></li>
<li>[x] IQDUBBING: Prosody modeling based on discrete self-supervised speech representation for expressive voice conversion (2022), Wendong Gan <a href="https://arxiv.org/pdf/2201.00269.pdf">[PDF]</a></li>
<li>[x] SoftVC: A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion (2022), Benjamin van Niekerk <a href="https://arxiv.org/pdf/2111.02392.pdf">[PDF]</a></li>
</ul>
<h3 id="Encodec-models"><a href="#Encodec-models" class="headerlink" title="Encodec models"></a>Encodec models</h3><ul>
<li>[x] High Fidelity Neural Audio Compression (2022), Alexandre Défossez <a href="https://arxiv.org/pdf/2210.13438.pdf">[PDF]</a></li>
<li>[x] SoundStream: An End-to-End Neural Audio Codec (2021), Neil Zeghidour <a href="https://arxiv.org/pdf/2107.03312.pdf">[PDF]</a></li>
<li>[x] HIFI-CODEC: GROUP-RESIDUAL VECTOR QUANTIZATION FOR HIGH FIDELITY AUDIO CODEC <a href="https://arxiv.org/pdf/2305.02765.pdf">[PDF]</a></li>
</ul>
<h3 id="System-level"><a href="#System-level" class="headerlink" title="System-level"></a>System-level</h3><ul>
<li>[x] AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head (2023), Rongjie Huang <a href="https://arxiv.org/pdf/2304.12995.pdf">[PDF]</a></li>
</ul>
<p>无 Code List:</p>
<ol>
<li><p><a href="https://arxiv.org/pdf/1809.10460.pdf">Sample Efficient Adaptive Text-to-Speech</a> (Yutian Chen, DeepMind &amp; Google, 2019 Jan, ICLR 2019)</p>
</li>
<li><p><a href="https://arxiv.org/abs/2005.08484">Attentron: Few-shot Text-to-Speech Exploiting Attention-based Variable Length Embedding</a> (Seungwoo Choi, Hyperconnect, 2020 Aug., Interspeech 2020)</p>
</li>
<li><p><a href="">BOFFIN TTS: Few-Shot Speaker Adaptation by Bayesian Optimization</a>(<a href="https://arxiv.org/abs/2002.01953">https://arxiv.org/abs/2002.01953</a>) (Henry B. Moss, Amazon, 2020 Feb., ICASSP 2020)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2102.00151.pdf">Expressive Neural Voice Cloning</a> (Paarth Neekhara, 2021 Jan, 加州大学圣地亚哥分校, PMLR 2021)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2103.04699.pdf">CUHK-EE VOICE CLONING SYSTEM FOR ICASSP 2021 M2VOC CHALLENGE</a> (Daxin Tan, 港中文, 2021 Jul)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2111.04040.pdf">Meta-TTS: Meta-Learning for Few-Shot Speaker Adaptive Text-to-Speech</a> (Sung-Feng Huang, National Taiwan Uni, 2021 Nov,  IEEE/ACM Transactions on Audio, Speech, and Language Processing)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1907.04448.pdf">Learning to Speak Fluently in a Foreign Language: Multilingual Speech Synthesis and Cross-Language Voice Cloning</a> (Yu Zhang, Google, 2019 Jul.)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2104.01818.pdf">The Multi-Speaker Multi-Style Voice Cloning Challenge 2021</a> (Qicong Xie, ASLP, 2021 Apr., ICASSP 2021)</p>
</li>
<li><p><a href="https://sci-hub.se/10.1109/icassp39728.2021.9414727">Dian: Duration Informed Auto-Regressive Network for Voice Cloning</a> (Wei song, JD, 2021 May, ICASSP 2021)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2204.03421.pdf">Self supervised learning for robust voice cloning</a> (Konstantinos Klapsas, Innoetics, 2022 Apr, submitted to Interspeech 2022)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2203.09708.pdf">Improve few-shot voice cloning using multi-modal learning</a> (Haitong Zhang, NetEase Games, 2022 Mar, 2022 IEEE International Conference on Acoustics, Speech and Signal Processing)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2110.03347.pdf">Cloning one’s voice using very limited data in the wild</a> (Dongyang Dai, (SAMI), ByteDance, 2021 Oct)</p>
</li>
<li><p><a href="https://arxiv.org/pdf/2111.12890.pdf">V2C: Visual Voice Cloning</a> (Qi Chen, 阿德莱德大学（澳洲），华南理工大学, 2021 Nov., )</p>
</li>
</ol>
<p>综述类:</p>
<ol>
<li><a href="https://www.irjmets.com/uploadedfiles/paper/issue_2_february_2022/18991/final/fin_irjmets1644437365.pdf">Voice cloning</a>(Saiesh Prabhu Verlekar, Shree Rayeshwar Institute Of Engineering And Information Technology Goa 2022 Feb, IRJETS 2022)</li>
</ol>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>few-shot-TTS-details</title>
    <url>/2022/05/30/few-shot-TTS-details/</url>
    <content><![CDATA[<h1 id="few-shot-tts-解决方案">Few-shot TTS 解决方案</h1>
<h3 id="目前的解决方案">== 目前的解决方案</h3>
<p>目前自研的解决方案是：</p>
<ul>
<li>需要30条target speaker的语音（3分钟）</li>
<li>训练10分钟</li>
<li>MOS: 4.5， 下限根据数据来看</li>
<li>鲁棒性99%， 下限根据数据来看</li>
<li>Similarity: 4.5， 下限根据数据来看</li>
<li>RTF：200ms/10s = 0,02，甚至可以再做压缩。</li>
</ul>
<p><strong>痛点：</strong></p>
<ol style="list-style-type: decimal">
<li>把10分钟训练时间砍掉，尽量做到实时的</li>
<li>30条语音数据需要的质量较高，采集有难度：方向1）降低数据所需质量，2）降低数据所需数量</li>
</ol>
<h2 id="with-open-source-code">With open-source code</h2>
<p>Quick Overview</p>
<table style="width:75%;">
<colgroup>
<col width="5%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
<col width="6%" />
</colgroup>
<thead>
<tr class="header">
<th>Code #</th>
<th>Model</th>
<th>Date</th>
<th>Institution</th>
<th>Author</th>
<th>Architecture</th>
<th># of tgt audios</th>
<th>MOS</th>
<th>Similarity</th>
<th>RTF</th>
<th>memo</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>4</td>
<td>YourTTS</td>
<td>Feb 2022</td>
<td>Brazil</td>
<td>Edresson Casanova</td>
<td>VITS+spk emb+language emb+SCL</td>
<td>1</td>
<td>3.97</td>
<td>3.07</td>
<td>2</td>
<td>🌟🌟🌟英文</td>
</tr>
<tr class="even">
<td>5</td>
<td>Meta-StyleSpeech</td>
<td>Jun 2021(ICML)</td>
<td>KAIST</td>
<td>Dongchan Min</td>
<td>FastSpeech+GST+episodic meta learning</td>
<td>1</td>
<td>3.82</td>
<td>2.6</td>
<td>NA</td>
<td>🌟🌟英文</td>
</tr>
<tr class="odd">
<td>10</td>
<td>Tortoise</td>
<td>Apr 2022</td>
<td>Google</td>
<td>Jbetker</td>
<td>autoregressive decoder+clvp+cvvp+diffusion decoder+vocoder+...</td>
<td>1~5</td>
<td>3.5</td>
<td>2.3</td>
<td>12</td>
<td>🌟英文</td>
</tr>
<tr class="even">
<td>2</td>
<td>Unet-TTS</td>
<td>ICASSP 2022</td>
<td>CloudMinds</td>
<td>Rui Li</td>
<td>Unet</td>
<td>1</td>
<td>3.3</td>
<td>2.5</td>
<td>NA</td>
<td>🌟中文</td>
</tr>
<tr class="odd">
<td>3</td>
<td>BaiduVC</td>
<td>Oct 2018</td>
<td>Baidu</td>
<td>Sercan Ö. Arık</td>
<td>Spk adaption</td>
<td>10</td>
<td>3.16</td>
<td>3.16</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td>8</td>
<td>OneModel</td>
<td>Jul 2021</td>
<td>HKUST</td>
<td>Mutian He</td>
<td>byte</td>
<td>10</td>
<td>3.7</td>
<td>NA</td>
<td>NA</td>
<td></td>
</tr>
<tr class="odd">
<td>1</td>
<td>SV2TTS-CN</td>
<td>NA</td>
<td>Mocking Bird</td>
<td>NA</td>
<td>Tacotron2/Transformer+spk emb</td>
<td>1</td>
<td>2.9</td>
<td>2.5</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td>6</td>
<td>Zero-shot VITS</td>
<td>Jan 2019</td>
<td>Seoul National University</td>
<td>Minchan Kim</td>
<td>VITS+wav2vec</td>
<td>10min</td>
<td>4.42</td>
<td>NA</td>
<td>NA</td>
<td></td>
</tr>
<tr class="odd">
<td>7</td>
<td>AdaSpeech 1&amp;2</td>
<td>March&amp;Apr 2021</td>
<td>Microsoft</td>
<td>Xu Tan</td>
<td>Fastspeech+ConLayerNorm</td>
<td>10-20</td>
<td>3.45</td>
<td>3.59</td>
<td>NA</td>
<td></td>
</tr>
<tr class="even">
<td>9</td>
<td>SV2TTS</td>
<td>Jan 2019</td>
<td>Google</td>
<td>Ye Jia</td>
<td>Tacotron2+spk emb</td>
<td>1</td>
<td>4.2</td>
<td>3.28</td>
<td>NA</td>
<td></td>
</tr>
</tbody>
</table>
<h3 id="code-1-realtime-voice-clone-for-chinese-mocking-bird">== [✅] Code 1: Realtime Voice Clone for Chinese (Mocking Bird)</h3>
<p>改自 仅支持英语的<a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Real-Time-Voice-Cloning</a> 分叉出来的支持中文的Repo</p>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>Voice cloning 语音克隆</li>
<li>语音转换 （PPG-based）</li>
<li>支持中文、英文</li>
</ol>
<p><strong>Conclusion</strong> 听demo效果有点差，电音很严重，mos估计3.0左右</p>
<h3 id="code-2-unet-tts-improving-unseen-speaker-and-style-transfer-in-one-shot-voice-cloning">== [✅] Code 2: Unet-TTS: Improving Unseen Speaker and Style Transfer in One-shot Voice Cloning</h3>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>一句参考语音可进行克隆TTS; 一句话语音风格迁移系统，给定reference audio，能够克隆目标说话人的语音和韵律，可实现多情感合成，但是MOS较低</li>
<li>支持中文</li>
</ol>
<p><strong>Conclusion</strong> 复现mos3.0左右，但可实现多情感；也有预训练好的中文模型，不需要从0训练</p>
<h3 id="code-3-neural-voice-cloning-with-a-few-samples-sercan-o.-arık-baidu-2018-dec.">== [❌] Code 3: Neural Voice Cloning with a Few Samples (Sercan Ö. Arık, Baidu, 2018 Dec.)</h3>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>一句参考语音可进行克隆TTS</li>
</ol>
<h3 id="code-4-yourtts-towards-zero-shot-multi-speaker-tts-and-zero-shot-voice-conversion-for-everyone-edresson-casanova-universidade-de-sa-o-paulo-2022-feb">== [✅] Code 4: YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice Conversion for everyone (Edresson Casanova, Universidade de Sa ̃o Paulo, 2022 Feb)</h3>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>单语种多说话人数据训练模型，实现单语种的zero-shot tts</li>
<li>bi/tri-lingual</li>
<li>tri-lingual for single language plus</li>
<li>Zero-shot cross-lingual VC</li>
<li>Speaker Adaption</li>
</ol>
<p><strong>Conclusion:</strong> - MOS: 3.8 连贯性还ok，问题1: 噪声， 问题2: 韵律 ||| 论文里的MOS 3.97左右 - Similarity: 3.0 比较难以听出是target speaker的声音，如果两个对比的话可以区分出来一点点区别 ||| 论文里的 Similarity 3.07 - RTF: 2 - coquiTTS的 <a href="https://github.com/coqui-ai/TTS/releases/download/v0.6.0_models/tts_models--multilingual--multi-dataset--your_tts.zip">tts_models--multilingual--multi-dataset--your_tts.zip</a> 测试了之后，可以实现one-shot TTS。 - 没有提供训练的方案，难以在其他语言中复现。</p>
<p>论文里的speaker adaption MOS提升了0.07左右，Similarity持平，并且听Demo也比较差，相似度低，仍存在zero-shot 的噪声和韵律问题，所以不准备再尝试了。</p>
<h3 id="code-5-meta-stylespeech-multi-style-adaptive-text-to-speech-generation">== [✅] Code 5: Meta-StyleSpeech: Multi-Style Adaptive Text to Speech Generation</h3>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>单语种zero-shot tts 且包含口音</li>
</ol>
<p><strong>Conclusion:</strong> MOS: 3.5, Similarity: 2.5。英文效果来说不及YourTTS</p>
<h3 id="code-6-transfer-learning-framework-for-low-resource-text-to-speech-using-a-large-scale-unlabeled-speech-corpus">== [❌] Code 6: Transfer Learning Framework for Low-Resource Text-to-Speech using a Large-Scale Unlabeled Speech Corpus</h3>
<p>支持的功能点有：</p>
<ol style="list-style-type: decimal">
<li>只需要最少10min的labeled数据集，利用了大量的unlabed的数据集</li>
</ol>
<p><strong>Conclusion:</strong> 10min数据太长了，目前只需要3min</p>
<h3 id="code-7-adaspeech">== [❌] Code 7: AdaSpeech</h3>
<p>AdaSpeech</p>
<ol style="list-style-type: decimal">
<li>voice cloning given (10~20)+ sentences of any speaker, adaptively training 2k steps</li>
<li>需要：1) LibriTTS (2456 speakers, 586 hours)</li>
</ol>
<p>AdaSpeech 2</p>
<ol style="list-style-type: decimal">
<li>voice cloning given 50 unlabeled sentences of any speaker, adaptively training1w+2k steps</li>
<li>需要：1) LibriTTS (2456 speakers, 586 hours)</li>
</ol>
<p><strong>Conclusion:</strong> 不及现有算法。samples和training time都达不到目前的算法预期。</p>
<h3 id="waiting-code-8-one-model-to-speak-them-all">== [Waiting] Code 8: One model to speak them all</h3>
<ol style="list-style-type: decimal">
<li>Byte2Speech, 40s 语音 实现voice cloning 和 low-resources languages tts。</li>
<li>可以使biaobei说英文，ljspeech说中文。</li>
<li>可以尝试小数据量下的粤语模型实现（按照github的recipe，30-100条【文本，音频】可以得到有效的粤语模型）</li>
</ol>
<h3 id="code-9-transfer-learning-from-speaker-verification-to-multispeaker-text-to-speech-synthesis">== [❌] Code 9: Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis</h3>
<p><strong>Conclusion:</strong> 复现过，效果差。以前复现过中文，效果一般，可以参照Mocking Bird的B站demo，mos估计3.0左右</p>
<h3 id="code-10-tortoise">== [✅] Code 10: TorToiSe</h3>
<p><strong>Conclusion:</strong> 1. 英文复现在部分说话人上还可以（比如ljspeech），2-5条可以得到不错的效果，但在任意给定的自然人上效果一般。 2. 生成速度略慢，按照原文是2分钟生成10s左右的语音。rtf：12。 3. 没有提供训练的方案，难以在其他语言中复现。</p>
<p>模型细节： 1. 由5个独立训练的模型组成: autoregressive decoder</p>
<h4 id="reference-clips">-- reference clips</h4>
<p>被ViT(一组reducing convolutions + 一组全注意力的transformers)的模块所读取，在head处，我们取序列中所有元素的mean。然后，我们取所有reference clip的latent的mean来生成一个single reference latent，在代码里被称作conditioning latent。</p>
<h4 id="语音和文本输入">-- 语音和文本输入</h4>
<p>自回归解码器主要负责从文本到语音的转换。首先，语音被采样到22kHz，再将其降采样 1024x，即将其编码为Mel频谱（一个256x的压缩），然后再通过一个在mel频谱上训练的VQVAE将其降低4x。</p>
<p>当压缩了1024x后，生成的语音有相较于对应文本2x的空间维度。这样，这个问题将变成一个容易处理的问题，即通过一个nlp中使用的flat transformer即可处理。</p>
<p>文本首先通过一个经典的256-token的BPE词典来转化为tokens，这个词典是在所使用的语音数据库的文本侧来训练的。语音通过上述提到的VQVAE方法，被转换为8192tokens。</p>
<h4 id="模型">-- 模型</h4>
<p>对于实际模型来说，我采用了多层transformer结构来实验。最吸引人的选项是标准的encoder-decoder结构。然而，当我伸缩decoder的深度或者隐藏层的大小时，我遇到了收敛问题。由于这个原因，我选择建立一个只有decoder的结构，就像DALLE中所完成的那样。类似于DALLE，我使用了一个从Huggingface Transformer library中的 <em>GPT-2模型</em> ，且做了一点小的改进以支持多种模态。</p>
<p>通过使用GPT-2， 我能够在一组transformer layers中执行所有的计算。就像通过OpenAI研究所展示的那样，这样模型的类型很难变换且在多模态情况下完成的更好。最终的GPT结构由30层组成，每一层有16个attention heads和一个1024维的隐藏层。这个总体上有420M参数。作为对比，GPT-2有1.5B参数，DALLE是12B，GPT-3是175B。</p>
<p>与DALLE不同的是，我在所有层上选择了全联接。</p>
<p>我在提交到全尺寸版本的时候训练了一系列的玩具模型，并且发布到了Tortoise。 更小的模型是成功的，但是他们的多说话人的能力比较差。我强烈相信，这个模型是一个用于伸缩变换的杰出的候选人。它欠拟合于数据，在3个epoch内收敛，且在训练和验证集之间没有显著差异。</p>
<h4 id="组成输入">-- 组成输入</h4>
<p>一旦文本和语音都被tokenized，并且条件隐状态被计算了，每个文本和语音序列都被他们自己的START和STOP tokens来padded，然后通过分别的embeddings来输入。然后我们建立了<em>GPT-2</em> transformer 的inputs通过拼接3个inputs如下：【conditioning_point_latent, text_embeddings, voice_embeddings】</p>
<p>输出的模型是一个 next-token-prediction模型配套了一个转换：它能够预测文本和语音tokens，后者通过首先给模型一些text tokens和一个START voice token。</p>
<h4 id="问题">-- 问题</h4>
<p>这个模型包含了一个概率性的理解，文本和语音如何关系到彼此通过给定上下文的声音。然而，如果你简单的常识解码高度压缩的语音表示，它将近乎不智能。这是因为这样巨大的压缩比例使得大部分的音频信息丢失。</p>
<p>这个模型的另一个问题是，解码本身是“有挑战性的”。执行束搜索和贪婪解码会使得，语音像一个长的“uhhhhhhhhhh”或者仅仅简单的静音。这个是自回归解码众所周知的问题。</p>
<p>以上问题的解决方案通过如下来解决。</p>
<h4 id="clvp">-- CLVP</h4>
<p>Contrastive Language-voice pretraining (CLVP)模型被构建来解决上述提到的解码器的问题。为了解决这个问题，我用了类似于DALLE的解决方案。nucleus sampling被使用来从自回归解码器生成大量的候选者，然后CLVP模型被用来从这些候选者之中选择最可能的text和voice对。</p>
<p>在实际操作中，这个步骤进展的很顺利，除了一个缺点，即你必须首先从自回归模型中计算出许多candidates。</p>
<h4 id="the-model">-- The model</h4>
<p>CLVP与CLIP的结构很相似，除了图像作为第二模态，CLVP吸收了自回归解码器生成的tokenized outputs （其实是高度压缩的语音片段）。tokenized inputs被使用而不是原始语音或者频谱，因为，解码这些tokens来省城语音采样点是非常计算昂贵的。CLVP倾向于在token-space运转的非常好。</p>
<p>CLVP的文本和语音编码器由一组12层全注意力的transformer层来组成。隐藏层维度是512维且包含8个注意力头。模型总共是67M参数</p>
<h4 id="cvvp">-- CVVP</h4>
<p>Contrastive voice-voice pretraining (CVVP)是一个对比模型学习pair一个clip of 某人说某事通过自回归解码器的tokenized outputs 当输入某个人说某件事。它的目的是steer自回归解码器以相似于reference clip的vocal质量。</p>
<p>CVVP与CLVP几乎是完全相似的，除了它输入文本而不是音频。</p>
<p>CVVP对Tortoise的贡献很小。它能够被完全忽略，并且你用剩下的部分仍旧能够获得一个很好的TTS程序。我没有一种方式来量化它对Tortoise的贡献，但我能主观上分辨出CVVP生成的输出与那些没有CVVP的输出</p>
<h4 id="diffusion-decoder">-- Diffusion decoder</h4>
<p>剩下我们需要解决的问题就是解码高度压缩的语音表示回能够用计算机播放的真实语音。即，一个super-resolution的问题。Diffusion models 是super-resolution问题的king（实际上Diffusion models是所有生成模型的king）。所以我自然的选择了这个模型。</p>
<p>Diffusion decoder采取自回归transformer的输出和reference clips并且用这两个输入来构建真实的mel频谱</p>
<h4 id="model">-- Model</h4>
<p>自回归输出首先被一组4x全注意力模块来预处理，并且一个额外的3组全注意力/resnet模块。</p>
<p>The reference clips被一个ViT风格的transformer栈来预处理为point-latent，相似于reference clips被自回归解码器预处理的方式。这个模块的输出称为&quot;conditioning latent&quot;.</p>
<p>The conditioning latent 被用来scale和shift mean和variance of the codes latents。这些突变的codes然后被拼接回diffusion decoder的输入在输入到主要的transformer stack之前。</p>
<p>大多数文献中的diffusion models 是Unets和full attention在底层。因为diffusion decoder 仅仅在mel频谱数据上操作，也是深度压缩的，我能够采用一个简单的结构。The diffusion decoder 由一组 10个可替换的full-attention层和残差卷积模块。隐藏维度是1024和16个attention heads。这个模块最终是292M 参数。</p>
<p>就像在文献中提到的，diffusion timestep 信号被输入到网络结构，且被用于计算残差模块输出的mean和variance。</p>
<h4 id="the-vocoder">-- The vocoder</h4>
<p>以上述描述的结构，我们有一个能生成mel频谱的模型当给定文本和一些reference语音数据。最后一步是将mel频谱转换为语音。</p>
<p>幸运的是，这种框架在TTS领域是很常见的，所以过去一些年，大部分研究被投入其中，开源的声码器如WaveGlow和Univnet提供了显著的效果来将mel频谱转化为语音，甚至能够对集外的声音进行转换。</p>
<p>由于这个原因，我选择使用off-the-shelf vocoder：univnet。这个vocoder与waveglow对比有显著的优势，但是推理更快，模型更小。</p>
<p>这个结论的一个结果是，我被迫在univnet期待的采样率输出语音：24KHz。这与Tortoise模型的其他部分不同，是在22kHz上运作的。相似的，我被迫使用从Tacotron中生成的Mel频谱来与univnet交互，而不是Pytorch libraries（这两种生成略微有点区别的频谱）。</p>
<h4 id="why-vocoder">-- Why vocoder</h4>
<p>其实Diffusion models已经能够输出原始语音了，那为何我还要采用一个vocoder？实际上我花费了大量的时间来用diffusion models直接输出语音。发现如下： 1. 在高维数据上进行操作意味着我不需要使用U-net结构，这个相对我使用的flat结构效果差些； 2. 在高维数据上运作意味着diffusion U-net不得不有一个非常小的channel 在顶层，这导致了严重的性能损失 3. 重复性的在高维空间操作convolutions在计算上是低效的（因为diffusion models在推理阶段是必须的） 4. Vocoders开源好用</p>
<h4 id="system-level-description">-- System Level Description</h4>
<p>最后我们整体上描述Tortoise的结构，来理解他们如何生成语音： 1. 自回归解码器输入文本和reference clips。输出latents和对应的token codes来代表高度压缩的语音数据 1） Nucleus sampling被用作decoding strategy 2） 这个步骤被执行了多次来生成许多“candidate” latents 2. CLVP和CVVP模型选择最好的candidate： 1） CLVP模型在输入文本和每个candidate code sequence之间生成来一个相似分数 2） CVVP模型啦在reference clips和每个candidate之间生成一个相似分数 3） 两个相似度分数被结合到一起。 4） candidate和最高的整体相似度被选择进行下一步骤 3. Diffusion decoder输入自回归latents和reference clips来生成mel频谱代表某些语音输出 4. Univnet声码器被用来将Mel频谱转换为实际的语音数据。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>llvm-bugs</title>
    <url>/2022/06/09/llvm-bugs/</url>
    <content><![CDATA[<h1 id="llvm-安装过程遇到的坑">LLVM 安装过程遇到的坑</h1>
<p>目标：本意只是想pip install numba==0.48 但是numba 依赖llvmlite，所以pip install llvmlite 报错</p>
<p>llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config</p>
<p>ll /usr/bin/llvm-<em>时发现有很多llvm-</em>的文件，但是就是没有llvm-config，原因排查中</p>
<p>emmm最后放弃了安装llvmlite</p>
<p>线索1: LLVM 3.4.2 安装之后不存在llvm-config</p>
<p>Which version of LLVM is required? LLVM 3.4.2 in Centos 6.x does not have llvm-config</p>
<p>线索2:<code>numba</code> <code>0.55</code> was just released (https://github.com/numba/numba/releases/tag/0.55.0). 解决了这个llvmlite的问题</p>
<p>Conclusion：最终放弃安装numba0.48，而是安装了numba 0.55 顺利通关</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>有关VC, SSL以及Feature Distanglement的论文总结</title>
    <url>/2022/12/11/vc/</url>
    <content><![CDATA[<h1 id="有关VC-SSL-Feature-Distanglement-的论文总结"><a href="#有关VC-SSL-Feature-Distanglement-的论文总结" class="headerlink" title="有关VC, SSL, Feature Distanglement 的论文总结"></a>有关VC, SSL, Feature Distanglement 的论文总结</h1><h2 id="Voice-Conversion-VC"><a href="#Voice-Conversion-VC" class="headerlink" title="=== Voice Conversion (VC)"></a>=== Voice Conversion (VC)</h2><p><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2021/eskimez21_interspeech.pdf">One-Shot Voice Conversion with Speaker-Agnostic StarGAN</a> | Microsoft | Interspeech 2021 | <a href="https://github.com/liusongxiang/StarGAN-Voice-Conversion">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.00043.pdf">StarGAN-ZSVC: Towards Zero-Shot Voice Conversion in Low-Resource Contexts</a> | Stellenbosch University | SACAIR 2021 | <a href="https://github.com/Top34051/stargan-zsvc">repo</a></p>
<p><a href="https://arxiv.org/pdf/1904.05742.pdf">One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</a> | National Taiwan University | Interspeech 2019 | <a href="https://github.com/jjery2243542/adaptive_voice_conversion">repo</a></p>
<p><a href="https://arxiv.org/pdf/1905.05879.pdf">AUTOVC: Zero-Shot Voice Style Transfer with Only Autoencoder Loss</a> | MIT-IBM | ICML 2019 | <a href="https://github.com/CODEJIN/AutoVC">repo</a></p>
<p><a href="https://arxiv.org/pdf/2006.04154.pdf">VQVC+: One-Shot Voice Conversion by Vector Quantization and U-Net architecture</a> | National Taiwan University | Interspeech 2020 | <a href="https://github.com/ericwudayi/SkipVQVC">repo</a></p>
<p><a href="https://arxiv.org/pdf/2011.00316.pdf">AGAIN-VC: A ONE-SHOT VOICE CONVERSION USING ACTIVATION GUIDANCE AND ADAPTIVE INSTANCE NORMALIZATION</a> | National Taiwan University | ICASSP 2021 | <a href="https://github.com/KimythAnly/AGAIN-VC">repo</a></p>
<p><a href="https://ieeexplore.ieee.org/document/9053854">One-Shot Voice Conversion by Vector Quantization</a>| National Taiwan University | ICASSP 2020</p>
<p><a href="https://www1.se.cuhk.edu.hk/~hccl/publications/pub/201909_INTERSPEECH_HuiLU.pdf">One-shot Voice Conversion with Global Speaker Embeddings</a> | Tsinghua-CUHK | Interspeech 2019</p>
<p><a href="https://arxiv.org/pdf/2010.12788.pdf">GAZEV: GAN-Based Zero-Shot Voice Conversion over Non-parallel Speech Corpus</a> | Yitu Technology | Interspeech 2020</p>
<p><a href="https://arxiv.org/pdf/2111.02392.pdf">A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion</a> | Ubisoft La Forge | ICASSP 2022 | <a href="https://github.com/bshall/soft-vc">repo</a></p>
<h2 id="Self-supervised-Learning-SSL"><a href="#Self-supervised-Learning-SSL" class="headerlink" title="=== Self-supervised Learning (SSL)"></a>=== Self-supervised Learning (SSL)</h2><p><a href="https://www.isca-speech.org/archive/pdfs/interspeech_2019/schneider19_interspeech.pdf">wav2vec: Unsupervised Pre-training for Speech Recognition</a> | Facebook AI Research | INTERSPEECH 2019 | <a href="https://github.com/eastonYi/wav2vec">repo</a></p>
<p><a href="https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</a> | Facebook AI | Neurips 2020 | <a href="https://github.com/TencentGameMate/chinese_speech_pretrain">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.07447.pdf">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</a> | Wei-Ning Hsu, Meta AI | TASLP 2021 | <a href="https://github.com/bshall/hubert">repo</a></p>
<p><a href="https://proceedings.mlr.press/v162/qian22b/qian22b.pdf">ContentVec: An Improved Self-Supervised Speech Representation by Disentangling Speakers</a> | MIT-IBM Watson AI Lab | PMLR 2022 | <a href="https://proceedings.mlr.press/v162/qian22b/qian22b.pdf">repo</a></p>
<p><a href="https://arxiv.org/pdf/2106.05933.pdf">PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition</a> | MIT CSAIL | Neurips 2021 | <a href="https://github.com/pytorch/fairseq/blob/master/examples/
 wav2vec/README.md">repo</a></p>
<p><a href="https://arxiv.org/pdf/2110.13900.pdf">WavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing</a> | Microsoft | JSTSP 2022 | <a href="https://github.com/microsoft/unilm/tree/master/wavlm">repo</a></p>
<p><a href="https://arxiv.org/pdf/2202.03555.pdf">data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language</a> | Wei-Ning Hsu, Meta AI | ICML 2022 | <a href="www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.">repo</a></p>
<h2 id="Feature-Disentanglement"><a href="#Feature-Disentanglement" class="headerlink" title="=== Feature Disentanglement"></a>=== Feature Disentanglement</h2><p><a href="https://arxiv.org/pdf/2203.14156.pdf">SpeechSplit 2.0: Unsupervised speech disentanglement for voice conversion Without tuning autoencoder Bottlenecks</a> | MIT-IBM Watson AI Lab | ICASSP 2022 | <a href="https://github.com/biggytruck/SpeechSplit2">repo</a></p>
<p><a href="https://arxiv.org/pdf/2210.13771.pdf">Disentangled Speech Representation Learning for One-Shot Cross-lingual Voice Conversion Using β-VAE</a> | CUHK | SLT 2022 | <a href="无">repo</a></p>
<p><a href="https://arxiv.org/pdf/2208.08757v1.pdf">Speech Representation Disentanglement with Adversarial Mutual Information Learning for One-shot Voice Conversion</a> | Tsinghua | Interspeech 2022 | <a href="https://github.com/YoungSeng/SRD-VC">repo</a></p>
<p><a href="https://arxiv.org/pdf/2203.01829.pdf">A Brief Overview of Unsupervised Neural Speech Representation Learning</a> | University of Copenhagen | AAAI SAS 2022</p>
<p><a href="https://openreview.net/pdf?id=TgSVWXw22FQ">Improving Zero-shot Voice Style Transfer via Disentangled Representation Learning</a> | Duke Uni | ICLR 2021</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>SUPERB</title>
    <url>/2023/07/25/SUPERB/</url>
    <content><![CDATA[
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
  <entry>
    <title>BigVGAN</title>
    <url>/2023/07/20/BigVGAN/</url>
    <content><![CDATA[<h1 id="bigvgan一种具有大规模训练的通用神经声码器">BIGVGAN：一种具有大规模训练的通用神经声码器</h1>
<h2 id="摘要">摘要</h2>
<p>尽管最近在基于生成对抗性网络（GAN）的声码器方面取得了进展，其中该模型根据声学特征生成原始波形，但在各种记录环境中为众多扬声器合成高保真音频是一项挑战。在这项工作中，我们介绍了BigVGAN，这是一种通用的声码器，可以很好地推广到各种分布外的场景，而无需进行微调。我们在GAN生成器中引入了<strong>周期激活函数</strong>和<strong>抗混叠表示</strong>，这为音频合成带来了所需的<strong>电感偏置（inductive bias）</strong>，并显著提高了音频质量。此外，我们以高达112M个参数的最大规模训练我们的GAN声码器，这在文献中是前所未有的。我们识别并解决了大规模音频GAN训练中的故障模式，同时在不过度正则化的情况下保持高保真度输出。我们的BigVGAN只接受过清洁语音（LibriTTS）培训，在各种零样本（非分发）条件下都能实现最先进的性能，包括看不见的扬声器、语言、录音环境、歌唱声音、音乐和乐器音频。我们在以下位置发布代码和模型：https://github.com/NVIDIA/BigVGAN.</p>
<h2 id="introduction">1. Introduction</h2>
<p>深度生成模型在对原始音频建模方面取得了显著的成功。成功的方法包括自回归模型、基于流的模型、基于GAN的模型和扩散模型。</p>
<p>在这些方法中，基于GAN的声码器可以生成以mel声谱图为条件的高保真原始音频，同时在单个GPU上合成比实时快数百倍。然而，由于模型容量有限，现有的GAN声码器仅限于在清洁环境中录制适量声音的环境。当模型以不同录音环境中看不见的说话人的mel频谱图为条件时，音频质量可能会严重降低。实际上，一个通用声码器，可以为分布外的样本进行零样本生成，在许多实际应用中都非常有价值，包括多个说话人的文本到语音转换、神经语音克隆，和神经音频编解码器。在这些应用中，神经声码器还需要很好地推广在各种条件下记录的音频。</p>
<p>在文本生成和图像合成中，放大模型大小以实现零样本性能是一个显著的趋势，但在音频合成中尚未探索。尽管基于似然的模型由于其简单的训练目标和稳定的优化而更容易缩放，但我们使用大规模GAN训练来构建我们的通用声码器，因为GAN声码器具有以下优点：i）与自回归或扩散模型相比，它是完全平行的，并且只需要一次正向传递就可以产生高维波形。ii）与基于流的模型相比，它没有强制执行任何架构约束（例如，仿射耦合层），以保持潜在和数据之间的双射。在给定相同数量的参数的情况下，这种架构约束可能会限制模型容量。</p>
<p>在这项工作中，我们提出了BigVGAN，一种大的Vocoding GAN，它可以在没有微调的情况下实现高保真度的分布外（OOD）生成。具体而言，我们做出了以下贡献： 1. 我们在生成器中引入<strong>周期性激活</strong>，为音频合成提供所需的电感偏置（inductive bias）。受其他领域提出的方法的启发，我们证明了周期激活在音频合成中的显著成功。 2. 提出了一种用于复杂音频波形建模的抗混叠多周期合成（AMP）模块。AMP组合具有可学习周期性的多个信号分量，并使用低通滤波器来减少高频伪像。 3. 我们在不规范生成器和鉴别器的情况下，通过模拟大规模GAN训练的失败模式，成功地将BigVGAN扩展到了112M参数。经验见解与Brock等人在图像领域不同（2019）。例如，正则化方法在音频合成中引入了相位失配伪影。 4. 我们证明，对于分布内和分布外样本，具有14M参数的BigVGAN基底优于具有可类比大小的最先进的神经声码器。特别是，对于各种OOD场景下的零样本生成，具有112M参数的BigVGAN在很大程度上优于最先进的模型，包括各种不可见录音环境中的不可见扬声器、新颖语言、歌唱声音、音乐和乐器音频。</p>
<p>我们将论文的其余部分整理如下。我们在§2中讨论了相关工作，并在§3中介绍了BigVGAN。我们在§4中报告了实证结果，并在§5中总结了论文。</p>
<h2 id="related-work">2. Related Work</h2>
<p>我们的工作建立在用于图像和音频合成的最先进的GANs的基础上。GAN首次被提出用于图像合成。从那时起，通过优化的架构或大规模训练获得了令人印象深刻的结果。</p>
<p>在音频合成中，以前的工作侧重于改进鉴别器架构或添加新的辅助训练损失。MelGAN引入了多尺度鉴别器（MSD），该鉴别器使用平均池在多个尺度上对原始波形进行下采样，并在每个尺度上分别应用基于窗口的鉴别器。它还通过鉴别器的l1特征匹配损失来加强输入mel谱图和生成的波形之间的映射。相比之下，GAN-TTS使用了一组在不同大小的随机窗口上操作的鉴别器，并使用条件鉴别器在调节器和波形之间进行对抗性映射。并行WaveGAN将单个短时傅立叶变换（STFT）损失扩展到多分辨率，并将其添加为GAN训练的辅助损失。通过引入多分辨率STFT损失，进一步改进了MelGAN。HiFi GAN重用了MelGAN的MSD，并引入了用于高保真合成的多周期鉴别器（MPD）。UnivNe使用多分辨率鉴别器（MRD），该鉴别器以多分辨率频谱图为输入，可以锐化合成波形的频谱结构。相反，CARGAN将部分自回归纳入生成器中，以提高基音和周期精度。</p>
<p>在这项工作中，我们专注于改进和扩大生成器的规模。我们介绍了音频合成的周期性电感偏差，并解决了非自回归生成器架构中的特征混叠问题。我们的建筑设计与时间序列预测、隐式神经表示和图像合成的最新结果有关。请注意，不同的生成器架构对于单说话人神经声码可以同样好地执行。我们证明，在具有挑战性的条件下，改进生成器架构对于通用神经语音编码至关重要。</p>
<p>由于存在明显的挑战，通用神经语音编码的成功有限。在之前的工作中，WaveRNN已被应用于通用声码任务。建立了基于流的通用声码器模型。最近发现GAN声码器是一个很好的候选者。</p>
<h2 id="method">3. Method</h2>
<p>在本节中，我们介绍了GAN声码器的预备知识，然后介绍了BigVGAN。如图1所示，有关体系结构的详细描述，请参阅附录A。</p>
<h3 id="gan声码器简介">3.1 GAN声码器简介</h3>
<p><strong>生成器</strong> 生成器网络以mel频谱图或其他特征作为输入，并输出相应的原始波形。在之前的研究中，已经应用了几种生成器架构，包括WaveNet，或卷积网络，其通过残差块堆栈将mel频谱图逐步上采样为高分辨率波形。我们选择HiFi GAN生成器作为基线架构。我们相信所提出的技术也适用于其他生成器架构。</p>
<p><strong>鉴别器</strong> 最先进的GAN声码器通常包括几个鉴别器，以引导发生器合成相干波形，同时最大限度地减少人类耳朵无法检测到的感知伪影。重要的是，每个鉴别器包含多个子鉴别器，在波形的不同分辨率窗口上操作。例如，HiFi GAN 应用了两种类型的鉴别器：i）多周期鉴别器（MPD），其中将一维信号重塑为具有不同高度和宽度的二维表示，以通过二维卷积分别捕获多个周期结构。ii）多尺度鉴别器（MSD），其中每个子鉴别器通过时域中的平均池来接收不同频率的下采样的一维信号。提出使用多分辨率鉴别器（MRD）在时频域上应用鉴别器，该鉴别器由几个子鉴别器组成，这些子鉴别器对具有不同STFT分辨率的多个二维线性频谱图进行操作。我们还发现，用MRD代替MSD提高了音频质量，同时减少了音调和周期伪影。</p>
<p><strong>训练目标</strong> 我们的训练目标与HiFi GAN类似，只是将MSD替换为MRD。它包括最小平方对抗性损失、特征匹配损失和mel谱图上的谱l1回归损失的加权和。我们在附录B中留下了每个损失和超参数的详细信息。</p>
<h3 id="周期性电感偏置">3.2 周期性电感偏置</h3>
<p>已知音频波形表现出高周期性，并且可以自然地表示为原始周期分量的组成（即，狄利克雷条件下的傅立叶级数）。这建议我们需要为生成器架构提供所需的电感偏置。然而，当前的非自回归GAN声码器仅依赖于膨胀卷积的层来学习不同频率下所需的周期分量。它们的激活函数（例如Leaky ReLU）可以产生具有必要非线性的新细节，但不提供任何周期性的电感偏置。此外，我们发现Leaky ReLU在波形域的外推表现不佳：尽管该模型在训练时可以在可见的录音环境中生成高质量的语音信号，但在分布外的场景中，如看不见的录音环境、非语音发声和乐器音频，其性能会显著下降。</p>
<p>我们通过应用最近提出的称为Snake函数的周期激活，将周期性的适当电感偏置引入生成器，定义为<span class="math inline">\(f_\alpha(x) = x + \frac{1}{\alpha}sin^2(\alpha x)\)</span>，其中<span class="math inline">\(\alpha\)</span>是控制信号周期分量频率的可训练参数，<span class="math inline">\(\alpha\)</span>越大，频率越高。<span class="math inline">\(\sin^2(x)\)</span>的使用确保了单调性，并使其易于优化。证明了这种周期性激活对温度和金融数据预测具有改进的外推能力。</p>
<p>在BigVGAN中，我们使用Snake激活<span class="math inline">\(f_{\alpha}(x)\)</span>和信道可训练参数<span class="math inline">\(\alpha \in \R^h\)</span>，这些参数定义了每个一维卷积信道的周期频率。采用这种具有学习频率控制的周期函数形式，卷积模块可以自然地拟合具有多个周期分量的原始波形。我们证明了所提出的基于Snake的生成器对训练过程中看不到的分布外音频样本更具鲁棒性，这表明在通用声码任务中具有强大的外推能力。示例见图2和附录D；使用Snake激活的BigVGAN基本无滤波器比HiFi GAN更接近地面实况样本。</p>
<h3 id="防泄密代表">3.3 防泄密代表</h3>
<p>Snake激活为建模原始波形提供了所需的周期性电感偏置，但它可以为连续时间信号产生任意高频细节，这些细节不能用网络的离散时间输出来表示，这可能导致混叠伪影。这种副作用可以通过应用低通滤波器来抑制。抗混叠非线性通过沿时间维度将信号上采样2×，应用Snake激活，然后将信号下采样2×来操作，这是受奈奎斯特-香农采样定理启发的常见实践。每个上采样和下采样操作都伴随着低通滤波器，该低通滤波器使用具有Kaiser窗口的窗口sinc滤波器。详见附录A。</p>
<p>我们将这种滤波后的Snake非线性应用于生成器内的每个残差扩张卷积层，以获得离散时间一维信号的抗混叠表示。该模块被命名为抗混叠多周期合成（AMP）。如图1所示。我们发现，结合滤波激活可以减少合成波形中的高频伪影；如图2所示，请参见BigVGAN基本无滤波器与BigVGAN基础（带滤波器）。我们将证明，它在各种客观和主观评价方面提供了显著的改进。请注意，我们还探索了抗锯齿上采样层，但这会导致显著的训练不稳定性，并导致大型模型的早期崩溃。详见附录C。</p>
<h3 id="采用大规模训练的bigvgan">3.4 采用大规模训练的BigVGAN</h3>
<p>在本小节中，我们通过将生成器的模型大小扩大到112M个参数来探索通用声码的局限性，同时保持GAN训练的稳定性和作为高速神经声码器的实用性。我们从我们的改进生成器开始，使用具有14M参数的可比HiFi GAN V1配置，表示为BigVGAN基础。我们通过增加每个块的上采样块和卷积通道的数量来增加BigVGAN基数。BigVGAN基础使用4个上采样块以[8，8，2，2]的比率对信号进行256×上采样。每个上采样块都伴随着具有扩张卷积的多个残差层，即AMP模块。我们进一步将256×上采样划分为6个块[4，4，2，2，2]，以进行更细粒度的特征细化。此外，我们将AMP模块（类似于HiFi GAN中的MRF）的通道数量从512个增加到1536个。我们将具有1536个通道和112M个参数的模型表示为BigVGAN。</p>
<p>我们发现，HiFi GAN中使用的<span class="math inline">\(2×10^{−4}\)</span>的默认学习率会导致BigVGAN训练的早期训练崩溃，其中鉴别器子模块的损失在数千次迭代后立即收敛为零。将学习率减半至1×10−4能够减少此类失败。我们还发现，大批量有助于减少训练中的模式崩溃。为了在训练效率和稳定性之间取得良好的平衡，我们只将批量大小从通常的16倍增加到32倍，因为神经声码器可能需要数百万步才能收敛。注意，这个推荐的批量大小仍然比图像合成的批量大小（例如，2048）小得多，因为神经声码具有强的条件信息。</p>
<p>即使有了上述变化，大型BigVGAN在训练初期仍然容易崩溃。我们在训练过程中跟踪了每个模块的梯度范数，并发现抗混叠非线性显著放大了MPD的梯度范数。因此，BigVGAN生成器在训练的早期接收到发散梯度，导致不稳定性和潜在的崩溃。我们在附录C的图4中可视化了每个模块的梯度范数。我们通过将梯度的全局范数裁剪为<span class="math inline">\(10^3\)</span>来缓解这个问题，该全局范数接近112M BigVGAN生成器的平均梯度范数。这种梯度剪裁防止了生成器的早期训练崩溃。请注意，梯度裁剪被发现对缓解图像合成的训练不稳定性无效（见Brock等人（2019）中的附录H），但它在我们的努力中非常有效。</p>
<p>除了上述努力，我们还探索了其他方向，包括改进模型架构的各种方法、稳定GAN训练的谱归一化，这对图像域中的大规模GAN训练至关重要，以及提高模型泛化能力的数据增加。不幸的是，在我们的研究中，所有这些试验都导致了更差的感知质量。详细信息可在附录C中找到。我们希望我们所学到的这些实际经验教训对未来的研究工作有用。</p>
<h2 id="results">4 Results</h2>
<h3 id="训练数据">4.1 训练数据</h3>
<p>我们使用原始采样率为24kHz的LibriTTS数据集进行训练。与之前只采用在清洁环境中记录的子集的研究不同，我们使用了所有训练数据，包括来自不同记录环境的子集，这在文献中是前所未有的。我们发现，训练数据的多样性对于实现使用BigVGAN的通用神经声码的目标很重要。对于OOD实验，如果需要，我们使用librosa包提供的kaiser-best算法将音频重新采样到24kHz。</p>
<p>传统的STFT参数被设计为具有有限的频带[0，8]kHz，通过切断高频细节以便于建模。相反，我们使用频率范围[0，12]kHz和100波段对数mel频谱图来训练所有模型（包括基线），这也用于最近对通用声码的研究。我们在之前的工作中设置了其他STFT参数，具有1024 FFT大小、1024 Hann窗口和256帧跳大小。</p>
<h3 id="模型">4.2 模型</h3>
<p>我们使用1M步的训练配置训练所有BigVGAN模型，包括消融模型和基线HiFi GAN。我们使用批量大小为32，分段大小为8192，初始学习率为<span class="math inline">\(1×10^{−4}\)</span>。所有其他配置，包括优化器、学习率调度器和损失项的标量权重，都遵循HiFi GAN的官方开源实现，没有修改，除了BigVGAN用MRD代替MSD作为鉴别器。所有型号均使用NVIDIA DGX-1和8个V100 GPU进行训练。有关详细的超参数，请参阅附录A中的表6。</p>
<p>我们包括了与SC-WaveRNN的比较，SC是一种最先进的基于WaveRNN的自回归通用神经声码器，使用官方实现。我们还包括两个流行的基于流的模型：WaveGlow和WaveFlow，使用它们的官方实现。对于分发外测试，我们包括UnivNet-c32的非官方开源实现，它使用train-clean-360子集进行训练，据报道在相同的训练配置下优于HiFi-GAN。详见附录E。</p>
<p>我们包括了与SC-WaveRNN的比较，SC是一种最先进的基于WaveRNN的自回归通用神经声码器，使用官方实现。我们还包括两个流行的基于流的模型：WaveGlow和WaveFlow，使用它们的官方实现。对于分发外测试，我们包括UnivNet-c32的非官方开源实现，它使用train-clean-360子集进行训练，据报道在相同的训练配置下优于HiFi-GAN。详见附录E。</p>
<p>表1总结了用于生成24kHz音频的基于流和GAN声码器的合成速度。我们省略了SC-WaveRNN，因为它要慢得多。具有14M参数的BigVGAN-base模型可以合成比实时快70.18倍的音频，这比HiFi GAN相对较慢，因为滤波后的Snake函数需要更多的计算。HiFi GAN和BigVGAN比基于流的模型更快，因为它们是完全平行的（WaveFlow具有部分自回归），并且具有更少的层（WaveGlow具有96层）。我们的BigVGAN具有112M参数，可以合成比实时速度快44.72倍的音频，并有望成为一种高速神经声码器。</p>
<h3 id="评价指标">4.3 评价指标</h3>
<p>我们收集的客观指标旨在测量地面实况音频和生成的样本之间的各种类型的距离。我们提供了5种不同的度量：1）多分辨率STFT（M-STFT），它测量多个分辨率之间的光谱距离。 2）语音质量感知评估（PESQ），一种广泛采用的语音质量自动评估。3）具有动态时间扭曲的梅尔倒谱失真（MCD），其测量梅尔倒谱之间的差异。4）周期性误差，以及5）浊音/清音分类的F1分数（V/UV F1），其被认为是来自非自回归GAN声码器的主要伪像。</p>
<p>传统的5-尺度平均意见得分（MOS）不足以对通用声码器进行主观评估，因为该度量需要区分在各种环境中记录的不同说话者身份的话语。例如，该模型可能总是输出一些非常自然的“平均”声音，这不是优选的，但在MOS评估中仍然可以得到人类工作者的高度评价。因此，我们还进行了5尺度相似性平均意见得分（SMOS）评估，其中要求参与者在并排听了基本事实音频和来自模型的样本后给出这对音频的相似性得分。SMOS提供了一种改进的方法来评估给定样本与地面实况的接近程度，其中地面实况录音可以具有不同的说话者身份，包含听众看不见的语言，并且可以在各种声学环境中录制。SMOS还直接适用于非语音样本，例如音乐。我们对Mechanical Turk进行了MOS和SMOS评估。更多详细信息见附录G。</p>
<h3 id="libritts-结果">LibriTTS 结果</h3>
<p>我们报告了BigVGAN的性能以及使用上述客观和主观指标在LibriTTS上评估的基线模型。我们对dev-clean和dev-ether进行了全面的客观评价，对test-clean与test-ether的组合进行了主观评价。LibriTTS的开发和测试部分包含训练期间看不见的说话人，但录制环境包含在训练部分中。</p>
<p>表2显示了LibriTTS上的分布中测试结果。除HiFi GAN之外的基线模型表现明显较差。这表明GAN声码器是最先进的通用神经声码器。BigVGAN显著改进了所有客观指标。特别是，与具有相同参数量的HiFi GAN（V1）相比，BigVGAN-base表现出持续改进的客观得分，这表明它对波形数据具有更好的周期性电感偏差。</p>
<p>HiFi GAN（V1）、BigVGAN底座和BigVGAN在MOS方面表现相当好，而无需并排收听地面实况音频。当听众可以并排比较模型样本和真实音频时，BigVGAN基础在SMOS（+0.05）方面明显优于HiFi GAN（V1），112M BigVGAN在SMOS方面明显优于高保真GAN（+0.11），因为它具有高的模型容量，可以进一步利用不同的训练数据获得更好的质量。</p>
<p>在附录E中，我们还报告了其他结果，包括UnivNet和基于LibriTTS开发集、未发现的VCTK和LJSpeech数据的BigVGAN消融模型。</p>
<h2 id="看不见的语言和多变的录音环境">4.5 看不见的语言和多变的录音环境</h2>
<p>在本小节中，我们通过测量BigVGAN在不可见数据集中具有不同类型记录环境的各种不可见语言的零样本性能来评估其通用语音编码能力。基于表2中的结果，我们仅将基于GAN的声码器作为最先进的基线。我们收集了三类公开可用的多语言数据集，根据记录环境中的噪声类型进行分类。 - 在无声的录音室环境中录制的资源不足的语言集：爪哇语、高棉语、尼泊尔语和巽他语。我们使用从组合数据集中随机选择的50个音频片段，这些片段在不同语言之间具有相等的平衡。 - 多语言TEDx语料库：包含西班牙语、法语、意大利语和葡萄牙语的TEDx演讲集。我们从IWSLT’21测试集中随机选择了50个语言均衡的音频片段。我们通过添加MS-SNSD中的随机环境噪声来模拟看不见的录制环境设置，如机场、咖啡馆、胡言乱语等。</p>
<p>表3总结了三种不同类型的未发现数据集的SMOS结果。我们只做了SMOS评估，因为数据集有人类听众看不见的语言，如果不与地面实况记录进行并排比较，很难确定质量。对于资源不足的干净语言数据集，模型之间的性能差距并不大。这表明在整个LibriTTS训练集上训练的通用声码器在干净的录音环境下对看不见的语言是鲁棒的。对于两种类型的看不见的记录环境（模拟或真实世界），BigVGAN的性能都大大优于基线模型。小容量BigVGAN基础也显示出与基线相比的改善，具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.05）。这表明，由于采用了AMP模块改进的发生器设计，BigVGAN对看不见的记录环境的鲁棒性明显更强。在附录F中，我们进一步证明了就多种语言的字符错误率（CER）而言，BigVGAN是语言上最准确的通用声码器。</p>
<p>我们用在train-clean-360子集上训练的预训练检查点测试了UnivNet的开源实现。与Jang等人的报告相反，UnivNet-c32优于HiFi GAN，我们发现在整个LibriTTS数据集上训练的未修改的HiFi GAN能够匹配或优于UnivNet-c32。我们还在LibriTTS上对UnivNet-c32进行了完整的训练，发现它并没有从更大的训练数据中受益。详细分析见附录E。</p>
<h2 id="分布外稳健性">4.6 分布外稳健性</h2>
<p>在本小节中，我们通过测量分布外数据的零样本性能来测试BigVGAN的稳健性和外推能力。我们使用MUSDB18-HQ进行了SMOS实验，这是一个多音轨音乐音频数据集，包含人声、鼓、贝斯、其他乐器和原始混音。测试集包含50首歌曲和5首曲目。我们为每首曲目和歌曲收集持续时间为10秒的歌曲中期剪辑。</p>
<p>表4显示了来自5个轨道的SMOS结果及其来自MUSDB18-HQ测试集的平均值。BigVGAN模型显示出在更宽的频带覆盖范围内大大提高了零样本生成性能，而基线模型无法在有限的频率范围外生成音频，并遭受严重失真。歌声（人声）、器乐音频（其他）和歌曲的完整混音（混音）的改进最为深刻，而鼓和低音的改进则不那么显著。</p>
<p>我们还对从真实世界录制环境中的YouTube视频中获得的音频进行了实验。BigVGAN还表现出对各种类型的分布外信号（如笑声）的鲁棒性。我们为我们的演示页面提供音频样本。</p>
<h2 id="消融研究">4.7 消融研究</h2>
<p><strong>模型架构</strong> 为了衡量BigVGAN生成器的有效性，我们根据MUSDB18-HQ数据对BigVGAN的消融模型进行了SMOS测试。表4显示，消融模型在各种场景下表现出明显的退化，如仪器音频（其他，混合）。从平均SMOS评级来看，1）禁用Snake激活的抗锯齿滤波器比BigVGAN基础表现更差，2）去除滤波器和Snake活化（即用MRD代替MSD训练的香草HiFi GAN）甚至比仅Snake消融模型更差，两者都具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.01）。这表明Leaky ReLU不够健壮，无法外推到学习的频率范围之外，并且在具有挑战性的设置中，混叠伪影会降低音频质量。结果表明，由于周期性归纳偏差和抗混叠特征表示的无缝集成，BigVGAN生成器对分布外场景表现出强大的鲁棒性和外推能力。关于BigVGAN中抗锯齿效果的可视化，请参见附录D。</p>
<p><strong>大模型</strong> 我们比较了具有最大112M参数的HiFi GAN和BigVGAN。我们使用与BigVGAN相同的训练设置来训练112M HiFi GAN。我们在MUSDB18-HQ的混合测试集上对两个模型进行了成对测试，这是一个具有挑战性的分布外数据。我们要求参与者在两个模型的样本之间选择声音更好的音频。测试显示，58%的评分投票给BigVGAN而不是大的HiFi GAN，并且BigVGAN的质量大于大的HiFi GAN，具有统计学意义（Wilcoxon符号秩检验的p值&lt;0.01）。结果进一步验证了BigVGAN在大规模环境中的架构优势。</p>
<p><strong>大型训练数据</strong> 为了验证使用大型训练数据的重要性，我们使用不太多样化的纯语音数据集训练我们的BigVGAN，使用相同的训练配置进行1M个步骤：1）LibriTTS的train-clean-360子集，或2）VCTK数据集。表5显示，在不太多样化的数据上训练BigVGAN显示了LibriTTS评估集上的客观指标和主观SMOS的退化。结果验证了使用不同训练数据的重要性，并证明了BigVGAN在大规模数据集上的有效性。</p>
<h1 id="结论">5 结论</h1>
<p>这项研究以前所未有的数据、模型和评估规模探索了通用神经语音编码的局限性。我们通过各种自动和人工评估来分析不同场景下的表现，包括看不见的演讲者、语言、录音环境和分布外的数据。我们通过引入具有学习频率控制的抗混叠周期激活函数，为BigVGAN提供了一种改进的生成器架构，该函数为波形生成注入了所需的电感偏置。基于改进的生成器，我们演示了最大的GAN声码器，它在各种OOD条件下具有强大的零样本性能，包括看不见的录制环境、歌唱声音和乐器声音。我们相信，BigVGAN结合从大规模培训中吸取的实际经验教训，将激励未来的通用声码努力，并提高现实世界应用的最先进成果，包括语音克隆、语音转换、语音翻译和音频编解码器。</p>
]]></content>
      <categories>
        <category>工作</category>
      </categories>
  </entry>
</search>
